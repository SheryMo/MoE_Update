import lm_eval
# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM
from lm_eval.models.huggingface import HFLM
import torch
import torch.nn as nn
import random
import time
import seaborn as sns
import matplotlib.pyplot as plt
import datetime
import re
import os
import numpy as np
from tqdm import tqdm
from typing import Optional, List
import random
import lm_eval
import numpy as np
import math
import argparse
from sklearn.metrics.pairwise import cosine_similarity
import copy
import gc
import torch
import inspect
import socket
import logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def get_expert_remap_mapping(group: List[int]) -> List[int]:
    """
    Convert arbitrary group labels into expert remap mapping.
    Each expert will be mapped to the first expert in its group.

    Args:
        group (List[int]): e.g., [10, 10, 11, 11, 15, 15]

    Returns:
        List[int]: Mapping for set_expert_mapping, e.g., [0, 0, 2, 2, 4, 4]
    """
    mapping = []
    group_tensor = torch.tensor(group, dtype=torch.long)
    unique_groups = group_tensor.unique(sorted=True)
    group_to_indices = {int(g): (group_tensor == g).nonzero(as_tuple=False).view(-1).tolist() for g in unique_groups}
    
    # Each group: map all indices to the first expert in that group
    for i in range(len(group_tensor)):
        current_group = int(group_tensor[i])
        target_idx = group_to_indices[current_group][0]  # use first expert in group as target
        mapping.append(target_idx)

    return mapping

def clean_and_report_cuda_tensors(context_label="æœªå‘½å"):
    print(f"\n===== ğŸš€ã€CUDA æ£€æŸ¥å¼€å§‹ã€‘[{context_label}] =====")
    
    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()
    torch.cuda.empty_cache()

    # è®°å½•å½“å‰ä»åœ¨ CUDA çš„ Tensor
    cuda_tensors = []
    total_size_mb = 0

    for obj in gc.get_objects():
        try:
            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):
                if obj.is_cuda:
                    size_mb = obj.element_size() * obj.nelement() / 1024**2
                    total_size_mb += size_mb
                    cuda_tensors.append((type(obj), tuple(obj.size()), size_mb))

        except Exception:
            continue

    if cuda_tensors:
        print(f"ğŸ” æ‰¾åˆ° {len(cuda_tensors)} ä¸ªä»é©»ç•™åœ¨ CUDA ä¸Šçš„å¼ é‡:")
        for obj_type, shape, size in cuda_tensors:
            print(f"  - ç±»å‹: {obj_type.__name__:<20} | å°ºå¯¸: {shape} | æ˜¾å­˜: {size:.2f} MB")
    else:
        print("âœ… æ²¡æœ‰å‘ç°ä»»ä½•é©»ç•™åœ¨ CUDA ä¸Šçš„å¼ é‡ï¼Œæ˜¾å­˜æ¸…ç†æˆåŠŸã€‚")

    print(f"ğŸ§  æ€»å ç”¨ CUDA æ˜¾å­˜ï¼ˆéç¼“å­˜ï¼‰: {total_size_mb:.2f} MB")
    print(f"===== âœ…ã€CUDA æ£€æŸ¥ç»“æŸã€‘[{context_label}] =====\n")
    
def list_tensors_on_cuda():
    print("="*50)
    print("ğŸ” å½“å‰ä»é©»ç•™åœ¨ CUDA ä¸Šçš„å¼ é‡ä¿¡æ¯ï¼š")
    total_mem = 0
    for obj in gc.get_objects():
        try:
            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):
                if obj.is_cuda:
                    size = obj.element_size() * obj.nelement() / 1024**2  # MB
                    total_mem += size
                    print(f"- ç±»å‹: {type(obj)}, å¤§å°: {obj.size()}, å ç”¨æ˜¾å­˜: {size:.2f} MB")
        except Exception as e:
            pass
    print(f"\nğŸ§  æ€»å…±å ç”¨ CUDA æ˜¾å­˜ï¼ˆéç¼“å­˜éƒ¨åˆ†ï¼‰: {total_mem:.2f} MB")
    print("="*50)

def extract_task_groups(log_file_path = 'logs11.log'):
    task_groups = []  # å­˜æ”¾ä»»åŠ¡ç»„çš„åˆ—è¡¨
    start_line = False  # æ ‡è®°æ˜¯å¦å·²ç»æ‰¾åˆ°ç›®æ ‡è¡Œ
    with open(log_file_path, 'r') as file:
        for line in file:
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¼€å§‹çš„åˆ†éš”è¡Œ
            if 'Group' in line and 'Config Location' in line:
                start_line = True
                print('Group Find')
                continue  # è·³è¿‡è¯¥è¡Œï¼Œç›´æ¥æ£€æŸ¥ä¸‹ä¸€è¡Œ
            
            # ä»ç›®æ ‡è¡Œçš„ä¸‹ä¸€è¡Œå¼€å§‹è¯»å–
            if start_line:
                # å¦‚æœé‡åˆ°ç©ºè¡Œåˆ™åœæ­¢
                if not line.strip():
                    break
                
                # ä½¿ç”¨æ­£åˆ™æå–ä»»åŠ¡ç»„åç§°ï¼ˆå³å‰é¢çš„éƒ¨åˆ†ï¼‰
                match = re.match(r'\| *(\S+)', line.strip())  # åŒ¹é…ä»|å¼€å§‹çš„ç¬¬ä¸€ä¸ªéç©ºç™½å­—ç¬¦ä¸²
                if match:
                    group = match.group(1).strip().split('|')
                    task_groups.append(group[0])  # æå–ä»»åŠ¡ç»„åç§°å¹¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­

    return task_groups

def extract_task_dict(log_file_path ,task_groups):
    task_dict = {}  # å­˜æ”¾æ¯ä¸ªtask_groupå¯¹åº”çš„ä»»åŠ¡åˆ—è¡¨
    start_line_task = False  # æ ‡è®°æ˜¯å¦æ‰¾åˆ°ä»»åŠ¡çš„éƒ¨åˆ†
    for task_group in task_groups:
        # å¦‚æœä»»åŠ¡ç»„ä¸å­˜åœ¨ï¼Œåˆ™åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
        if task_group not in task_dict:
            task_dict[task_group] = []
            
    # è¯»å–æ—¥å¿—æ–‡ä»¶
    with open(log_file_path, 'r') as file:
        for line in file:
            # 2. æŸ¥æ‰¾ä»»åŠ¡ï¼ˆtaskï¼‰éƒ¨åˆ†
            if 'Task' in line and 'Config Location' in line and 'Output Type' in line:
                start_line_task = True
                continue  # è·³è¿‡è¯¥è¡Œï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€è¡Œ
            
            if start_line_task:
                if not line.strip():  # å¦‚æœé‡åˆ°ç©ºè¡Œåˆ™åœæ­¢
                    break
                # ä½¿ç”¨æ­£åˆ™æå–ä»»åŠ¡åç§°ï¼ˆå³å‰é¢çš„éƒ¨åˆ†ï¼‰
                match = re.match(r'\| *(\S+)', line.strip())  # åŒ¹é…ä»»åŠ¡åç§°
                if match:
                    task_name = match.group(1).strip()  # æå–ä»»åŠ¡åç§°å¹¶å»æ‰ç©ºæ ¼
                    # éå†æ‰€æœ‰task_groupå¹¶æ£€æŸ¥ä»»åŠ¡åç§°æ˜¯å¦åŒ…å«task_groupä¸­æ¯ä¸€éƒ¨åˆ†
                    for task_group in task_groups:
                        # åˆ†å‰²task_groupä¸ºå¤šä¸ªéƒ¨åˆ†
                        task_group_parts = task_group.split('_')
                        # åˆ¤æ–­ä»»åŠ¡åç§°æ˜¯å¦åŒ…å«æ‰€æœ‰çš„task_group_partsä¸­çš„å…ƒç´ 
                        if all(part in task_name for part in task_group_parts):
                            task_dict[task_group].append(task_name)  # å°†ä»»åŠ¡åç§°æ·»åŠ åˆ°å¯¹åº”çš„task_groupåˆ—è¡¨ä¸­

    return task_dict

def compute_frequency_variance(frequency_list):
    """
    Compute the variance of frequencies for each expert in the given frequency list.
    
    Args:
        frequency_list (List[float]): The list of frequencies for each expert in the layer.
    
    Returns:
        List[float]: Variance of frequencies for each expert.
    """
    mean_frequency = np.mean(frequency_list)
    variance_list = [(freq - mean_frequency) ** 2 for freq in frequency_list]
    return variance_list

def compute_similarity_between_layers(layer_variance_1, layer_variance_2):
    """
    Compute the cosine similarity between the frequency variance of two layers.
    
    Args:
        layer_variance_1 (List[float]): Variance of frequencies for each expert in the first layer.
        layer_variance_2 (List[float]): Variance of frequencies for each expert in the second layer.
    
    Returns:
        float: Cosine similarity between the two variance lists.
    """
    cos_sim = cosine_similarity(
        np.array(layer_variance_1).reshape(1, -1),
        np.array(layer_variance_2).reshape(1, -1)
    )[0][0]
    return cos_sim

def map_to_range(data: dict, name_list=None, new_min=0.1, new_max=0.9):
    """
    å°† data ä¸­çš„æ¯ä¸ª expert å‘é‡å€¼å½’ä¸€åŒ–åˆ° [new_min, new_max] åŒºé—´ã€‚
    å¦‚æœ name_list ä¸º Noneï¼Œåˆ™ä½¿ç”¨ data.keys() é¡ºåºï¼›å¦åˆ™ä¸¥æ ¼æŒ‰ name_list é¡ºåºæå–ã€‚

    è¿”å›:
        result: List[List[float]] å½’ä¸€åŒ–åçš„äºŒç»´æ•°ç»„
        names:  List[str] å¯¹åº”æ¯ä¸€è¡Œçš„ key å
    """
    result = []
    names = name_list if name_list else list(data.keys())

    for key in names:
        values = data.get(key, [])

        if not values:
            result.append([0.0] * 8)  # æˆ–å…¶ä»–é»˜è®¤å€¼
            continue

        values = [0.0 if math.isnan(v) else v for v in values]

        old_min = min(values)
        old_max = max(values)

        # é¿å…é™¤ä»¥ 0 çš„æƒ…å†µï¼ˆæ‰€æœ‰å€¼ç›¸ç­‰ï¼‰
        if old_max == old_min:
            scaled = [new_min for _ in values]
        else:
            scaled = [
                new_min + (v - old_min) / (old_max - old_min) * (new_max - new_min)
                for v in values
            ]
        result.append(scaled)

    return result, names

def get_model_size(model):
    total_size = 0
    for param in model.parameters():
        total_size += param.nelement() * param.element_size()
    # Convert bytes to megabytes (MB)
    total_size_mb = total_size / (1024 ** 3)
    return total_size_mb

def wait_for_port(ip, port, timeout=30.0, logger=logger):
    """
    Wait for a specific IP and port to become available (open for TCP connection).

    Args:
        ip (str): The IP address to check.
        port (int): The port to check.
        timeout (float): Max time to wait (in seconds).
        logger (logging.Logger, optional): Optional logger for debug output.

    Returns:
        bool: True if port is open before timeout, False otherwise.
    """
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                sock.settimeout(1.0)  # 1 second timeout
                result = sock.connect_ex((ip, port))
                if result == 0:
                    if logger:
                        logger.debug(f"Port {port} at {ip} is open.")
                    return True
                else:
                    if logger:
                        logger.debug(f"Port {port} at {ip} not open yet (code {result}).")
        except Exception as e:
            if logger:
                logger.warning(f"Error checking port {port} at {ip}: {e}")
        time.sleep(0.5)

    if logger:
        logger.warning(f"Timeout waiting for port {port} at {ip}.")
    return False
    
def safe_model_clone(model):
    # ä¿å­˜å½“å‰æ¨¡å‹çš„ state_dict
    state_dict = copy.deepcopy(model.state_dict())  # deepcopy here is OK
    # æ–°å»ºåŒç»“æ„æ¨¡å‹
    new_model = type(model)(model.config)
    new_model.load_state_dict(state_dict)
    return new_model