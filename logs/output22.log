/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|                                                                 | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|███████████▍                                             | 1/5 [00:00<00:03,  1.09it/s]Loading checkpoint shards:  40%|██████████████████████▊                                  | 2/5 [00:01<00:02,  1.13it/s]Loading checkpoint shards:  60%|██████████████████████████████████▏                      | 3/5 [00:02<00:01,  1.14it/s]Loading checkpoint shards:  80%|█████████████████████████████████████████████▌           | 4/5 [00:03<00:00,  1.15it/s]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.55it/s]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.33it/s]
Some weights of PhiMoEForCausalLM were not initialized from the model checkpoint at /root/autodl-tmp/lm-evaluation-harness/phiMergedMoE and are newly initialized: ['model.layers.0.block_sparse_moe.experts.10.w1.weight', 'model.layers.0.block_sparse_moe.experts.10.w2.weight', 'model.layers.0.block_sparse_moe.experts.10.w3.weight', 'model.layers.0.block_sparse_moe.experts.11.w1.weight', 'model.layers.0.block_sparse_moe.experts.11.w2.weight', 'model.layers.0.block_sparse_moe.experts.11.w3.weight', 'model.layers.0.block_sparse_moe.experts.12.w1.weight', 'model.layers.0.block_sparse_moe.experts.12.w2.weight', 'model.layers.0.block_sparse_moe.experts.12.w3.weight', 'model.layers.0.block_sparse_moe.experts.13.w1.weight', 'model.layers.0.block_sparse_moe.experts.13.w2.weight', 'model.layers.0.block_sparse_moe.experts.13.w3.weight', 'model.layers.0.block_sparse_moe.experts.14.w1.weight', 'model.layers.0.block_sparse_moe.experts.14.w2.weight', 'model.layers.0.block_sparse_moe.experts.14.w3.weight', 'model.layers.0.block_sparse_moe.experts.15.w1.weight', 'model.layers.0.block_sparse_moe.experts.15.w2.weight', 'model.layers.0.block_sparse_moe.experts.15.w3.weight', 'model.layers.0.block_sparse_moe.experts.3.w1.weight', 'model.layers.0.block_sparse_moe.experts.3.w2.weight', 'model.layers.0.block_sparse_moe.experts.3.w3.weight', 'model.layers.0.block_sparse_moe.experts.4.w1.weight', 'model.layers.0.block_sparse_moe.experts.4.w2.weight', 'model.layers.0.block_sparse_moe.experts.4.w3.weight', 'model.layers.0.block_sparse_moe.experts.6.w1.weight', 'model.layers.0.block_sparse_moe.experts.6.w2.weight', 'model.layers.0.block_sparse_moe.experts.6.w3.weight', 'model.layers.0.block_sparse_moe.experts.7.w1.weight', 'model.layers.0.block_sparse_moe.experts.7.w2.weight', 'model.layers.0.block_sparse_moe.experts.7.w3.weight', 'model.layers.0.block_sparse_moe.experts.8.w1.weight', 'model.layers.0.block_sparse_moe.experts.8.w2.weight', 'model.layers.0.block_sparse_moe.experts.8.w3.weight', 'model.layers.0.block_sparse_moe.experts.9.w1.weight', 'model.layers.0.block_sparse_moe.experts.9.w2.weight', 'model.layers.0.block_sparse_moe.experts.9.w3.weight', 'model.layers.1.block_sparse_moe.experts.10.w1.weight', 'model.layers.1.block_sparse_moe.experts.10.w2.weight', 'model.layers.1.block_sparse_moe.experts.10.w3.weight', 'model.layers.1.block_sparse_moe.experts.11.w1.weight', 'model.layers.1.block_sparse_moe.experts.11.w2.weight', 'model.layers.1.block_sparse_moe.experts.11.w3.weight', 'model.layers.1.block_sparse_moe.experts.12.w1.weight', 'model.layers.1.block_sparse_moe.experts.12.w2.weight', 'model.layers.1.block_sparse_moe.experts.12.w3.weight', 'model.layers.1.block_sparse_moe.experts.13.w1.weight', 'model.layers.1.block_sparse_moe.experts.13.w2.weight', 'model.layers.1.block_sparse_moe.experts.13.w3.weight', 'model.layers.1.block_sparse_moe.experts.14.w1.weight', 'model.layers.1.block_sparse_moe.experts.14.w2.weight', 'model.layers.1.block_sparse_moe.experts.14.w3.weight', 'model.layers.1.block_sparse_moe.experts.15.w1.weight', 'model.layers.1.block_sparse_moe.experts.15.w2.weight', 'model.layers.1.block_sparse_moe.experts.15.w3.weight', 'model.layers.1.block_sparse_moe.experts.2.w1.weight', 'model.layers.1.block_sparse_moe.experts.2.w2.weight', 'model.layers.1.block_sparse_moe.experts.2.w3.weight', 'model.layers.1.block_sparse_moe.experts.4.w1.weight', 'model.layers.1.block_sparse_moe.experts.4.w2.weight', 'model.layers.1.block_sparse_moe.experts.4.w3.weight', 'model.layers.1.block_sparse_moe.experts.5.w1.weight', 'model.layers.1.block_sparse_moe.experts.5.w2.weight', 'model.layers.1.block_sparse_moe.experts.5.w3.weight', 'model.layers.1.block_sparse_moe.experts.6.w1.weight', 'model.layers.1.block_sparse_moe.experts.6.w2.weight', 'model.layers.1.block_sparse_moe.experts.6.w3.weight', 'model.layers.1.block_sparse_moe.experts.7.w1.weight', 'model.layers.1.block_sparse_moe.experts.7.w2.weight', 'model.layers.1.block_sparse_moe.experts.7.w3.weight', 'model.layers.1.block_sparse_moe.experts.8.w1.weight', 'model.layers.1.block_sparse_moe.experts.8.w2.weight', 'model.layers.1.block_sparse_moe.experts.8.w3.weight', 'model.layers.1.block_sparse_moe.experts.9.w1.weight', 'model.layers.1.block_sparse_moe.experts.9.w2.weight', 'model.layers.1.block_sparse_moe.experts.9.w3.weight', 'model.layers.10.block_sparse_moe.experts.10.w1.weight', 'model.layers.10.block_sparse_moe.experts.10.w2.weight', 'model.layers.10.block_sparse_moe.experts.10.w3.weight', 'model.layers.10.block_sparse_moe.experts.11.w1.weight', 'model.layers.10.block_sparse_moe.experts.11.w2.weight', 'model.layers.10.block_sparse_moe.experts.11.w3.weight', 'model.layers.10.block_sparse_moe.experts.13.w1.weight', 'model.layers.10.block_sparse_moe.experts.13.w2.weight', 'model.layers.10.block_sparse_moe.experts.13.w3.weight', 'model.layers.10.block_sparse_moe.experts.14.w1.weight', 'model.layers.10.block_sparse_moe.experts.14.w2.weight', 'model.layers.10.block_sparse_moe.experts.14.w3.weight', 'model.layers.10.block_sparse_moe.experts.15.w1.weight', 'model.layers.10.block_sparse_moe.experts.15.w2.weight', 'model.layers.10.block_sparse_moe.experts.15.w3.weight', 'model.layers.10.block_sparse_moe.experts.2.w1.weight', 'model.layers.10.block_sparse_moe.experts.2.w2.weight', 'model.layers.10.block_sparse_moe.experts.2.w3.weight', 'model.layers.10.block_sparse_moe.experts.4.w1.weight', 'model.layers.10.block_sparse_moe.experts.4.w2.weight', 'model.layers.10.block_sparse_moe.experts.4.w3.weight', 'model.layers.10.block_sparse_moe.experts.5.w1.weight', 'model.layers.10.block_sparse_moe.experts.5.w2.weight', 'model.layers.10.block_sparse_moe.experts.5.w3.weight', 'model.layers.10.block_sparse_moe.experts.6.w1.weight', 'model.layers.10.block_sparse_moe.experts.6.w2.weight', 'model.layers.10.block_sparse_moe.experts.6.w3.weight', 'model.layers.10.block_sparse_moe.experts.7.w1.weight', 'model.layers.10.block_sparse_moe.experts.7.w2.weight', 'model.layers.10.block_sparse_moe.experts.7.w3.weight', 'model.layers.10.block_sparse_moe.experts.8.w1.weight', 'model.layers.10.block_sparse_moe.experts.8.w2.weight', 'model.layers.10.block_sparse_moe.experts.8.w3.weight', 'model.layers.10.block_sparse_moe.experts.9.w1.weight', 'model.layers.10.block_sparse_moe.experts.9.w2.weight', 'model.layers.10.block_sparse_moe.experts.9.w3.weight', 'model.layers.11.block_sparse_moe.experts.10.w1.weight', 'model.layers.11.block_sparse_moe.experts.10.w2.weight', 'model.layers.11.block_sparse_moe.experts.10.w3.weight', 'model.layers.11.block_sparse_moe.experts.11.w1.weight', 'model.layers.11.block_sparse_moe.experts.11.w2.weight', 'model.layers.11.block_sparse_moe.experts.11.w3.weight', 'model.layers.11.block_sparse_moe.experts.12.w1.weight', 'model.layers.11.block_sparse_moe.experts.12.w2.weight', 'model.layers.11.block_sparse_moe.experts.12.w3.weight', 'model.layers.11.block_sparse_moe.experts.13.w1.weight', 'model.layers.11.block_sparse_moe.experts.13.w2.weight', 'model.layers.11.block_sparse_moe.experts.13.w3.weight', 'model.layers.11.block_sparse_moe.experts.14.w1.weight', 'model.layers.11.block_sparse_moe.experts.14.w2.weight', 'model.layers.11.block_sparse_moe.experts.14.w3.weight', 'model.layers.11.block_sparse_moe.experts.15.w1.weight', 'model.layers.11.block_sparse_moe.experts.15.w2.weight', 'model.layers.11.block_sparse_moe.experts.15.w3.weight', 'model.layers.11.block_sparse_moe.experts.2.w1.weight', 'model.layers.11.block_sparse_moe.experts.2.w2.weight', 'model.layers.11.block_sparse_moe.experts.2.w3.weight', 'model.layers.11.block_sparse_moe.experts.3.w1.weight', 'model.layers.11.block_sparse_moe.experts.3.w2.weight', 'model.layers.11.block_sparse_moe.experts.3.w3.weight', 'model.layers.11.block_sparse_moe.experts.5.w1.weight', 'model.layers.11.block_sparse_moe.experts.5.w2.weight', 'model.layers.11.block_sparse_moe.experts.5.w3.weight', 'model.layers.11.block_sparse_moe.experts.6.w1.weight', 'model.layers.11.block_sparse_moe.experts.6.w2.weight', 'model.layers.11.block_sparse_moe.experts.6.w3.weight', 'model.layers.11.block_sparse_moe.experts.7.w1.weight', 'model.layers.11.block_sparse_moe.experts.7.w2.weight', 'model.layers.11.block_sparse_moe.experts.7.w3.weight', 'model.layers.11.block_sparse_moe.experts.9.w1.weight', 'model.layers.11.block_sparse_moe.experts.9.w2.weight', 'model.layers.11.block_sparse_moe.experts.9.w3.weight', 'model.layers.12.block_sparse_moe.experts.10.w1.weight', 'model.layers.12.block_sparse_moe.experts.10.w2.weight', 'model.layers.12.block_sparse_moe.experts.10.w3.weight', 'model.layers.12.block_sparse_moe.experts.11.w1.weight', 'model.layers.12.block_sparse_moe.experts.11.w2.weight', 'model.layers.12.block_sparse_moe.experts.11.w3.weight', 'model.layers.12.block_sparse_moe.experts.12.w1.weight', 'model.layers.12.block_sparse_moe.experts.12.w2.weight', 'model.layers.12.block_sparse_moe.experts.12.w3.weight', 'model.layers.12.block_sparse_moe.experts.13.w1.weight', 'model.layers.12.block_sparse_moe.experts.13.w2.weight', 'model.layers.12.block_sparse_moe.experts.13.w3.weight', 'model.layers.12.block_sparse_moe.experts.14.w1.weight', 'model.layers.12.block_sparse_moe.experts.14.w2.weight', 'model.layers.12.block_sparse_moe.experts.14.w3.weight', 'model.layers.12.block_sparse_moe.experts.15.w1.weight', 'model.layers.12.block_sparse_moe.experts.15.w2.weight', 'model.layers.12.block_sparse_moe.experts.15.w3.weight', 'model.layers.12.block_sparse_moe.experts.2.w1.weight', 'model.layers.12.block_sparse_moe.experts.2.w2.weight', 'model.layers.12.block_sparse_moe.experts.2.w3.weight', 'model.layers.12.block_sparse_moe.experts.3.w1.weight', 'model.layers.12.block_sparse_moe.experts.3.w2.weight', 'model.layers.12.block_sparse_moe.experts.3.w3.weight', 'model.layers.12.block_sparse_moe.experts.5.w1.weight', 'model.layers.12.block_sparse_moe.experts.5.w2.weight', 'model.layers.12.block_sparse_moe.experts.5.w3.weight', 'model.layers.12.block_sparse_moe.experts.6.w1.weight', 'model.layers.12.block_sparse_moe.experts.6.w2.weight', 'model.layers.12.block_sparse_moe.experts.6.w3.weight', 'model.layers.12.block_sparse_moe.experts.7.w1.weight', 'model.layers.12.block_sparse_moe.experts.7.w2.weight', 'model.layers.12.block_sparse_moe.experts.7.w3.weight', 'model.layers.12.block_sparse_moe.experts.8.w1.weight', 'model.layers.12.block_sparse_moe.experts.8.w2.weight', 'model.layers.12.block_sparse_moe.experts.8.w3.weight', 'model.layers.12.block_sparse_moe.experts.9.w1.weight', 'model.layers.12.block_sparse_moe.experts.9.w2.weight', 'model.layers.12.block_sparse_moe.experts.9.w3.weight', 'model.layers.13.block_sparse_moe.experts.1.w1.weight', 'model.layers.13.block_sparse_moe.experts.1.w2.weight', 'model.layers.13.block_sparse_moe.experts.1.w3.weight', 'model.layers.13.block_sparse_moe.experts.10.w1.weight', 'model.layers.13.block_sparse_moe.experts.10.w2.weight', 'model.layers.13.block_sparse_moe.experts.10.w3.weight', 'model.layers.13.block_sparse_moe.experts.11.w1.weight', 'model.layers.13.block_sparse_moe.experts.11.w2.weight', 'model.layers.13.block_sparse_moe.experts.11.w3.weight', 'model.layers.13.block_sparse_moe.experts.12.w1.weight', 'model.layers.13.block_sparse_moe.experts.12.w2.weight', 'model.layers.13.block_sparse_moe.experts.12.w3.weight', 'model.layers.13.block_sparse_moe.experts.13.w1.weight', 'model.layers.13.block_sparse_moe.experts.13.w2.weight', 'model.layers.13.block_sparse_moe.experts.13.w3.weight', 'model.layers.13.block_sparse_moe.experts.14.w1.weight', 'model.layers.13.block_sparse_moe.experts.14.w2.weight', 'model.layers.13.block_sparse_moe.experts.14.w3.weight', 'model.layers.13.block_sparse_moe.experts.15.w1.weight', 'model.layers.13.block_sparse_moe.experts.15.w2.weight', 'model.layers.13.block_sparse_moe.experts.15.w3.weight', 'model.layers.13.block_sparse_moe.experts.2.w1.weight', 'model.layers.13.block_sparse_moe.experts.2.w2.weight', 'model.layers.13.block_sparse_moe.experts.2.w3.weight', 'model.layers.13.block_sparse_moe.experts.5.w1.weight', 'model.layers.13.block_sparse_moe.experts.5.w2.weight', 'model.layers.13.block_sparse_moe.experts.5.w3.weight', 'model.layers.13.block_sparse_moe.experts.6.w1.weight', 'model.layers.13.block_sparse_moe.experts.6.w2.weight', 'model.layers.13.block_sparse_moe.experts.6.w3.weight', 'model.layers.13.block_sparse_moe.experts.7.w1.weight', 'model.layers.13.block_sparse_moe.experts.7.w2.weight', 'model.layers.13.block_sparse_moe.experts.7.w3.weight', 'model.layers.13.block_sparse_moe.experts.8.w1.weight', 'model.layers.13.block_sparse_moe.experts.8.w2.weight', 'model.layers.13.block_sparse_moe.experts.8.w3.weight', 'model.layers.13.block_sparse_moe.experts.9.w1.weight', 'model.layers.13.block_sparse_moe.experts.9.w2.weight', 'model.layers.13.block_sparse_moe.experts.9.w3.weight', 'model.layers.14.block_sparse_moe.experts.1.w1.weight', 'model.layers.14.block_sparse_moe.experts.1.w2.weight', 'model.layers.14.block_sparse_moe.experts.1.w3.weight', 'model.layers.14.block_sparse_moe.experts.10.w1.weight', 'model.layers.14.block_sparse_moe.experts.10.w2.weight', 'model.layers.14.block_sparse_moe.experts.10.w3.weight', 'model.layers.14.block_sparse_moe.experts.11.w1.weight', 'model.layers.14.block_sparse_moe.experts.11.w2.weight', 'model.layers.14.block_sparse_moe.experts.11.w3.weight', 'model.layers.14.block_sparse_moe.experts.12.w1.weight', 'model.layers.14.block_sparse_moe.experts.12.w2.weight', 'model.layers.14.block_sparse_moe.experts.12.w3.weight', 'model.layers.14.block_sparse_moe.experts.13.w1.weight', 'model.layers.14.block_sparse_moe.experts.13.w2.weight', 'model.layers.14.block_sparse_moe.experts.13.w3.weight', 'model.layers.14.block_sparse_moe.experts.14.w1.weight', 'model.layers.14.block_sparse_moe.experts.14.w2.weight', 'model.layers.14.block_sparse_moe.experts.14.w3.weight', 'model.layers.14.block_sparse_moe.experts.15.w1.weight', 'model.layers.14.block_sparse_moe.experts.15.w2.weight', 'model.layers.14.block_sparse_moe.experts.15.w3.weight', 'model.layers.14.block_sparse_moe.experts.4.w1.weight', 'model.layers.14.block_sparse_moe.experts.4.w2.weight', 'model.layers.14.block_sparse_moe.experts.4.w3.weight', 'model.layers.14.block_sparse_moe.experts.5.w1.weight', 'model.layers.14.block_sparse_moe.experts.5.w2.weight', 'model.layers.14.block_sparse_moe.experts.5.w3.weight', 'model.layers.14.block_sparse_moe.experts.6.w1.weight', 'model.layers.14.block_sparse_moe.experts.6.w2.weight', 'model.layers.14.block_sparse_moe.experts.6.w3.weight', 'model.layers.14.block_sparse_moe.experts.7.w1.weight', 'model.layers.14.block_sparse_moe.experts.7.w2.weight', 'model.layers.14.block_sparse_moe.experts.7.w3.weight', 'model.layers.14.block_sparse_moe.experts.8.w1.weight', 'model.layers.14.block_sparse_moe.experts.8.w2.weight', 'model.layers.14.block_sparse_moe.experts.8.w3.weight', 'model.layers.14.block_sparse_moe.experts.9.w1.weight', 'model.layers.14.block_sparse_moe.experts.9.w2.weight', 'model.layers.14.block_sparse_moe.experts.9.w3.weight', 'model.layers.15.block_sparse_moe.experts.1.w1.weight', 'model.layers.15.block_sparse_moe.experts.1.w2.weight', 'model.layers.15.block_sparse_moe.experts.1.w3.weight', 'model.layers.15.block_sparse_moe.experts.10.w1.weight', 'model.layers.15.block_sparse_moe.experts.10.w2.weight', 'model.layers.15.block_sparse_moe.experts.10.w3.weight', 'model.layers.15.block_sparse_moe.experts.11.w1.weight', 'model.layers.15.block_sparse_moe.experts.11.w2.weight', 'model.layers.15.block_sparse_moe.experts.11.w3.weight', 'model.layers.15.block_sparse_moe.experts.12.w1.weight', 'model.layers.15.block_sparse_moe.experts.12.w2.weight', 'model.layers.15.block_sparse_moe.experts.12.w3.weight', 'model.layers.15.block_sparse_moe.experts.13.w1.weight', 'model.layers.15.block_sparse_moe.experts.13.w2.weight', 'model.layers.15.block_sparse_moe.experts.13.w3.weight', 'model.layers.15.block_sparse_moe.experts.14.w1.weight', 'model.layers.15.block_sparse_moe.experts.14.w2.weight', 'model.layers.15.block_sparse_moe.experts.14.w3.weight', 'model.layers.15.block_sparse_moe.experts.15.w1.weight', 'model.layers.15.block_sparse_moe.experts.15.w2.weight', 'model.layers.15.block_sparse_moe.experts.15.w3.weight', 'model.layers.15.block_sparse_moe.experts.3.w1.weight', 'model.layers.15.block_sparse_moe.experts.3.w2.weight', 'model.layers.15.block_sparse_moe.experts.3.w3.weight', 'model.layers.15.block_sparse_moe.experts.5.w1.weight', 'model.layers.15.block_sparse_moe.experts.5.w2.weight', 'model.layers.15.block_sparse_moe.experts.5.w3.weight', 'model.layers.15.block_sparse_moe.experts.6.w1.weight', 'model.layers.15.block_sparse_moe.experts.6.w2.weight', 'model.layers.15.block_sparse_moe.experts.6.w3.weight', 'model.layers.15.block_sparse_moe.experts.8.w1.weight', 'model.layers.15.block_sparse_moe.experts.8.w2.weight', 'model.layers.15.block_sparse_moe.experts.8.w3.weight', 'model.layers.15.block_sparse_moe.experts.9.w1.weight', 'model.layers.15.block_sparse_moe.experts.9.w2.weight', 'model.layers.15.block_sparse_moe.experts.9.w3.weight', 'model.layers.16.block_sparse_moe.experts.10.w1.weight', 'model.layers.16.block_sparse_moe.experts.10.w2.weight', 'model.layers.16.block_sparse_moe.experts.10.w3.weight', 'model.layers.16.block_sparse_moe.experts.11.w1.weight', 'model.layers.16.block_sparse_moe.experts.11.w2.weight', 'model.layers.16.block_sparse_moe.experts.11.w3.weight', 'model.layers.16.block_sparse_moe.experts.12.w1.weight', 'model.layers.16.block_sparse_moe.experts.12.w2.weight', 'model.layers.16.block_sparse_moe.experts.12.w3.weight', 'model.layers.16.block_sparse_moe.experts.13.w1.weight', 'model.layers.16.block_sparse_moe.experts.13.w2.weight', 'model.layers.16.block_sparse_moe.experts.13.w3.weight', 'model.layers.16.block_sparse_moe.experts.14.w1.weight', 'model.layers.16.block_sparse_moe.experts.14.w2.weight', 'model.layers.16.block_sparse_moe.experts.14.w3.weight', 'model.layers.16.block_sparse_moe.experts.15.w1.weight', 'model.layers.16.block_sparse_moe.experts.15.w2.weight', 'model.layers.16.block_sparse_moe.experts.15.w3.weight', 'model.layers.16.block_sparse_moe.experts.4.w1.weight', 'model.layers.16.block_sparse_moe.experts.4.w2.weight', 'model.layers.16.block_sparse_moe.experts.4.w3.weight', 'model.layers.16.block_sparse_moe.experts.5.w1.weight', 'model.layers.16.block_sparse_moe.experts.5.w2.weight', 'model.layers.16.block_sparse_moe.experts.5.w3.weight', 'model.layers.16.block_sparse_moe.experts.6.w1.weight', 'model.layers.16.block_sparse_moe.experts.6.w2.weight', 'model.layers.16.block_sparse_moe.experts.6.w3.weight', 'model.layers.16.block_sparse_moe.experts.7.w1.weight', 'model.layers.16.block_sparse_moe.experts.7.w2.weight', 'model.layers.16.block_sparse_moe.experts.7.w3.weight', 'model.layers.16.block_sparse_moe.experts.8.w1.weight', 'model.layers.16.block_sparse_moe.experts.8.w2.weight', 'model.layers.16.block_sparse_moe.experts.8.w3.weight', 'model.layers.16.block_sparse_moe.experts.9.w1.weight', 'model.layers.16.block_sparse_moe.experts.9.w2.weight', 'model.layers.16.block_sparse_moe.experts.9.w3.weight', 'model.layers.17.block_sparse_moe.experts.10.w1.weight', 'model.layers.17.block_sparse_moe.experts.10.w2.weight', 'model.layers.17.block_sparse_moe.experts.10.w3.weight', 'model.layers.17.block_sparse_moe.experts.11.w1.weight', 'model.layers.17.block_sparse_moe.experts.11.w2.weight', 'model.layers.17.block_sparse_moe.experts.11.w3.weight', 'model.layers.17.block_sparse_moe.experts.12.w1.weight', 'model.layers.17.block_sparse_moe.experts.12.w2.weight', 'model.layers.17.block_sparse_moe.experts.12.w3.weight', 'model.layers.17.block_sparse_moe.experts.13.w1.weight', 'model.layers.17.block_sparse_moe.experts.13.w2.weight', 'model.layers.17.block_sparse_moe.experts.13.w3.weight', 'model.layers.17.block_sparse_moe.experts.14.w1.weight', 'model.layers.17.block_sparse_moe.experts.14.w2.weight', 'model.layers.17.block_sparse_moe.experts.14.w3.weight', 'model.layers.17.block_sparse_moe.experts.15.w1.weight', 'model.layers.17.block_sparse_moe.experts.15.w2.weight', 'model.layers.17.block_sparse_moe.experts.15.w3.weight', 'model.layers.17.block_sparse_moe.experts.3.w1.weight', 'model.layers.17.block_sparse_moe.experts.3.w2.weight', 'model.layers.17.block_sparse_moe.experts.3.w3.weight', 'model.layers.17.block_sparse_moe.experts.4.w1.weight', 'model.layers.17.block_sparse_moe.experts.4.w2.weight', 'model.layers.17.block_sparse_moe.experts.4.w3.weight', 'model.layers.17.block_sparse_moe.experts.5.w1.weight', 'model.layers.17.block_sparse_moe.experts.5.w2.weight', 'model.layers.17.block_sparse_moe.experts.5.w3.weight', 'model.layers.17.block_sparse_moe.experts.6.w1.weight', 'model.layers.17.block_sparse_moe.experts.6.w2.weight', 'model.layers.17.block_sparse_moe.experts.6.w3.weight', 'model.layers.17.block_sparse_moe.experts.7.w1.weight', 'model.layers.17.block_sparse_moe.experts.7.w2.weight', 'model.layers.17.block_sparse_moe.experts.7.w3.weight', 'model.layers.17.block_sparse_moe.experts.8.w1.weight', 'model.layers.17.block_sparse_moe.experts.8.w2.weight', 'model.layers.17.block_sparse_moe.experts.8.w3.weight', 'model.layers.17.block_sparse_moe.experts.9.w1.weight', 'model.layers.17.block_sparse_moe.experts.9.w2.weight', 'model.layers.17.block_sparse_moe.experts.9.w3.weight', 'model.layers.18.block_sparse_moe.experts.10.w1.weight', 'model.layers.18.block_sparse_moe.experts.10.w2.weight', 'model.layers.18.block_sparse_moe.experts.10.w3.weight', 'model.layers.18.block_sparse_moe.experts.11.w1.weight', 'model.layers.18.block_sparse_moe.experts.11.w2.weight', 'model.layers.18.block_sparse_moe.experts.11.w3.weight', 'model.layers.18.block_sparse_moe.experts.13.w1.weight', 'model.layers.18.block_sparse_moe.experts.13.w2.weight', 'model.layers.18.block_sparse_moe.experts.13.w3.weight', 'model.layers.18.block_sparse_moe.experts.14.w1.weight', 'model.layers.18.block_sparse_moe.experts.14.w2.weight', 'model.layers.18.block_sparse_moe.experts.14.w3.weight', 'model.layers.18.block_sparse_moe.experts.15.w1.weight', 'model.layers.18.block_sparse_moe.experts.15.w2.weight', 'model.layers.18.block_sparse_moe.experts.15.w3.weight', 'model.layers.18.block_sparse_moe.experts.2.w1.weight', 'model.layers.18.block_sparse_moe.experts.2.w2.weight', 'model.layers.18.block_sparse_moe.experts.2.w3.weight', 'model.layers.18.block_sparse_moe.experts.4.w1.weight', 'model.layers.18.block_sparse_moe.experts.4.w2.weight', 'model.layers.18.block_sparse_moe.experts.4.w3.weight', 'model.layers.18.block_sparse_moe.experts.5.w1.weight', 'model.layers.18.block_sparse_moe.experts.5.w2.weight', 'model.layers.18.block_sparse_moe.experts.5.w3.weight', 'model.layers.18.block_sparse_moe.experts.6.w1.weight', 'model.layers.18.block_sparse_moe.experts.6.w2.weight', 'model.layers.18.block_sparse_moe.experts.6.w3.weight', 'model.layers.18.block_sparse_moe.experts.7.w1.weight', 'model.layers.18.block_sparse_moe.experts.7.w2.weight', 'model.layers.18.block_sparse_moe.experts.7.w3.weight', 'model.layers.18.block_sparse_moe.experts.8.w1.weight', 'model.layers.18.block_sparse_moe.experts.8.w2.weight', 'model.layers.18.block_sparse_moe.experts.8.w3.weight', 'model.layers.18.block_sparse_moe.experts.9.w1.weight', 'model.layers.18.block_sparse_moe.experts.9.w2.weight', 'model.layers.18.block_sparse_moe.experts.9.w3.weight', 'model.layers.19.block_sparse_moe.experts.10.w1.weight', 'model.layers.19.block_sparse_moe.experts.10.w2.weight', 'model.layers.19.block_sparse_moe.experts.10.w3.weight', 'model.layers.19.block_sparse_moe.experts.11.w1.weight', 'model.layers.19.block_sparse_moe.experts.11.w2.weight', 'model.layers.19.block_sparse_moe.experts.11.w3.weight', 'model.layers.19.block_sparse_moe.experts.12.w1.weight', 'model.layers.19.block_sparse_moe.experts.12.w2.weight', 'model.layers.19.block_sparse_moe.experts.12.w3.weight', 'model.layers.19.block_sparse_moe.experts.13.w1.weight', 'model.layers.19.block_sparse_moe.experts.13.w2.weight', 'model.layers.19.block_sparse_moe.experts.13.w3.weight', 'model.layers.19.block_sparse_moe.experts.14.w1.weight', 'model.layers.19.block_sparse_moe.experts.14.w2.weight', 'model.layers.19.block_sparse_moe.experts.14.w3.weight', 'model.layers.19.block_sparse_moe.experts.15.w1.weight', 'model.layers.19.block_sparse_moe.experts.15.w2.weight', 'model.layers.19.block_sparse_moe.experts.15.w3.weight', 'model.layers.19.block_sparse_moe.experts.2.w1.weight', 'model.layers.19.block_sparse_moe.experts.2.w2.weight', 'model.layers.19.block_sparse_moe.experts.2.w3.weight', 'model.layers.19.block_sparse_moe.experts.3.w1.weight', 'model.layers.19.block_sparse_moe.experts.3.w2.weight', 'model.layers.19.block_sparse_moe.experts.3.w3.weight', 'model.layers.19.block_sparse_moe.experts.4.w1.weight', 'model.layers.19.block_sparse_moe.experts.4.w2.weight', 'model.layers.19.block_sparse_moe.experts.4.w3.weight', 'model.layers.19.block_sparse_moe.experts.5.w1.weight', 'model.layers.19.block_sparse_moe.experts.5.w2.weight', 'model.layers.19.block_sparse_moe.experts.5.w3.weight', 'model.layers.19.block_sparse_moe.experts.7.w1.weight', 'model.layers.19.block_sparse_moe.experts.7.w2.weight', 'model.layers.19.block_sparse_moe.experts.7.w3.weight', 'model.layers.19.block_sparse_moe.experts.8.w1.weight', 'model.layers.19.block_sparse_moe.experts.8.w2.weight', 'model.layers.19.block_sparse_moe.experts.8.w3.weight', 'model.layers.19.block_sparse_moe.experts.9.w1.weight', 'model.layers.19.block_sparse_moe.experts.9.w2.weight', 'model.layers.19.block_sparse_moe.experts.9.w3.weight', 'model.layers.2.block_sparse_moe.experts.1.w1.weight', 'model.layers.2.block_sparse_moe.experts.1.w2.weight', 'model.layers.2.block_sparse_moe.experts.1.w3.weight', 'model.layers.2.block_sparse_moe.experts.10.w1.weight', 'model.layers.2.block_sparse_moe.experts.10.w2.weight', 'model.layers.2.block_sparse_moe.experts.10.w3.weight', 'model.layers.2.block_sparse_moe.experts.11.w1.weight', 'model.layers.2.block_sparse_moe.experts.11.w2.weight', 'model.layers.2.block_sparse_moe.experts.11.w3.weight', 'model.layers.2.block_sparse_moe.experts.13.w1.weight', 'model.layers.2.block_sparse_moe.experts.13.w2.weight', 'model.layers.2.block_sparse_moe.experts.13.w3.weight', 'model.layers.2.block_sparse_moe.experts.14.w1.weight', 'model.layers.2.block_sparse_moe.experts.14.w2.weight', 'model.layers.2.block_sparse_moe.experts.14.w3.weight', 'model.layers.2.block_sparse_moe.experts.15.w1.weight', 'model.layers.2.block_sparse_moe.experts.15.w2.weight', 'model.layers.2.block_sparse_moe.experts.15.w3.weight', 'model.layers.2.block_sparse_moe.experts.2.w1.weight', 'model.layers.2.block_sparse_moe.experts.2.w2.weight', 'model.layers.2.block_sparse_moe.experts.2.w3.weight', 'model.layers.2.block_sparse_moe.experts.5.w1.weight', 'model.layers.2.block_sparse_moe.experts.5.w2.weight', 'model.layers.2.block_sparse_moe.experts.5.w3.weight', 'model.layers.2.block_sparse_moe.experts.6.w1.weight', 'model.layers.2.block_sparse_moe.experts.6.w2.weight', 'model.layers.2.block_sparse_moe.experts.6.w3.weight', 'model.layers.2.block_sparse_moe.experts.7.w1.weight', 'model.layers.2.block_sparse_moe.experts.7.w2.weight', 'model.layers.2.block_sparse_moe.experts.7.w3.weight', 'model.layers.2.block_sparse_moe.experts.8.w1.weight', 'model.layers.2.block_sparse_moe.experts.8.w2.weight', 'model.layers.2.block_sparse_moe.experts.8.w3.weight', 'model.layers.2.block_sparse_moe.experts.9.w1.weight', 'model.layers.2.block_sparse_moe.experts.9.w2.weight', 'model.layers.2.block_sparse_moe.experts.9.w3.weight', 'model.layers.20.block_sparse_moe.experts.10.w1.weight', 'model.layers.20.block_sparse_moe.experts.10.w2.weight', 'model.layers.20.block_sparse_moe.experts.10.w3.weight', 'model.layers.20.block_sparse_moe.experts.11.w1.weight', 'model.layers.20.block_sparse_moe.experts.11.w2.weight', 'model.layers.20.block_sparse_moe.experts.11.w3.weight', 'model.layers.20.block_sparse_moe.experts.12.w1.weight', 'model.layers.20.block_sparse_moe.experts.12.w2.weight', 'model.layers.20.block_sparse_moe.experts.12.w3.weight', 'model.layers.20.block_sparse_moe.experts.13.w1.weight', 'model.layers.20.block_sparse_moe.experts.13.w2.weight', 'model.layers.20.block_sparse_moe.experts.13.w3.weight', 'model.layers.20.block_sparse_moe.experts.14.w1.weight', 'model.layers.20.block_sparse_moe.experts.14.w2.weight', 'model.layers.20.block_sparse_moe.experts.14.w3.weight', 'model.layers.20.block_sparse_moe.experts.15.w1.weight', 'model.layers.20.block_sparse_moe.experts.15.w2.weight', 'model.layers.20.block_sparse_moe.experts.15.w3.weight', 'model.layers.20.block_sparse_moe.experts.3.w1.weight', 'model.layers.20.block_sparse_moe.experts.3.w2.weight', 'model.layers.20.block_sparse_moe.experts.3.w3.weight', 'model.layers.20.block_sparse_moe.experts.5.w1.weight', 'model.layers.20.block_sparse_moe.experts.5.w2.weight', 'model.layers.20.block_sparse_moe.experts.5.w3.weight', 'model.layers.20.block_sparse_moe.experts.6.w1.weight', 'model.layers.20.block_sparse_moe.experts.6.w2.weight', 'model.layers.20.block_sparse_moe.experts.6.w3.weight', 'model.layers.20.block_sparse_moe.experts.7.w1.weight', 'model.layers.20.block_sparse_moe.experts.7.w2.weight', 'model.layers.20.block_sparse_moe.experts.7.w3.weight', 'model.layers.20.block_sparse_moe.experts.8.w1.weight', 'model.layers.20.block_sparse_moe.experts.8.w2.weight', 'model.layers.20.block_sparse_moe.experts.8.w3.weight', 'model.layers.20.block_sparse_moe.experts.9.w1.weight', 'model.layers.20.block_sparse_moe.experts.9.w2.weight', 'model.layers.20.block_sparse_moe.experts.9.w3.weight', 'model.layers.21.block_sparse_moe.experts.10.w1.weight', 'model.layers.21.block_sparse_moe.experts.10.w2.weight', 'model.layers.21.block_sparse_moe.experts.10.w3.weight', 'model.layers.21.block_sparse_moe.experts.11.w1.weight', 'model.layers.21.block_sparse_moe.experts.11.w2.weight', 'model.layers.21.block_sparse_moe.experts.11.w3.weight', 'model.layers.21.block_sparse_moe.experts.12.w1.weight', 'model.layers.21.block_sparse_moe.experts.12.w2.weight', 'model.layers.21.block_sparse_moe.experts.12.w3.weight', 'model.layers.21.block_sparse_moe.experts.13.w1.weight', 'model.layers.21.block_sparse_moe.experts.13.w2.weight', 'model.layers.21.block_sparse_moe.experts.13.w3.weight', 'model.layers.21.block_sparse_moe.experts.14.w1.weight', 'model.layers.21.block_sparse_moe.experts.14.w2.weight', 'model.layers.21.block_sparse_moe.experts.14.w3.weight', 'model.layers.21.block_sparse_moe.experts.15.w1.weight', 'model.layers.21.block_sparse_moe.experts.15.w2.weight', 'model.layers.21.block_sparse_moe.experts.15.w3.weight', 'model.layers.21.block_sparse_moe.experts.3.w1.weight', 'model.layers.21.block_sparse_moe.experts.3.w2.weight', 'model.layers.21.block_sparse_moe.experts.3.w3.weight', 'model.layers.21.block_sparse_moe.experts.4.w1.weight', 'model.layers.21.block_sparse_moe.experts.4.w2.weight', 'model.layers.21.block_sparse_moe.experts.4.w3.weight', 'model.layers.21.block_sparse_moe.experts.5.w1.weight', 'model.layers.21.block_sparse_moe.experts.5.w2.weight', 'model.layers.21.block_sparse_moe.experts.5.w3.weight', 'model.layers.21.block_sparse_moe.experts.6.w1.weight', 'model.layers.21.block_sparse_moe.experts.6.w2.weight', 'model.layers.21.block_sparse_moe.experts.6.w3.weight', 'model.layers.21.block_sparse_moe.experts.7.w1.weight', 'model.layers.21.block_sparse_moe.experts.7.w2.weight', 'model.layers.21.block_sparse_moe.experts.7.w3.weight', 'model.layers.21.block_sparse_moe.experts.8.w1.weight', 'model.layers.21.block_sparse_moe.experts.8.w2.weight', 'model.layers.21.block_sparse_moe.experts.8.w3.weight', 'model.layers.21.block_sparse_moe.experts.9.w1.weight', 'model.layers.21.block_sparse_moe.experts.9.w2.weight', 'model.layers.21.block_sparse_moe.experts.9.w3.weight', 'model.layers.22.block_sparse_moe.experts.10.w1.weight', 'model.layers.22.block_sparse_moe.experts.10.w2.weight', 'model.layers.22.block_sparse_moe.experts.10.w3.weight', 'model.layers.22.block_sparse_moe.experts.11.w1.weight', 'model.layers.22.block_sparse_moe.experts.11.w2.weight', 'model.layers.22.block_sparse_moe.experts.11.w3.weight', 'model.layers.22.block_sparse_moe.experts.12.w1.weight', 'model.layers.22.block_sparse_moe.experts.12.w2.weight', 'model.layers.22.block_sparse_moe.experts.12.w3.weight', 'model.layers.22.block_sparse_moe.experts.13.w1.weight', 'model.layers.22.block_sparse_moe.experts.13.w2.weight', 'model.layers.22.block_sparse_moe.experts.13.w3.weight', 'model.layers.22.block_sparse_moe.experts.14.w1.weight', 'model.layers.22.block_sparse_moe.experts.14.w2.weight', 'model.layers.22.block_sparse_moe.experts.14.w3.weight', 'model.layers.22.block_sparse_moe.experts.15.w1.weight', 'model.layers.22.block_sparse_moe.experts.15.w2.weight', 'model.layers.22.block_sparse_moe.experts.15.w3.weight', 'model.layers.22.block_sparse_moe.experts.2.w1.weight', 'model.layers.22.block_sparse_moe.experts.2.w2.weight', 'model.layers.22.block_sparse_moe.experts.2.w3.weight', 'model.layers.22.block_sparse_moe.experts.3.w1.weight', 'model.layers.22.block_sparse_moe.experts.3.w2.weight', 'model.layers.22.block_sparse_moe.experts.3.w3.weight', 'model.layers.22.block_sparse_moe.experts.4.w1.weight', 'model.layers.22.block_sparse_moe.experts.4.w2.weight', 'model.layers.22.block_sparse_moe.experts.4.w3.weight', 'model.layers.22.block_sparse_moe.experts.5.w1.weight', 'model.layers.22.block_sparse_moe.experts.5.w2.weight', 'model.layers.22.block_sparse_moe.experts.5.w3.weight', 'model.layers.22.block_sparse_moe.experts.7.w1.weight', 'model.layers.22.block_sparse_moe.experts.7.w2.weight', 'model.layers.22.block_sparse_moe.experts.7.w3.weight', 'model.layers.22.block_sparse_moe.experts.8.w1.weight', 'model.layers.22.block_sparse_moe.experts.8.w2.weight', 'model.layers.22.block_sparse_moe.experts.8.w3.weight', 'model.layers.22.block_sparse_moe.experts.9.w1.weight', 'model.layers.22.block_sparse_moe.experts.9.w2.weight', 'model.layers.22.block_sparse_moe.experts.9.w3.weight', 'model.layers.23.block_sparse_moe.experts.10.w1.weight', 'model.layers.23.block_sparse_moe.experts.10.w2.weight', 'model.layers.23.block_sparse_moe.experts.10.w3.weight', 'model.layers.23.block_sparse_moe.experts.11.w1.weight', 'model.layers.23.block_sparse_moe.experts.11.w2.weight', 'model.layers.23.block_sparse_moe.experts.11.w3.weight', 'model.layers.23.block_sparse_moe.experts.12.w1.weight', 'model.layers.23.block_sparse_moe.experts.12.w2.weight', 'model.layers.23.block_sparse_moe.experts.12.w3.weight', 'model.layers.23.block_sparse_moe.experts.13.w1.weight', 'model.layers.23.block_sparse_moe.experts.13.w2.weight', 'model.layers.23.block_sparse_moe.experts.13.w3.weight', 'model.layers.23.block_sparse_moe.experts.14.w1.weight', 'model.layers.23.block_sparse_moe.experts.14.w2.weight', 'model.layers.23.block_sparse_moe.experts.14.w3.weight', 'model.layers.23.block_sparse_moe.experts.15.w1.weight', 'model.layers.23.block_sparse_moe.experts.15.w2.weight', 'model.layers.23.block_sparse_moe.experts.15.w3.weight', 'model.layers.23.block_sparse_moe.experts.2.w1.weight', 'model.layers.23.block_sparse_moe.experts.2.w2.weight', 'model.layers.23.block_sparse_moe.experts.2.w3.weight', 'model.layers.23.block_sparse_moe.experts.3.w1.weight', 'model.layers.23.block_sparse_moe.experts.3.w2.weight', 'model.layers.23.block_sparse_moe.experts.3.w3.weight', 'model.layers.23.block_sparse_moe.experts.5.w1.weight', 'model.layers.23.block_sparse_moe.experts.5.w2.weight', 'model.layers.23.block_sparse_moe.experts.5.w3.weight', 'model.layers.23.block_sparse_moe.experts.6.w1.weight', 'model.layers.23.block_sparse_moe.experts.6.w2.weight', 'model.layers.23.block_sparse_moe.experts.6.w3.weight', 'model.layers.23.block_sparse_moe.experts.7.w1.weight', 'model.layers.23.block_sparse_moe.experts.7.w2.weight', 'model.layers.23.block_sparse_moe.experts.7.w3.weight', 'model.layers.23.block_sparse_moe.experts.8.w1.weight', 'model.layers.23.block_sparse_moe.experts.8.w2.weight', 'model.layers.23.block_sparse_moe.experts.8.w3.weight', 'model.layers.23.block_sparse_moe.experts.9.w1.weight', 'model.layers.23.block_sparse_moe.experts.9.w2.weight', 'model.layers.23.block_sparse_moe.experts.9.w3.weight', 'model.layers.24.block_sparse_moe.experts.10.w1.weight', 'model.layers.24.block_sparse_moe.experts.10.w2.weight', 'model.layers.24.block_sparse_moe.experts.10.w3.weight', 'model.layers.24.block_sparse_moe.experts.11.w1.weight', 'model.layers.24.block_sparse_moe.experts.11.w2.weight', 'model.layers.24.block_sparse_moe.experts.11.w3.weight', 'model.layers.24.block_sparse_moe.experts.12.w1.weight', 'model.layers.24.block_sparse_moe.experts.12.w2.weight', 'model.layers.24.block_sparse_moe.experts.12.w3.weight', 'model.layers.24.block_sparse_moe.experts.13.w1.weight', 'model.layers.24.block_sparse_moe.experts.13.w2.weight', 'model.layers.24.block_sparse_moe.experts.13.w3.weight', 'model.layers.24.block_sparse_moe.experts.14.w1.weight', 'model.layers.24.block_sparse_moe.experts.14.w2.weight', 'model.layers.24.block_sparse_moe.experts.14.w3.weight', 'model.layers.24.block_sparse_moe.experts.15.w1.weight', 'model.layers.24.block_sparse_moe.experts.15.w2.weight', 'model.layers.24.block_sparse_moe.experts.15.w3.weight', 'model.layers.24.block_sparse_moe.experts.2.w1.weight', 'model.layers.24.block_sparse_moe.experts.2.w2.weight', 'model.layers.24.block_sparse_moe.experts.2.w3.weight', 'model.layers.24.block_sparse_moe.experts.4.w1.weight', 'model.layers.24.block_sparse_moe.experts.4.w2.weight', 'model.layers.24.block_sparse_moe.experts.4.w3.weight', 'model.layers.24.block_sparse_moe.experts.5.w1.weight', 'model.layers.24.block_sparse_moe.experts.5.w2.weight', 'model.layers.24.block_sparse_moe.experts.5.w3.weight', 'model.layers.24.block_sparse_moe.experts.6.w1.weight', 'model.layers.24.block_sparse_moe.experts.6.w2.weight', 'model.layers.24.block_sparse_moe.experts.6.w3.weight', 'model.layers.24.block_sparse_moe.experts.8.w1.weight', 'model.layers.24.block_sparse_moe.experts.8.w2.weight', 'model.layers.24.block_sparse_moe.experts.8.w3.weight', 'model.layers.24.block_sparse_moe.experts.9.w1.weight', 'model.layers.24.block_sparse_moe.experts.9.w2.weight', 'model.layers.24.block_sparse_moe.experts.9.w3.weight', 'model.layers.25.block_sparse_moe.experts.10.w1.weight', 'model.layers.25.block_sparse_moe.experts.10.w2.weight', 'model.layers.25.block_sparse_moe.experts.10.w3.weight', 'model.layers.25.block_sparse_moe.experts.11.w1.weight', 'model.layers.25.block_sparse_moe.experts.11.w2.weight', 'model.layers.25.block_sparse_moe.experts.11.w3.weight', 'model.layers.25.block_sparse_moe.experts.12.w1.weight', 'model.layers.25.block_sparse_moe.experts.12.w2.weight', 'model.layers.25.block_sparse_moe.experts.12.w3.weight', 'model.layers.25.block_sparse_moe.experts.13.w1.weight', 'model.layers.25.block_sparse_moe.experts.13.w2.weight', 'model.layers.25.block_sparse_moe.experts.13.w3.weight', 'model.layers.25.block_sparse_moe.experts.14.w1.weight', 'model.layers.25.block_sparse_moe.experts.14.w2.weight', 'model.layers.25.block_sparse_moe.experts.14.w3.weight', 'model.layers.25.block_sparse_moe.experts.15.w1.weight', 'model.layers.25.block_sparse_moe.experts.15.w2.weight', 'model.layers.25.block_sparse_moe.experts.15.w3.weight', 'model.layers.25.block_sparse_moe.experts.2.w1.weight', 'model.layers.25.block_sparse_moe.experts.2.w2.weight', 'model.layers.25.block_sparse_moe.experts.2.w3.weight', 'model.layers.25.block_sparse_moe.experts.4.w1.weight', 'model.layers.25.block_sparse_moe.experts.4.w2.weight', 'model.layers.25.block_sparse_moe.experts.4.w3.weight', 'model.layers.25.block_sparse_moe.experts.5.w1.weight', 'model.layers.25.block_sparse_moe.experts.5.w2.weight', 'model.layers.25.block_sparse_moe.experts.5.w3.weight', 'model.layers.25.block_sparse_moe.experts.6.w1.weight', 'model.layers.25.block_sparse_moe.experts.6.w2.weight', 'model.layers.25.block_sparse_moe.experts.6.w3.weight', 'model.layers.25.block_sparse_moe.experts.7.w1.weight', 'model.layers.25.block_sparse_moe.experts.7.w2.weight', 'model.layers.25.block_sparse_moe.experts.7.w3.weight', 'model.layers.25.block_sparse_moe.experts.8.w1.weight', 'model.layers.25.block_sparse_moe.experts.8.w2.weight', 'model.layers.25.block_sparse_moe.experts.8.w3.weight', 'model.layers.25.block_sparse_moe.experts.9.w1.weight', 'model.layers.25.block_sparse_moe.experts.9.w2.weight', 'model.layers.25.block_sparse_moe.experts.9.w3.weight', 'model.layers.26.block_sparse_moe.experts.10.w1.weight', 'model.layers.26.block_sparse_moe.experts.10.w2.weight', 'model.layers.26.block_sparse_moe.experts.10.w3.weight', 'model.layers.26.block_sparse_moe.experts.12.w1.weight', 'model.layers.26.block_sparse_moe.experts.12.w2.weight', 'model.layers.26.block_sparse_moe.experts.12.w3.weight', 'model.layers.26.block_sparse_moe.experts.13.w1.weight', 'model.layers.26.block_sparse_moe.experts.13.w2.weight', 'model.layers.26.block_sparse_moe.experts.13.w3.weight', 'model.layers.26.block_sparse_moe.experts.14.w1.weight', 'model.layers.26.block_sparse_moe.experts.14.w2.weight', 'model.layers.26.block_sparse_moe.experts.14.w3.weight', 'model.layers.26.block_sparse_moe.experts.15.w1.weight', 'model.layers.26.block_sparse_moe.experts.15.w2.weight', 'model.layers.26.block_sparse_moe.experts.15.w3.weight', 'model.layers.26.block_sparse_moe.experts.2.w1.weight', 'model.layers.26.block_sparse_moe.experts.2.w2.weight', 'model.layers.26.block_sparse_moe.experts.2.w3.weight', 'model.layers.26.block_sparse_moe.experts.3.w1.weight', 'model.layers.26.block_sparse_moe.experts.3.w2.weight', 'model.layers.26.block_sparse_moe.experts.3.w3.weight', 'model.layers.26.block_sparse_moe.experts.5.w1.weight', 'model.layers.26.block_sparse_moe.experts.5.w2.weight', 'model.layers.26.block_sparse_moe.experts.5.w3.weight', 'model.layers.26.block_sparse_moe.experts.6.w1.weight', 'model.layers.26.block_sparse_moe.experts.6.w2.weight', 'model.layers.26.block_sparse_moe.experts.6.w3.weight', 'model.layers.26.block_sparse_moe.experts.7.w1.weight', 'model.layers.26.block_sparse_moe.experts.7.w2.weight', 'model.layers.26.block_sparse_moe.experts.7.w3.weight', 'model.layers.26.block_sparse_moe.experts.8.w1.weight', 'model.layers.26.block_sparse_moe.experts.8.w2.weight', 'model.layers.26.block_sparse_moe.experts.8.w3.weight', 'model.layers.26.block_sparse_moe.experts.9.w1.weight', 'model.layers.26.block_sparse_moe.experts.9.w2.weight', 'model.layers.26.block_sparse_moe.experts.9.w3.weight', 'model.layers.27.block_sparse_moe.experts.10.w1.weight', 'model.layers.27.block_sparse_moe.experts.10.w2.weight', 'model.layers.27.block_sparse_moe.experts.10.w3.weight', 'model.layers.27.block_sparse_moe.experts.11.w1.weight', 'model.layers.27.block_sparse_moe.experts.11.w2.weight', 'model.layers.27.block_sparse_moe.experts.11.w3.weight', 'model.layers.27.block_sparse_moe.experts.12.w1.weight', 'model.layers.27.block_sparse_moe.experts.12.w2.weight', 'model.layers.27.block_sparse_moe.experts.12.w3.weight', 'model.layers.27.block_sparse_moe.experts.13.w1.weight', 'model.layers.27.block_sparse_moe.experts.13.w2.weight', 'model.layers.27.block_sparse_moe.experts.13.w3.weight', 'model.layers.27.block_sparse_moe.experts.14.w1.weight', 'model.layers.27.block_sparse_moe.experts.14.w2.weight', 'model.layers.27.block_sparse_moe.experts.14.w3.weight', 'model.layers.27.block_sparse_moe.experts.15.w1.weight', 'model.layers.27.block_sparse_moe.experts.15.w2.weight', 'model.layers.27.block_sparse_moe.experts.15.w3.weight', 'model.layers.27.block_sparse_moe.experts.2.w1.weight', 'model.layers.27.block_sparse_moe.experts.2.w2.weight', 'model.layers.27.block_sparse_moe.experts.2.w3.weight', 'model.layers.27.block_sparse_moe.experts.4.w1.weight', 'model.layers.27.block_sparse_moe.experts.4.w2.weight', 'model.layers.27.block_sparse_moe.experts.4.w3.weight', 'model.layers.27.block_sparse_moe.experts.5.w1.weight', 'model.layers.27.block_sparse_moe.experts.5.w2.weight', 'model.layers.27.block_sparse_moe.experts.5.w3.weight', 'model.layers.27.block_sparse_moe.experts.6.w1.weight', 'model.layers.27.block_sparse_moe.experts.6.w2.weight', 'model.layers.27.block_sparse_moe.experts.6.w3.weight', 'model.layers.27.block_sparse_moe.experts.7.w1.weight', 'model.layers.27.block_sparse_moe.experts.7.w2.weight', 'model.layers.27.block_sparse_moe.experts.7.w3.weight', 'model.layers.27.block_sparse_moe.experts.8.w1.weight', 'model.layers.27.block_sparse_moe.experts.8.w2.weight', 'model.layers.27.block_sparse_moe.experts.8.w3.weight', 'model.layers.28.block_sparse_moe.experts.10.w1.weight', 'model.layers.28.block_sparse_moe.experts.10.w2.weight', 'model.layers.28.block_sparse_moe.experts.10.w3.weight', 'model.layers.28.block_sparse_moe.experts.11.w1.weight', 'model.layers.28.block_sparse_moe.experts.11.w2.weight', 'model.layers.28.block_sparse_moe.experts.11.w3.weight', 'model.layers.28.block_sparse_moe.experts.12.w1.weight', 'model.layers.28.block_sparse_moe.experts.12.w2.weight', 'model.layers.28.block_sparse_moe.experts.12.w3.weight', 'model.layers.28.block_sparse_moe.experts.13.w1.weight', 'model.layers.28.block_sparse_moe.experts.13.w2.weight', 'model.layers.28.block_sparse_moe.experts.13.w3.weight', 'model.layers.28.block_sparse_moe.experts.14.w1.weight', 'model.layers.28.block_sparse_moe.experts.14.w2.weight', 'model.layers.28.block_sparse_moe.experts.14.w3.weight', 'model.layers.28.block_sparse_moe.experts.15.w1.weight', 'model.layers.28.block_sparse_moe.experts.15.w2.weight', 'model.layers.28.block_sparse_moe.experts.15.w3.weight', 'model.layers.28.block_sparse_moe.experts.2.w1.weight', 'model.layers.28.block_sparse_moe.experts.2.w2.weight', 'model.layers.28.block_sparse_moe.experts.2.w3.weight', 'model.layers.28.block_sparse_moe.experts.3.w1.weight', 'model.layers.28.block_sparse_moe.experts.3.w2.weight', 'model.layers.28.block_sparse_moe.experts.3.w3.weight', 'model.layers.28.block_sparse_moe.experts.5.w1.weight', 'model.layers.28.block_sparse_moe.experts.5.w2.weight', 'model.layers.28.block_sparse_moe.experts.5.w3.weight', 'model.layers.28.block_sparse_moe.experts.6.w1.weight', 'model.layers.28.block_sparse_moe.experts.6.w2.weight', 'model.layers.28.block_sparse_moe.experts.6.w3.weight', 'model.layers.28.block_sparse_moe.experts.7.w1.weight', 'model.layers.28.block_sparse_moe.experts.7.w2.weight', 'model.layers.28.block_sparse_moe.experts.7.w3.weight', 'model.layers.28.block_sparse_moe.experts.8.w1.weight', 'model.layers.28.block_sparse_moe.experts.8.w2.weight', 'model.layers.28.block_sparse_moe.experts.8.w3.weight', 'model.layers.28.block_sparse_moe.experts.9.w1.weight', 'model.layers.28.block_sparse_moe.experts.9.w2.weight', 'model.layers.28.block_sparse_moe.experts.9.w3.weight', 'model.layers.29.block_sparse_moe.experts.10.w1.weight', 'model.layers.29.block_sparse_moe.experts.10.w2.weight', 'model.layers.29.block_sparse_moe.experts.10.w3.weight', 'model.layers.29.block_sparse_moe.experts.11.w1.weight', 'model.layers.29.block_sparse_moe.experts.11.w2.weight', 'model.layers.29.block_sparse_moe.experts.11.w3.weight', 'model.layers.29.block_sparse_moe.experts.12.w1.weight', 'model.layers.29.block_sparse_moe.experts.12.w2.weight', 'model.layers.29.block_sparse_moe.experts.12.w3.weight', 'model.layers.29.block_sparse_moe.experts.13.w1.weight', 'model.layers.29.block_sparse_moe.experts.13.w2.weight', 'model.layers.29.block_sparse_moe.experts.13.w3.weight', 'model.layers.29.block_sparse_moe.experts.14.w1.weight', 'model.layers.29.block_sparse_moe.experts.14.w2.weight', 'model.layers.29.block_sparse_moe.experts.14.w3.weight', 'model.layers.29.block_sparse_moe.experts.15.w1.weight', 'model.layers.29.block_sparse_moe.experts.15.w2.weight', 'model.layers.29.block_sparse_moe.experts.15.w3.weight', 'model.layers.29.block_sparse_moe.experts.2.w1.weight', 'model.layers.29.block_sparse_moe.experts.2.w2.weight', 'model.layers.29.block_sparse_moe.experts.2.w3.weight', 'model.layers.29.block_sparse_moe.experts.4.w1.weight', 'model.layers.29.block_sparse_moe.experts.4.w2.weight', 'model.layers.29.block_sparse_moe.experts.4.w3.weight', 'model.layers.29.block_sparse_moe.experts.5.w1.weight', 'model.layers.29.block_sparse_moe.experts.5.w2.weight', 'model.layers.29.block_sparse_moe.experts.5.w3.weight', 'model.layers.29.block_sparse_moe.experts.6.w1.weight', 'model.layers.29.block_sparse_moe.experts.6.w2.weight', 'model.layers.29.block_sparse_moe.experts.6.w3.weight', 'model.layers.29.block_sparse_moe.experts.7.w1.weight', 'model.layers.29.block_sparse_moe.experts.7.w2.weight', 'model.layers.29.block_sparse_moe.experts.7.w3.weight', 'model.layers.29.block_sparse_moe.experts.8.w1.weight', 'model.layers.29.block_sparse_moe.experts.8.w2.weight', 'model.layers.29.block_sparse_moe.experts.8.w3.weight', 'model.layers.29.block_sparse_moe.experts.9.w1.weight', 'model.layers.29.block_sparse_moe.experts.9.w2.weight', 'model.layers.29.block_sparse_moe.experts.9.w3.weight', 'model.layers.3.block_sparse_moe.experts.10.w1.weight', 'model.layers.3.block_sparse_moe.experts.10.w2.weight', 'model.layers.3.block_sparse_moe.experts.10.w3.weight', 'model.layers.3.block_sparse_moe.experts.11.w1.weight', 'model.layers.3.block_sparse_moe.experts.11.w2.weight', 'model.layers.3.block_sparse_moe.experts.11.w3.weight', 'model.layers.3.block_sparse_moe.experts.12.w1.weight', 'model.layers.3.block_sparse_moe.experts.12.w2.weight', 'model.layers.3.block_sparse_moe.experts.12.w3.weight', 'model.layers.3.block_sparse_moe.experts.13.w1.weight', 'model.layers.3.block_sparse_moe.experts.13.w2.weight', 'model.layers.3.block_sparse_moe.experts.13.w3.weight', 'model.layers.3.block_sparse_moe.experts.14.w1.weight', 'model.layers.3.block_sparse_moe.experts.14.w2.weight', 'model.layers.3.block_sparse_moe.experts.14.w3.weight', 'model.layers.3.block_sparse_moe.experts.15.w1.weight', 'model.layers.3.block_sparse_moe.experts.15.w2.weight', 'model.layers.3.block_sparse_moe.experts.15.w3.weight', 'model.layers.3.block_sparse_moe.experts.3.w1.weight', 'model.layers.3.block_sparse_moe.experts.3.w2.weight', 'model.layers.3.block_sparse_moe.experts.3.w3.weight', 'model.layers.3.block_sparse_moe.experts.4.w1.weight', 'model.layers.3.block_sparse_moe.experts.4.w2.weight', 'model.layers.3.block_sparse_moe.experts.4.w3.weight', 'model.layers.3.block_sparse_moe.experts.5.w1.weight', 'model.layers.3.block_sparse_moe.experts.5.w2.weight', 'model.layers.3.block_sparse_moe.experts.5.w3.weight', 'model.layers.3.block_sparse_moe.experts.6.w1.weight', 'model.layers.3.block_sparse_moe.experts.6.w2.weight', 'model.layers.3.block_sparse_moe.experts.6.w3.weight', 'model.layers.3.block_sparse_moe.experts.7.w1.weight', 'model.layers.3.block_sparse_moe.experts.7.w2.weight', 'model.layers.3.block_sparse_moe.experts.7.w3.weight', 'model.layers.3.block_sparse_moe.experts.8.w1.weight', 'model.layers.3.block_sparse_moe.experts.8.w2.weight', 'model.layers.3.block_sparse_moe.experts.8.w3.weight', 'model.layers.3.block_sparse_moe.experts.9.w1.weight', 'model.layers.3.block_sparse_moe.experts.9.w2.weight', 'model.layers.3.block_sparse_moe.experts.9.w3.weight', 'model.layers.30.block_sparse_moe.experts.10.w1.weight', 'model.layers.30.block_sparse_moe.experts.10.w2.weight', 'model.layers.30.block_sparse_moe.experts.10.w3.weight', 'model.layers.30.block_sparse_moe.experts.11.w1.weight', 'model.layers.30.block_sparse_moe.experts.11.w2.weight', 'model.layers.30.block_sparse_moe.experts.11.w3.weight', 'model.layers.30.block_sparse_moe.experts.12.w1.weight', 'model.layers.30.block_sparse_moe.experts.12.w2.weight', 'model.layers.30.block_sparse_moe.experts.12.w3.weight', 'model.layers.30.block_sparse_moe.experts.13.w1.weight', 'model.layers.30.block_sparse_moe.experts.13.w2.weight', 'model.layers.30.block_sparse_moe.experts.13.w3.weight', 'model.layers.30.block_sparse_moe.experts.14.w1.weight', 'model.layers.30.block_sparse_moe.experts.14.w2.weight', 'model.layers.30.block_sparse_moe.experts.14.w3.weight', 'model.layers.30.block_sparse_moe.experts.15.w1.weight', 'model.layers.30.block_sparse_moe.experts.15.w2.weight', 'model.layers.30.block_sparse_moe.experts.15.w3.weight', 'model.layers.30.block_sparse_moe.experts.2.w1.weight', 'model.layers.30.block_sparse_moe.experts.2.w2.weight', 'model.layers.30.block_sparse_moe.experts.2.w3.weight', 'model.layers.30.block_sparse_moe.experts.3.w1.weight', 'model.layers.30.block_sparse_moe.experts.3.w2.weight', 'model.layers.30.block_sparse_moe.experts.3.w3.weight', 'model.layers.30.block_sparse_moe.experts.4.w1.weight', 'model.layers.30.block_sparse_moe.experts.4.w2.weight', 'model.layers.30.block_sparse_moe.experts.4.w3.weight', 'model.layers.30.block_sparse_moe.experts.5.w1.weight', 'model.layers.30.block_sparse_moe.experts.5.w2.weight', 'model.layers.30.block_sparse_moe.experts.5.w3.weight', 'model.layers.30.block_sparse_moe.experts.6.w1.weight', 'model.layers.30.block_sparse_moe.experts.6.w2.weight', 'model.layers.30.block_sparse_moe.experts.6.w3.weight', 'model.layers.30.block_sparse_moe.experts.8.w1.weight', 'model.layers.30.block_sparse_moe.experts.8.w2.weight', 'model.layers.30.block_sparse_moe.experts.8.w3.weight', 'model.layers.30.block_sparse_moe.experts.9.w1.weight', 'model.layers.30.block_sparse_moe.experts.9.w2.weight', 'model.layers.30.block_sparse_moe.experts.9.w3.weight', 'model.layers.31.block_sparse_moe.experts.10.w1.weight', 'model.layers.31.block_sparse_moe.experts.10.w2.weight', 'model.layers.31.block_sparse_moe.experts.10.w3.weight', 'model.layers.31.block_sparse_moe.experts.11.w1.weight', 'model.layers.31.block_sparse_moe.experts.11.w2.weight', 'model.layers.31.block_sparse_moe.experts.11.w3.weight', 'model.layers.31.block_sparse_moe.experts.12.w1.weight', 'model.layers.31.block_sparse_moe.experts.12.w2.weight', 'model.layers.31.block_sparse_moe.experts.12.w3.weight', 'model.layers.31.block_sparse_moe.experts.13.w1.weight', 'model.layers.31.block_sparse_moe.experts.13.w2.weight', 'model.layers.31.block_sparse_moe.experts.13.w3.weight', 'model.layers.31.block_sparse_moe.experts.14.w1.weight', 'model.layers.31.block_sparse_moe.experts.14.w2.weight', 'model.layers.31.block_sparse_moe.experts.14.w3.weight', 'model.layers.31.block_sparse_moe.experts.15.w1.weight', 'model.layers.31.block_sparse_moe.experts.15.w2.weight', 'model.layers.31.block_sparse_moe.experts.15.w3.weight', 'model.layers.31.block_sparse_moe.experts.2.w1.weight', 'model.layers.31.block_sparse_moe.experts.2.w2.weight', 'model.layers.31.block_sparse_moe.experts.2.w3.weight', 'model.layers.31.block_sparse_moe.experts.3.w1.weight', 'model.layers.31.block_sparse_moe.experts.3.w2.weight', 'model.layers.31.block_sparse_moe.experts.3.w3.weight', 'model.layers.31.block_sparse_moe.experts.6.w1.weight', 'model.layers.31.block_sparse_moe.experts.6.w2.weight', 'model.layers.31.block_sparse_moe.experts.6.w3.weight', 'model.layers.31.block_sparse_moe.experts.7.w1.weight', 'model.layers.31.block_sparse_moe.experts.7.w2.weight', 'model.layers.31.block_sparse_moe.experts.7.w3.weight', 'model.layers.31.block_sparse_moe.experts.8.w1.weight', 'model.layers.31.block_sparse_moe.experts.8.w2.weight', 'model.layers.31.block_sparse_moe.experts.8.w3.weight', 'model.layers.31.block_sparse_moe.experts.9.w1.weight', 'model.layers.31.block_sparse_moe.experts.9.w2.weight', 'model.layers.31.block_sparse_moe.experts.9.w3.weight', 'model.layers.4.block_sparse_moe.experts.1.w1.weight', 'model.layers.4.block_sparse_moe.experts.1.w2.weight', 'model.layers.4.block_sparse_moe.experts.1.w3.weight', 'model.layers.4.block_sparse_moe.experts.10.w1.weight', 'model.layers.4.block_sparse_moe.experts.10.w2.weight', 'model.layers.4.block_sparse_moe.experts.10.w3.weight', 'model.layers.4.block_sparse_moe.experts.11.w1.weight', 'model.layers.4.block_sparse_moe.experts.11.w2.weight', 'model.layers.4.block_sparse_moe.experts.11.w3.weight', 'model.layers.4.block_sparse_moe.experts.12.w1.weight', 'model.layers.4.block_sparse_moe.experts.12.w2.weight', 'model.layers.4.block_sparse_moe.experts.12.w3.weight', 'model.layers.4.block_sparse_moe.experts.13.w1.weight', 'model.layers.4.block_sparse_moe.experts.13.w2.weight', 'model.layers.4.block_sparse_moe.experts.13.w3.weight', 'model.layers.4.block_sparse_moe.experts.14.w1.weight', 'model.layers.4.block_sparse_moe.experts.14.w2.weight', 'model.layers.4.block_sparse_moe.experts.14.w3.weight', 'model.layers.4.block_sparse_moe.experts.15.w1.weight', 'model.layers.4.block_sparse_moe.experts.15.w2.weight', 'model.layers.4.block_sparse_moe.experts.15.w3.weight', 'model.layers.4.block_sparse_moe.experts.2.w1.weight', 'model.layers.4.block_sparse_moe.experts.2.w2.weight', 'model.layers.4.block_sparse_moe.experts.2.w3.weight', 'model.layers.4.block_sparse_moe.experts.4.w1.weight', 'model.layers.4.block_sparse_moe.experts.4.w2.weight', 'model.layers.4.block_sparse_moe.experts.4.w3.weight', 'model.layers.4.block_sparse_moe.experts.6.w1.weight', 'model.layers.4.block_sparse_moe.experts.6.w2.weight', 'model.layers.4.block_sparse_moe.experts.6.w3.weight', 'model.layers.4.block_sparse_moe.experts.7.w1.weight', 'model.layers.4.block_sparse_moe.experts.7.w2.weight', 'model.layers.4.block_sparse_moe.experts.7.w3.weight', 'model.layers.4.block_sparse_moe.experts.8.w1.weight', 'model.layers.4.block_sparse_moe.experts.8.w2.weight', 'model.layers.4.block_sparse_moe.experts.8.w3.weight', 'model.layers.4.block_sparse_moe.experts.9.w1.weight', 'model.layers.4.block_sparse_moe.experts.9.w2.weight', 'model.layers.4.block_sparse_moe.experts.9.w3.weight', 'model.layers.5.block_sparse_moe.experts.1.w1.weight', 'model.layers.5.block_sparse_moe.experts.1.w2.weight', 'model.layers.5.block_sparse_moe.experts.1.w3.weight', 'model.layers.5.block_sparse_moe.experts.10.w1.weight', 'model.layers.5.block_sparse_moe.experts.10.w2.weight', 'model.layers.5.block_sparse_moe.experts.10.w3.weight', 'model.layers.5.block_sparse_moe.experts.11.w1.weight', 'model.layers.5.block_sparse_moe.experts.11.w2.weight', 'model.layers.5.block_sparse_moe.experts.11.w3.weight', 'model.layers.5.block_sparse_moe.experts.12.w1.weight', 'model.layers.5.block_sparse_moe.experts.12.w2.weight', 'model.layers.5.block_sparse_moe.experts.12.w3.weight', 'model.layers.5.block_sparse_moe.experts.13.w1.weight', 'model.layers.5.block_sparse_moe.experts.13.w2.weight', 'model.layers.5.block_sparse_moe.experts.13.w3.weight', 'model.layers.5.block_sparse_moe.experts.14.w1.weight', 'model.layers.5.block_sparse_moe.experts.14.w2.weight', 'model.layers.5.block_sparse_moe.experts.14.w3.weight', 'model.layers.5.block_sparse_moe.experts.15.w1.weight', 'model.layers.5.block_sparse_moe.experts.15.w2.weight', 'model.layers.5.block_sparse_moe.experts.15.w3.weight', 'model.layers.5.block_sparse_moe.experts.2.w1.weight', 'model.layers.5.block_sparse_moe.experts.2.w2.weight', 'model.layers.5.block_sparse_moe.experts.2.w3.weight', 'model.layers.5.block_sparse_moe.experts.5.w1.weight', 'model.layers.5.block_sparse_moe.experts.5.w2.weight', 'model.layers.5.block_sparse_moe.experts.5.w3.weight', 'model.layers.5.block_sparse_moe.experts.6.w1.weight', 'model.layers.5.block_sparse_moe.experts.6.w2.weight', 'model.layers.5.block_sparse_moe.experts.6.w3.weight', 'model.layers.5.block_sparse_moe.experts.7.w1.weight', 'model.layers.5.block_sparse_moe.experts.7.w2.weight', 'model.layers.5.block_sparse_moe.experts.7.w3.weight', 'model.layers.5.block_sparse_moe.experts.9.w1.weight', 'model.layers.5.block_sparse_moe.experts.9.w2.weight', 'model.layers.5.block_sparse_moe.experts.9.w3.weight', 'model.layers.6.block_sparse_moe.experts.1.w1.weight', 'model.layers.6.block_sparse_moe.experts.1.w2.weight', 'model.layers.6.block_sparse_moe.experts.1.w3.weight', 'model.layers.6.block_sparse_moe.experts.11.w1.weight', 'model.layers.6.block_sparse_moe.experts.11.w2.weight', 'model.layers.6.block_sparse_moe.experts.11.w3.weight', 'model.layers.6.block_sparse_moe.experts.12.w1.weight', 'model.layers.6.block_sparse_moe.experts.12.w2.weight', 'model.layers.6.block_sparse_moe.experts.12.w3.weight', 'model.layers.6.block_sparse_moe.experts.13.w1.weight', 'model.layers.6.block_sparse_moe.experts.13.w2.weight', 'model.layers.6.block_sparse_moe.experts.13.w3.weight', 'model.layers.6.block_sparse_moe.experts.14.w1.weight', 'model.layers.6.block_sparse_moe.experts.14.w2.weight', 'model.layers.6.block_sparse_moe.experts.14.w3.weight', 'model.layers.6.block_sparse_moe.experts.15.w1.weight', 'model.layers.6.block_sparse_moe.experts.15.w2.weight', 'model.layers.6.block_sparse_moe.experts.15.w3.weight', 'model.layers.6.block_sparse_moe.experts.2.w1.weight', 'model.layers.6.block_sparse_moe.experts.2.w2.weight', 'model.layers.6.block_sparse_moe.experts.2.w3.weight', 'model.layers.6.block_sparse_moe.experts.4.w1.weight', 'model.layers.6.block_sparse_moe.experts.4.w2.weight', 'model.layers.6.block_sparse_moe.experts.4.w3.weight', 'model.layers.6.block_sparse_moe.experts.6.w1.weight', 'model.layers.6.block_sparse_moe.experts.6.w2.weight', 'model.layers.6.block_sparse_moe.experts.6.w3.weight', 'model.layers.6.block_sparse_moe.experts.7.w1.weight', 'model.layers.6.block_sparse_moe.experts.7.w2.weight', 'model.layers.6.block_sparse_moe.experts.7.w3.weight', 'model.layers.6.block_sparse_moe.experts.8.w1.weight', 'model.layers.6.block_sparse_moe.experts.8.w2.weight', 'model.layers.6.block_sparse_moe.experts.8.w3.weight', 'model.layers.6.block_sparse_moe.experts.9.w1.weight', 'model.layers.6.block_sparse_moe.experts.9.w2.weight', 'model.layers.6.block_sparse_moe.experts.9.w3.weight', 'model.layers.7.block_sparse_moe.experts.10.w1.weight', 'model.layers.7.block_sparse_moe.experts.10.w2.weight', 'model.layers.7.block_sparse_moe.experts.10.w3.weight', 'model.layers.7.block_sparse_moe.experts.11.w1.weight', 'model.layers.7.block_sparse_moe.experts.11.w2.weight', 'model.layers.7.block_sparse_moe.experts.11.w3.weight', 'model.layers.7.block_sparse_moe.experts.12.w1.weight', 'model.layers.7.block_sparse_moe.experts.12.w2.weight', 'model.layers.7.block_sparse_moe.experts.12.w3.weight', 'model.layers.7.block_sparse_moe.experts.13.w1.weight', 'model.layers.7.block_sparse_moe.experts.13.w2.weight', 'model.layers.7.block_sparse_moe.experts.13.w3.weight', 'model.layers.7.block_sparse_moe.experts.14.w1.weight', 'model.layers.7.block_sparse_moe.experts.14.w2.weight', 'model.layers.7.block_sparse_moe.experts.14.w3.weight', 'model.layers.7.block_sparse_moe.experts.15.w1.weight', 'model.layers.7.block_sparse_moe.experts.15.w2.weight', 'model.layers.7.block_sparse_moe.experts.15.w3.weight', 'model.layers.7.block_sparse_moe.experts.2.w1.weight', 'model.layers.7.block_sparse_moe.experts.2.w2.weight', 'model.layers.7.block_sparse_moe.experts.2.w3.weight', 'model.layers.7.block_sparse_moe.experts.3.w1.weight', 'model.layers.7.block_sparse_moe.experts.3.w2.weight', 'model.layers.7.block_sparse_moe.experts.3.w3.weight', 'model.layers.7.block_sparse_moe.experts.5.w1.weight', 'model.layers.7.block_sparse_moe.experts.5.w2.weight', 'model.layers.7.block_sparse_moe.experts.5.w3.weight', 'model.layers.7.block_sparse_moe.experts.6.w1.weight', 'model.layers.7.block_sparse_moe.experts.6.w2.weight', 'model.layers.7.block_sparse_moe.experts.6.w3.weight', 'model.layers.7.block_sparse_moe.experts.7.w1.weight', 'model.layers.7.block_sparse_moe.experts.7.w2.weight', 'model.layers.7.block_sparse_moe.experts.7.w3.weight', 'model.layers.7.block_sparse_moe.experts.8.w1.weight', 'model.layers.7.block_sparse_moe.experts.8.w2.weight', 'model.layers.7.block_sparse_moe.experts.8.w3.weight', 'model.layers.7.block_sparse_moe.experts.9.w1.weight', 'model.layers.7.block_sparse_moe.experts.9.w2.weight', 'model.layers.7.block_sparse_moe.experts.9.w3.weight', 'model.layers.8.block_sparse_moe.experts.10.w1.weight', 'model.layers.8.block_sparse_moe.experts.10.w2.weight', 'model.layers.8.block_sparse_moe.experts.10.w3.weight', 'model.layers.8.block_sparse_moe.experts.11.w1.weight', 'model.layers.8.block_sparse_moe.experts.11.w2.weight', 'model.layers.8.block_sparse_moe.experts.11.w3.weight', 'model.layers.8.block_sparse_moe.experts.12.w1.weight', 'model.layers.8.block_sparse_moe.experts.12.w2.weight', 'model.layers.8.block_sparse_moe.experts.12.w3.weight', 'model.layers.8.block_sparse_moe.experts.13.w1.weight', 'model.layers.8.block_sparse_moe.experts.13.w2.weight', 'model.layers.8.block_sparse_moe.experts.13.w3.weight', 'model.layers.8.block_sparse_moe.experts.14.w1.weight', 'model.layers.8.block_sparse_moe.experts.14.w2.weight', 'model.layers.8.block_sparse_moe.experts.14.w3.weight', 'model.layers.8.block_sparse_moe.experts.15.w1.weight', 'model.layers.8.block_sparse_moe.experts.15.w2.weight', 'model.layers.8.block_sparse_moe.experts.15.w3.weight', 'model.layers.8.block_sparse_moe.experts.3.w1.weight', 'model.layers.8.block_sparse_moe.experts.3.w2.weight', 'model.layers.8.block_sparse_moe.experts.3.w3.weight', 'model.layers.8.block_sparse_moe.experts.4.w1.weight', 'model.layers.8.block_sparse_moe.experts.4.w2.weight', 'model.layers.8.block_sparse_moe.experts.4.w3.weight', 'model.layers.8.block_sparse_moe.experts.5.w1.weight', 'model.layers.8.block_sparse_moe.experts.5.w2.weight', 'model.layers.8.block_sparse_moe.experts.5.w3.weight', 'model.layers.8.block_sparse_moe.experts.6.w1.weight', 'model.layers.8.block_sparse_moe.experts.6.w2.weight', 'model.layers.8.block_sparse_moe.experts.6.w3.weight', 'model.layers.8.block_sparse_moe.experts.7.w1.weight', 'model.layers.8.block_sparse_moe.experts.7.w2.weight', 'model.layers.8.block_sparse_moe.experts.7.w3.weight', 'model.layers.8.block_sparse_moe.experts.8.w1.weight', 'model.layers.8.block_sparse_moe.experts.8.w2.weight', 'model.layers.8.block_sparse_moe.experts.8.w3.weight', 'model.layers.8.block_sparse_moe.experts.9.w1.weight', 'model.layers.8.block_sparse_moe.experts.9.w2.weight', 'model.layers.8.block_sparse_moe.experts.9.w3.weight', 'model.layers.9.block_sparse_moe.experts.10.w1.weight', 'model.layers.9.block_sparse_moe.experts.10.w2.weight', 'model.layers.9.block_sparse_moe.experts.10.w3.weight', 'model.layers.9.block_sparse_moe.experts.11.w1.weight', 'model.layers.9.block_sparse_moe.experts.11.w2.weight', 'model.layers.9.block_sparse_moe.experts.11.w3.weight', 'model.layers.9.block_sparse_moe.experts.12.w1.weight', 'model.layers.9.block_sparse_moe.experts.12.w2.weight', 'model.layers.9.block_sparse_moe.experts.12.w3.weight', 'model.layers.9.block_sparse_moe.experts.13.w1.weight', 'model.layers.9.block_sparse_moe.experts.13.w2.weight', 'model.layers.9.block_sparse_moe.experts.13.w3.weight', 'model.layers.9.block_sparse_moe.experts.14.w1.weight', 'model.layers.9.block_sparse_moe.experts.14.w2.weight', 'model.layers.9.block_sparse_moe.experts.14.w3.weight', 'model.layers.9.block_sparse_moe.experts.15.w1.weight', 'model.layers.9.block_sparse_moe.experts.15.w2.weight', 'model.layers.9.block_sparse_moe.experts.15.w3.weight', 'model.layers.9.block_sparse_moe.experts.2.w1.weight', 'model.layers.9.block_sparse_moe.experts.2.w2.weight', 'model.layers.9.block_sparse_moe.experts.2.w3.weight', 'model.layers.9.block_sparse_moe.experts.3.w1.weight', 'model.layers.9.block_sparse_moe.experts.3.w2.weight', 'model.layers.9.block_sparse_moe.experts.3.w3.weight', 'model.layers.9.block_sparse_moe.experts.5.w1.weight', 'model.layers.9.block_sparse_moe.experts.5.w2.weight', 'model.layers.9.block_sparse_moe.experts.5.w3.weight', 'model.layers.9.block_sparse_moe.experts.6.w1.weight', 'model.layers.9.block_sparse_moe.experts.6.w2.weight', 'model.layers.9.block_sparse_moe.experts.6.w3.weight', 'model.layers.9.block_sparse_moe.experts.8.w1.weight', 'model.layers.9.block_sparse_moe.experts.8.w2.weight', 'model.layers.9.block_sparse_moe.experts.8.w3.weight', 'model.layers.9.block_sparse_moe.experts.9.w1.weight', 'model.layers.9.block_sparse_moe.experts.9.w2.weight', 'model.layers.9.block_sparse_moe.experts.9.w3.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-01-08:16:20:13,246 WARNING  [huggingface.py:98] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
2025-01-08:16:20:13,246 INFO     [huggingface.py:496] Model type cannot be determined. Using default model type 'causal'
2025-01-08:16:20:13,260 WARNING  [huggingface.py:279] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
0
tensor(1)
tensor(0)
tensor(3)
done!
1
tensor(0)
tensor(2)
tensor(1)
done!
2
tensor(4)
tensor(0)
tensor(3)
done!
3
tensor(1)
tensor(2)
tensor(0)
tensor(4)
done!
4
tensor(1)
tensor(2)
tensor(0)
done!
5
tensor(1)
tensor(0)
tensor(3)
done!
6
tensor(2)
tensor(1)
tensor(11)
tensor(0)
done!
7
tensor(4)
tensor(0)
tensor(2)
done!
8
tensor(0)
tensor(2)
tensor(1)
done!
9
tensor(1)
tensor(0)
tensor(7)
tensor(2)
done!
10
tensor(0)
tensor(1)
tensor(2)
done!
11
tensor(0)
tensor(4)
tensor(3)
tensor(1)
done!
12
tensor(2)
tensor(1)
tensor(3)
tensor(0)
done!
13
tensor(6)
tensor(0)
tensor(1)
tensor(15)
done!
14
tensor(0)
tensor(8)
tensor(2)
tensor(3)
done!
15
tensor(0)
tensor(2)
tensor(3)
done!
16
tensor(4)
tensor(1)
tensor(0)
done!
17
tensor(1)
tensor(0)
tensor(2)
done!
18
tensor(4)
tensor(1)
tensor(0)
tensor(9)
done!
19
tensor(5)
tensor(1)
tensor(0)
tensor(2)
done!
20
tensor(1)
tensor(6)
tensor(5)
tensor(0)
done!
21
tensor(0)
tensor(2)
tensor(4)
done!
22
tensor(2)
tensor(1)
tensor(0)
tensor(3)
done!
23
tensor(0)
tensor(1)
tensor(5)
tensor(2)
done!
24
tensor(2)
tensor(0)
tensor(3)
tensor(13)
done!
25
tensor(3)
tensor(0)
tensor(10)
tensor(9)
done!
26
tensor(2)
tensor(0)
tensor(6)
done!
27
tensor(0)
tensor(3)
tensor(2)
done!
28
tensor(1)
tensor(0)
tensor(3)
done!
29
tensor(1)
tensor(0)
tensor(4)
tensor(11)
done!
30
tensor(0)
tensor(1)
tensor(5)
tensor(3)
done!
31
tensor(3)
tensor(0)
tensor(4)
tensor(6)
done!
all done!
all_gate number:32

PhiMoEForCausalLM(
  (model): PhiMoEModel(
    (embed_tokens): Embedding(32064, 4096)
    (layers): ModuleList(
      (0-31): 32 x PhiMoEDecoderLayer(
        (self_attn): PhiMoESdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
        )
        (block_sparse_moe): PhiMoESparseMoeBlock(
          (gate): Linear(in_features=4096, out_features=16, bias=False)
          (experts): ModuleList(
            (0-15): 16 x PhiMoEBlockSparseTop2MLP(
              (w1): Linear(in_features=4096, out_features=6400, bias=False)
              (w2): Linear(in_features=6400, out_features=4096, bias=False)
              (w3): Linear(in_features=4096, out_features=6400, bias=False)
              (act_fn): SiLU()
            )
          )
        )
        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=4096, out_features=32064, bias=True)
)
model
PhiMoEModel(
  (embed_tokens): Embedding(32064, 4096)
  (layers): ModuleList(
    (0-31): 32 x PhiMoEDecoderLayer(
      (self_attn): PhiMoESdpaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
        (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
        (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
        (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
      )
      (block_sparse_moe): PhiMoESparseMoeBlock(
        (gate): Linear(in_features=4096, out_features=16, bias=False)
        (experts): ModuleList(
          (0-15): 16 x PhiMoEBlockSparseTop2MLP(
            (w1): Linear(in_features=4096, out_features=6400, bias=False)
            (w2): Linear(in_features=6400, out_features=4096, bias=False)
            (w3): Linear(in_features=4096, out_features=6400, bias=False)
            (act_fn): SiLU()
          )
        )
      )
      (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.embed_tokens
Embedding(32064, 4096)
model.layers
ModuleList(
  (0-31): 32 x PhiMoEDecoderLayer(
    (self_attn): PhiMoESdpaAttention(
      (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
      (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
      (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
      (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
      (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
    )
    (block_sparse_moe): PhiMoESparseMoeBlock(
      (gate): Linear(in_features=4096, out_features=16, bias=False)
      (experts): ModuleList(
        (0-15): 16 x PhiMoEBlockSparseTop2MLP(
          (w1): Linear(in_features=4096, out_features=6400, bias=False)
          (w2): Linear(in_features=6400, out_features=4096, bias=False)
          (w3): Linear(in_features=4096, out_features=6400, bias=False)
          (act_fn): SiLU()
        )
      )
    )
    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  )
)
model.layers.0
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.0.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.0.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.0.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.0.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.0.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.0.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.0.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.0.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.0.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.0.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.0.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.0.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.1
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.1.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.1.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.1.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.1.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.1.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.1.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.1.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.1.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.1.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.1.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.1.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.1.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.2
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.2.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.2.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.2.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.2.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.2.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.2.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.2.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.2.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.2.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.2.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.2.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.2.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.3
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.3.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.3.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.3.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.3.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.3.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.3.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.3.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.3.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.3.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.3.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.3.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.3.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.4
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.4.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.4.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.4.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.4.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.4.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.4.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.4.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.4.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.4.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.4.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.4.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.4.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.5
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.5.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.5.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.5.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.5.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.5.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.5.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.5.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.5.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.5.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.5.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.5.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.5.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.6
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.6.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.6.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.6.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.6.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.6.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.6.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.6.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.6.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.6.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.6.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.6.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.6.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.7
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.7.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.7.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.7.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.7.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.7.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.7.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.7.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.7.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.7.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.7.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.7.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.7.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.8
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.8.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.8.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.8.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.8.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.8.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.8.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.8.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.8.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.8.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.8.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.8.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.8.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.9
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.9.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.9.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.9.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.9.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.9.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.9.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.9.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.9.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.9.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.9.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.9.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.9.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.10
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.10.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.10.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.10.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.10.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.10.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.10.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.10.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.10.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.10.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.10.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.10.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.10.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.11
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.11.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.11.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.11.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.11.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.11.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.11.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.11.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.11.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.11.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.11.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.11.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.11.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.12
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.12.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.12.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.12.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.12.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.12.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.12.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.12.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.12.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.12.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.12.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.12.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.12.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.13
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.13.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.13.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.13.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.13.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.13.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.13.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.13.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.13.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.13.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.13.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.13.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.13.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.14
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.14.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.14.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.14.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.14.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.14.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.14.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.14.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.14.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.14.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.14.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.14.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.14.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.15
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.15.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.15.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.15.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.15.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.15.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.15.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.15.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.15.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.15.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.15.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.15.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.15.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.16
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.16.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.16.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.16.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.16.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.16.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.16.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.16.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.16.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.16.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.16.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.16.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.16.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.17
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.17.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.17.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.17.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.17.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.17.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.17.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.17.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.17.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.17.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.17.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.17.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.17.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.18
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.18.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.18.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.18.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.18.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.18.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.18.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.18.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.18.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.18.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.18.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.18.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.18.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.19
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.19.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.19.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.19.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.19.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.19.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.19.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.19.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.19.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.19.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.19.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.19.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.19.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.20
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.20.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.20.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.20.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.20.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.20.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.20.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.20.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.20.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.20.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.20.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.20.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.20.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.21
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.21.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.21.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.21.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.21.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.21.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.21.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.21.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.21.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.21.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.21.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.21.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.21.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.22
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.22.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.22.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.22.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.22.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.22.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.22.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.22.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.22.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.22.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.22.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.22.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.22.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.23
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.23.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.23.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.23.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.23.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.23.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.23.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.23.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.23.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.23.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.23.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.23.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.23.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.24
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.24.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.24.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.24.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.24.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.24.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.24.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.24.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.24.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.24.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.24.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.24.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.24.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.25
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.25.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.25.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.25.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.25.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.25.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.25.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.25.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.25.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.25.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.25.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.25.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.25.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.26
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.26.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.26.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.26.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.26.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.26.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.26.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.26.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.26.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.26.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.26.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.26.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.26.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.27
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.27.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.27.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.27.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.27.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.27.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.27.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.27.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.27.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.27.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.27.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.27.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.27.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.28
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.28.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.28.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.28.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.28.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.28.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.28.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.28.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.28.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.28.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.28.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.28.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.28.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.29
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.29.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.29.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.29.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.29.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.29.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.29.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.29.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.29.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.29.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.29.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.29.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.29.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.30
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.30.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.30.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.30.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.30.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.30.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.30.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.30.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.30.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.30.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.30.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.5
2025-01-08:16:20:27,787 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-01-08:16:20:27,787 INFO     [evaluator.py:217] Using pre-initialized model
Using the latest cached version of the module from /root/autodl-tmp/huggingface/modules/datasets_modules/datasets/hails--mmlu_no_train/6e75f77a579ee4ce0a66d822aa8a2660c81c6c5b49d1a3489f5a787e37886aa9 (last modified on Tue Jan  7 01:08:17 2025) since it couldn't be found locally at hails/mmlu_no_train, or remotely on the Hugging Face Hub.
2025-01-08:16:22:07,830 WARNING  [load.py:1569] Using the latest cached version of the module from /root/autodl-tmp/huggingface/modules/datasets_modules/datasets/hails--mmlu_no_train/6e75f77a579ee4ce0a66d822aa8a2660c81c6c5b49d1a3489f5a787e37886aa9 (last modified on Tue Jan  7 01:08:17 2025) since it couldn't be found locally at hails/mmlu_no_train, or remotely on the Hugging Face Hub.
2025-01-08:16:22:07,897 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_formal_logic from None to 5
2025-01-08:16:22:07,897 INFO     [task.py:415] Building contexts for mmlu_formal_logic on rank 0...
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.30.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.30.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.31
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.31.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.31.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.31.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.31.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.31.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.31.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.31.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.31.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.31.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.31.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.31.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.31.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.norm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
lm_head
Linear(in_features=4096, out_features=32064, bias=True)
  0%|                                                                                           | 0/50 [00:00<?, ?it/s] 20%|████████████████▍                                                                 | 10/50 [00:00<00:00, 97.68it/s] 40%|████████████████████████████████▊                                                 | 20/50 [00:00<00:00, 97.15it/s] 60%|█████████████████████████████████████████████████▏                                | 30/50 [00:00<00:00, 97.56it/s] 80%|█████████████████████████████████████████████████████████████████▌                | 40/50 [00:00<00:00, 97.58it/s]100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 97.74it/s]100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 97.58it/s]
2025-01-08:16:22:08,413 INFO     [evaluator.py:496] Running loglikelihood requests
Running loglikelihood requests:   0%|                                                          | 0/200 [00:00<?, ?it/s]Layer: gate_0 - Captured router_logits: [0.13254255056381226, 0.1486671417951584, 0.1392432451248169, -0.247202530503273, -0.23955142498016357, -0.1248696967959404, 0.15123286843299866, -0.20318827033042908, 0.10521744191646576, 0.12474643439054489, 0.12517130374908447, 0.09170890599489212, 0.11192772537469864, 0.13452504575252533, -1.1366933584213257, 0.14799034595489502]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09402970969676971, 0.057678110897541046, 0.04530045762658119, 0.06943544000387192, 0.09461663663387299, 0.04340218007564545, 0.05878884345293045, 0.07865336537361145, 0.014691376127302647, 0.06453090161085129, -0.1758018136024475, 0.06010831519961357, -0.0036127192433923483, -0.0017292851116508245, 0.016264721751213074, 0.026514394208788872]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0768049955368042, 0.05254239961504936, 0.09832517802715302, 0.05062532424926758, 0.10109253972768784, 0.11029472202062607, 0.05855415016412735, -0.1388007551431656, 0.0664106160402298, 0.0850297212600708, -0.00021645371452905238, 0.0794660672545433, -0.211162269115448, -0.01331338007003069, -0.06110432371497154, 0.12157511711120605]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.2162247598171234, 0.16373050212860107, 0.13635064661502838, 0.14971444010734558, 0.0528230145573616, 0.0605490505695343, -0.057159096002578735, 0.17411479353904724, 0.15442155301570892, -0.4550063908100128, 0.024761265143752098, 0.16936388611793518, 0.0848955288529396, -0.3017210364341736, -0.08430881798267365, -0.03877631947398186]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07866348326206207, -0.0321868397295475, -0.07291647791862488, 0.12972474098205566, -0.0650777518749237, -0.15263217687606812, 0.10091660171747208, 0.05965336412191391, 0.0611131489276886, 0.2069748342037201, -0.29267123341560364, 0.02631254307925701, 0.003839631797745824, -0.17367739975452423, 0.08084612339735031, 0.021390782669186592]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.22714561223983765, 0.25315916538238525, 0.1639922857284546, 0.06820061802864075, -0.3260383903980255, -0.03844073787331581, 0.08508431911468506, 0.001987712224945426, 0.178339421749115, -0.048679426312446594, -0.5700045228004456, 0.07426043599843979, -0.3173811137676239, 0.28098756074905396, 0.16409039497375488, 0.17494839429855347]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.31858396530151367, 0.34970179200172424, 0.21938015520572662, 0.2641330063343048, 0.29965582489967346, 0.24501363933086395, 0.12712135910987854, 0.16284926235675812, -0.41961348056793213, 0.39705801010131836, -0.0979447215795517, 0.29737716913223267, 0.11786368489265442, -0.0824797973036766, 0.2998336851596832, 0.19124409556388855]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05327722430229187, 0.20700323581695557, 0.11714659631252289, 0.15949265658855438, -0.3645632266998291, 0.19207994639873505, 0.0648491382598877, 0.214478000998497, -0.032605160027742386, -0.1688799262046814, 0.055366817861795425, -0.056482598185539246, 0.1573326587677002, 0.11556975543498993, -0.04278017207980156, 0.12013882398605347]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.22052185237407684, 0.2928244471549988, -0.10057451575994492, 0.3374306559562683, 0.4575546681880951, 0.40425345301628113, 0.12241479009389877, -0.04415226727724075, 0.6413930654525757, -0.1606101244688034, 0.2597977817058563, 0.05163176357746124, 0.6888338923454285, -0.2296682745218277, -0.029266472905874252, 0.20368263125419617]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.866723895072937, 0.7515944242477417, 0.6303815841674805, 0.595391571521759, 0.8291144967079163, 0.42571526765823364, 1.1100081205368042, 1.0965431928634644, 0.08200681209564209, 0.008300784043967724, 0.4109747111797333, 0.7971333861351013, 0.7115206718444824, 0.8118684887886047, 1.1350996494293213, 0.7863190174102783]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1233779191970825, 1.1573160886764526, 0.6367106437683105, 1.013458251953125, 0.9483163356781006, 0.39236944913864136, 0.9791765809059143, 1.424589991569519, 0.7575889825820923, 0.28056418895721436, 1.0858731269836426, 1.0969399213790894, 0.5640450716018677, 1.0994670391082764, 0.4993882477283478, 0.9916524291038513]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2435804605484009, 1.614417314529419, 1.5324888229370117, 1.4354678392410278, 1.3953157663345337, 0.7799959778785706, 1.7601783275604248, 1.2265946865081787, 1.4832969903945923, 1.4713200330734253, 1.2297017574310303, 1.169734001159668, 1.129238247871399, 1.3218716382980347, 1.1935665607452393, 1.1869491338729858]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0195058584213257, 1.5020233392715454, 1.6525530815124512, 1.7958604097366333, 1.2422330379486084, 1.5850602388381958, 1.3154454231262207, 1.550549864768982, 1.3464645147323608, 0.5639911890029907, 1.7038841247558594, 1.9612197875976562, 1.7582063674926758, 1.6632345914840698, 1.4135481119155884, 1.5241044759750366]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0481061935424805, 3.8692972660064697, 2.996495008468628, 2.609377384185791, 3.4612808227539062, 2.035102367401123, 3.665372610092163, 3.199561595916748, 3.829193353652954, 3.3626856803894043, 3.1940231323242188, 3.8997116088867188, 2.1053168773651123, 3.949759006500244, 3.166964054107666, 2.7809269428253174]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3857955932617188, 2.3702902793884277, 2.848280906677246, 2.4294180870056152, 2.8097798824310303, 2.0520026683807373, 1.550239086151123, 2.1260993480682373, 2.6438796520233154, 1.9510325193405151, 1.3473271131515503, 1.9353879690170288, 2.402304172515869, 2.667048692703247, 2.633500099182129, 1.9964219331741333]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5683393478393555, 3.402019500732422, 3.0594615936279297, 3.1106181144714355, 3.0508978366851807, 2.8309051990509033, 2.807908535003662, 2.0710012912750244, 3.2778737545013428, 2.9416298866271973, 2.750056505203247, 1.3668391704559326, 2.932671546936035, 2.755998134613037, 3.038954973220825, 2.0545639991760254]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9109535217285156, 1.8457709550857544, 2.6321537494659424, 2.5857856273651123, 2.613661050796509, 1.9762916564941406, 1.7957923412322998, 3.1001367568969727, 0.5697833299636841, 2.1664204597473145, 0.9991196990013123, 2.4501969814300537, 2.7506330013275146, 2.924605369567871, 2.956380844116211, 1.4308232069015503]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6922175884246826, 3.9665651321411133, 5.577171325683594, 4.95991849899292, 5.508030891418457, 5.694666385650635, 5.134487152099609, 5.125734329223633, 6.1593146324157715, 5.064245223999023, 5.779329776763916, 6.091649532318115, 4.859915733337402, 5.418302536010742, 4.322770595550537, 4.974287033081055]
Running loglikelihood requests:   0%|▎                                                 | 1/200 [00:10<35:02, 10.57s/it]Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.202898979187012, 4.559000492095947, 5.574416160583496, 5.160213470458984, 3.698601007461548, 4.4569196701049805, 4.892738342285156, 5.313803672790527, 5.357581615447998, 4.922553539276123, 4.27034330368042, 3.3507604598999023, 4.411663055419922, 3.3832173347473145, 4.9242753982543945, 4.502079486846924]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.877331733703613, 5.840376377105713, 6.020401954650879, 5.625530242919922, 6.060311317443848, 4.816841125488281, 5.682024002075195, 5.643614768981934, 5.312038898468018, 5.353951454162598, 6.195103645324707, 5.887454986572266, 4.921103477478027, 5.848262786865234, 6.040686130523682, 5.853712558746338]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.9805643558502197, 2.970918655395508, 2.806734800338745, 2.2615060806274414, -0.40100908279418945, 3.0336227416992188, 2.7211601734161377, 1.8635475635528564, 2.7269654273986816, 3.462092638015747, 2.684459686279297, 3.2285594940185547, 2.8608953952789307, 2.949371337890625, 2.350811719894409, 1.7468396425247192]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.5209736824035645, 6.787817001342773, 6.511117458343506, 6.281683444976807, 6.439307689666748, 6.457635402679443, 6.720682621002197, 6.595990180969238, 6.615363121032715, 6.3028669357299805, 6.896373271942139, 6.237955570220947, 5.263485908508301, 7.1679911613464355, 6.46043586730957, 6.411074638366699]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [4.995814800262451, 5.330416679382324, 4.942221641540527, 4.03344202041626, 4.893352508544922, 5.230085372924805, 4.621087074279785, 4.764090538024902, 5.053583145141602, 4.83060359954834, 5.2405829429626465, 4.865983009338379, 5.123637676239014, 5.4235734939575195, 4.582207679748535, 5.469923973083496]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.108579635620117, 4.640695095062256, 3.320773124694824, 4.178071975708008, 4.418075084686279, 4.053586959838867, 4.222897052764893, 4.217972755432129, 4.338570594787598, 4.4411749839782715, 4.035141944885254, 4.4007768630981445, 3.6583473682403564, 2.0130631923675537, 3.363048553466797, 4.002002239227295]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.25640869140625, 6.221091270446777, 5.886478424072266, 6.066015720367432, 5.408997535705566, 5.835064888000488, 6.159425258636475, 5.690140247344971, 5.3358259201049805, 5.7453131675720215, 6.425929069519043, 5.508316993713379, 5.744402885437012, 6.1890997886657715, 6.209881782531738, 5.773303031921387]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.824054002761841, 3.660810708999634, 3.778761148452759, 3.9317896366119385, 2.680389642715454, 3.8420498371124268, 3.929152488708496, 3.6374213695526123, 3.9757513999938965, 3.600052833557129, 3.679849147796631, 3.606449604034424, 3.6484830379486084, 3.5920584201812744, 3.685487985610962, 3.883556842803955]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.443683624267578, 3.510702133178711, 3.403369665145874, 3.548513412475586, 3.377852439880371, 3.6031763553619385, 3.5601260662078857, 3.3595821857452393, 3.5866944789886475, 3.2810184955596924, 3.586294174194336, 3.4397876262664795, 3.4306442737579346, 2.7226407527923584, 3.6344454288482666, 3.5464065074920654]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.167306900024414, 3.194326877593994, 2.9788453578948975, 2.8874592781066895, 2.8143811225891113, 3.074625015258789, 3.169567108154297, 3.2696616649627686, 3.0273237228393555, 3.000417470932007, 2.2800967693328857, 2.9901487827301025, 3.141014814376831, 3.044933319091797, 3.084165096282959, 2.8763248920440674]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.369492053985596, 4.485512733459473, 4.570766448974609, 4.378593921661377, 4.485946178436279, 4.464509010314941, 4.214879989624023, 4.70890998840332, 4.491588115692139, 4.46473503112793, 4.4719767570495605, 4.055366516113281, 4.19605016708374, 4.059017658233643, 4.583070755004883, 4.373637676239014]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.585575103759766, 8.89503002166748, 8.794232368469238, 8.475288391113281, 8.551257133483887, 8.257527351379395, 8.62802505493164, 8.542418479919434, 8.629385948181152, 8.497335433959961, 8.60108757019043, 8.229212760925293, 8.31915283203125, 9.003849029541016, 8.893218040466309, 8.509368896484375]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.88125467300415, 5.420595169067383, 5.382789134979248, 4.78309440612793, 5.129881381988525, 5.087730884552002, 4.661452770233154, 5.1211700439453125, 5.229809284210205, 4.91831111907959, 4.832267761230469, 4.717676639556885, 4.856942653656006, 4.9813337326049805, 5.060062885284424, 4.823737621307373]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.239739179611206, 3.004286050796509, 3.0988264083862305, 2.7617709636688232, 3.0218069553375244, 3.3164479732513428, 2.756340980529785, 3.185192823410034, 3.091230869293213, 3.1910626888275146, 3.175349473953247, 3.254847288131714, 3.0343241691589355, 2.9854090213775635, 3.1179606914520264, 3.186497688293457]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.1209174171090126, 0.1339551955461502, 0.12755177915096283, -0.23724745213985443, -0.22882622480392456, -0.09481137990951538, 0.1427534520626068, -0.19160796701908112, 0.09607744216918945, 0.11582008004188538, 0.11300322413444519, 0.07775858789682388, 0.10570511221885681, 0.12496472150087357, -1.082400918006897, 0.13655628263950348]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08835954964160919, 0.0509505420923233, 0.03760150074958801, 0.06692531704902649, 0.08109994977712631, 0.044209301471710205, 0.047239333391189575, 0.0746227353811264, 0.014971431344747543, 0.0601026825606823, -0.16309157013893127, 0.060262955725193024, -0.0015354650095105171, 0.004248849581927061, 0.016665756702423096, 0.022991344332695007]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06727980077266693, 0.050271522253751755, 0.09287595748901367, 0.043470412492752075, 0.08720549196004868, 0.10670667886734009, 0.05539305880665779, -0.13123248517513275, 0.06372088938951492, 0.08312540501356125, -0.0027630669064819813, 0.07896454632282257, -0.19369813799858093, -0.01197725534439087, -0.05418702960014343, 0.12015646696090698]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.20946884155273438, 0.14671632647514343, 0.13227234780788422, 0.13835209608078003, 0.04650329425930977, 0.06013541296124458, -0.08176998794078827, 0.17276133596897125, 0.14709943532943726, -0.43206527829170227, 0.043360352516174316, 0.1635725349187851, 0.07823454588651657, -0.28532058000564575, -0.07749854028224945, -0.02694348618388176]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07380956411361694, -0.029785986989736557, -0.07224828749895096, 0.13266965746879578, -0.05654138699173927, -0.1467445194721222, 0.08620796352624893, 0.05176557973027229, 0.05694131553173065, 0.19621291756629944, -0.2778957486152649, 0.0295789185911417, 0.0037619811482727528, -0.1735682636499405, 0.08516275882720947, 0.005479465238749981]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.2159487009048462, 0.2522542476654053, 0.15538948774337769, 0.06889588385820389, -0.3268791735172272, -0.04871603101491928, 0.08659347891807556, 5.617974966298789e-05, 0.17520573735237122, -0.0465296171605587, -0.5645192265510559, 0.07308198511600494, -0.3153119385242462, 0.28178006410598755, 0.1591029018163681, 0.1741265505552292]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3193836808204651, 0.33986204862594604, 0.2112337350845337, 0.2637898325920105, 0.28798630833625793, 0.24409133195877075, 0.11159960180521011, 0.151744544506073, -0.41978925466537476, 0.3992217481136322, -0.08931322395801544, 0.3010598123073578, 0.11318923532962799, -0.08346815407276154, 0.29704099893569946, 0.1884162873029709]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05781480669975281, 0.20314955711364746, 0.1110481321811676, 0.16267184913158417, -0.36017319560050964, 0.1894606351852417, 0.07041270285844803, 0.1830856204032898, -0.02946382574737072, -0.17187084257602692, 0.04918519780039787, -0.06305728852748871, 0.15797710418701172, 0.11956390738487244, -0.04299727454781532, 0.12283852696418762]
Layer: gate_7 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2194947898387909, 0.28823912143707275, -0.11720890551805496, 0.33637917041778564, 0.4615660309791565, 0.4055924415588379, 0.1150684654712677, -0.039600178599357605, 0.6443299055099487, -0.1627333015203476, 0.2644200921058655, 0.040284402668476105, 0.6892264485359192, -0.2242007702589035, -0.04587320238351822, 0.19727174937725067]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.861068069934845, 0.7515580654144287, 0.6210489869117737, 0.5890110731124878, 0.8226883411407471, 0.41873699426651, 1.086197853088379, 1.0872563123703003, 0.06717138737440109, 0.007501622196286917, 0.39993032813072205, 0.7857800126075745, 0.7099981307983398, 0.8041046857833862, 1.1222225427627563, 0.77741938829422]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1057859659194946, 1.1450767517089844, 0.6304276585578918, 1.0059518814086914, 0.9309294819831848, 0.37439659237861633, 0.9736202359199524, 1.4182004928588867, 0.748126745223999, 0.2792428731918335, 1.0742634534835815, 1.0854319334030151, 0.568985641002655, 1.0943224430084229, 0.4957369863986969, 0.9700106382369995]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.220962405204773, 1.5956575870513916, 1.5231010913848877, 1.4170140027999878, 1.3873838186264038, 0.7795882821083069, 1.7410273551940918, 1.1965824365615845, 1.4705803394317627, 1.4585692882537842, 1.2200984954833984, 1.1542532444000244, 1.1102252006530762, 1.312544584274292, 1.191163420677185, 1.172662377357483]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.015274167060852, 1.482676386833191, 1.6453529596328735, 1.7814326286315918, 1.2354508638381958, 1.5844132900238037, 1.3020961284637451, 1.5337207317352295, 1.3350319862365723, 0.5750759243965149, 1.702826976776123, 1.9530272483825684, 1.7405701875686646, 1.6555935144424438, 1.4078007936477661, 1.518685221672058]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0508525371551514, 3.8598408699035645, 3.0060768127441406, 2.6039609909057617, 3.4622488021850586, 2.040916681289673, 3.6501009464263916, 3.190837860107422, 3.8140249252319336, 3.3608288764953613, 3.1780953407287598, 3.8872122764587402, 2.1184306144714355, 3.938188076019287, 3.140841245651245, 2.77699875831604]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.381779670715332, 2.364759683609009, 2.8351683616638184, 2.4168992042541504, 2.8028130531311035, 2.0512943267822266, 1.564271092414856, 2.129512310028076, 2.6373488903045654, 1.9526574611663818, 1.3446736335754395, 1.9311633110046387, 2.409975528717041, 2.664337635040283, 2.6281514167785645, 1.9900373220443726]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.568814277648926, 3.3989813327789307, 3.0518953800201416, 3.1046812534332275, 3.0448265075683594, 2.827775001525879, 2.8059191703796387, 2.0644278526306152, 3.2640645503997803, 2.9391298294067383, 2.740257501602173, 1.381227970123291, 2.9325387477874756, 2.751997947692871, 3.0260562896728516, 2.0542943477630615]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.919156551361084, 1.844455599784851, 2.6388354301452637, 2.5908689498901367, 2.61993670463562, 1.983749270439148, 1.7847013473510742, 3.104820489883423, 0.5943729281425476, 2.181086540222168, 1.014181137084961, 2.4554085731506348, 2.747053861618042, 2.927365779876709, 2.9581611156463623, 1.435055136680603]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6941938400268555, 3.97792649269104, 5.576441287994385, 4.961389541625977, 5.518368244171143, 5.690008163452148, 5.138629913330078, 5.1210784912109375, 6.160956859588623, 5.070474147796631, 5.776054382324219, 6.081563472747803, 4.8533430099487305, 5.419457912445068, 4.3336944580078125, 4.9760918617248535]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.20837926864624, 4.553524494171143, 5.581437110900879, 5.163822174072266, 3.70558500289917, 4.4688944816589355, 4.886950969696045, 5.313596725463867, 5.361794948577881, 4.92404317855835, 4.27424955368042, 3.3607442378997803, 4.409295082092285, 3.3846678733825684, 4.924526691436768, 4.499159812927246]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.874640464782715, 5.839852809906006, 6.018444061279297, 5.626351356506348, 6.0592217445373535, 4.817781448364258, 5.679675579071045, 5.640988349914551, 5.310215473175049, 5.347545146942139, 6.195590496063232, 5.881051063537598, 4.921805381774902, 5.849720001220703, 6.041712284088135, 5.854602813720703]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.974743366241455, 2.9756832122802734, 2.812439203262329, 2.261892080307007, -0.3863493502140045, 3.0385794639587402, 2.723227024078369, 1.8597067594528198, 2.722968101501465, 3.4629082679748535, 2.6800525188446045, 3.2336747646331787, 2.861652374267578, 2.9488604068756104, 2.3492798805236816, 1.7490509748458862]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.521590232849121, 6.7907023429870605, 6.512691497802734, 6.284539699554443, 6.439666748046875, 6.460531711578369, 6.725117206573486, 6.593565464019775, 6.617245674133301, 6.305087089538574, 6.897352695465088, 6.23555850982666, 5.271180629730225, 7.164989948272705, 6.45875358581543, 6.413585662841797]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:   2%|█▎                                                | 5/200 [00:19<11:31,  3.55s/it]Layer: gate_22 - Captured router_logits: [4.9967732429504395, 5.335666179656982, 4.941822052001953, 4.039204120635986, 4.896755218505859, 5.23407506942749, 4.624285697937012, 4.7692131996154785, 5.058881759643555, 4.837407112121582, 5.246663570404053, 4.868526935577393, 5.127046585083008, 5.4256367683410645, 4.586287975311279, 5.478295803070068]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.113381862640381, 4.646584987640381, 3.3276829719543457, 4.178348541259766, 4.427249908447266, 4.057998180389404, 4.228282451629639, 4.220951557159424, 4.343423366546631, 4.442105770111084, 4.040077209472656, 4.408283710479736, 3.6625053882598877, 2.0217413902282715, 3.365034580230713, 4.0075225830078125]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.2680158615112305, 6.233666896820068, 5.8978376388549805, 6.073790073394775, 5.417937755584717, 5.84854793548584, 6.170691967010498, 5.703790187835693, 5.3475542068481445, 5.756843090057373, 6.440311908721924, 5.523557662963867, 5.756710052490234, 6.201427936553955, 6.22275972366333, 5.784480094909668]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.832341194152832, 3.6669280529022217, 3.7891757488250732, 3.943748712539673, 2.691403865814209, 3.852405548095703, 3.9364736080169678, 3.648975372314453, 3.9854884147644043, 3.611262559890747, 3.6904940605163574, 3.617429256439209, 3.659564971923828, 3.601959705352783, 3.6960625648498535, 3.892242193222046]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.44484543800354, 3.5136687755584717, 3.408236265182495, 3.5525741577148438, 3.3847711086273193, 3.6060211658477783, 3.565509796142578, 3.3644073009490967, 3.589827537536621, 3.285297155380249, 3.5916128158569336, 3.4462854862213135, 3.4341440200805664, 2.7278099060058594, 3.6372668743133545, 3.549985647201538]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.171384811401367, 3.1973531246185303, 2.9833037853240967, 2.889397621154785, 2.8160557746887207, 3.0784952640533447, 3.1720871925354004, 3.2729687690734863, 3.031602621078491, 3.0026915073394775, 2.2845911979675293, 2.9934513568878174, 3.1450297832489014, 3.0514001846313477, 3.087942600250244, 2.8821723461151123]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.372678756713867, 4.487431526184082, 4.574704170227051, 4.385056972503662, 4.499732494354248, 4.467798709869385, 4.221213340759277, 4.714630126953125, 4.497375965118408, 4.4708709716796875, 4.479195594787598, 4.062682628631592, 4.204043865203857, 4.065139293670654, 4.587743759155273, 4.378361701965332]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.591351509094238, 8.900185585021973, 8.802778244018555, 8.480649948120117, 8.556387901306152, 8.266342163085938, 8.637694358825684, 8.547811508178711, 8.64220905303955, 8.504500389099121, 8.609163284301758, 8.236178398132324, 8.323281288146973, 9.013754844665527, 8.899670600891113, 8.512948989868164]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.884650707244873, 5.419149875640869, 5.384498596191406, 4.781185626983643, 5.130433559417725, 5.092158794403076, 4.658616065979004, 5.121933460235596, 5.230796813964844, 4.917551040649414, 4.833828926086426, 4.71998405456543, 4.858453273773193, 4.983040809631348, 5.059537887573242, 4.829110145568848]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.237861394882202, 3.0074031352996826, 3.097437620162964, 2.7625491619110107, 3.0206496715545654, 3.3166117668151855, 2.756660223007202, 3.1880135536193848, 3.088986873626709, 3.1899759769439697, 3.173625946044922, 3.24981689453125, 3.0328290462493896, 2.98691725730896, 3.117741107940674, 3.1843576431274414]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.13673146069049835, 0.1495891511440277, 0.13960006833076477, -0.24032625555992126, -0.23246437311172485, -0.12754841148853302, 0.15530633926391602, -0.21047599613666534, 0.1079980880022049, 0.12749944627285004, 0.12694263458251953, 0.0790279433131218, 0.11520319432020187, 0.1348249912261963, -1.1284459829330444, 0.14843574166297913]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09583967179059982, 0.05895759537816048, 0.042312536388635635, 0.06641128659248352, 0.09237917512655258, 0.04329606518149376, 0.05639132112264633, 0.07419513911008835, 0.017821332439780235, 0.06588077545166016, -0.17424991726875305, 0.060443807393312454, -0.003444541245698929, 0.0009361893753521144, 0.011586658656597137, 0.0185251347720623]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06738169491291046, 0.05801147222518921, 0.09255505353212357, 0.05037649720907211, 0.09090372174978256, 0.10896243155002594, 0.05771707370877266, -0.13420435786247253, 0.05992239713668823, 0.0784701406955719, -0.004827394615858793, 0.07804077863693237, -0.19716908037662506, -0.003786667948588729, -0.047054123133420944, 0.12333066016435623]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21046045422554016, 0.15152566134929657, 0.13213735818862915, 0.1479424387216568, 0.04598262161016464, 0.06015600264072418, -0.071355901658535, 0.19003994762897491, 0.15446904301643372, -0.436208039522171, 0.019880473613739014, 0.16403554379940033, 0.0666181892156601, -0.2649809718132019, -0.10289856046438217, -0.03361843153834343]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06772416830062866, -0.027598584070801735, -0.0745435357093811, 0.11504273861646652, -0.08174462616443634, -0.1522740125656128, 0.0945054367184639, 0.05257884040474892, 0.08197130262851715, 0.20247912406921387, -0.2628677189350128, 0.0308793094009161, 0.0024698327761143446, -0.16577978432178497, 0.07017005234956741, 0.037605222314596176]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.22103124856948853, 0.25847700238227844, 0.160195991396904, 0.061794377863407135, -0.3207319378852844, -0.03498285636305809, 0.08692046999931335, 0.00528296735137701, 0.17308616638183594, -0.048509903252124786, -0.5674219727516174, 0.06969773024320602, -0.31041380763053894, 0.274710088968277, 0.16418564319610596, 0.17294394969940186]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3137000799179077, 0.3409607410430908, 0.21110104024410248, 0.2614264190196991, 0.28398391604423523, 0.24078068137168884, 0.1296406090259552, 0.16278794407844543, -0.4112413227558136, 0.3875240981578827, -0.08094284683465958, 0.2930222749710083, 0.1009218767285347, -0.07216613739728928, 0.29085177183151245, 0.18515917658805847]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.061377543956041336, 0.20082895457744598, 0.11212097853422165, 0.159245565533638, -0.35808685421943665, 0.18741528689861298, 0.05912608280777931, 0.20266680419445038, -0.04237009584903717, -0.168019101023674, 0.05913640931248665, -0.06311694532632828, 0.16859082877635956, 0.10681624710559845, -0.03187083825469017, 0.12885892391204834]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21166904270648956, 0.28299179673194885, -0.10360067337751389, 0.3312279284000397, 0.45499488711357117, 0.40081503987312317, 0.11821294575929642, -0.030994543805718422, 0.639101505279541, -0.17526696622371674, 0.25250768661499023, 0.04994270205497742, 0.6770219206809998, -0.21554696559906006, -0.03428203612565994, 0.19078366458415985]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8564774394035339, 0.7604518532752991, 0.628376841545105, 0.5732365250587463, 0.8279811143875122, 0.4301394820213318, 1.0908472537994385, 1.0810807943344116, 0.05520734563469887, 0.006629031151533127, 0.41668274998664856, 0.7875781655311584, 0.7218363285064697, 0.8244699239730835, 1.125466227531433, 0.7759968042373657]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1197246313095093, 1.144689679145813, 0.6296584606170654, 1.0140396356582642, 0.9363805055618286, 0.3958069384098053, 0.96808922290802, 1.4112350940704346, 0.7685848474502563, 0.29071131348609924, 1.0711815357208252, 1.0848315954208374, 0.5918047428131104, 1.0932501554489136, 0.4845181405544281, 0.9842297434806824]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2164976596832275, 1.5936739444732666, 1.5168976783752441, 1.4286692142486572, 1.383140206336975, 0.801078736782074, 1.7402786016464233, 1.2103209495544434, 1.4697023630142212, 1.4453661441802979, 1.2314081192016602, 1.1631733179092407, 1.1284499168395996, 1.3225823640823364, 1.2085683345794678, 1.182822346687317]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0314531326293945, 1.4874988794326782, 1.6356834173202515, 1.7774150371551514, 1.248246431350708, 1.5797150135040283, 1.29653799533844, 1.5226067304611206, 1.3485760688781738, 0.585451066493988, 1.6864697933197021, 1.9432015419006348, 1.7467950582504272, 1.648956060409546, 1.4035027027130127, 1.5141568183898926]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.056180477142334, 3.843001365661621, 2.990077018737793, 2.6179702281951904, 3.437730312347412, 2.0592198371887207, 3.6395387649536133, 3.1628262996673584, 3.7984282970428467, 3.3495993614196777, 3.160533905029297, 3.8764350414276123, 2.1252288818359375, 3.9458651542663574, 3.12890625, 2.780660629272461]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3647069931030273, 2.3526082038879395, 2.816711664199829, 2.403836488723755, 2.7869179248809814, 2.0291268825531006, 1.5745128393173218, 2.102729320526123, 2.621873378753662, 1.9458199739456177, 1.3567919731140137, 1.9279534816741943, 2.3974101543426514, 2.6442582607269287, 2.614236831665039, 1.9903147220611572]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5654988288879395, 3.389247417449951, 3.0449368953704834, 3.0901150703430176, 3.0453760623931885, 2.8196377754211426, 2.778332233428955, 2.0544040203094482, 3.2615599632263184, 2.926799774169922, 2.7389204502105713, 1.3850966691970825, 2.9119203090667725, 2.743440866470337, 3.019299030303955, 2.046405076980591]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9170522689819336, 1.852752447128296, 2.6302754878997803, 2.5769965648651123, 2.609593152999878, 1.9837843179702759, 1.786076545715332, 3.0955097675323486, 0.6089203953742981, 2.173267126083374, 1.029384970664978, 2.454616069793701, 2.7434494495391846, 2.913170337677002, 2.942213296890259, 1.4384639263153076]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.708965539932251, 3.969235420227051, 5.558803558349609, 4.961491107940674, 5.502310276031494, 5.670902252197266, 5.116459846496582, 5.11294412612915, 6.136364459991455, 5.064435958862305, 5.757485866546631, 6.06329870223999, 4.851437568664551, 5.403079986572266, 4.335157871246338, 4.9732985496521]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.193006992340088, 4.5402374267578125, 5.564306259155273, 5.148331642150879, 3.702871799468994, 4.466275215148926, 4.878433704376221, 5.305883407592773, 5.340758800506592, 4.909986972808838, 4.275325298309326, 3.3658525943756104, 4.410758018493652, 3.3984780311584473, 4.917967796325684, 4.493351936340332]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.8618974685668945, 5.822951793670654, 5.996037483215332, 5.604761600494385, 6.037538528442383, 4.811776638031006, 5.665482997894287, 5.630537986755371, 5.291670799255371, 5.33442497253418, 6.170997619628906, 5.864395618438721, 4.917667388916016, 5.831104755401611, 6.018985748291016, 5.833581924438477]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.96565318107605, 2.9691452980041504, 2.801694631576538, 2.256225109100342, -0.3729884624481201, 3.0201122760772705, 2.7168002128601074, 1.8588272333145142, 2.714371681213379, 3.4491167068481445, 2.6770248413085938, 3.219438314437866, 2.8516480922698975, 2.9408442974090576, 2.3514018058776855, 1.7537394762039185]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.498322486877441, 6.764232635498047, 6.48405122756958, 6.265769958496094, 6.415677070617676, 6.433769226074219, 6.700039386749268, 6.569189071655273, 6.592785835266113, 6.28450870513916, 6.871232509613037, 6.206783771514893, 5.256128787994385, 7.136201858520508, 6.440243244171143, 6.38903284072876]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [4.9835734367370605, 5.3160552978515625, 4.92290735244751, 4.0301995277404785, 4.883081912994385, 5.217434406280518, 4.615865707397461, 4.758903503417969, 5.043343544006348, 4.826558589935303, 5.228277206420898, 4.85112190246582, 5.11076021194458, 5.406367778778076, 4.576695442199707, 5.4611945152282715]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.095448970794678, 4.629514694213867, 3.3190348148345947, 4.162522792816162, 4.412600517272949, 4.043094635009766, 4.210536956787109, 4.207955360412598, 4.328451156616211, 4.421595096588135, 4.026711463928223, 4.391571521759033, 3.6526033878326416, 2.0164356231689453, 3.3584563732147217, 3.9950294494628906]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.2482781410217285, 6.213355541229248, 5.878262042999268, 6.056917190551758, 5.404515266418457, 5.831766605377197, 6.151713848114014, 5.68792200088501, 5.336212635040283, 5.741368770599365, 6.420154571533203, 5.5077338218688965, 5.738612651824951, 6.183392524719238, 6.205694198608398, 5.770799160003662]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.820709228515625, 3.6542742252349854, 3.7743942737579346, 3.928745985031128, 2.684344530105591, 3.8395652770996094, 3.9217689037323, 3.632369041442871, 3.970731258392334, 3.5994832515716553, 3.6764683723449707, 3.604590654373169, 3.6451761722564697, 3.590890645980835, 3.679594039916992, 3.8778083324432373]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:   4%|██▎                                               | 9/200 [00:28<08:54,  2.80s/it]Layer: gate_26 - Captured router_logits: [3.433473587036133, 3.4969301223754883, 3.3939244747161865, 3.5402939319610596, 3.3690881729125977, 3.5909712314605713, 3.550283908843994, 3.3532371520996094, 3.5774409770965576, 3.2742063999176025, 3.573775291442871, 3.4349100589752197, 3.424482583999634, 2.719451665878296, 3.6223487854003906, 3.5347986221313477]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.157470941543579, 3.1850242614746094, 2.972660541534424, 2.8793437480926514, 2.8078174591064453, 3.066537857055664, 3.159654140472412, 3.2571027278900146, 3.0214216709136963, 2.9926540851593018, 2.2778520584106445, 2.9837806224823, 3.1310477256774902, 3.0376548767089844, 3.0769033432006836, 2.869676351547241]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.364514350891113, 4.476469039916992, 4.563876628875732, 4.374553680419922, 4.486103057861328, 4.454941749572754, 4.203193664550781, 4.69948148727417, 4.48646879196167, 4.45976448059082, 4.464983940124512, 4.053712368011475, 4.195964336395264, 4.0562744140625, 4.5743727684021, 4.365484714508057]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.573379516601562, 8.87590503692627, 8.78454303741455, 8.46936321258545, 8.54178524017334, 8.25106430053711, 8.622736930847168, 8.538071632385254, 8.626801490783691, 8.49193000793457, 8.593238830566406, 8.220575332641602, 8.306489944458008, 8.995048522949219, 8.880698204040527, 8.496310234069824]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.87841272354126, 5.4103779792785645, 5.378934383392334, 4.775773525238037, 5.12382173538208, 5.0851826667785645, 4.660246849060059, 5.117844581604004, 5.2260355949401855, 4.913456916809082, 4.831409931182861, 4.719911098480225, 4.857639312744141, 4.97769832611084, 5.0547194480896, 4.821740627288818]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2335898876190186, 2.9992382526397705, 3.089970350265503, 2.7565958499908447, 3.0150692462921143, 3.309654474258423, 2.7524425983428955, 3.1853079795837402, 3.0806870460510254, 3.1858766078948975, 3.1666431427001953, 3.244311809539795, 3.0298802852630615, 2.984055519104004, 3.114429473876953, 3.1813578605651855]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.13348761200904846, 0.14984464645385742, 0.1396765410900116, -0.24629336595535278, -0.23508301377296448, -0.12466822564601898, 0.1545269936323166, -0.20590025186538696, 0.1071910634636879, 0.12724153697490692, 0.12578769028186798, 0.08438674360513687, 0.11390413343906403, 0.1336725950241089, -1.1396193504333496, 0.14911890029907227]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0956704318523407, 0.054198238998651505, 0.043669044971466064, 0.0665009543299675, 0.09168428182601929, 0.043068937957286835, 0.052921272814273834, 0.07617921382188797, 0.016113299876451492, 0.06243064999580383, -0.1753551959991455, 0.06090947613120079, 0.0009109113598242402, -0.0030099593568593264, 0.009282007813453674, 0.023800186812877655]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07002212107181549, 0.047851961106061935, 0.09101766347885132, 0.050335340201854706, 0.09150529652833939, 0.10845819115638733, 0.06373824924230576, -0.13939730823040009, 0.059524934738874435, 0.07512329518795013, -0.0018006745958700776, 0.07907717674970627, -0.20732715725898743, -0.008090636692941189, -0.04850112274289131, 0.12555259466171265]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21396854519844055, 0.1507980227470398, 0.1284017413854599, 0.14760687947273254, 0.041523780673742294, 0.05765550956130028, -0.06423623859882355, 0.1866130232810974, 0.14973072707653046, -0.42665231227874756, 0.01662970334291458, 0.161050483584404, 0.07008316367864609, -0.2596404552459717, -0.10117077827453613, -0.039286836981773376]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07721316069364548, -0.031132793053984642, -0.06950747221708298, 0.12279094010591507, -0.07724464684724808, -0.15371496975421906, 0.09980790317058563, 0.051545146852731705, 0.07415667921304703, 0.2069980651140213, -0.2695730924606323, 0.02908172830939293, 0.003089162055402994, -0.17625460028648376, 0.07390428334474564, 0.039231814444065094]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.2154320925474167, 0.2545970678329468, 0.14976194500923157, 0.05593254789710045, -0.31004393100738525, -0.04182810336351395, 0.08546629548072815, 0.007326515857130289, 0.16628645360469818, -0.050242625176906586, -0.5385415554046631, 0.060813628137111664, -0.3214331269264221, 0.2669101357460022, 0.15829019248485565, 0.16516582667827606]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3131369650363922, 0.3427978456020355, 0.2054833024740219, 0.2633849084377289, 0.27804648876190186, 0.24154283106327057, 0.13320983946323395, 0.15900005400180817, -0.40711793303489685, 0.3896034061908722, -0.07420502603054047, 0.3040922284126282, 0.09288866072893143, -0.06978201866149902, 0.2821296751499176, 0.180621936917305]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05838336795568466, 0.20086859166622162, 0.11430060118436813, 0.157301127910614, -0.3519856929779053, 0.19492703676223755, 0.057329628616571426, 0.20771555602550507, -0.03923648223280907, -0.16784784197807312, 0.06558845192193985, -0.06641591340303421, 0.1734851747751236, 0.10411304235458374, -0.020165108144283295, 0.13826878368854523]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2155262678861618, 0.28450196981430054, -0.09684596210718155, 0.32875508069992065, 0.45389869809150696, 0.40056586265563965, 0.11791044473648071, -0.023290477693080902, 0.6282004714012146, -0.18265020847320557, 0.25296342372894287, 0.049900516867637634, 0.6739077568054199, -0.20882059633731842, -0.02549477107822895, 0.19051693379878998]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8611975312232971, 0.7584152817726135, 0.6246941089630127, 0.5720468163490295, 0.834855854511261, 0.43674683570861816, 1.1004436016082764, 1.0774272680282593, 0.06685833632946014, 0.01735534518957138, 0.42631587386131287, 0.7885725498199463, 0.7298511266708374, 0.8314762115478516, 1.1213006973266602, 0.7793343663215637]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1335949897766113, 1.157430648803711, 0.6412510871887207, 1.0322072505950928, 0.9459398984909058, 0.39207014441490173, 0.9770148396492004, 1.4179819822311401, 0.7752634882926941, 0.29223406314849854, 1.0846706628799438, 1.095671534538269, 0.6097708344459534, 1.1007776260375977, 0.4907991290092468, 0.9984891414642334]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.231197714805603, 1.5976319313049316, 1.5217689275741577, 1.4219944477081299, 1.3988062143325806, 0.8139945268630981, 1.7473477125167847, 1.2176233530044556, 1.476601004600525, 1.4454989433288574, 1.24226713180542, 1.160115361213684, 1.134895920753479, 1.32801353931427, 1.2039815187454224, 1.1901766061782837]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.042184591293335, 1.4992642402648926, 1.6432303190231323, 1.7874391078948975, 1.2586262226104736, 1.5839345455169678, 1.2988531589508057, 1.526512861251831, 1.3517920970916748, 0.6034830212593079, 1.6956690549850464, 1.9479753971099854, 1.761771559715271, 1.6624916791915894, 1.4149105548858643, 1.5225708484649658]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0701537132263184, 3.8537065982818604, 3.01408314704895, 2.6343133449554443, 3.4628303050994873, 2.0716679096221924, 3.653001308441162, 3.1747660636901855, 3.8149144649505615, 3.3697996139526367, 3.1767666339874268, 3.9031291007995605, 2.1304633617401123, 3.961906909942627, 3.1456141471862793, 2.79072642326355]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.37827205657959, 2.3597352504730225, 2.8313639163970947, 2.4168925285339355, 2.797914981842041, 2.040297269821167, 1.5744699239730835, 2.102297306060791, 2.628248453140259, 1.966870665550232, 1.3487763404846191, 1.9328533411026, 2.4118824005126953, 2.658393144607544, 2.6293132305145264, 1.9940541982650757]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5660223960876465, 3.4044902324676514, 3.0518479347229004, 3.1048617362976074, 3.0444774627685547, 2.832756519317627, 2.7826130390167236, 2.0543203353881836, 3.271376848220825, 2.936396837234497, 2.7527904510498047, 1.3792520761489868, 2.915308713912964, 2.756711006164551, 3.0278117656707764, 2.0564494132995605]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9181418418884277, 1.847759485244751, 2.635162591934204, 2.5820271968841553, 2.6187846660614014, 1.9874101877212524, 1.7818140983581543, 3.097055435180664, 0.5907549858093262, 2.176318645477295, 1.0026935338974, 2.452709436416626, 2.7467434406280518, 2.9179930686950684, 2.948723793029785, 1.430723786354065]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6958634853363037, 3.963531494140625, 5.562361717224121, 4.957277774810791, 5.507059097290039, 5.677261829376221, 5.128082752227783, 5.11479377746582, 6.147141456604004, 5.060919761657715, 5.766519546508789, 6.072207927703857, 4.847383975982666, 5.404560565948486, 4.322528839111328, 4.967650413513184]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.203831195831299, 4.55133581161499, 5.55905818939209, 5.160017967224121, 3.7066919803619385, 4.46578311920166, 4.881664752960205, 5.30348539352417, 5.352169513702393, 4.914193153381348, 4.273075103759766, 3.3600077629089355, 4.4101057052612305, 3.392357349395752, 4.925212860107422, 4.49790620803833]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.871303558349609, 5.83183479309082, 6.014020919799805, 5.616165637969971, 6.055561542510986, 4.815067768096924, 5.676888465881348, 5.634422302246094, 5.297689914703369, 5.3473896980285645, 6.191447734832764, 5.88332462310791, 4.931171894073486, 5.844051361083984, 6.0310235023498535, 5.849706649780273]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.97987961769104, 2.9720101356506348, 2.809483051300049, 2.2551345825195312, -0.4019322693347931, 3.031545877456665, 2.719468593597412, 1.8585782051086426, 2.717607259750366, 3.46051287651062, 2.6843302249908447, 3.2355175018310547, 2.8595526218414307, 2.9486639499664307, 2.3444266319274902, 1.7495957612991333]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.524534702301025, 6.792414665222168, 6.511298656463623, 6.283575057983398, 6.439383506774902, 6.455077648162842, 6.721950054168701, 6.601766109466553, 6.6171956062316895, 6.305919170379639, 6.898037910461426, 6.229745388031006, 5.268881797790527, 7.1680731773376465, 6.46195125579834, 6.412568092346191]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [4.998301029205322, 5.3364691734313965, 4.944498062133789, 4.040136814117432, 4.895141124725342, 5.235143184661865, 4.6249098777771, 4.769955635070801, 5.059062957763672, 4.8389458656311035, 5.250509262084961, 4.86627721786499, 5.1288676261901855, 5.422307968139648, 4.590199947357178, 5.470889091491699]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.120511531829834, 4.650505542755127, 3.329716205596924, 4.1808295249938965, 4.431929588317871, 4.06203556060791, 4.227593898773193, 4.220269680023193, 4.347052574157715, 4.4467949867248535, 4.043664455413818, 4.412993907928467, 3.6640679836273193, 2.021698474884033, 3.370171070098877, 4.008978843688965]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.267430305480957, 6.225430965423584, 5.8947529792785645, 6.071046352386475, 5.418271064758301, 5.840887069702148, 6.165672302246094, 5.695939064025879, 5.34105920791626, 5.747683525085449, 6.429402828216553, 5.514100551605225, 5.753256320953369, 6.196585178375244, 6.216773986816406, 5.778059959411621]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8280961513519287, 3.664250135421753, 3.7802979946136475, 3.938140630722046, 2.686880588531494, 3.8479206562042236, 3.9312899112701416, 3.645570755004883, 3.981809616088867, 3.607167959213257, 3.687412977218628, 3.6136765480041504, 3.652906894683838, 3.5952582359313965, 3.692164659500122, 3.8866822719573975]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.442498207092285, 3.512399673461914, 3.401458501815796, 3.54795503616333, 3.37727689743042, 3.6071009635925293, 3.561877727508545, 3.3584561347961426, 3.5902676582336426, 3.286397933959961, 3.587697982788086, 3.441216230392456, 3.430908203125, 2.7228071689605713, 3.6373331546783447, 3.550158977508545]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1711266040802, 3.1983606815338135, 2.980156421661377, 2.888617753982544, 2.814826011657715, 3.0768778324127197, 3.172048330307007, 3.268317222595215, 3.0287346839904785, 3.001098871231079, 2.2843172550201416, 2.9926767349243164, 3.1448304653167725, 3.051684856414795, 3.0809786319732666, 2.8793959617614746]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.37300968170166, 4.486912250518799, 4.568949222564697, 4.37770414352417, 4.491383075714111, 4.463168144226074, 4.211544513702393, 4.7071757316589355, 4.491349697113037, 4.462305068969727, 4.470040798187256, 4.05629825592041, 4.198894500732422, 4.058773994445801, 4.578271389007568, 4.37168550491333]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.5820894241333, 8.894726753234863, 8.787354469299316, 8.472654342651367, 8.552824020385742, 8.25973892211914, 8.632051467895508, 8.54448127746582, 8.630218505859375, 8.496000289916992, 8.598450660705566, 8.230141639709473, 8.322386741638184, 9.004014015197754, 8.893403053283691, 8.503114700317383]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:   6%|███▏                                             | 13/200 [00:37<08:02,  2.58s/it]Layer: gate_30 - Captured router_logits: [4.882687568664551, 5.420905113220215, 5.384383678436279, 4.781728267669678, 5.132181644439697, 5.0870137214660645, 4.664676666259766, 5.126515865325928, 5.233808517456055, 4.919817924499512, 4.83259916305542, 4.7260212898254395, 4.861851215362549, 4.979846477508545, 5.061506271362305, 4.826457500457764]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2342007160186768, 2.9998576641082764, 3.0964889526367188, 2.761043071746826, 3.0195817947387695, 3.3101868629455566, 2.75551700592041, 3.1838386058807373, 3.084934711456299, 3.184767961502075, 3.1692025661468506, 3.2541580200195312, 3.0311801433563232, 2.9823222160339355, 3.1177499294281006, 3.1802964210510254]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11574360728263855, 0.13002228736877441, 0.12696444988250732, -0.2533006966114044, -0.2548607885837555, -0.08811741322278976, 0.13482333719730377, -0.147933229804039, 0.09507124125957489, 0.11045005917549133, 0.10800207406282425, 0.06284889578819275, 0.10171160846948624, 0.12516754865646362, -1.1091309785842896, 0.1332920789718628]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08567574620246887, 0.0481393039226532, 0.03822670131921768, 0.06348294764757156, 0.0809141993522644, 0.034145064651966095, 0.05395026504993439, 0.08648373186588287, 0.03134126961231232, 0.06213049218058586, -0.1840047985315323, 0.06307382136583328, 0.0016747239278629422, -0.0051321000792086124, 0.025770464912056923, 0.030999505892395973]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07094229757785797, 0.04138647019863129, 0.08539770543575287, 0.049877576529979706, 0.09898975491523743, 0.09996026754379272, 0.06071734055876732, -0.1400306075811386, 0.06931429356336594, 0.095316082239151, 0.0037027020007371902, 0.07527074962854385, -0.2175929993391037, -0.010710394941270351, -0.056192945688962936, 0.1254209280014038]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.2136974185705185, 0.14571577310562134, 0.12535300850868225, 0.1395787000656128, 0.04950467869639397, 0.05005035549402237, -0.10656564682722092, 0.17092475295066833, 0.15374033153057098, -0.45511606335639954, 0.04175620153546333, 0.1574689894914627, 0.10793211311101913, -0.30440008640289307, -0.050762273371219635, -0.04548869654536247]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07255961745977402, -0.02509962022304535, -0.0735752210021019, 0.1536816507577896, -0.05676108971238136, -0.14496955275535583, 0.08239151537418365, 0.049052078276872635, 0.05189812183380127, 0.19236308336257935, -0.29764223098754883, 0.020732177421450615, 0.030831923708319664, -0.18344087898731232, 0.08982454985380173, 0.005605536047369242]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.20907503366470337, 0.245950847864151, 0.15243695676326752, 0.06497567147016525, -0.3283613622188568, -0.06309954077005386, 0.09528202563524246, -0.0012923136819154024, 0.17783096432685852, -0.05339054390788078, -0.5301138758659363, 0.05916144698858261, -0.3409486711025238, 0.273190975189209, 0.1578354686498642, 0.16382895410060883]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.32524043321609497, 0.3320865333080292, 0.21193112432956696, 0.2655792236328125, 0.2834254205226898, 0.2486732453107834, 0.10382552444934845, 0.14776182174682617, -0.4035916030406952, 0.3997842073440552, -0.1049220934510231, 0.31203073263168335, 0.10865775495767593, -0.10037936270236969, 0.30600765347480774, 0.18601390719413757]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.04476679861545563, 0.1961652785539627, 0.11358203738927841, 0.16297759115695953, -0.37062883377075195, 0.20323850214481354, 0.06567741185426712, 0.17891481518745422, -0.0122219854965806, -0.16612672805786133, 0.04530679062008858, -0.05994807183742523, 0.15551087260246277, 0.12215585261583328, -0.05406384542584419, 0.12260694056749344]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21772950887680054, 0.28533321619033813, -0.11333183199167252, 0.3317064344882965, 0.4720715582370758, 0.40120500326156616, 0.1184055358171463, -0.028476610779762268, 0.6339136362075806, -0.16165299713611603, 0.2606016993522644, 0.0463143028318882, 0.6869231462478638, -0.23403048515319824, -0.05545155704021454, 0.1971239298582077]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8564562201499939, 0.742347776889801, 0.6192467212677002, 0.6002581119537354, 0.8219911456108093, 0.4173966646194458, 1.0878063440322876, 1.0792872905731201, 0.07973027974367142, 0.005630836822092533, 0.40071848034858704, 0.7944871783256531, 0.7194437384605408, 0.7943617701530457, 1.1253373622894287, 0.7852033972740173]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1326667070388794, 1.1699377298355103, 0.6372657418251038, 1.0258339643478394, 0.9344490170478821, 0.3657926023006439, 0.9761005640029907, 1.421504259109497, 0.7454525828361511, 0.2664111852645874, 1.0924113988876343, 1.0959845781326294, 0.5833916664123535, 1.0853406190872192, 0.5074960589408875, 0.9791545271873474]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2309834957122803, 1.6034646034240723, 1.521757960319519, 1.4026703834533691, 1.4037419557571411, 0.7827219367027283, 1.75045907497406, 1.1937402486801147, 1.4829561710357666, 1.4662004709243774, 1.2314562797546387, 1.1492230892181396, 1.108933448791504, 1.3066298961639404, 1.1689544916152954, 1.1678227186203003]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0102570056915283, 1.480169653892517, 1.650321364402771, 1.7831966876983643, 1.2224384546279907, 1.5740514993667603, 1.2946065664291382, 1.5296788215637207, 1.3276745080947876, 0.5882353186607361, 1.6994006633758545, 1.947506308555603, 1.7389758825302124, 1.6626291275024414, 1.405875325202942, 1.5192025899887085]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0491201877593994, 3.863302707672119, 3.017191171646118, 2.608095407485962, 3.473287343978882, 2.041200876235962, 3.653132200241089, 3.1955032348632812, 3.815702199935913, 3.3676280975341797, 3.180443048477173, 3.904024600982666, 2.102605104446411, 3.92802095413208, 3.144425868988037, 2.768951654434204]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3830490112304688, 2.363603353500366, 2.839946746826172, 2.421919584274292, 2.802440881729126, 2.061112403869629, 1.5602619647979736, 2.1245992183685303, 2.635288953781128, 1.9711241722106934, 1.3321524858474731, 1.9285602569580078, 2.418480157852173, 2.6712913513183594, 2.636181592941284, 1.9834880828857422]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5508201122283936, 3.399942398071289, 3.045677900314331, 3.105102062225342, 3.026700735092163, 2.827216386795044, 2.798496961593628, 2.0594029426574707, 3.264674663543701, 2.933479070663452, 2.7398626804351807, 1.3637453317642212, 2.920433282852173, 2.748767614364624, 3.020827531814575, 2.0519802570343018]
Running loglikelihood requests:   8%|████▏                                            | 17/200 [00:46<07:22,  2.42s/it]Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9119136333465576, 1.8340710401535034, 2.6291983127593994, 2.587028741836548, 2.6165595054626465, 1.9753042459487915, 1.773406744003296, 3.090912103652954, 0.5744746923446655, 2.1806206703186035, 0.9812119603157043, 2.4439682960510254, 2.7394659519195557, 2.916562795639038, 2.9464223384857178, 1.4232722520828247]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6573991775512695, 3.9444046020507812, 5.546061038970947, 4.9252119064331055, 5.4930853843688965, 5.659224510192871, 5.118271827697754, 5.092028617858887, 6.14077615737915, 5.030416011810303, 5.753429412841797, 6.051329612731934, 4.827641487121582, 5.389221668243408, 4.2864274978637695, 4.938118934631348]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.196996212005615, 4.544829845428467, 5.549198627471924, 5.161100387573242, 3.6898274421691895, 4.450750350952148, 4.871523857116699, 5.290412902832031, 5.35541296005249, 4.9063262939453125, 4.253242492675781, 3.338094711303711, 4.390798091888428, 3.369046449661255, 4.907803058624268, 4.488987445831299]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.8585309982299805, 5.823739528656006, 6.012902736663818, 5.609356880187988, 6.0509562492370605, 4.796488285064697, 5.663393974304199, 5.621076583862305, 5.297768592834473, 5.34092903137207, 6.192846775054932, 5.8749284744262695, 4.920175552368164, 5.837287902832031, 6.027772426605225, 5.847158908843994]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.97759747505188, 2.9698803424835205, 2.805586576461792, 2.243652582168579, -0.4215807020664215, 3.0377092361450195, 2.7133355140686035, 1.8520829677581787, 2.7130372524261475, 3.460287570953369, 2.6741809844970703, 3.2366843223571777, 2.8554840087890625, 2.944232225418091, 2.3310067653656006, 1.7374812364578247]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.520223140716553, 6.789967060089111, 6.511056423187256, 6.279011249542236, 6.432685852050781, 6.456083297729492, 6.716152667999268, 6.597797870635986, 6.618342399597168, 6.299615859985352, 6.8956732749938965, 6.231225490570068, 5.259801387786865, 7.165445804595947, 6.452978134155273, 6.4091644287109375]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [4.9921088218688965, 5.3360066413879395, 4.943709373474121, 4.035372734069824, 4.889151096343994, 5.2305827140808105, 4.61673641204834, 4.764429092407227, 5.054958343505859, 4.833117961883545, 5.250196933746338, 4.864360809326172, 5.124917030334473, 5.418653964996338, 4.579771518707275, 5.464001655578613]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.123303413391113, 4.650432586669922, 3.3253116607666016, 4.181098937988281, 4.433339595794678, 4.062320232391357, 4.2281107902526855, 4.218140602111816, 4.3439621925354, 4.449301719665527, 4.0405497550964355, 4.41423225402832, 3.6619927883148193, 2.0180628299713135, 3.3638885021209717, 4.007555961608887]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.2696404457092285, 6.227402687072754, 5.8982014656066895, 6.068704605102539, 5.420475959777832, 5.841618061065674, 6.168409824371338, 5.695730686187744, 5.341941833496094, 5.749622821807861, 6.429829120635986, 5.513075828552246, 5.7576003074646, 6.199003219604492, 6.2157301902771, 5.779817581176758]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.826681137084961, 3.663804054260254, 3.7795886993408203, 3.938187599182129, 2.6853487491607666, 3.8494460582733154, 3.9273087978363037, 3.6457810401916504, 3.981595754623413, 3.6065306663513184, 3.689203977584839, 3.6119680404663086, 3.654568910598755, 3.59378981590271, 3.6932427883148193, 3.88789963722229]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.444033145904541, 3.5135695934295654, 3.4012458324432373, 3.548543930053711, 3.3803064823150635, 3.608973264694214, 3.5638322830200195, 3.3616273403167725, 3.58941650390625, 3.286386489868164, 3.5931529998779297, 3.442384719848633, 3.430386781692505, 2.722527503967285, 3.640796184539795, 3.5535027980804443]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1732821464538574, 3.198848009109497, 2.980675220489502, 2.8906362056732178, 2.815185785293579, 3.077720880508423, 3.173342227935791, 3.2679829597473145, 3.0281264781951904, 3.002089500427246, 2.28273868560791, 2.992969274520874, 3.1487932205200195, 3.060715913772583, 3.080291509628296, 2.8813962936401367]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.374153137207031, 4.487789154052734, 4.573354244232178, 4.379851818084717, 4.494197368621826, 4.46491813659668, 4.219393730163574, 4.7081170082092285, 4.49501895904541, 4.463801860809326, 4.47353458404541, 4.059826374053955, 4.203266143798828, 4.061368465423584, 4.582953929901123, 4.3769683837890625]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.57735824584961, 8.896830558776855, 8.783766746520996, 8.46826457977295, 8.551030158996582, 8.265141487121582, 8.630005836486816, 8.54560661315918, 8.629035949707031, 8.492834091186523, 8.599008560180664, 8.229385375976562, 8.324419975280762, 9.000167846679688, 8.89100170135498, 8.49971866607666]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.871460914611816, 5.405228614807129, 5.370384216308594, 4.769476413726807, 5.122323036193848, 5.077664375305176, 4.652421474456787, 5.11367130279541, 5.226266860961914, 4.909533500671387, 4.824343204498291, 4.712471961975098, 4.850636959075928, 4.973441123962402, 5.047905445098877, 4.8169450759887695]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2255091667175293, 2.9942901134490967, 3.0880649089813232, 2.752828359603882, 3.011409044265747, 3.300600051879883, 2.7480387687683105, 3.1766650676727295, 3.076045036315918, 3.1766788959503174, 3.166043758392334, 3.2476966381073, 3.026238441467285, 2.977548360824585, 3.1164662837982178, 3.1734931468963623]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12093126773834229, 0.13399450480937958, 0.12684713304042816, -0.2380170226097107, -0.22867213189601898, -0.09799923002719879, 0.14268994331359863, -0.1812632977962494, 0.09775736927986145, 0.11508923023939133, 0.11442507803440094, 0.07465790957212448, 0.10643736273050308, 0.12436062842607498, -1.0785512924194336, 0.13669021427631378]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08839735388755798, 0.049282170832157135, 0.03919742628931999, 0.06541413813829422, 0.08094596862792969, 0.04282612353563309, 0.04834786802530289, 0.07452396303415298, 0.014024345204234123, 0.06201697140932083, -0.16993878781795502, 0.05881829559803009, 0.0015822657151147723, 2.0654688341892324e-05, 0.01597856730222702, 0.02439308352768421]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06638871133327484, 0.04833526909351349, 0.0928046777844429, 0.047503091394901276, 0.08774861693382263, 0.10389723628759384, 0.053171031177043915, -0.1363699734210968, 0.060749635100364685, 0.08228890597820282, -0.001714996644295752, 0.07838872820138931, -0.19918599724769592, -0.01360827125608921, -0.05614157393574715, 0.12182119488716125]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21091574430465698, 0.1468420773744583, 0.13055606186389923, 0.14197635650634766, 0.04292048141360283, 0.05725310742855072, -0.08898171037435532, 0.1746535450220108, 0.1493334323167801, -0.4317281246185303, 0.034784648567438126, 0.163201242685318, 0.08082730323076248, -0.2785528600215912, -0.08220410346984863, -0.03350148722529411]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07389110326766968, -0.02904690057039261, -0.07089873403310776, 0.13396403193473816, -0.062468383461236954, -0.14726823568344116, 0.08818729221820831, 0.05098624527454376, 0.06450895220041275, 0.1999252438545227, -0.27823564410209656, 0.02785770408809185, 0.010752269998192787, -0.1761116087436676, 0.08325748890638351, 0.01407169084995985]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21108375489711761, 0.2448570877313614, 0.1506519466638565, 0.06045623868703842, -0.3161342740058899, -0.05469685420393944, 0.09215870499610901, 0.002091937232762575, 0.16809199750423431, -0.05271607264876366, -0.5330120921134949, 0.0630074217915535, -0.3248104155063629, 0.2699790894985199, 0.15574580430984497, 0.16322027146816254]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3160265386104584, 0.3387405574321747, 0.20445282757282257, 0.26633787155151367, 0.2785966992378235, 0.24285171926021576, 0.11698216199874878, 0.14935316145420074, -0.4065168797969818, 0.394115149974823, -0.08061959594488144, 0.307969331741333, 0.09990695118904114, -0.0779619961977005, 0.2884284257888794, 0.18141739070415497]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05679076537489891, 0.19912253320217133, 0.11235376447439194, 0.1587393581867218, -0.34908342361450195, 0.19598235189914703, 0.0618148148059845, 0.20095495879650116, -0.03192395344376564, -0.17012490332126617, 0.05774547904729843, -0.0690307542681694, 0.16862450540065765, 0.10987205803394318, -0.02636084146797657, 0.13840781152248383]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21342872083187103, 0.28327706456184387, -0.10376794636249542, 0.3306639492511749, 0.4616217315196991, 0.40199899673461914, 0.11448127776384354, -0.02559261955320835, 0.6332419514656067, -0.17826169729232788, 0.26147565245628357, 0.050441060215234756, 0.6794431805610657, -0.21387039124965668, -0.03809155151247978, 0.194564089179039]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8634081482887268, 0.7537412643432617, 0.6211955547332764, 0.5790206789970398, 0.8347310423851013, 0.42929744720458984, 1.0963112115859985, 1.0816388130187988, 0.07555711269378662, 0.01907275803387165, 0.41620731353759766, 0.7917292714118958, 0.7297626733779907, 0.8129094243049622, 1.1210873126983643, 0.7855241894721985]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.130820631980896, 1.160259485244751, 0.6479974985122681, 1.03408944606781, 0.9431437253952026, 0.37856951355934143, 0.9849181771278381, 1.4249292612075806, 0.7615571022033691, 0.28357526659965515, 1.0864421129226685, 1.100584626197815, 0.6041895151138306, 1.0984883308410645, 0.49577566981315613, 0.9884012341499329]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2342864274978638, 1.6025432348251343, 1.5310287475585938, 1.417197585105896, 1.410399079322815, 0.8113076090812683, 1.7496591806411743, 1.2162278890609741, 1.4801273345947266, 1.456595778465271, 1.2438509464263916, 1.1641234159469604, 1.1317023038864136, 1.3284008502960205, 1.2065736055374146, 1.1886712312698364]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.036112666130066, 1.4981842041015625, 1.6514755487442017, 1.7938928604125977, 1.2513351440429688, 1.5913705825805664, 1.2998260259628296, 1.5356242656707764, 1.3480044603347778, 0.6088056564331055, 1.705683708190918, 1.9584921598434448, 1.7550917863845825, 1.6706622838974, 1.4224785566329956, 1.53207528591156]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0740973949432373, 3.8728415966033936, 3.0302767753601074, 2.634824275970459, 3.480609655380249, 2.0719778537750244, 3.667189598083496, 3.1869637966156006, 3.8300676345825195, 3.3811912536621094, 3.196678638458252, 3.923794984817505, 2.136132001876831, 3.959622621536255, 3.15621018409729, 2.7991037368774414]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3921899795532227, 2.373987913131714, 2.848013401031494, 2.4283742904663086, 2.811610221862793, 2.063091278076172, 1.5750333070755005, 2.1192424297332764, 2.64400577545166, 1.9764316082000732, 1.350341796875, 1.9434962272644043, 2.427046775817871, 2.6764094829559326, 2.6433959007263184, 2.0008254051208496]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5766658782958984, 3.4111692905426025, 3.0596301555633545, 3.11592698097229, 3.0488810539245605, 2.8417983055114746, 2.7987937927246094, 2.0636074542999268, 3.2781009674072266, 2.949636459350586, 2.758789300918579, 1.3807086944580078, 2.931657552719116, 2.767261505126953, 3.039191722869873, 2.063753843307495]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9276726245880127, 1.8440485000610352, 2.6437599658966064, 2.5977394580841064, 2.630342721939087, 1.9912374019622803, 1.7810297012329102, 3.1026570796966553, 0.5824114084243774, 2.1803925037384033, 1.002139687538147, 2.457960367202759, 2.7542521953582764, 2.9323620796203613, 2.9629673957824707, 1.4320169687271118]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.694730520248413, 3.9729719161987305, 5.574378967285156, 4.961910247802734, 5.522400856018066, 5.691357135772705, 5.14244270324707, 5.127090930938721, 6.166543006896973, 5.06516170501709, 5.780836582183838, 6.087712287902832, 4.849496364593506, 5.415304183959961, 4.3294548988342285, 4.976438045501709]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.216878890991211, 4.562976360321045, 5.574550628662109, 5.174947261810303, 3.7102231979370117, 4.472550392150879, 4.889446258544922, 5.311790943145752, 5.370660781860352, 4.926623821258545, 4.280412673950195, 3.3602657318115234, 4.4138383865356445, 3.3906302452087402, 4.929145336151123, 4.502751350402832]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.884231090545654, 5.847613334655762, 6.03434419631958, 5.633117198944092, 6.074347496032715, 4.8239922523498535, 5.690843105316162, 5.6471967697143555, 5.313620567321777, 5.36042594909668, 6.2126030921936035, 5.899213790893555, 4.941054344177246, 5.861502170562744, 6.049996852874756, 5.86820650100708]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Running loglikelihood requests:  10%|█████▏                                           | 21/200 [00:54<06:51,  2.30s/it]Layer: gate_20 - Captured router_logits: [2.988460063934326, 2.9774463176727295, 2.815286159515381, 2.254704713821411, -0.41260623931884766, 3.0426602363586426, 2.7247815132141113, 1.8598904609680176, 2.7210261821746826, 3.469024181365967, 2.6861321926116943, 3.2443346977233887, 2.8639965057373047, 2.9538605213165283, 2.3432679176330566, 1.748278260231018]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.5420756340026855, 6.813107490539551, 6.531959056854248, 6.301349639892578, 6.457118511199951, 6.476608753204346, 6.74259090423584, 6.620138168334961, 6.6363091468811035, 6.32454252243042, 6.918220520019531, 6.253806114196777, 5.283473968505859, 7.191431522369385, 6.4789204597473145, 6.432450771331787]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.007978916168213, 5.348025798797607, 4.955392837524414, 4.046980857849121, 4.904416561126709, 5.246303558349609, 4.6325764656066895, 4.780831813812256, 5.069954872131348, 4.847264766693115, 5.263558864593506, 4.878478050231934, 5.14023494720459, 5.434967994689941, 4.59780216217041, 5.483286380767822]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.135828971862793, 4.661286354064941, 3.337106227874756, 4.191516399383545, 4.446341514587402, 4.071772575378418, 4.239979267120361, 4.2307658195495605, 4.356462478637695, 4.460977077484131, 4.052478790283203, 4.423861026763916, 3.6725122928619385, 2.027414560317993, 3.377448081970215, 4.018921852111816]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.283130168914795, 6.240460395812988, 5.910166263580322, 6.0830607414245605, 5.42788553237915, 5.8549394607543945, 6.178925037384033, 5.708827972412109, 5.351722717285156, 5.761343955993652, 6.446530342102051, 5.5294904708862305, 5.767359256744385, 6.211180210113525, 6.23072624206543, 5.79047966003418]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.837101936340332, 3.6721997261047363, 3.788691520690918, 3.9488234519958496, 2.6918866634368896, 3.8602449893951416, 3.9406545162200928, 3.655817985534668, 3.9914774894714355, 3.6144893169403076, 3.695493459701538, 3.6222431659698486, 3.6638317108154297, 3.602374315261841, 3.7027931213378906, 3.8956398963928223]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.45035719871521, 3.522731304168701, 3.409107208251953, 3.555691957473755, 3.3863871097564697, 3.615396738052368, 3.570805072784424, 3.3662638664245605, 3.597465753555298, 3.2929434776306152, 3.5981810092926025, 3.448714017868042, 3.437255620956421, 2.729607105255127, 3.645244836807251, 3.5595173835754395]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1782760620117188, 3.2065072059631348, 2.9859871864318848, 2.894874334335327, 2.818953037261963, 3.0843935012817383, 3.1797127723693848, 3.277022361755371, 3.034729480743408, 3.006093978881836, 2.2882072925567627, 2.998548984527588, 3.1524171829223633, 3.058932065963745, 3.0870120525360107, 2.885542869567871]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.376075267791748, 4.491207122802734, 4.572820663452148, 4.3822431564331055, 4.499981880187988, 4.468155860900879, 4.222405910491943, 4.714170932769775, 4.497096538543701, 4.467108726501465, 4.476994037628174, 4.060590744018555, 4.202202320098877, 4.063048362731934, 4.5848894119262695, 4.377610206604004]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.586860656738281, 8.904488563537598, 8.79620361328125, 8.475537300109863, 8.554757118225098, 8.26546859741211, 8.637401580810547, 8.544979095458984, 8.635641098022461, 8.498828887939453, 8.602560043334961, 8.233598709106445, 8.329119682312012, 9.009172439575195, 8.899290084838867, 8.510181427001953]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.885571002960205, 5.423440933227539, 5.385344505310059, 4.785229206085205, 5.134278774261475, 5.090728759765625, 4.662773132324219, 5.126058101654053, 5.232753276824951, 4.921080112457275, 4.83338737487793, 4.7245917320251465, 4.860083103179932, 4.981572151184082, 5.061995029449463, 4.830029010772705]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2379794120788574, 3.0054399967193604, 3.1018850803375244, 2.7640891075134277, 3.023638963699341, 3.3161885738372803, 2.757204532623291, 3.1856343746185303, 3.089493989944458, 3.1884169578552246, 3.1752572059631348, 3.2577059268951416, 3.0339579582214355, 2.9849212169647217, 3.120037317276001, 3.1824920177459717]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12187989056110382, 0.13708685338497162, 0.1310681253671646, -0.2530871629714966, -0.2530699074268341, -0.1004360243678093, 0.1446378529071808, -0.17165766656398773, 0.09702987968921661, 0.11706316471099854, 0.11725188791751862, 0.07617126405239105, 0.10656778514385223, 0.12611611187458038, -1.1390520334243774, 0.1377812772989273]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0891382098197937, 0.05080242455005646, 0.04211897403001785, 0.060527365654706955, 0.08443769812583923, 0.03405104577541351, 0.052093781530857086, 0.080191969871521, 0.0220867320895195, 0.06289657205343246, -0.1849566549062729, 0.06310325860977173, 0.0012603825889527798, -0.008119218051433563, 0.020848587155342102, 0.030037006363272667]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07312706112861633, 0.041310712695121765, 0.0895979031920433, 0.050868112593889236, 0.09336818754673004, 0.10360366851091385, 0.05816939100623131, -0.1476346254348755, 0.06344819813966751, 0.08795060217380524, 0.0037000481970608234, 0.07630524784326553, -0.22128808498382568, -0.008910325355827808, -0.06201649829745293, 0.12050376087427139]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21547140181064606, 0.14914122223854065, 0.12214801460504532, 0.1397348940372467, 0.0443224236369133, 0.05425860360264778, -0.09289835393428802, 0.175059974193573, 0.15371136367321014, -0.44850996136665344, 0.02924216538667679, 0.1579694002866745, 0.0955810472369194, -0.291505366563797, -0.07020731270313263, -0.048790186643600464]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07506629824638367, -0.03277257829904556, -0.06620226800441742, 0.14704081416130066, -0.06501003354787827, -0.15082594752311707, 0.08825823664665222, 0.0484214685857296, 0.0640387162566185, 0.20521588623523712, -0.29210612177848816, 0.026098264381289482, 0.01693582907319069, -0.19029848277568817, 0.08315642178058624, 0.022061791270971298]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21239617466926575, 0.24138213694095612, 0.14564931392669678, 0.06010650470852852, -0.3125818371772766, -0.06118239834904671, 0.0928623229265213, 0.0016942352522164583, 0.17026686668395996, -0.05855298787355423, -0.5117501020431519, 0.05336981639266014, -0.33973997831344604, 0.2652660310268402, 0.15437179803848267, 0.15650460124015808]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3142780065536499, 0.32826822996139526, 0.196109801530838, 0.265798419713974, 0.2752508223056793, 0.24517180025577545, 0.11506012827157974, 0.14709481596946716, -0.40169844031333923, 0.3903123140335083, -0.09295657277107239, 0.3121289908885956, 0.0948934406042099, -0.08162142336368561, 0.2934326231479645, 0.18271830677986145]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05185617133975029, 0.19598540663719177, 0.11371854692697525, 0.1594514101743698, -0.3627677857875824, 0.20008133351802826, 0.05810534954071045, 0.19692054390907288, -0.026422278955578804, -0.17351432144641876, 0.05609545856714249, -0.06679033488035202, 0.166377454996109, 0.10884572565555573, -0.03324078768491745, 0.14202134311199188]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21267977356910706, 0.283353328704834, -0.10069217532873154, 0.3271404206752777, 0.46353068947792053, 0.398347407579422, 0.1144138053059578, -0.017255982384085655, 0.6288360357284546, -0.1816604882478714, 0.2550673186779022, 0.05278821289539337, 0.6788974404335022, -0.21942761540412903, -0.03694632276892662, 0.1926470547914505]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8601876497268677, 0.7493993043899536, 0.6216074824333191, 0.5835614800453186, 0.8343794345855713, 0.43162116408348083, 1.098776936531067, 1.0785752534866333, 0.08406171202659607, 0.013564097695052624, 0.4159422218799591, 0.7937706708908081, 0.7325507998466492, 0.8131308555603027, 1.1228344440460205, 0.7871699929237366]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1446754932403564, 1.1695411205291748, 0.6497524380683899, 1.0420992374420166, 0.946831464767456, 0.3871757388114929, 0.9864493608474731, 1.426737904548645, 0.7591521143913269, 0.2823268473148346, 1.0957075357437134, 1.1068345308303833, 0.6042107343673706, 1.098876714706421, 0.5026798248291016, 0.9912713170051575]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.237123727798462, 1.606224775314331, 1.529443383216858, 1.419471263885498, 1.4172965288162231, 0.8119856119155884, 1.7537578344345093, 1.216549277305603, 1.4854416847229004, 1.4607160091400146, 1.2459101676940918, 1.1597858667373657, 1.1266905069351196, 1.3213196992874146, 1.1964517831802368, 1.1888371706008911]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0340551137924194, 1.4999762773513794, 1.6536537408828735, 1.7962578535079956, 1.2423100471496582, 1.5875604152679443, 1.3007014989852905, 1.5354362726211548, 1.3455222845077515, 0.6095336079597473, 1.7063308954238892, 1.956127643585205, 1.7569340467453003, 1.6750649213790894, 1.4210044145584106, 1.528359293937683]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0685389041900635, 3.870182991027832, 3.026961326599121, 2.6355643272399902, 3.4843709468841553, 2.0672104358673096, 3.6644818782806396, 3.181924819946289, 3.831368923187256, 3.378145933151245, 3.196512460708618, 3.9223575592041016, 2.1224234104156494, 3.955761671066284, 3.1576356887817383, 2.7930104732513428]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3869998455047607, 2.369783878326416, 2.8463919162750244, 2.4278292655944824, 2.8063125610351562, 2.0609118938446045, 1.5646419525146484, 2.1133522987365723, 2.640618085861206, 1.9756298065185547, 1.3388983011245728, 1.9331496953964233, 2.422382354736328, 2.6748244762420654, 2.6421239376068115, 1.9926292896270752]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.563494920730591, 3.4096603393554688, 3.052149772644043, 3.114881992340088, 3.0353808403015137, 2.8370165824890137, 2.7916953563690186, 2.0573012828826904, 3.276838779449463, 2.942317485809326, 2.753652572631836, 1.363981008529663, 2.923431634902954, 2.7600622177124023, 3.034351110458374, 2.059922933578491]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9173663854599, 1.8385134935379028, 2.6378867626190186, 2.5921247005462646, 2.6262404918670654, 1.9855519533157349, 1.7832841873168945, 3.0981428623199463, 0.5671689510345459, 2.1787798404693604, 0.985832691192627, 2.4513731002807617, 2.746180772781372, 2.9261815547943115, 2.9548637866973877, 1.4321165084838867]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6770412921905518, 3.952672004699707, 5.560956001281738, 4.943512439727783, 5.511475563049316, 5.676463603973389, 5.13115930557251, 5.110562801361084, 6.154101371765137, 5.051670551300049, 5.770120620727539, 6.073449611663818, 4.833425045013428, 5.40542459487915, 4.305497646331787, 4.959516525268555]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.207579135894775, 4.554708480834961, 5.560372352600098, 5.16973352432251, 3.699317693710327, 4.46162748336792, 4.880599021911621, 5.2987446784973145, 5.363675594329834, 4.916167259216309, 4.265906810760498, 3.3504533767700195, 4.406236171722412, 3.3819055557250977, 4.9204816818237305, 4.495957374572754]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.870613098144531, 5.836475849151611, 6.024412631988525, 5.621338367462158, 6.063928604125977, 4.8061699867248535, 5.676555156707764, 5.631586074829102, 5.301445007324219, 5.347525596618652, 6.204037189483643, 5.890711784362793, 4.931896209716797, 5.849310398101807, 6.037713050842285, 5.857204914093018]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.988659620285034, 2.9763031005859375, 2.8136680126190186, 2.250885486602783, -0.42651695013046265, 3.0425078868865967, 2.719285011291504, 1.8587477207183838, 2.7186496257781982, 3.4669880867004395, 2.685032606124878, 3.2441959381103516, 2.8615660667419434, 2.950363874435425, 2.338007688522339, 1.7436416149139404]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.5347900390625, 6.804453372955322, 6.523601531982422, 6.292821884155273, 6.447295188903809, 6.469111919403076, 6.733175754547119, 6.61713981628418, 6.628985404968262, 6.3133111000061035, 6.9108357429504395, 6.244930267333984, 5.270698547363281, 7.183775424957275, 6.470264911651611, 6.423941612243652]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.003177642822266, 5.344742774963379, 4.9524359703063965, 4.041181564331055, 4.89667272567749, 5.241186141967773, 4.625913619995117, 4.774411201477051, 5.0643744468688965, 4.842405796051025, 5.259932041168213, 4.8717942237854, 5.136185169219971, 5.428892612457275, 4.591587066650391, 5.474941253662109]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.135905742645264, 4.661194801330566, 3.334662437438965, 4.189249038696289, 4.444555282592773, 4.07098913192749, 4.239029884338379, 4.22865629196167, 4.355087757110596, 4.462262153625488, 4.051373481750488, 4.423277854919434, 3.670295000076294, 2.0211055278778076, 3.3765082359313965, 4.014969348907471]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  12%|██████▏                                          | 25/200 [01:03<06:32,  2.24s/it]Layer: gate_24 - Captured router_logits: [6.2792158126831055, 6.235287189483643, 5.906469345092773, 6.078588008880615, 5.425717830657959, 5.847485065460205, 6.174665927886963, 5.703813552856445, 5.346587657928467, 5.755130767822266, 6.438487529754639, 5.521300315856934, 5.764396667480469, 6.20639181137085, 6.22496223449707, 5.783642768859863]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8338816165924072, 3.6692662239074707, 3.7852542400360107, 3.9438626766204834, 2.686885118484497, 3.8586347103118896, 3.9368207454681396, 3.6517794132232666, 3.9885079860687256, 3.610837697982788, 3.694610834121704, 3.6177399158477783, 3.6601362228393555, 3.5979535579681396, 3.6995601654052734, 3.8922226428985596]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.449983596801758, 3.5215063095092773, 3.4052207469940186, 3.5542285442352295, 3.38478946685791, 3.616380214691162, 3.570976972579956, 3.3668272495269775, 3.597707509994507, 3.2934482097625732, 3.5991592407226562, 3.4476828575134277, 3.4354395866394043, 2.7276365756988525, 3.6453440189361572, 3.5607457160949707]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1800262928009033, 3.2062506675720215, 2.9855339527130127, 2.896040439605713, 2.8199243545532227, 3.085155487060547, 3.1796557903289795, 3.276130199432373, 3.0340847969055176, 3.006922960281372, 2.286930561065674, 2.998260021209717, 3.154402732849121, 3.0612683296203613, 3.0866076946258545, 2.885457754135132]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.375927925109863, 4.492072582244873, 4.573040008544922, 4.381078243255615, 4.4989542961120605, 4.467337608337402, 4.220914363861084, 4.712264060974121, 4.496531963348389, 4.465075492858887, 4.475537300109863, 4.058447360992432, 4.203498363494873, 4.061923503875732, 4.5838422775268555, 4.378976345062256]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.58785343170166, 8.909586906433105, 8.793290138244629, 8.4778413772583, 8.55980110168457, 8.270951271057129, 8.641181945800781, 8.549127578735352, 8.636836051940918, 8.500840187072754, 8.606072425842285, 8.238563537597656, 8.334362983703613, 9.009603500366211, 8.90341854095459, 8.51406478881836]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.886964797973633, 5.425007343292236, 5.386258602142334, 4.785145282745361, 5.136070251464844, 5.09230899810791, 4.665954113006592, 5.129283428192139, 5.236013412475586, 4.924064636230469, 4.837339878082275, 4.727653980255127, 4.863011360168457, 4.985002517700195, 5.0651535987854, 4.831528663635254]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2336740493774414, 3.0018622875213623, 3.0986547470092773, 2.764087200164795, 3.0210704803466797, 3.309790849685669, 2.7574782371520996, 3.181518316268921, 3.0861711502075195, 3.1842596530914307, 3.1731936931610107, 3.2570462226867676, 3.0329840183258057, 2.9814019203186035, 3.1198952198028564, 3.179715394973755]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12444891035556793, 0.1397022008895874, 0.13209028542041779, -0.2494547814130783, -0.24654953181743622, -0.10410328954458237, 0.1458864063024521, -0.1806442141532898, 0.09829378128051758, 0.11931182444095612, 0.11987495422363281, 0.07793337106704712, 0.10898634791374207, 0.12826576828956604, -1.1478476524353027, 0.1399848312139511]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0898858904838562, 0.052585721015930176, 0.04350142553448677, 0.06407572329044342, 0.08540388941764832, 0.03562222793698311, 0.052819348871707916, 0.07999458909034729, 0.021433217450976372, 0.0638277679681778, -0.17924216389656067, 0.05964803695678711, -0.0018199997721239924, -0.0030518958810716867, 0.017268428578972816, 0.027300728484988213]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07195831835269928, 0.04288586974143982, 0.09206113964319229, 0.0513828843832016, 0.09408746659755707, 0.10541961342096329, 0.05944342538714409, -0.1446532905101776, 0.06287539750337601, 0.08604194968938828, -0.0018114666454494, 0.07635661214590073, -0.21086321771144867, -0.008545070886611938, -0.05723800137639046, 0.12244739383459091]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.2167094647884369, 0.1503007560968399, 0.12484435737133026, 0.15092462301254272, 0.0479867048561573, 0.05439535155892372, -0.08749359846115112, 0.1791166365146637, 0.15323713421821594, -0.4384642243385315, 0.02682388946413994, 0.1588582992553711, 0.09108573198318481, -0.2804781496524811, -0.07802213728427887, -0.048974279314279556]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07625259459018707, -0.032336555421352386, -0.06847254931926727, 0.1429934799671173, -0.06338264793157578, -0.14868824183940887, 0.08934814482927322, 0.05220651254057884, 0.06216143071651459, 0.2005014270544052, -0.2841048240661621, 0.024518603459000587, 0.01134162675589323, -0.18179179728031158, 0.08095759898424149, 0.02110769972205162]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21389015018939972, 0.24471823871135712, 0.14775577187538147, 0.05882149562239647, -0.3147668242454529, -0.05723005160689354, 0.09306070953607559, 0.0004892201977781951, 0.169678196310997, -0.05640082806348801, -0.5195041298866272, 0.0559881292283535, -0.33686497807502747, 0.26557812094688416, 0.15506896376609802, 0.15950727462768555]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3156494200229645, 0.3313479423522949, 0.19960208237171173, 0.26532018184661865, 0.2773776948451996, 0.24481917917728424, 0.11806920170783997, 0.15347275137901306, -0.4027494490146637, 0.3917521834373474, -0.08926063030958176, 0.31315699219703674, 0.09859777241945267, -0.08063250035047531, 0.29271775484085083, 0.1813880354166031]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05223478749394417, 0.19812753796577454, 0.11680037528276443, 0.1596411168575287, -0.3632713854312897, 0.2003955990076065, 0.060053348541259766, 0.2018120139837265, -0.027045302093029022, -0.17004244029521942, 0.05628426745533943, -0.06559403240680695, 0.16626952588558197, 0.10984797775745392, -0.03238634765148163, 0.14026854932308197]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2161705195903778, 0.2852630317211151, -0.09901650249958038, 0.3283398151397705, 0.4651568531990051, 0.4005740284919739, 0.11707410216331482, -0.019816434010863304, 0.6295353770256042, -0.18128427863121033, 0.25607675313949585, 0.052311524748802185, 0.6796689033508301, -0.21854236721992493, -0.03883735463023186, 0.1942308098077774]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8619297742843628, 0.7518681883811951, 0.6230442523956299, 0.5837379097938538, 0.8339013457298279, 0.43289849162101746, 1.1020433902740479, 1.0799823999404907, 0.08666209131479263, 0.01733560301363468, 0.41731691360473633, 0.7960910201072693, 0.7336650490760803, 0.814159631729126, 1.1238694190979004, 0.7879048585891724]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1443301439285278, 1.1682358980178833, 0.6492967009544373, 1.04085111618042, 0.9477823972702026, 0.3865150809288025, 0.9852510094642639, 1.4257107973098755, 0.7600739002227783, 0.28439202904701233, 1.095299482345581, 1.1051288843154907, 0.6083386540412903, 1.0957781076431274, 0.500906229019165, 0.9924960136413574]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2384343147277832, 1.6065223217010498, 1.5284265279769897, 1.4204596281051636, 1.4177072048187256, 0.8121942281723022, 1.7535231113433838, 1.2178152799606323, 1.4850711822509766, 1.4598108530044556, 1.2475779056549072, 1.1627944707870483, 1.12742280960083, 1.3245275020599365, 1.2007843255996704, 1.1886780261993408]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.032771348953247, 1.500641107559204, 1.6532748937606812, 1.7932124137878418, 1.2438148260116577, 1.5874319076538086, 1.300423264503479, 1.532943606376648, 1.3431239128112793, 0.6069517135620117, 1.7050654888153076, 1.9529227018356323, 1.7574048042297363, 1.674527883529663, 1.4220713376998901, 1.527195930480957]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0689504146575928, 3.869173765182495, 3.0289769172668457, 2.6369025707244873, 3.4838812351226807, 2.066697597503662, 3.6653335094451904, 3.181394100189209, 3.83012056350708, 3.379145860671997, 3.1962835788726807, 3.9226698875427246, 2.127429723739624, 3.9579992294311523, 3.1563353538513184, 2.79553484916687]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.391857624053955, 2.3684208393096924, 2.8486242294311523, 2.43241548538208, 2.807675838470459, 2.066446304321289, 1.5710663795471191, 2.11244535446167, 2.641193151473999, 1.9777978658676147, 1.3431674242019653, 1.9374680519104004, 2.423208475112915, 2.6759703159332275, 2.6422128677368164, 1.9950966835021973]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.566293478012085, 3.4083127975463867, 3.0544426441192627, 3.1146676540374756, 3.040900945663452, 2.837878942489624, 2.795135021209717, 2.0595407485961914, 3.2775278091430664, 2.943387746810913, 2.7573347091674805, 1.367909550666809, 2.9255776405334473, 2.761754274368286, 3.037444591522217, 2.061906576156616]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.918351173400879, 1.8441064357757568, 2.6380832195281982, 2.593223810195923, 2.6259312629699707, 1.984473705291748, 1.7810908555984497, 3.0956976413726807, 0.5686099529266357, 2.1752047538757324, 0.9881811141967773, 2.451962947845459, 2.746551752090454, 2.927049160003662, 2.956569194793701, 1.4331235885620117]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.683169364929199, 3.9571471214294434, 5.564034461975098, 4.945314407348633, 5.517773628234863, 5.678130626678467, 5.1332197189331055, 5.115229606628418, 6.157525539398193, 5.051753997802734, 5.773418426513672, 6.078281402587891, 4.838047981262207, 5.409596920013428, 4.31087589263916, 4.960926055908203]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.209445476531982, 4.559581756591797, 5.56176233291626, 5.171982765197754, 3.7027814388275146, 4.4663848876953125, 4.882822036743164, 5.300139427185059, 5.365999698638916, 4.917759895324707, 4.2692389488220215, 3.3509225845336914, 4.409414768218994, 3.3828747272491455, 4.920943737030029, 4.496459007263184]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.875306606292725, 5.838309288024902, 6.028567314147949, 5.624887943267822, 6.067615032196045, 4.811304092407227, 5.681457042694092, 5.635992050170898, 5.304040908813477, 5.352870464324951, 6.207401275634766, 5.89388370513916, 4.93379545211792, 5.852504253387451, 6.041388511657715, 5.860947132110596]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.9894323348999023, 2.9740936756134033, 2.8122549057006836, 2.2480111122131348, -0.4255034625530243, 3.0410938262939453, 2.719594717025757, 1.8588848114013672, 2.71891450881958, 3.4672343730926514, 2.6846656799316406, 3.2426071166992188, 2.8587024211883545, 2.947545051574707, 2.3341610431671143, 1.741892695426941]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.5334248542785645, 6.8036274909973145, 6.5243377685546875, 6.292886734008789, 6.447178363800049, 6.468493461608887, 6.731937885284424, 6.617153167724609, 6.627546787261963, 6.312210559844971, 6.910190582275391, 6.245029926300049, 5.268625259399414, 7.1849446296691895, 6.469458103179932, 6.424123287200928]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.003304481506348, 5.344249725341797, 4.95345401763916, 4.042922496795654, 4.896960258483887, 5.240808963775635, 4.625154972076416, 4.775326728820801, 5.06591272354126, 4.840728759765625, 5.260778427124023, 4.871351718902588, 5.136187553405762, 5.428464889526367, 4.590281009674072, 5.473046779632568]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.133296012878418, 4.659017086029053, 3.330852508544922, 4.189899921417236, 4.4428839683532715, 4.0687174797058105, 4.237607002258301, 4.2275309562683105, 4.3532395362854, 4.460224151611328, 4.048491477966309, 4.4208807945251465, 3.667264938354492, 2.020049810409546, 3.3726630210876465, 4.013391971588135]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.280622482299805, 6.235865592956543, 5.907733917236328, 6.078030586242676, 5.42573881149292, 5.848051071166992, 6.174209117889404, 5.704333305358887, 5.347309112548828, 5.754901885986328, 6.438733100891113, 5.519420623779297, 5.7644572257995605, 6.206589221954346, 6.224360942840576, 5.783700466156006]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.834120512008667, 3.668875217437744, 3.784724712371826, 3.9442379474639893, 2.6868958473205566, 3.858203172683716, 3.938133716583252, 3.652865409851074, 3.989190101623535, 3.6118874549865723, 3.6953628063201904, 3.6180596351623535, 3.660932779312134, 3.599550247192383, 3.6999661922454834, 3.8931405544281006]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4486308097839355, 3.520968198776245, 3.403883457183838, 3.5531797409057617, 3.3840136528015137, 3.6162266731262207, 3.5709989070892334, 3.3668453693389893, 3.5965349674224854, 3.2931013107299805, 3.598405122756958, 3.446265935897827, 3.433840036392212, 2.7262399196624756, 3.6453473567962646, 3.55985951423645]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1802871227264404, 3.2069807052612305, 2.985590934753418, 2.8966002464294434, 2.8196861743927, 3.084298610687256, 3.1803085803985596, 3.275278329849243, 3.0339505672454834, 3.005572557449341, 2.2866287231445312, 2.9981651306152344, 3.1548476219177246, 3.0612540245056152, 3.0857622623443604, 2.8857076168060303]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  14%|███████                                          | 29/200 [01:12<06:26,  2.26s/it]Layer: gate_28 - Captured router_logits: [4.377824783325195, 4.493122577667236, 4.573306560516357, 4.380980014801025, 4.496938705444336, 4.468631267547607, 4.2220048904418945, 4.7137675285339355, 4.497938632965088, 4.4654927253723145, 4.476185321807861, 4.059328556060791, 4.203831195831299, 4.062432289123535, 4.5844950675964355, 4.379216194152832]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.593979835510254, 8.916499137878418, 8.80030345916748, 8.486493110656738, 8.567939758300781, 8.278740882873535, 8.6478271484375, 8.554743766784668, 8.642219543457031, 8.507314682006836, 8.6126070022583, 8.244430541992188, 8.341434478759766, 9.014933586120605, 8.91010570526123, 8.519604682922363]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.8897223472595215, 5.427092552185059, 5.3867716789245605, 4.785930156707764, 5.136054515838623, 5.092311382293701, 4.668449401855469, 5.131723880767822, 5.237854957580566, 4.926299095153809, 4.838726043701172, 4.72839879989624, 4.86461067199707, 4.98554801940918, 5.067160606384277, 4.832797527313232]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2354137897491455, 3.0032355785369873, 3.100707530975342, 2.764660596847534, 3.0235023498535156, 3.3105556964874268, 2.7582690715789795, 3.181952953338623, 3.087049961090088, 3.1858692169189453, 3.1755285263061523, 3.2593722343444824, 3.0351576805114746, 2.983414888381958, 3.1222567558288574, 3.1811370849609375]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11540547013282776, 0.1298983246088028, 0.1267586648464203, -0.2529463469982147, -0.2565954625606537, -0.09161372482776642, 0.13613629341125488, -0.15331624448299408, 0.0969451367855072, 0.11008003354072571, 0.10947078466415405, 0.06674924492835999, 0.10226574540138245, 0.12393874675035477, -1.1157710552215576, 0.1339191347360611]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08624149858951569, 0.04493187367916107, 0.039671048521995544, 0.06213952600955963, 0.0806099995970726, 0.033069927245378494, 0.05111139267683029, 0.0836869478225708, 0.02938578464090824, 0.06080775707960129, -0.18348564207553864, 0.06203785911202431, -0.0006645471439696848, -0.004504386335611343, 0.02323884144425392, 0.03011518530547619]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07037353515625, 0.04108448326587677, 0.08898428827524185, 0.05123775452375412, 0.10005681216716766, 0.09809581190347672, 0.057920437306165695, -0.142610564827919, 0.06362473964691162, 0.09326459467411041, 0.005295733921229839, 0.07742617279291153, -0.22046113014221191, -0.01086693350225687, -0.054597821086645126, 0.1230221763253212]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.2148234099149704, 0.14420746266841888, 0.12372045964002609, 0.1431736946105957, 0.04468134418129921, 0.054114628583192825, -0.09466701000928879, 0.17568841576576233, 0.1575663536787033, -0.46042004227638245, 0.038196686655282974, 0.15417268872261047, 0.11023715138435364, -0.30515581369400024, -0.058422353118658066, -0.04393533989787102]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07481234520673752, -0.027001293376088142, -0.06937559694051743, 0.15044042468070984, -0.05894434452056885, -0.14666298031806946, 0.08594325929880142, 0.04450392723083496, 0.058165378868579865, 0.19952453672885895, -0.2997448742389679, 0.022039638832211494, 0.028280461207032204, -0.1848415583372116, 0.08858729898929596, 0.010382902808487415]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21058505773544312, 0.2424451857805252, 0.14697733521461487, 0.06046956405043602, -0.3203357458114624, -0.06088606268167496, 0.09782450646162033, 0.0020431634038686752, 0.17498517036437988, -0.05793008580803871, -0.5196971297264099, 0.05463024228811264, -0.34375786781311035, 0.26705214381217957, 0.15768010914325714, 0.1583692878484726]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.32090869545936584, 0.3337356746196747, 0.20481346547603607, 0.2666584849357605, 0.2768552303314209, 0.2486603856086731, 0.1109079122543335, 0.14790666103363037, -0.4009086787700653, 0.39539024233818054, -0.10150600969791412, 0.3123047649860382, 0.10024398565292358, -0.09166564792394638, 0.2972562313079834, 0.17963416874408722]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.047249291092157364, 0.19770856201648712, 0.11428553611040115, 0.16026118397712708, -0.3686911165714264, 0.20283406972885132, 0.061501890420913696, 0.1970745176076889, -0.017310963943600655, -0.16567930579185486, 0.05165960267186165, -0.061516355723142624, 0.16356225311756134, 0.11594448238611221, -0.044761572033166885, 0.13180100917816162]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21408401429653168, 0.2840241491794586, -0.1042751744389534, 0.3282325267791748, 0.4677688777446747, 0.399572491645813, 0.11882486194372177, -0.020572595298290253, 0.62578284740448, -0.17436380684375763, 0.25737154483795166, 0.053029585629701614, 0.6813568472862244, -0.22721819579601288, -0.04767869412899017, 0.19617827236652374]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8596826195716858, 0.741291344165802, 0.6215941905975342, 0.5906572937965393, 0.8314279913902283, 0.42218536138534546, 1.0949914455413818, 1.0761818885803223, 0.08267096430063248, 0.007131156977266073, 0.40960800647735596, 0.7952473759651184, 0.7277631163597107, 0.8000440001487732, 1.1252000331878662, 0.7878992557525635]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1441049575805664, 1.174737811088562, 0.647827684879303, 1.0395457744598389, 0.9421007633209229, 0.3706355392932892, 0.9840124249458313, 1.422371745109558, 0.7521576285362244, 0.26754364371299744, 1.09750497341156, 1.1039752960205078, 0.5940302014350891, 1.0893189907073975, 0.5046091675758362, 0.9912405610084534]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2366676330566406, 1.6081171035766602, 1.528560996055603, 1.4023526906967163, 1.415399432182312, 0.8006089925765991, 1.7538820505142212, 1.205876111984253, 1.4880681037902832, 1.464219331741333, 1.2440540790557861, 1.1562466621398926, 1.1229534149169922, 1.3157373666763306, 1.1791136264801025, 1.1793707609176636]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0253255367279053, 1.4930503368377686, 1.6560962200164795, 1.7925198078155518, 1.2327193021774292, 1.579060673713684, 1.293046474456787, 1.5341706275939941, 1.3362475633621216, 0.6032488942146301, 1.7026948928833008, 1.9515163898468018, 1.7509567737579346, 1.6687463521957397, 1.4163057804107666, 1.528773307800293]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0637505054473877, 3.8704347610473633, 3.0300192832946777, 2.6257216930389404, 3.4872779846191406, 2.0567800998687744, 3.664092540740967, 3.191042184829712, 3.8291125297546387, 3.3831186294555664, 3.19374418258667, 3.920673370361328, 2.1130638122558594, 3.945171594619751, 3.1583380699157715, 2.7821226119995117]
Running loglikelihood requests:  16%|████████                                         | 33/200 [01:20<06:10,  2.22s/it]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3948402404785156, 2.373418092727661, 2.851236343383789, 2.4317080974578857, 2.8119688034057617, 2.0716049671173096, 1.5621845722198486, 2.1245505809783936, 2.640747308731079, 1.9856798648834229, 1.3361310958862305, 1.9416576623916626, 2.4317281246185303, 2.6823766231536865, 2.647843360900879, 1.9963953495025635]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.558797836303711, 3.4080147743225098, 3.053596258163452, 3.117664337158203, 3.03129506111145, 2.83758544921875, 2.794552803039551, 2.0581846237182617, 3.2759900093078613, 2.9425735473632812, 2.7549805641174316, 1.3624200820922852, 2.921661615371704, 2.761820077896118, 3.029597759246826, 2.0603325366973877]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9204835891723633, 1.8352938890457153, 2.6389713287353516, 2.5944535732269287, 2.628648519515991, 1.9854310750961304, 1.773899793624878, 3.098074197769165, 0.5642966032028198, 2.1819489002227783, 0.9758635759353638, 2.447046995162964, 2.7508976459503174, 2.926431179046631, 2.95621395111084, 1.4248051643371582]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.655184030532837, 3.9423975944519043, 5.553236484527588, 4.9342522621154785, 5.50079870223999, 5.668338775634766, 5.130127429962158, 5.103743553161621, 6.152651309967041, 5.031606674194336, 5.763108730316162, 6.064621925354004, 4.828179836273193, 5.394303798675537, 4.290225505828857, 4.943594932556152]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.20200252532959, 4.548450946807861, 5.545573711395264, 5.164985656738281, 3.6895806789398193, 4.448518753051758, 4.871853351593018, 5.288486480712891, 5.3581461906433105, 4.908165454864502, 4.257178783416748, 3.3368489742279053, 4.394959449768066, 3.3685836791992188, 4.913806915283203, 4.489315509796143]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.865781307220459, 5.8307695388793945, 6.022928237915039, 5.617133617401123, 6.06101655960083, 4.800447940826416, 5.6697998046875, 5.62490177154541, 5.299067974090576, 5.349384784698486, 6.202221870422363, 5.887329578399658, 4.931183815002441, 5.8460211753845215, 6.03335428237915, 5.854884147644043]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.9873266220092773, 2.971623659133911, 2.809964179992676, 2.2439639568328857, -0.43795037269592285, 3.041618824005127, 2.7162058353424072, 1.8532894849777222, 2.716240406036377, 3.4662413597106934, 2.6804022789001465, 3.2456002235412598, 2.859997272491455, 2.9513142108917236, 2.328956365585327, 1.7362924814224243]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.533929824829102, 6.805105209350586, 6.520892143249512, 6.2894744873046875, 6.443589210510254, 6.46586799621582, 6.7280378341674805, 6.616215705871582, 6.627715587615967, 6.312137603759766, 6.9095916748046875, 6.243641376495361, 5.267386436462402, 7.182969093322754, 6.466777324676514, 6.421164512634277]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.003823280334473, 5.346947193145752, 4.954501628875732, 4.042703628540039, 4.898237228393555, 5.2429938316345215, 4.626059055328369, 4.777013301849365, 5.065220355987549, 4.843447208404541, 5.264523029327393, 4.872997760772705, 5.1379594802856445, 5.429312229156494, 4.590891361236572, 5.472502708435059]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.1372551918029785, 4.662525177001953, 3.3328990936279297, 4.193429470062256, 4.4471116065979, 4.073934078216553, 4.237368583679199, 4.227801322937012, 4.355912208557129, 4.464462757110596, 4.052159309387207, 4.424483776092529, 3.671088933944702, 2.023202896118164, 3.3735556602478027, 4.017805099487305]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.2803802490234375, 6.234162330627441, 5.907168388366699, 6.078140735626221, 5.426733016967773, 5.847084999084473, 6.17530632019043, 5.702847480773926, 5.346007347106934, 5.753816604614258, 6.434246063232422, 5.5208892822265625, 5.764992713928223, 6.207225322723389, 6.224059581756592, 5.785848617553711]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.833906888961792, 3.670203685760498, 3.7840750217437744, 3.944805860519409, 2.6886298656463623, 3.8588685989379883, 3.9354138374328613, 3.6540510654449463, 3.9891810417175293, 3.6125283241271973, 3.6957883834838867, 3.6200125217437744, 3.661787271499634, 3.597853183746338, 3.701683759689331, 3.8939871788024902]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4495320320129395, 3.5216481685638428, 3.40437650680542, 3.554011583328247, 3.385136842727661, 3.6171255111694336, 3.570279359817505, 3.3661651611328125, 3.5961592197418213, 3.292811632156372, 3.599477529525757, 3.44665265083313, 3.4352293014526367, 2.7272756099700928, 3.647244930267334, 3.5610849857330322]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1809029579162598, 3.207608938217163, 2.9864048957824707, 2.8976478576660156, 2.820035934448242, 3.085703134536743, 3.1813740730285645, 3.274993658065796, 3.034182071685791, 3.0077641010284424, 2.2874677181243896, 2.9994888305664062, 3.1565170288085938, 3.067993640899658, 3.084719181060791, 2.8871004581451416]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.377438068389893, 4.492872714996338, 4.573940277099609, 4.38067626953125, 4.497128486633301, 4.467341899871826, 4.2227959632873535, 4.71135950088501, 4.498614311218262, 4.4647698402404785, 4.475501537322998, 4.060503959655762, 4.203845977783203, 4.0619611740112305, 4.583694934844971, 4.3797736167907715]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.584696769714355, 8.90633487701416, 8.787240028381348, 8.473462104797363, 8.555953025817871, 8.269036293029785, 8.63620376586914, 8.548967361450195, 8.632155418395996, 8.497529029846191, 8.604493141174316, 8.233887672424316, 8.335637092590332, 9.00518798828125, 8.898181915283203, 8.50706672668457]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.883462905883789, 5.420205593109131, 5.380959987640381, 4.780966758728027, 5.133246421813965, 5.0869598388671875, 4.662258148193359, 5.123939037322998, 5.237305164337158, 4.920714378356934, 4.83302640914917, 4.723623275756836, 4.860930442810059, 4.982325553894043, 5.059901714324951, 4.828390598297119]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2287330627441406, 2.996832847595215, 3.0933029651641846, 2.7581074237823486, 3.0159788131713867, 3.303624391555786, 2.7509055137634277, 3.176815986633301, 3.080712080001831, 3.178908586502075, 3.1690449714660645, 3.253565549850464, 3.02866792678833, 2.978125810623169, 3.1193480491638184, 3.174389123916626]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11609751731157303, 0.13037976622581482, 0.1273130625486374, -0.25416508316993713, -0.2566452622413635, -0.09181933104991913, 0.13678611814975739, -0.15543445944786072, 0.09723350405693054, 0.11118046939373016, 0.11007962375879288, 0.06618035584688187, 0.10278167575597763, 0.12551644444465637, -1.1159518957138062, 0.13379448652267456]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08645959198474884, 0.047418709844350815, 0.03983791545033455, 0.06213747337460518, 0.08145762979984283, 0.03394974395632744, 0.05381331220269203, 0.08335218578577042, 0.03021366335451603, 0.061265744268894196, -0.18230435252189636, 0.06137753650546074, -0.0008321922505274415, -0.003727205330505967, 0.023710204288363457, 0.029703032225370407]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06982342898845673, 0.039838649332523346, 0.09019487351179123, 0.05164548009634018, 0.10182711482048035, 0.1003541424870491, 0.05744322016835213, -0.1413060575723648, 0.06781898438930511, 0.09449039399623871, 0.00454848213121295, 0.07570251077413559, -0.21826007962226868, -0.012930654920637608, -0.054015323519706726, 0.12302319705486298]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21501001715660095, 0.14560692012310028, 0.12481158971786499, 0.1435718983411789, 0.04741840809583664, 0.054508257657289505, -0.09330558776855469, 0.17454633116722107, 0.1592605710029602, -0.46368908882141113, 0.039272554218769073, 0.15492519736289978, 0.10708500444889069, -0.30141985416412354, -0.05675910413265228, -0.04370354861021042]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07429246604442596, -0.027391821146011353, -0.06995982676744461, 0.15110623836517334, -0.05834117531776428, -0.1468498259782791, 0.08672968298196793, 0.046241115778684616, 0.056528251618146896, 0.1975245475769043, -0.29674839973449707, 0.0200295839458704, 0.024253906682133675, -0.18191222846508026, 0.08656772971153259, 0.01257342379540205]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.20822623372077942, 0.2421986311674118, 0.1485765129327774, 0.061470381915569305, -0.3205747902393341, -0.0621742308139801, 0.09688246995210648, 0.0022139609791338444, 0.17394933104515076, -0.0579664520919323, -0.5201972723007202, 0.05520898848772049, -0.343418151140213, 0.2675451338291168, 0.1568402498960495, 0.15855829417705536]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3223685026168823, 0.3346706032752991, 0.20491783320903778, 0.2668313682079315, 0.2778145670890808, 0.24902302026748657, 0.10834627598524094, 0.14710302650928497, -0.40027379989624023, 0.39656129479408264, -0.10183029621839523, 0.31253740191459656, 0.10130114108324051, -0.09248431771993637, 0.2959252893924713, 0.17981553077697754]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.045570459216833115, 0.19766895473003387, 0.11526589840650558, 0.16082563996315002, -0.36905863881111145, 0.20362304151058197, 0.06357014924287796, 0.19485771656036377, -0.01624739170074463, -0.16513821482658386, 0.0493670254945755, -0.06138280779123306, 0.16183865070343018, 0.11735939234495163, -0.04834745079278946, 0.1297338455915451]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21378442645072937, 0.2843095362186432, -0.10458893328905106, 0.32892847061157227, 0.4683501124382019, 0.40012869238853455, 0.1183428168296814, -0.02119387313723564, 0.6273658871650696, -0.17230309545993805, 0.2587982416152954, 0.051411762833595276, 0.6820132732391357, -0.22893302142620087, -0.04980127885937691, 0.1967143565416336]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8594727516174316, 0.7402105927467346, 0.6205955743789673, 0.5907805562019348, 0.8294529914855957, 0.4218536913394928, 1.0941262245178223, 1.0762088298797607, 0.08473093062639236, 0.008661346510052681, 0.4073620140552521, 0.7943079471588135, 0.7274119853973389, 0.7965388894081116, 1.1242139339447021, 0.7875629663467407]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.143215537071228, 1.1729563474655151, 0.6454929113388062, 1.0384646654129028, 0.9414340257644653, 0.36897042393684387, 0.9824375510215759, 1.4208201169967651, 0.7485619783401489, 0.2665700614452362, 1.0962971448898315, 1.1025670766830444, 0.591870903968811, 1.0867290496826172, 0.5066152811050415, 0.9889283180236816]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2364885807037354, 1.6043264865875244, 1.5264464616775513, 1.4007532596588135, 1.4129449129104614, 0.7997889518737793, 1.7505353689193726, 1.2027071714401245, 1.4857813119888306, 1.4638381004333496, 1.2420905828475952, 1.152957558631897, 1.1183040142059326, 1.31344735622406, 1.1745080947875977, 1.1756870746612549]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0226343870162964, 1.491573452949524, 1.653541922569275, 1.7910715341567993, 1.2306257486343384, 1.577041745185852, 1.2938319444656372, 1.5329501628875732, 1.3338431119918823, 0.6034324169158936, 1.702169418334961, 1.950505018234253, 1.7481998205184937, 1.6660516262054443, 1.4138798713684082, 1.5273633003234863]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0624680519104004, 3.8701579570770264, 3.0302629470825195, 2.62436842918396, 3.486786365509033, 2.056703567504883, 3.6634202003479004, 3.192702054977417, 3.8273582458496094, 3.3813681602478027, 3.192107915878296, 3.920644521713257, 2.1122140884399414, 3.943631649017334, 3.1556591987609863, 2.7791192531585693]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.392592668533325, 2.3718326091766357, 2.849917411804199, 2.430744171142578, 2.811601161956787, 2.0708231925964355, 1.5593323707580566, 2.12611722946167, 2.6399314403533936, 1.9834727048873901, 1.3321590423583984, 1.939062476158142, 2.429626226425171, 2.681143045425415, 2.6457631587982178, 1.9937174320220947]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5578508377075195, 3.4083597660064697, 3.0529701709747314, 3.1172571182250977, 3.0319695472717285, 2.8372628688812256, 2.7967543601989746, 2.0579893589019775, 3.2763352394104004, 2.9425435066223145, 2.754140853881836, 1.3632817268371582, 2.9223172664642334, 2.760666608810425, 3.029170513153076, 2.0613794326782227]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9154196977615356, 1.8337267637252808, 2.635735273361206, 2.591764450073242, 2.625430107116699, 1.9824950695037842, 1.7742013931274414, 3.094590187072754, 0.5636717677116394, 2.178741931915283, 0.9724043011665344, 2.445876359939575, 2.7483160495758057, 2.922346591949463, 2.952636480331421, 1.4231657981872559]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6568920612335205, 3.943359613418579, 5.551528453826904, 4.931775093078613, 5.500768184661865, 5.666529178619385, 5.127655506134033, 5.101856231689453, 6.149112224578857, 5.030356407165527, 5.762136936187744, 6.062760353088379, 4.8274993896484375, 5.393741130828857, 4.2879157066345215, 4.942001819610596]
Running loglikelihood requests:  18%|█████████                                        | 37/200 [01:29<05:58,  2.20s/it]Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.201373100280762, 4.549182891845703, 5.544651508331299, 5.164980411529541, 3.6899337768554688, 4.44761323928833, 4.873083591461182, 5.2881317138671875, 5.3578362464904785, 4.908259391784668, 4.256601333618164, 3.336656332015991, 4.3947038650512695, 3.3686087131500244, 4.912102222442627, 4.489497661590576]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.863041400909424, 5.828022480010986, 6.021071434020996, 5.614102840423584, 6.0580315589904785, 4.798584461212158, 5.66785192489624, 5.622422218322754, 5.297589302062988, 5.347570419311523, 6.199492931365967, 5.885051727294922, 4.9289703369140625, 5.842702865600586, 6.030506134033203, 5.852453231811523]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.9881343841552734, 2.972520589828491, 2.8091225624084473, 2.2444698810577393, -0.4371595084667206, 3.042684555053711, 2.71689772605896, 1.8542698621749878, 2.7152862548828125, 3.4651613235473633, 2.6813902854919434, 3.2447128295898438, 2.860125780105591, 2.949902057647705, 2.329531669616699, 1.7375062704086304]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.5312299728393555, 6.801805019378662, 6.519529819488525, 6.286283016204834, 6.442503452301025, 6.463813304901123, 6.725587368011475, 6.612448215484619, 6.624892711639404, 6.309546947479248, 6.9062395095825195, 6.240485191345215, 5.264867782592773, 7.179875373840332, 6.463894367218018, 6.418749809265137]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.001129627227783, 5.345053195953369, 4.953247547149658, 4.039823532104492, 4.895125389099121, 5.239781379699707, 4.623008728027344, 4.773636341094971, 5.062396049499512, 4.840127468109131, 5.261252403259277, 4.87102746963501, 5.135386943817139, 5.425673961639404, 4.587460041046143, 5.469658851623535]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.135537147521973, 4.660342216491699, 3.330930709838867, 4.191914081573486, 4.444313049316406, 4.0717010498046875, 4.234829902648926, 4.225472450256348, 4.353601455688477, 4.462045669555664, 4.049557685852051, 4.42313289642334, 3.6689369678497314, 2.0210511684417725, 3.371211528778076, 4.015791416168213]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.276275157928467, 6.229709148406982, 5.903407573699951, 6.073405742645264, 5.424315929412842, 5.843208312988281, 6.17128849029541, 5.6982340812683105, 5.3415207862854, 5.749909400939941, 6.4308247566223145, 5.516009330749512, 5.761313438415527, 6.202627658843994, 6.218866348266602, 5.781306266784668]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.831434965133667, 3.669200897216797, 3.7820510864257812, 3.9428257942199707, 2.6867575645446777, 3.85624098777771, 3.933133602142334, 3.651972532272339, 3.9871644973754883, 3.6100571155548096, 3.694044589996338, 3.6176698207855225, 3.6593172550201416, 3.5958173274993896, 3.7000176906585693, 3.892094373703003]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4482996463775635, 3.5212349891662598, 3.403480052947998, 3.5525002479553223, 3.383843183517456, 3.616014242172241, 3.5687367916107178, 3.36460280418396, 3.5955400466918945, 3.291600227355957, 3.59816312789917, 3.445237636566162, 3.4336986541748047, 2.725721836090088, 3.645862340927124, 3.5599262714385986]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.179603099822998, 3.2062313556671143, 2.9845502376556396, 2.896221160888672, 2.8187763690948486, 3.0843141078948975, 3.180079460144043, 3.2733612060546875, 3.0325188636779785, 3.006288766860962, 2.285719394683838, 2.998612642288208, 3.155094861984253, 3.066460371017456, 3.082972764968872, 2.886007070541382]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.376434326171875, 4.490968227386475, 4.5721845626831055, 4.378726005554199, 4.494369983673096, 4.4656662940979, 4.2207560539245605, 4.709217071533203, 4.496676445007324, 4.4627485275268555, 4.473114967346191, 4.058154582977295, 4.202328681945801, 4.06018590927124, 4.581573963165283, 4.378037929534912]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.582341194152832, 8.904581069946289, 8.786458015441895, 8.471501350402832, 8.555121421813965, 8.268245697021484, 8.635026931762695, 8.546794891357422, 8.630400657653809, 8.495993614196777, 8.602527618408203, 8.232333183288574, 8.334321975708008, 9.003653526306152, 8.896862030029297, 8.504586219787598]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.8817362785339355, 5.417852878570557, 5.379340648651123, 4.779528617858887, 5.132041931152344, 5.085915565490723, 4.661281585693359, 5.123630046844482, 5.234859466552734, 4.919499397277832, 4.831664085388184, 4.721318244934082, 4.859399318695068, 4.9807891845703125, 5.058243751525879, 4.826590061187744]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.228555679321289, 2.997037649154663, 3.093160629272461, 2.756565570831299, 3.0160717964172363, 3.3033764362335205, 2.750673532485962, 3.176109552383423, 3.0805370807647705, 3.1785576343536377, 3.16916561126709, 3.253601551055908, 3.0291929244995117, 2.97906231880188, 3.1192867755889893, 3.1749460697174072]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12214679270982742, 0.1378248929977417, 0.1311836689710617, -0.24934189021587372, -0.250427782535553, -0.1038486659526825, 0.1446412205696106, -0.171176940202713, 0.09918265044689178, 0.11697632074356079, 0.1176602840423584, 0.07556504011154175, 0.1069561317563057, 0.12648652493953705, -1.1368941068649292, 0.13842733204364777]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08922804147005081, 0.052247337996959686, 0.04280374199151993, 0.0606912262737751, 0.08517317473888397, 0.035458747297525406, 0.053295720368623734, 0.0804704949259758, 0.020117849111557007, 0.06167356297373772, -0.18481764197349548, 0.06157175824046135, 0.001060405862517655, -0.008086750283837318, 0.020279821008443832, 0.030715694651007652]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07387959957122803, 0.04109467566013336, 0.09021309018135071, 0.05204709246754646, 0.09345012158155441, 0.10427319258451462, 0.05987542122602463, -0.14665809273719788, 0.06455578655004501, 0.0870920941233635, 0.001089461729861796, 0.07645125687122345, -0.21952402591705322, -0.011469416320323944, -0.062361981719732285, 0.1221240758895874]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.2150629758834839, 0.15023791790008545, 0.12292565405368805, 0.14177532494068146, 0.044365350157022476, 0.05310499668121338, -0.09363152086734772, 0.1769419014453888, 0.1529349535703659, -0.4455077052116394, 0.02764437347650528, 0.15891127288341522, 0.09437645226716995, -0.2865932285785675, -0.07329604029655457, -0.04924493655562401]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07512308657169342, -0.03145206347107887, -0.06830098479986191, 0.14488455653190613, -0.06623008847236633, -0.15041086077690125, 0.08692701160907745, 0.04784109443426132, 0.06595329940319061, 0.20561814308166504, -0.28661057353019714, 0.027590876445174217, 0.016546258702874184, -0.1866704821586609, 0.08432114869356155, 0.021974951028823853]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21027812361717224, 0.2410411238670349, 0.144556924700737, 0.05908118933439255, -0.3104219138622284, -0.061154548078775406, 0.09345021843910217, 0.0021821933332830667, 0.16844333708286285, -0.05818725377321243, -0.5079731941223145, 0.05167340487241745, -0.33848151564598083, 0.26260998845100403, 0.15339581668376923, 0.15534275770187378]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3133000433444977, 0.3280065357685089, 0.1951388716697693, 0.26629602909088135, 0.27165934443473816, 0.24413804709911346, 0.11663166433572769, 0.14795690774917603, -0.39706289768218994, 0.389140248298645, -0.08958753943443298, 0.3130939304828644, 0.0919637382030487, -0.07865217328071594, 0.29058197140693665, 0.18024803698062897]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.050977446138858795, 0.19544242322444916, 0.11481177061796188, 0.15890884399414062, -0.36076048016548157, 0.20077528059482574, 0.057025689631700516, 0.20011760294437408, -0.026517722755670547, -0.17222361266613007, 0.058132655918598175, -0.0672805905342102, 0.16942870616912842, 0.10776684433221817, -0.028544757515192032, 0.14434845745563507]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21224182844161987, 0.2826858460903168, -0.09848975390195847, 0.3269134759902954, 0.4634994566440582, 0.3982788920402527, 0.11500073224306107, -0.015375577844679356, 0.6257310509681702, -0.1845172941684723, 0.254729300737381, 0.053849589079618454, 0.6768108010292053, -0.21530354022979736, -0.03542628139257431, 0.19288058578968048]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8611175417900085, 0.7496451139450073, 0.6219280362129211, 0.5818143486976624, 0.836827278137207, 0.4337071180343628, 1.1000481843948364, 1.0767194032669067, 0.08617165684700012, 0.01745557226240635, 0.4204001724720001, 0.7943588495254517, 0.7365602850914001, 0.8149540424346924, 1.1217477321624756, 0.7887336611747742]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1488635540008545, 1.1724469661712646, 0.6532034277915955, 1.0477370023727417, 0.9487781524658203, 0.38746485114097595, 0.9887785911560059, 1.4273349046707153, 0.761659562587738, 0.2847437560558319, 1.0983996391296387, 1.109288215637207, 0.6111528873443604, 1.0986559391021729, 0.5037604570388794, 0.9945300817489624]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2402411699295044, 1.6063461303710938, 1.5303733348846436, 1.4171890020370483, 1.4203124046325684, 0.8176949620246887, 1.7541521787643433, 1.2183294296264648, 1.4860997200012207, 1.4590662717819214, 1.2507152557373047, 1.1619199514389038, 1.1301865577697754, 1.3243348598480225, 1.197077751159668, 1.1895499229431152]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.037090539932251, 1.5005983114242554, 1.653764247894287, 1.7965917587280273, 1.2452309131622314, 1.5870991945266724, 1.2991191148757935, 1.534425973892212, 1.3458495140075684, 0.6153364777565002, 1.705255389213562, 1.9558414220809937, 1.7585058212280273, 1.6776540279388428, 1.4232840538024902, 1.530503749847412]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0721535682678223, 3.8707525730133057, 3.0314414501190186, 2.639707565307617, 3.488114356994629, 2.0713789463043213, 3.666257381439209, 3.181274652481079, 3.833192825317383, 3.3824002742767334, 3.197842597961426, 3.9279978275299072, 2.1247637271881104, 3.956909656524658, 3.1584811210632324, 2.796811819076538]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3893988132476807, 2.37156343460083, 2.848356246948242, 2.430551528930664, 2.808652877807617, 2.0623903274536133, 1.5658278465270996, 2.111035108566284, 2.640868902206421, 1.9813225269317627, 1.336397409439087, 1.9366720914840698, 2.426424026489258, 2.6790103912353516, 2.644899845123291, 1.9945424795150757]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5645854473114014, 3.413137674331665, 3.0539093017578125, 3.117518663406372, 3.036078929901123, 2.83992862701416, 2.790517807006836, 2.058112144470215, 3.2808315753936768, 2.9452123641967773, 2.757333993911743, 1.3630213737487793, 2.9231951236724854, 2.7635276317596436, 3.03700590133667, 2.060938596725464]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.922244668006897, 1.839269757270813, 2.6418447494506836, 2.5972039699554443, 2.630838394165039, 1.9889445304870605, 1.7817682027816772, 3.0980453491210938, 0.5642184019088745, 2.180511713027954, 0.9812588095664978, 2.4524099826812744, 2.7499475479125977, 2.928412437438965, 2.9593000411987305, 1.431928038597107]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6780643463134766, 3.9517412185668945, 5.5638203620910645, 4.946065425872803, 5.516021728515625, 5.6790876388549805, 5.135522842407227, 5.1143693923950195, 6.160460948944092, 5.051161766052246, 5.774648189544678, 6.077978134155273, 4.836820602416992, 5.407249450683594, 4.305277347564697, 4.960427761077881]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.211772441864014, 4.559622764587402, 5.558145523071289, 5.1743011474609375, 3.7011570930480957, 4.464003562927246, 4.882219314575195, 5.298372745513916, 5.367344379425049, 4.918290138244629, 4.268834590911865, 3.350970506668091, 4.407827854156494, 3.3834540843963623, 4.922693252563477, 4.4981794357299805]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.874857425689697, 5.839588165283203, 6.0307297706604, 5.624332427978516, 6.068927764892578, 4.809466361999512, 5.680789947509766, 5.634799957275391, 5.303814888000488, 5.353902816772461, 6.209853172302246, 5.896315097808838, 4.939708232879639, 5.85384464263916, 6.041349411010742, 5.862220287322998]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.9916443824768066, 2.9758450984954834, 2.81325364112854, 2.2476158142089844, -0.4348452389240265, 3.042656421661377, 2.7198421955108643, 1.8575630187988281, 2.7175841331481934, 3.4677305221557617, 2.686235189437866, 3.246633768081665, 2.8602404594421387, 2.9497671127319336, 2.3340280055999756, 1.74144446849823]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.541441917419434, 6.811909198760986, 6.529865264892578, 6.298654079437256, 6.453141212463379, 6.473730564117432, 6.738208770751953, 6.625296115875244, 6.635210037231445, 6.319684028625488, 6.91719388961792, 6.2524847984313965, 5.275101661682129, 7.191771030426025, 6.476406097412109, 6.4295268058776855]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  20%|██████████                                       | 41/200 [01:37<05:44,  2.17s/it]Layer: gate_22 - Captured router_logits: [5.006951332092285, 5.349125862121582, 4.956442356109619, 4.045006275177002, 4.900784015655518, 5.246028900146484, 4.629023551940918, 4.779031276702881, 5.0696916580200195, 4.846359729766846, 5.266334533691406, 4.876359939575195, 5.140882968902588, 5.431692600250244, 4.595317363739014, 5.476918697357178]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.141531944274902, 4.665551662445068, 3.336759090423584, 4.193596363067627, 4.450528621673584, 4.075455188751221, 4.242115020751953, 4.2315545082092285, 4.359339237213135, 4.467111110687256, 4.054316520690918, 4.4268388748168945, 3.672802209854126, 2.023200750350952, 3.3780534267425537, 4.018280029296875]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.285051345825195, 6.238814353942871, 5.911322593688965, 6.082697868347168, 5.429896831512451, 5.851325511932373, 6.178464412689209, 5.706472396850586, 5.349771499633789, 5.757199764251709, 6.441464900970459, 5.523990154266357, 5.7696309089660645, 6.210922718048096, 6.228255748748779, 5.787522792816162]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8361544609069824, 3.671581506729126, 3.7858526706695557, 3.9470014572143555, 2.6884090900421143, 3.8623602390289307, 3.9393298625946045, 3.6553964614868164, 3.9913711547851562, 3.6135871410369873, 3.6979434490203857, 3.6207449436187744, 3.6623635292053223, 3.6003170013427734, 3.7033870220184326, 3.8946640491485596]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4515669345855713, 3.52382493019104, 3.4058971405029297, 3.555280923843384, 3.3861236572265625, 3.6193976402282715, 3.573002338409424, 3.3685495853424072, 3.599924087524414, 3.2958948612213135, 3.6006710529327393, 3.4485387802124023, 3.4361398220062256, 2.727993965148926, 3.6483027935028076, 3.5636942386627197]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1820666790008545, 3.2085630893707275, 2.9862611293792725, 2.897756338119507, 2.82051682472229, 3.086562395095825, 3.181647300720215, 3.276372194290161, 3.0348708629608154, 3.007512092590332, 2.2879011631011963, 2.999342679977417, 3.1565098762512207, 3.063591718673706, 3.085453748703003, 2.8868391513824463]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.376670837402344, 4.492178916931152, 4.572076797485352, 4.379696369171143, 4.498018741607666, 4.467154502868652, 4.222400188446045, 4.712116718292236, 4.496857166290283, 4.463902950286865, 4.474242210388184, 4.057984828948975, 4.202515125274658, 4.060867786407471, 4.5821332931518555, 4.378646373748779]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.586921691894531, 8.91103744506836, 8.792325019836426, 8.478343963623047, 8.559693336486816, 8.272149085998535, 8.642428398132324, 8.548397064208984, 8.636253356933594, 8.500533103942871, 8.605952262878418, 8.238221168518066, 8.336804389953613, 9.008480072021484, 8.903717041015625, 8.51455020904541]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.889044761657715, 5.4273457527160645, 5.387793064117432, 4.78734016418457, 5.137259006500244, 5.091536998748779, 4.669281959533691, 5.131930828094482, 5.238523006439209, 4.9266157150268555, 4.8393754959106445, 4.731231689453125, 4.865777492523193, 4.984920024871826, 5.067152500152588, 4.8331990242004395]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2343530654907227, 3.0018064975738525, 3.0998423099517822, 2.7631289958953857, 3.0226080417633057, 3.309743642807007, 2.7568583488464355, 3.180889368057251, 3.0863850116729736, 3.184774398803711, 3.1736810207366943, 3.2599246501922607, 3.0337955951690674, 2.981961488723755, 3.1207189559936523, 3.179802656173706]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12423934042453766, 0.140196293592453, 0.13299965858459473, -0.2504711151123047, -0.25093328952789307, -0.10282677412033081, 0.14625157415866852, -0.1789809912443161, 0.1004660427570343, 0.1192958801984787, 0.11894024163484573, 0.07800988852977753, 0.10867911577224731, 0.12790031731128693, -1.1446095705032349, 0.1403398960828781]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0909232348203659, 0.05157859995961189, 0.043988801538944244, 0.06250789016485214, 0.0863562673330307, 0.035987336188554764, 0.05316966772079468, 0.0797337144613266, 0.020836398005485535, 0.06329204887151718, -0.18540766835212708, 0.06279933452606201, 0.0002528982004150748, -0.006737997755408287, 0.0198673065751791, 0.030039522796869278]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07429272681474686, 0.04177390784025192, 0.09034200012683868, 0.051785729825496674, 0.09405241161584854, 0.10504463315010071, 0.05873967707157135, -0.14728184044361115, 0.06388504058122635, 0.08712171763181686, 0.0006360349361784756, 0.07534283399581909, -0.217927947640419, -0.009303650818765163, -0.06008992716670036, 0.12363364547491074]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21461209654808044, 0.1486189365386963, 0.1241346076130867, 0.14410698413848877, 0.04438384994864464, 0.05254077911376953, -0.08940870314836502, 0.17765115201473236, 0.15280680358409882, -0.4440482556819916, 0.026582397520542145, 0.16017462313175201, 0.08966576308012009, -0.2862934172153473, -0.07493908703327179, -0.04911923408508301]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07589631527662277, -0.03041731007397175, -0.06762274354696274, 0.14430879056453705, -0.06575001031160355, -0.15260061621665955, 0.08793028444051743, 0.04904597997665405, 0.06564661860466003, 0.20493872463703156, -0.2856524586677551, 0.027560245245695114, 0.01304696500301361, -0.18646134436130524, 0.0837559923529625, 0.023106656968593597]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21218307316303253, 0.24067385494709015, 0.14474068582057953, 0.05704379826784134, -0.3103196620941162, -0.06083432212471962, 0.09291864186525345, 0.003382675815373659, 0.16660866141319275, -0.05807066708803177, -0.5087714791297913, 0.052281979471445084, -0.34014734625816345, 0.2621372640132904, 0.15322047472000122, 0.15592971444129944]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.31358060240745544, 0.3292303681373596, 0.1954648196697235, 0.26589077711105347, 0.27160313725471497, 0.2440645694732666, 0.1173219308257103, 0.14923302829265594, -0.39605969190597534, 0.39066213369369507, -0.08914444595575333, 0.31555885076522827, 0.09378249943256378, -0.07839157432317734, 0.2876632511615753, 0.1791946142911911]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05137103423476219, 0.19717292487621307, 0.1167682334780693, 0.15959066152572632, -0.3604055941104889, 0.20150026679039001, 0.05843718349933624, 0.20420387387275696, -0.026085853576660156, -0.17131447792053223, 0.05740389600396156, -0.0672900602221489, 0.16940195858478546, 0.10871898382902145, -0.03038322925567627, 0.14428938925266266]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21207302808761597, 0.2833437919616699, -0.09586817026138306, 0.3264046907424927, 0.4621977210044861, 0.3991962969303131, 0.11517389118671417, -0.013588189147412777, 0.6256780028343201, -0.18602699041366577, 0.25558415055274963, 0.05135960131883621, 0.6763380765914917, -0.21481464803218842, -0.03487926721572876, 0.19222205877304077]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.862019956111908, 0.7511269450187683, 0.6212089657783508, 0.5781430602073669, 0.835624098777771, 0.4345272183418274, 1.1017574071884155, 1.0748339891433716, 0.08664275705814362, 0.018574615940451622, 0.42067793011665344, 0.7934857606887817, 0.7387039661407471, 0.8165969252586365, 1.1194382905960083, 0.7890422344207764]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1490063667297363, 1.170188069343567, 0.6525720953941345, 1.0488442182540894, 0.9491143822669983, 0.3822042644023895, 0.9883635640144348, 1.4255223274230957, 0.7632598876953125, 0.28375959396362305, 1.0984556674957275, 1.1069802045822144, 0.6154298186302185, 1.0963020324707031, 0.5030462145805359, 0.9957594871520996]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2404237985610962, 1.6043553352355957, 1.5306367874145508, 1.4146560430526733, 1.4210965633392334, 0.8217673897743225, 1.7538846731185913, 1.2187068462371826, 1.4868866205215454, 1.457349181175232, 1.2523335218429565, 1.1628727912902832, 1.1332075595855713, 1.3263964653015137, 1.1990225315093994, 1.1911317110061646]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0371969938278198, 1.5028892755508423, 1.6531052589416504, 1.7941222190856934, 1.2469289302825928, 1.5861846208572388, 1.2967990636825562, 1.5298652648925781, 1.3463159799575806, 0.6181328296661377, 1.7045120000839233, 1.9543566703796387, 1.7598788738250732, 1.6776971817016602, 1.4230140447616577, 1.5302910804748535]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.074270009994507, 3.8690478801727295, 3.034823179244995, 2.644113779067993, 3.4881577491760254, 2.074927568435669, 3.6662023067474365, 3.1775996685028076, 3.8315224647521973, 3.3840653896331787, 3.1963632106781006, 3.9296536445617676, 2.1288723945617676, 3.9586002826690674, 3.159893751144409, 2.796917200088501]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.391057252883911, 2.3715193271636963, 2.8492238521575928, 2.431691884994507, 2.8087785243988037, 2.0641260147094727, 1.5684627294540405, 2.1083805561065674, 2.6407299041748047, 1.9825361967086792, 1.339073896408081, 1.9368501901626587, 2.4275529384613037, 2.679133653640747, 2.644108295440674, 1.9956517219543457]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5654053688049316, 3.412543296813965, 3.0546653270721436, 3.1174888610839844, 3.0376124382019043, 2.8411800861358643, 2.7898266315460205, 2.05841064453125, 3.2792298793792725, 2.9459128379821777, 2.760313034057617, 1.3670376539230347, 2.923444986343384, 2.7647809982299805, 3.0368540287017822, 2.061901330947876]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9215353727340698, 1.8409667015075684, 2.641028642654419, 2.595712184906006, 2.6298985481262207, 1.9887011051177979, 1.7786825895309448, 3.0950684547424316, 0.5651670098304749, 2.1779675483703613, 0.9812105298042297, 2.4519803524017334, 2.7491610050201416, 2.9276773929595947, 2.9587948322296143, 1.4304792881011963]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6826393604278564, 3.9555749893188477, 5.5643630027771, 4.94732141494751, 5.5186991691589355, 5.680574893951416, 5.136590480804443, 5.1170830726623535, 6.1609787940979, 5.05059814453125, 5.774892330169678, 6.079272270202637, 4.839900970458984, 5.408560752868652, 4.30776309967041, 4.961724758148193]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.213135242462158, 4.561557769775391, 5.556778907775879, 5.1751580238342285, 3.703371524810791, 4.464692115783691, 4.883087158203125, 5.298633575439453, 5.367654800415039, 4.918453216552734, 4.270662307739258, 3.3520641326904297, 4.409882068634033, 3.3858673572540283, 4.923058032989502, 4.498960018157959]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.877321243286133, 5.8407087326049805, 6.032651424407959, 5.626408576965332, 6.071136474609375, 4.813090801239014, 5.684206008911133, 5.636733055114746, 5.304730415344238, 5.3567352294921875, 6.211792469024658, 5.8987135887146, 4.942862510681152, 5.855681896209717, 6.043209552764893, 5.864460468292236]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.992539882659912, 2.975233793258667, 2.8136727809906006, 2.2472424507141113, -0.4349248707294464, 3.043558359146118, 2.72086501121521, 1.8587812185287476, 2.718839645385742, 3.468806743621826, 2.6880881786346436, 3.2467548847198486, 2.860560655593872, 2.9494075775146484, 2.333381175994873, 1.7420705556869507]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.545246601104736, 6.815713405609131, 6.5336809158325195, 6.302313804626465, 6.457526683807373, 6.4777512550354, 6.7421393394470215, 6.629202842712402, 6.638495445251465, 6.323585033416748, 6.921361446380615, 6.255956649780273, 5.27788782119751, 7.196943759918213, 6.481198310852051, 6.434168815612793]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.008173942565918, 5.350368022918701, 4.957752227783203, 4.0459818840026855, 4.901241779327393, 5.246828079223633, 4.630035877227783, 4.779361248016357, 5.07061767578125, 4.846140384674072, 5.267516613006592, 4.876668930053711, 5.141902923583984, 5.43196964263916, 4.5960917472839355, 5.476862907409668]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.142870903015137, 4.664958477020264, 3.335505723953247, 4.194091320037842, 4.450186252593994, 4.075192928314209, 4.241611957550049, 4.230929374694824, 4.359346866607666, 4.467442512512207, 4.053407669067383, 4.425987243652344, 3.6724565029144287, 2.0222561359405518, 3.3774478435516357, 4.016964435577393]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.284946918487549, 6.238174915313721, 5.910727500915527, 6.082102298736572, 5.429161071777344, 5.8506011962890625, 6.177511215209961, 5.704685688018799, 5.348735332489014, 5.756025791168213, 6.440604209899902, 5.522345066070557, 5.7689595222473145, 6.210225582122803, 6.226839065551758, 5.786125183105469]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8364617824554443, 3.6714162826538086, 3.7847814559936523, 3.9472641944885254, 2.688039541244507, 3.861633062362671, 3.939974546432495, 3.6557159423828125, 3.991501569747925, 3.6135616302490234, 3.6978237628936768, 3.620694637298584, 3.6621336936950684, 3.60014009475708, 3.703477144241333, 3.8951003551483154]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  22%|███████████                                      | 45/200 [01:46<05:35,  2.17s/it]Layer: gate_26 - Captured router_logits: [3.45158314704895, 3.5247249603271484, 3.4057538509368896, 3.5552191734313965, 3.3851258754730225, 3.6205101013183594, 3.5728626251220703, 3.3679168224334717, 3.600310802459717, 3.2961361408233643, 3.600813865661621, 3.4483721256256104, 3.435772657394409, 2.727527618408203, 3.6490676403045654, 3.5644214153289795]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1823272705078125, 3.208224058151245, 2.985538959503174, 2.8975820541381836, 2.8205039501190186, 3.0856499671936035, 3.181570053100586, 3.2761502265930176, 3.034036874771118, 3.0065548419952393, 2.2877655029296875, 2.999020576477051, 3.1567001342773438, 3.063218355178833, 3.0843000411987305, 2.886549472808838]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.374842166900635, 4.490650177001953, 4.569879055023193, 4.377411365509033, 4.494767189025879, 4.466183662414551, 4.220108509063721, 4.710813999176025, 4.494855880737305, 4.461665153503418, 4.472009181976318, 4.055762767791748, 4.200311660766602, 4.058440208435059, 4.580168724060059, 4.375984191894531]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.585468292236328, 8.910521507263184, 8.790816307067871, 8.476289749145508, 8.558098793029785, 8.270048141479492, 8.640056610107422, 8.544527053833008, 8.633269309997559, 8.498067855834961, 8.60374641418457, 8.236421585083008, 8.335113525390625, 9.006414413452148, 8.902703285217285, 8.512718200683594]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.888728618621826, 5.4272050857543945, 5.386826038360596, 4.7869744300842285, 5.136003494262695, 5.0901007652282715, 4.669247627258301, 5.131940841674805, 5.237839698791504, 4.926521301269531, 4.837759971618652, 4.730453968048096, 4.86509370803833, 4.983229637145996, 5.067203521728516, 4.832578659057617]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.233651638031006, 3.0012214183807373, 3.1004247665405273, 2.7622175216674805, 3.0227558612823486, 3.3095510005950928, 2.756035566329956, 3.17939829826355, 3.0853078365325928, 3.183750629425049, 3.1733577251434326, 3.2593750953674316, 3.0332281589508057, 2.981356143951416, 3.1201398372650146, 3.1789190769195557]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12206588685512543, 0.13776342570781708, 0.13045144081115723, -0.2480146735906601, -0.24998146295547485, -0.10189730674028397, 0.14444617927074432, -0.16893646121025085, 0.0999697670340538, 0.116721510887146, 0.1163787841796875, 0.07566674798727036, 0.10768003761768341, 0.12660081684589386, -1.1307313442230225, 0.1376798152923584]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08907300978899002, 0.051311101764440536, 0.041989780962467194, 0.06121167168021202, 0.08457939326763153, 0.035735808312892914, 0.05370304360985756, 0.08046809583902359, 0.019243549555540085, 0.061811767518520355, -0.1844322681427002, 0.06011244282126427, 0.0018641313072293997, -0.007221858482807875, 0.020421620458364487, 0.030780330300331116]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0722842663526535, 0.04069583863019943, 0.08996488898992538, 0.05315708741545677, 0.0932161957025528, 0.10325304418802261, 0.05849013477563858, -0.14594048261642456, 0.06309564411640167, 0.086211659014225, 0.0013434940483421087, 0.07529406249523163, -0.21778883039951324, -0.010959661565721035, -0.05973516404628754, 0.12423282861709595]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.2137981504201889, 0.149526908993721, 0.1236889511346817, 0.14284399151802063, 0.04453624039888382, 0.052359651774168015, -0.09022074937820435, 0.17784558236598969, 0.15245500206947327, -0.4451947510242462, 0.0273190476000309, 0.159518301486969, 0.09366943687200546, -0.2876025140285492, -0.07550911605358124, -0.0498119480907917]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0749848410487175, -0.031338032335042953, -0.06785553693771362, 0.14444074034690857, -0.06767257302999496, -0.15143075585365295, 0.08750370144844055, 0.04860231280326843, 0.06669079512357712, 0.206655815243721, -0.28516072034835815, 0.02759762853384018, 0.014483529143035412, -0.18574170768260956, 0.08253452181816101, 0.02070147544145584]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21085819602012634, 0.2402065098285675, 0.14408376812934875, 0.05688353255391121, -0.3090004622936249, -0.06129411235451698, 0.09368675202131271, 0.003096272936090827, 0.16704238951206207, -0.05980636551976204, -0.5065242648124695, 0.05135512351989746, -0.34040579199790955, 0.2608755826950073, 0.1543867439031601, 0.15465517342090607]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.31330037117004395, 0.3295957148075104, 0.1942598670721054, 0.26663312315940857, 0.27072829008102417, 0.24460400640964508, 0.11790333688259125, 0.14910179376602173, -0.3952668011188507, 0.38935670256614685, -0.08826610445976257, 0.31377944350242615, 0.092151939868927, -0.07701270282268524, 0.2889430820941925, 0.1794629693031311]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.050839535892009735, 0.19644534587860107, 0.11633183807134628, 0.15866419672966003, -0.3609788715839386, 0.2024787962436676, 0.057666368782520294, 0.20293028652668, -0.026396138593554497, -0.17150042951107025, 0.058622654527425766, -0.06699421256780624, 0.1702590435743332, 0.10802236199378967, -0.02883513830602169, 0.14542622864246368]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21117039024829865, 0.282303124666214, -0.0958610400557518, 0.3263677954673767, 0.4628356099128723, 0.3986624479293823, 0.11519204825162888, -0.01286802813410759, 0.6240633130073547, -0.18644732236862183, 0.2548525929450989, 0.0543227382004261, 0.6755858659744263, -0.21399301290512085, -0.03432075306773186, 0.19284915924072266]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8618072271347046, 0.750748872756958, 0.6214654445648193, 0.5799546241760254, 0.8367943167686462, 0.43548619747161865, 1.1017664670944214, 1.0749483108520508, 0.08787878602743149, 0.01843094639480114, 0.42251551151275635, 0.7941045165061951, 0.7393103837966919, 0.8156988620758057, 1.120941162109375, 0.7889658808708191]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1507055759429932, 1.1728179454803467, 0.6530476808547974, 1.04997718334198, 0.9492788910865784, 0.3858725428581238, 0.9885979890823364, 1.4257116317749023, 0.7645818591117859, 0.2839788794517517, 1.0988291501998901, 1.1084401607513428, 0.6158602833747864, 1.0960376262664795, 0.502840518951416, 0.9972361922264099]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.24057137966156, 1.6063209772109985, 1.5299248695373535, 1.4157441854476929, 1.4215178489685059, 0.8243618607521057, 1.753298044204712, 1.2204746007919312, 1.4868440628051758, 1.4572149515151978, 1.253174901008606, 1.163989782333374, 1.1355390548706055, 1.327163815498352, 1.1995576620101929, 1.1915757656097412]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0397690534591675, 1.5035004615783691, 1.6524399518966675, 1.7952361106872559, 1.2476582527160645, 1.5860244035720825, 1.2963711023330688, 1.5311284065246582, 1.347640872001648, 0.6206108331680298, 1.7045518159866333, 1.95356023311615, 1.7594894170761108, 1.6771836280822754, 1.4226490259170532, 1.5307365655899048]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0770795345306396, 3.8709447383880615, 3.0357937812805176, 2.646146535873413, 3.4891481399536133, 2.0792269706726074, 3.667551279067993, 3.1788294315338135, 3.8334383964538574, 3.3851304054260254, 3.197772264480591, 3.930863618850708, 2.1305997371673584, 3.9605045318603516, 3.1613500118255615, 2.7989704608917236]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.391389846801758, 2.3702516555786133, 2.8490633964538574, 2.4321162700653076, 2.808499574661255, 2.0639493465423584, 1.5683830976486206, 2.107759475708008, 2.641350746154785, 1.9837161302566528, 1.340425729751587, 1.9396300315856934, 2.4279510974884033, 2.67891263961792, 2.64556622505188, 1.9962081909179688]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.567121744155884, 3.4144420623779297, 3.055952548980713, 3.118479013442993, 3.039257049560547, 2.8425872325897217, 2.789715051651001, 2.0577526092529297, 3.281155824661255, 2.946261405944824, 2.7618002891540527, 1.3666250705718994, 2.923762559890747, 2.7649850845336914, 3.0379011631011963, 2.062312602996826]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9218348264694214, 1.8419010639190674, 2.640805721282959, 2.596086025238037, 2.6311988830566406, 1.9891808032989502, 1.77899968624115, 3.0961685180664062, 0.5644198656082153, 2.176894426345825, 0.9813966751098633, 2.4521148204803467, 2.749603748321533, 2.9269096851348877, 2.9578771591186523, 1.4298360347747803]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6819827556610107, 3.953632354736328, 5.563629627227783, 4.947968006134033, 5.518168926239014, 5.680309295654297, 5.1356000900268555, 5.116628170013428, 6.160476207733154, 5.04899263381958, 5.774466514587402, 6.078824043273926, 4.836732864379883, 5.408209323883057, 4.30759859085083, 4.961040019989014]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.212541580200195, 4.560321807861328, 5.555271148681641, 5.174458026885986, 3.7019693851470947, 4.463963985443115, 4.881503582000732, 5.298177719116211, 5.36702823638916, 4.9178690910339355, 4.270370960235596, 3.351719617843628, 4.409761905670166, 3.385470151901245, 4.921915054321289, 4.498841762542725]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.876376628875732, 5.839445114135742, 6.032261848449707, 5.625615119934082, 6.070460319519043, 4.811089515686035, 5.682566165924072, 5.635217189788818, 5.303300380706787, 5.355738639831543, 6.211418628692627, 5.898167133331299, 4.942532539367676, 5.8551788330078125, 6.042060852050781, 5.863395690917969]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.992959976196289, 2.9755046367645264, 2.8135807514190674, 2.2466483116149902, -0.43716228008270264, 3.043412208557129, 2.720351457595825, 1.8577767610549927, 2.719161033630371, 3.4682915210723877, 2.6872689723968506, 3.2471823692321777, 2.8608107566833496, 2.9502763748168945, 2.3333475589752197, 1.7409257888793945]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.544044494628906, 6.813995361328125, 6.532227993011475, 6.2999348640441895, 6.455571174621582, 6.47590446472168, 6.74017858505249, 6.62811803817749, 6.636836051940918, 6.321863651275635, 6.919418811798096, 6.253674030303955, 5.2760090827941895, 7.1943488121032715, 6.479030132293701, 6.431859493255615]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.008252143859863, 5.350522994995117, 4.958186149597168, 4.045877456665039, 4.901086807250977, 5.247033596038818, 4.629642486572266, 4.7792863845825195, 5.070511341094971, 4.8467698097229, 5.267757892608643, 4.876028060913086, 5.142043590545654, 5.431800365447998, 4.59619140625, 5.476552486419678]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.142958641052246, 4.665646076202393, 3.3358726501464844, 4.194186210632324, 4.450878620147705, 4.075671672821045, 4.2412590980529785, 4.230961322784424, 4.359784126281738, 4.4673051834106445, 4.054049015045166, 4.427186489105225, 3.6725213527679443, 2.0217397212982178, 3.3773910999298096, 4.0179362297058105]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.284759998321533, 6.237764835357666, 5.910701751708984, 6.0823516845703125, 5.429266929626465, 5.850203990936279, 6.177492141723633, 5.704771518707275, 5.348188877105713, 5.756102085113525, 6.440039157867432, 5.522394180297852, 5.769577980041504, 6.210456371307373, 6.227166175842285, 5.786228656768799]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8367438316345215, 3.671970844268799, 3.7855474948883057, 3.947615146636963, 2.6881558895111084, 3.8622336387634277, 3.939713954925537, 3.6563456058502197, 3.991861581802368, 3.6138718128204346, 3.6985957622528076, 3.6213362216949463, 3.662548780441284, 3.6001508235931396, 3.7037858963012695, 3.89555025100708]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4518957138061523, 3.52504563331604, 3.405895709991455, 3.5555076599121094, 3.3861498832702637, 3.6209075450897217, 3.5732946395874023, 3.3682539463043213, 3.6008193492889404, 3.296313762664795, 3.600996971130371, 3.448500394821167, 3.436549186706543, 2.7273809909820557, 3.6492464542388916, 3.564974784851074]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.182666778564453, 3.2086877822875977, 2.9859189987182617, 2.8977043628692627, 2.8207662105560303, 3.086853265762329, 3.1820735931396484, 3.276336431503296, 3.0344126224517822, 3.0073776245117188, 2.2877390384674072, 2.9994394779205322, 3.1570115089416504, 3.0641021728515625, 3.0846714973449707, 2.887012004852295]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.376055717468262, 4.4917192459106445, 4.571193218231201, 4.378267288208008, 4.495761394500732, 4.467031002044678, 4.220913887023926, 4.711734771728516, 4.496335983276367, 4.462681293487549, 4.4730706214904785, 4.056909561157227, 4.201714515686035, 4.059606075286865, 4.58062744140625, 4.37729549407959]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.586894035339355, 8.911759376525879, 8.792009353637695, 8.477397918701172, 8.560032844543457, 8.272215843200684, 8.641888618469238, 8.547523498535156, 8.635225296020508, 8.499682426452637, 8.60517692565918, 8.237371444702148, 8.336822509765625, 9.007994651794434, 8.904195785522461, 8.513958930969238]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  24%|████████████                                     | 49/200 [01:54<05:14,  2.08s/it]Layer: gate_30 - Captured router_logits: [4.889463901519775, 5.428605079650879, 5.387711524963379, 4.788142681121826, 5.137461185455322, 5.092238903045654, 4.670094013214111, 5.133181095123291, 5.239370346069336, 4.927125930786133, 4.838907718658447, 4.731584072113037, 4.866073131561279, 4.984634876251221, 5.068027496337891, 4.833202362060547]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.234635353088379, 3.0020217895507812, 3.1015214920043945, 2.7632296085357666, 3.0240061283111572, 3.3103976249694824, 2.7571535110473633, 3.180812358856201, 3.086763381958008, 3.1850624084472656, 3.1745569705963135, 3.2607507705688477, 3.0346920490264893, 2.9825797080993652, 3.1219005584716797, 3.1802797317504883]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.1227487325668335, 0.13893860578536987, 0.13104687631130219, -0.24448804557323456, -0.2459927648305893, -0.10411255061626434, 0.14481908082962036, -0.18269041180610657, 0.09991006553173065, 0.11763440072536469, 0.11723646521568298, 0.0770912617444992, 0.10779248178005219, 0.1267419159412384, -1.1253578662872314, 0.1386282742023468]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0907958447933197, 0.05236085504293442, 0.04237800091505051, 0.06152937933802605, 0.08534462749958038, 0.03676021471619606, 0.05355009809136391, 0.0770559310913086, 0.01674823649227619, 0.062128324061632156, -0.18208998441696167, 0.06256628036499023, 0.0014875192428007722, -0.006351307034492493, 0.019643831998109818, 0.030973490327596664]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07451924681663513, 0.04317062348127365, 0.09021592140197754, 0.05175064504146576, 0.09234973043203354, 0.10450293868780136, 0.05718623101711273, -0.14476731419563293, 0.06478884071111679, 0.08417588472366333, 0.00024919756106100976, 0.0754442811012268, -0.21481938660144806, -0.010264373384416103, -0.061473339796066284, 0.1244378536939621]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.2155836522579193, 0.15010352432727814, 0.12490858137607574, 0.14426732063293457, 0.042258694767951965, 0.05363290011882782, -0.08568819612264633, 0.17645122110843658, 0.15124258399009705, -0.4362201392650604, 0.02516149915754795, 0.16130979359149933, 0.08627906441688538, -0.2765004634857178, -0.07887090742588043, -0.04701235145330429]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07620794326066971, -0.03175044059753418, -0.0673048198223114, 0.14012011885643005, -0.06832940876483917, -0.14809644222259521, 0.08944929391145706, 0.04893428087234497, 0.07207880169153214, 0.20561575889587402, -0.2794538140296936, 0.026630515232682228, 0.015148361213505268, -0.18434520065784454, 0.08135224133729935, 0.025364432483911514]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21073244512081146, 0.24078784883022308, 0.14311403036117554, 0.05583580583333969, -0.30710312724113464, -0.0582776814699173, 0.09291202574968338, 0.004876089747995138, 0.16523997485637665, -0.05770169943571091, -0.5071396231651306, 0.05081009119749069, -0.33724215626716614, 0.25946202874183655, 0.15233953297138214, 0.1549365222454071]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3119480311870575, 0.33084166049957275, 0.19546915590763092, 0.26728585362434387, 0.269767165184021, 0.24147048592567444, 0.12138043344020844, 0.14841517806053162, -0.39248937368392944, 0.3896399140357971, -0.08096281439065933, 0.3130706250667572, 0.09084390848875046, -0.07401808351278305, 0.28362250328063965, 0.17600081861019135]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05224272236227989, 0.19789005815982819, 0.11673605442047119, 0.15874932706356049, -0.3564828038215637, 0.20118460059165955, 0.05733662098646164, 0.20841218531131744, -0.029868638142943382, -0.17144091427326202, 0.06079581752419472, -0.06779318302869797, 0.17260858416557312, 0.10588245838880539, -0.023516831919550896, 0.1455548256635666]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21023020148277283, 0.2818509340286255, -0.09101510047912598, 0.3261151611804962, 0.4610038697719574, 0.3986918032169342, 0.11504827439785004, -0.012646731920540333, 0.6233331561088562, -0.19002357125282288, 0.25460124015808105, 0.0538952462375164, 0.6729745268821716, -0.20884370803833008, -0.030918443575501442, 0.1916869878768921]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8626687526702881, 0.7543515563011169, 0.6185713410377502, 0.5730302929878235, 0.8386533260345459, 0.4374246597290039, 1.102694034576416, 1.0727763175964355, 0.08796662837266922, 0.023410074412822723, 0.4277016818523407, 0.7915117144584656, 0.7411929368972778, 0.8214248418807983, 1.1181786060333252, 0.7879915833473206]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1477692127227783, 1.1695975065231323, 0.6546821594238281, 1.051719307899475, 0.9500412344932556, 0.3832387626171112, 0.9881680607795715, 1.4239790439605713, 0.7697039842605591, 0.2891365885734558, 1.0976002216339111, 1.1070027351379395, 0.6236638426780701, 1.0986757278442383, 0.49970054626464844, 1.0004702806472778]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.241336703300476, 1.6038002967834473, 1.5299152135849, 1.4138379096984863, 1.4213086366653442, 0.830079197883606, 1.7501904964447021, 1.2241777181625366, 1.482818603515625, 1.4528207778930664, 1.2549419403076172, 1.1677100658416748, 1.1402692794799805, 1.3320266008377075, 1.207571268081665, 1.1949957609176636]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0447357892990112, 1.5053364038467407, 1.6499290466308594, 1.7946052551269531, 1.2551186084747314, 1.5877254009246826, 1.2949217557907104, 1.5290530920028687, 1.3511887788772583, 0.6245437860488892, 1.7016804218292236, 1.9531923532485962, 1.7614235877990723, 1.6765977144241333, 1.4231680631637573, 1.5306307077407837]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0771279335021973, 3.8646273612976074, 3.03298020362854, 2.650369167327881, 3.485053777694702, 2.0824365615844727, 3.663464069366455, 3.1729354858398438, 3.8307104110717773, 3.382765293121338, 3.1939260959625244, 3.9302611351013184, 2.134556293487549, 3.961409330368042, 3.1588001251220703, 2.8014163970947266]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3900256156921387, 2.369779348373413, 2.848644971847534, 2.431492328643799, 2.8067057132720947, 2.0648350715637207, 1.5744212865829468, 2.103116273880005, 2.641514778137207, 1.9841108322143555, 1.3444877862930298, 1.939483642578125, 2.426884412765503, 2.675621747970581, 2.6429953575134277, 1.9980665445327759]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.568903684616089, 3.4144818782806396, 3.0554656982421875, 3.115905284881592, 3.0403645038604736, 2.842742443084717, 2.788079261779785, 2.0561580657958984, 3.2796082496643066, 2.945944309234619, 2.7635579109191895, 1.3703447580337524, 2.9212329387664795, 2.7666382789611816, 3.0377068519592285, 2.063960313796997]
Running loglikelihood requests:  26%|████████████▉                                    | 53/200 [02:01<04:57,  2.02s/it]Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9228863716125488, 1.843351125717163, 2.6414577960968018, 2.5947394371032715, 2.630357503890991, 1.9907788038253784, 1.7790164947509766, 3.0943808555603027, 0.5678196549415588, 2.1773688793182373, 0.9857213497161865, 2.453036069869995, 2.7496752738952637, 2.928164482116699, 2.9589385986328125, 1.4317116737365723]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6836957931518555, 3.953819990158081, 5.561370849609375, 4.947874546051025, 5.514461994171143, 5.678252696990967, 5.132709503173828, 5.1155781745910645, 6.156702518463135, 5.049487113952637, 5.771542072296143, 6.075921058654785, 4.833654403686523, 5.402458667755127, 4.3083930015563965, 4.960449695587158]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.2122015953063965, 4.559713840484619, 5.554157257080078, 5.172289848327637, 3.7043070793151855, 4.465309143066406, 4.879507541656494, 5.296796798706055, 5.365478992462158, 4.9169721603393555, 4.271050453186035, 3.3524439334869385, 4.409351348876953, 3.385812520980835, 4.922049045562744, 4.497398853302002]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.87736177444458, 5.839959621429443, 6.032755374908447, 5.625885963439941, 6.07199764251709, 4.814051151275635, 5.68479585647583, 5.636525630950928, 5.302524566650391, 5.355311393737793, 6.211450576782227, 5.898683547973633, 4.943949222564697, 5.856484413146973, 6.0424981117248535, 5.864302635192871]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.99310302734375, 2.9738426208496094, 2.812584400177002, 2.2464206218719482, -0.4362857937812805, 3.041217803955078, 2.7201788425445557, 1.85673987865448, 2.7171645164489746, 3.4682652950286865, 2.6863627433776855, 3.2456090450286865, 2.860055446624756, 2.948967933654785, 2.332551956176758, 1.7398700714111328]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.545783996582031, 6.815439701080322, 6.532803058624268, 6.3015899658203125, 6.456212997436523, 6.476294040679932, 6.7417473793029785, 6.630641937255859, 6.637227535247803, 6.323472499847412, 6.921168327331543, 6.253235340118408, 5.277930736541748, 7.1969828605651855, 6.480818271636963, 6.433796405792236]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.008284568786621, 5.350291728973389, 4.95943021774292, 4.046215534210205, 4.900548934936523, 5.2473859786987305, 4.629872798919678, 4.7809553146362305, 5.070577621459961, 4.845902442932129, 5.26726770401001, 4.8749308586120605, 5.142350673675537, 5.43255090713501, 4.597344398498535, 5.476512908935547]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.143557548522949, 4.666348457336426, 3.337141513824463, 4.1947021484375, 4.453347682952881, 4.076159477233887, 4.242451190948486, 4.231807231903076, 4.360775947570801, 4.4684977531433105, 4.0549468994140625, 4.4279584884643555, 3.6729798316955566, 2.0236213207244873, 3.3789401054382324, 4.018073558807373]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.28312349319458, 6.236016273498535, 5.9101409912109375, 6.0807576179504395, 5.4264678955078125, 5.848448753356934, 6.1758503913879395, 5.703081130981445, 5.345609188079834, 5.7536091804504395, 6.437310218811035, 5.521097660064697, 5.766586780548096, 6.208542346954346, 6.225594997406006, 5.7840352058410645]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8354005813598633, 3.670228958129883, 3.784554958343506, 3.945929527282715, 2.686837911605835, 3.8615520000457764, 3.939271926879883, 3.6547060012817383, 3.990896701812744, 3.6124205589294434, 3.6965537071228027, 3.619823932647705, 3.6612582206726074, 3.5986552238464355, 3.702680826187134, 3.8938863277435303]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.450467824935913, 3.5253074169158936, 3.404979944229126, 3.554781913757324, 3.3839964866638184, 3.620690107345581, 3.5727596282958984, 3.3660972118377686, 3.6001410484313965, 3.2955057621002197, 3.600010871887207, 3.4476211071014404, 3.435347080230713, 2.726391315460205, 3.648432493209839, 3.564426898956299]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.181735038757324, 3.208085536956787, 2.9845097064971924, 2.8958678245544434, 2.819066286087036, 3.085500717163086, 3.181701421737671, 3.2752609252929688, 3.0333359241485596, 3.005469799041748, 2.2868716716766357, 2.9984359741210938, 3.155986785888672, 3.062750816345215, 3.0836830139160156, 2.8858091831207275]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.373333930969238, 4.489792823791504, 4.567380428314209, 4.374284744262695, 4.492149829864502, 4.463752269744873, 4.217120170593262, 4.708252429962158, 4.493229389190674, 4.4588093757629395, 4.470098495483398, 4.053185939788818, 4.197616100311279, 4.056553363800049, 4.577398777008057, 4.374077796936035]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.580870628356934, 8.906128883361816, 8.786060333251953, 8.4708890914917, 8.554473876953125, 8.265358924865723, 8.635035514831543, 8.5402250289917, 8.627449989318848, 8.493728637695312, 8.59762191772461, 8.231382369995117, 8.330495834350586, 9.001448631286621, 8.898494720458984, 8.506797790527344]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.888027667999268, 5.4281158447265625, 5.386228561401367, 4.787431240081787, 5.136256694793701, 5.089210510253906, 4.66855001449585, 5.1319804191589355, 5.237386226654053, 4.925976753234863, 4.8362603187561035, 4.730347633361816, 4.863983631134033, 4.981605052947998, 5.066633224487305, 4.831820964813232]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2345292568206787, 3.001133918762207, 3.1018869876861572, 2.76308012008667, 3.02374529838562, 3.3104939460754395, 2.756448984146118, 3.17948842048645, 3.0867037773132324, 3.184091329574585, 3.1740267276763916, 3.260819673538208, 3.0336387157440186, 2.98075270652771, 3.1208417415618896, 3.1798648834228516]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12225595116615295, 0.13704945147037506, 0.1308000236749649, -0.24843351542949677, -0.24366723001003265, -0.10714604705572128, 0.1425657719373703, -0.17495661973953247, 0.10439296066761017, 0.11637446284294128, 0.11590176075696945, 0.08009134978055954, 0.10783408582210541, 0.1273365169763565, -1.1181933879852295, 0.13879647850990295]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.090164914727211, 0.051539164036512375, 0.041791461408138275, 0.06236196681857109, 0.08447463065385818, 0.03831903263926506, 0.05131123214960098, 0.07829941809177399, 0.02022126503288746, 0.061008114367723465, -0.18236255645751953, 0.06133430078625679, 0.0013279989361763, -0.004112327005714178, 0.017783574759960175, 0.02544582635164261]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06935001164674759, 0.04255391284823418, 0.09253351390361786, 0.05228770151734352, 0.09610117226839066, 0.09915127605199814, 0.05823548883199692, -0.1432497203350067, 0.06366270780563354, 0.08528236299753189, 0.0009150676196441054, 0.07534927874803543, -0.21328876912593842, -0.01023837924003601, -0.05505777522921562, 0.1252986192703247]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21245074272155762, 0.14544817805290222, 0.12376397103071213, 0.14515908062458038, 0.04388013109564781, 0.05809454619884491, -0.08679988235235214, 0.17677854001522064, 0.15167108178138733, -0.4447621703147888, 0.023223016411066055, 0.16631929576396942, 0.09476110339164734, -0.28492215275764465, -0.07836506515741348, -0.04224998131394386]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07667629420757294, -0.0320272222161293, -0.06764515489339828, 0.14136838912963867, -0.06554993987083435, -0.15024857223033905, 0.0903579592704773, 0.04825739189982414, 0.0647289901971817, 0.2025572508573532, -0.28360000252723694, 0.02203485555946827, 0.013447214849293232, -0.17648029327392578, 0.08351520448923111, 0.0277179516851902]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.2103801816701889, 0.24049602448940277, 0.14543206989765167, 0.05979133024811745, -0.3101854920387268, -0.05635961890220642, 0.09399542957544327, 0.004733020905405283, 0.16654473543167114, -0.05814746394753456, -0.5154407620429993, 0.05284630134701729, -0.3398456275463104, 0.261374831199646, 0.15639284253120422, 0.1551426500082016]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3161638379096985, 0.33391737937927246, 0.19996307790279388, 0.2648756504058838, 0.27205923199653625, 0.24576374888420105, 0.12041280418634415, 0.15129579603672028, -0.39404016733169556, 0.39254269003868103, -0.08985473215579987, 0.31511104106903076, 0.09555639326572418, -0.0776311457157135, 0.28618496656417847, 0.17936135828495026]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.053026411682367325, 0.1982814371585846, 0.11757674068212509, 0.1588219851255417, -0.36263415217399597, 0.20326268672943115, 0.05928875878453255, 0.20789220929145813, -0.0278859194368124, -0.16864646971225739, 0.059262849390506744, -0.06587638705968857, 0.1686677634716034, 0.10846254974603653, -0.028666222468018532, 0.1360781192779541]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2123870998620987, 0.28225842118263245, -0.09463998675346375, 0.3246423304080963, 0.46223148703575134, 0.40103277564048767, 0.11721669882535934, -0.013758665882050991, 0.62225341796875, -0.18451035022735596, 0.25544801354408264, 0.04934448003768921, 0.6742547750473022, -0.21402350068092346, -0.0308295339345932, 0.1921146810054779]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8618170022964478, 0.7451426982879639, 0.6183509230613708, 0.5770534873008728, 0.8369039297103882, 0.43364208936691284, 1.1021649837493896, 1.070716142654419, 0.08472971618175507, 0.018369799479842186, 0.4215329587459564, 0.7907115817070007, 0.7364262342453003, 0.8189175724983215, 1.119879126548767, 0.7892064452171326]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1471527814865112, 1.1671092510223389, 0.6498870849609375, 1.0482450723648071, 0.94851154088974, 0.37345802783966064, 0.9835163950920105, 1.4173918962478638, 0.7604131102561951, 0.2763329744338989, 1.0955946445465088, 1.1033326387405396, 0.6103616952896118, 1.0936321020126343, 0.49946826696395874, 0.9962811470031738]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2345901727676392, 1.5978786945343018, 1.5244876146316528, 1.4045535326004028, 1.4165353775024414, 0.8254831433296204, 1.7520523071289062, 1.2159010171890259, 1.4826912879943848, 1.4515409469604492, 1.248144507408142, 1.1601063013076782, 1.1355661153793335, 1.3235849142074585, 1.1913745403289795, 1.1875065565109253]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0373884439468384, 1.5027790069580078, 1.6489999294281006, 1.7907295227050781, 1.2435911893844604, 1.5797065496444702, 1.2922085523605347, 1.5285509824752808, 1.3458330631256104, 0.6238848567008972, 1.6987535953521729, 1.9497870206832886, 1.760055422782898, 1.6711972951889038, 1.4211732149124146, 1.5292308330535889]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0692038536071777, 3.861506462097168, 3.0297458171844482, 2.6387622356414795, 3.4851326942443848, 2.0761444568634033, 3.65926456451416, 3.1757705211639404, 3.825106143951416, 3.3810882568359375, 3.191504955291748, 3.922938585281372, 2.1229732036590576, 3.954191207885742, 3.1579222679138184, 2.7887964248657227]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3972699642181396, 2.373098134994507, 2.848451852798462, 2.4311907291412354, 2.8083765506744385, 2.072113275527954, 1.5731241703033447, 2.115022659301758, 2.639070510864258, 1.9910435676574707, 1.3487473726272583, 1.9423179626464844, 2.434321403503418, 2.6776821613311768, 2.6442177295684814, 2.0004899501800537]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5639772415161133, 3.411287784576416, 3.0542547702789307, 3.116039752960205, 3.032372236251831, 2.840074062347412, 2.790585517883301, 2.055379867553711, 3.279494047164917, 2.9437196254730225, 2.7628886699676514, 1.3673551082611084, 2.916900157928467, 2.763169050216675, 3.034099817276001, 2.064646005630493]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9239472150802612, 1.8373744487762451, 2.640479326248169, 2.5925309658050537, 2.6288633346557617, 1.9907158613204956, 1.7828495502471924, 3.095838785171509, 0.56777024269104, 2.1781017780303955, 0.983398973941803, 2.4522876739501953, 2.7486069202423096, 2.9267406463623047, 2.9575490951538086, 1.432045340538025]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.674898147583008, 3.9485201835632324, 5.55573034286499, 4.937980651855469, 5.504811763763428, 5.6722564697265625, 5.131338596343994, 5.108266830444336, 6.150730133056641, 5.039750576019287, 5.765255928039551, 6.069166660308838, 4.829602241516113, 5.395456314086914, 4.300554275512695, 4.9531168937683105]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.203693866729736, 4.55449104309082, 5.546280860900879, 5.1649932861328125, 3.6963512897491455, 4.4539875984191895, 4.876542091369629, 5.287906646728516, 5.358046054840088, 4.9105353355407715, 4.263495445251465, 3.3463034629821777, 4.402473449707031, 3.3812315464019775, 4.919106483459473, 4.492633819580078]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.871032238006592, 5.83481502532959, 6.026845455169678, 5.619846343994141, 6.065547466278076, 4.808605194091797, 5.677894115447998, 5.630457878112793, 5.300778388977051, 5.350659370422363, 6.204923152923584, 5.893909931182861, 4.941139221191406, 5.850244045257568, 6.036492347717285, 5.858280181884766]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Running loglikelihood requests:  28%|█████████████▉                                   | 57/200 [02:09<04:45,  2.00s/it]Layer: gate_20 - Captured router_logits: [2.989978790283203, 2.9708776473999023, 2.810760736465454, 2.244276285171509, -0.4343578815460205, 3.039382219314575, 2.7174806594848633, 1.8561652898788452, 2.716336965560913, 3.4668846130371094, 2.684358596801758, 3.2452802658081055, 2.8604769706726074, 2.950305938720703, 2.33188796043396, 1.7413088083267212]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.53394889831543, 6.8058061599731445, 6.522369384765625, 6.291496276855469, 6.446741580963135, 6.465500354766846, 6.729918479919434, 6.618077278137207, 6.626253604888916, 6.313270568847656, 6.910423278808594, 6.245215892791748, 5.269382476806641, 7.184790134429932, 6.469113826751709, 6.423297882080078]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.00491189956665, 5.346713066101074, 4.953638553619385, 4.042628765106201, 4.896851539611816, 5.242650985717773, 4.6264262199401855, 4.778510093688965, 5.065245151519775, 4.843103408813477, 5.263955593109131, 4.873143672943115, 5.1377129554748535, 5.428319931030273, 4.593223571777344, 5.472162246704102]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.143037796020508, 4.661443710327148, 3.333631753921509, 4.191773891448975, 4.448845863342285, 4.072947025299072, 4.237218856811523, 4.2262420654296875, 4.356021404266357, 4.463611602783203, 4.052189826965332, 4.423557281494141, 3.6704776287078857, 2.0232620239257812, 3.374225616455078, 4.015105247497559]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.276243686676025, 6.228745937347412, 5.9029059410095215, 6.0739264488220215, 5.422220230102539, 5.8416008949279785, 6.169297218322754, 5.696838855743408, 5.339330673217773, 5.747849941253662, 6.428916931152344, 5.515284538269043, 5.759804725646973, 6.201245307922363, 6.218967437744141, 5.778147220611572]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8275723457336426, 3.6636815071105957, 3.777482271194458, 3.9395065307617188, 2.683013677597046, 3.8532021045684814, 3.931210517883301, 3.649698257446289, 3.983229875564575, 3.6056997776031494, 3.688427448272705, 3.613455057144165, 3.6553494930267334, 3.5915822982788086, 3.695774555206299, 3.8867595195770264]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4448928833007812, 3.5195212364196777, 3.3993942737579346, 3.548823833465576, 3.3792643547058105, 3.6143381595611572, 3.5657644271850586, 3.3607311248779297, 3.5936405658721924, 3.290336847305298, 3.5945301055908203, 3.4413487911224365, 3.4295578002929688, 2.723748207092285, 3.6432807445526123, 3.5580592155456543]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.175170421600342, 3.201225519180298, 2.9793076515197754, 2.8906490802764893, 2.813628673553467, 3.079925060272217, 3.1762874126434326, 3.269099235534668, 3.027223587036133, 3.00095272064209, 2.283803701400757, 2.992424964904785, 3.1494386196136475, 3.0561437606811523, 3.0771515369415283, 2.8798885345458984]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.366829872131348, 4.483258247375488, 4.5612006187438965, 4.367901802062988, 4.484335422515869, 4.456886291503906, 4.21073579788208, 4.700796604156494, 4.4864397048950195, 4.451462745666504, 4.463454723358154, 4.047793865203857, 4.191420078277588, 4.050203800201416, 4.5707106590271, 4.367067337036133]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.56606674194336, 8.886096000671387, 8.770363807678223, 8.45425796508789, 8.53713321685791, 8.250211715698242, 8.616982460021973, 8.526864051818848, 8.61327838897705, 8.477723121643066, 8.583778381347656, 8.215991973876953, 8.316078186035156, 8.983925819396973, 8.879494667053223, 8.488780975341797]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.876248359680176, 5.4144697189331055, 5.374093055725098, 4.776517868041992, 5.123172760009766, 5.074467658996582, 4.656064987182617, 5.118605136871338, 5.224178791046143, 4.912867546081543, 4.822896957397461, 4.718443393707275, 4.85288143157959, 4.9697771072387695, 5.053208351135254, 4.821218490600586]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2247166633605957, 2.993130922317505, 3.093095064163208, 2.75256085395813, 3.0146467685699463, 3.302698850631714, 2.746276378631592, 3.17156720161438, 3.077038526535034, 3.1754674911499023, 3.1659977436065674, 3.252192735671997, 3.024445056915283, 2.972296714782715, 3.1130964756011963, 3.169680118560791]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12225879728794098, 0.13829916715621948, 0.13080300390720367, -0.242745041847229, -0.2436724752187729, -0.10526535660028458, 0.14471924304962158, -0.18000297248363495, 0.09948775172233582, 0.11733110249042511, 0.11758247017860413, 0.07702808827161789, 0.10716418921947479, 0.1260174959897995, -1.1212273836135864, 0.13847984373569489]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09080014377832413, 0.05318306386470795, 0.04265810176730156, 0.06120336428284645, 0.08548460155725479, 0.03675338625907898, 0.05329711362719536, 0.07791870832443237, 0.018359243869781494, 0.0619930624961853, -0.1821412742137909, 0.06227107346057892, 0.0012769678141921759, -0.007160172797739506, 0.018381500616669655, 0.03113377094268799]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07415734976530075, 0.04147261381149292, 0.08999597281217575, 0.051503829658031464, 0.09314193576574326, 0.1050921306014061, 0.058470468968153, -0.1445515900850296, 0.06408315896987915, 0.08581346273422241, -0.001282163430005312, 0.07626695185899734, -0.21501696109771729, -0.010326066985726357, -0.061079636216163635, 0.12380141019821167]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21461252868175507, 0.15044426918029785, 0.12380065023899078, 0.1429966390132904, 0.042533084750175476, 0.05209744721651077, -0.09027615934610367, 0.17700500786304474, 0.15107153356075287, -0.4351184666156769, 0.024846402928233147, 0.16174674034118652, 0.08675628155469894, -0.2783629894256592, -0.07664233446121216, -0.04632075875997543]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07494186609983444, -0.03242724761366844, -0.06849677860736847, 0.13990455865859985, -0.06874918937683105, -0.14698995649814606, 0.08730185776948929, 0.049390118569135666, 0.070151686668396, 0.20480476319789886, -0.27864527702331543, 0.02683860808610916, 0.015254279598593712, -0.18187512457370758, 0.08229480683803558, 0.02520597167313099]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.2105167955160141, 0.2401074320077896, 0.14255087077617645, 0.05640159547328949, -0.3073621392250061, -0.05878906697034836, 0.0928468108177185, 0.004582114052027464, 0.16506683826446533, -0.056783732026815414, -0.506145715713501, 0.05111774429678917, -0.3358481228351593, 0.2598070204257965, 0.15234394371509552, 0.15554770827293396]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3123241364955902, 0.3295604884624481, 0.19548115134239197, 0.26740342378616333, 0.2695370018482208, 0.24125553667545319, 0.12158527970314026, 0.14970749616622925, -0.3926420509815216, 0.38971254229545593, -0.08019053936004639, 0.31345710158348083, 0.09116190671920776, -0.07378897070884705, 0.28373485803604126, 0.1752738356590271]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05243660509586334, 0.19677680730819702, 0.11676864326000214, 0.15919822454452515, -0.3563644587993622, 0.2015717625617981, 0.05762481689453125, 0.20692291855812073, -0.029443640261888504, -0.17109902203083038, 0.061438750475645065, -0.06775719672441483, 0.17314250767230988, 0.10634396970272064, -0.022364024072885513, 0.14491981267929077]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21101360023021698, 0.28188228607177734, -0.09234313666820526, 0.32680660486221313, 0.4614274203777313, 0.3983016312122345, 0.11485655605792999, -0.012557901442050934, 0.6225019097328186, -0.18958164751529694, 0.25501683354377747, 0.05347060039639473, 0.6734089851379395, -0.20743001997470856, -0.031470101326704025, 0.1920435130596161]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8624664545059204, 0.7533085346221924, 0.6193844079971313, 0.5739999413490295, 0.8398504853248596, 0.4384564161300659, 1.102269172668457, 1.0727958679199219, 0.08822169899940491, 0.02493082545697689, 0.4282734990119934, 0.79137122631073, 0.7418336272239685, 0.8229259848594666, 1.1184107065200806, 0.7887522578239441]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1489555835723877, 1.170414924621582, 0.6559745669364929, 1.0524733066558838, 0.9509178400039673, 0.3847145438194275, 0.9881855249404907, 1.4241650104522705, 0.7703167200088501, 0.2903558611869812, 1.0985780954360962, 1.1080355644226074, 0.6246944069862366, 1.0990715026855469, 0.5007069110870361, 0.9998505115509033]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2416257858276367, 1.6036181449890137, 1.5299378633499146, 1.4145214557647705, 1.4211188554763794, 0.8296041488647461, 1.7502866983413696, 1.2239006757736206, 1.4823637008666992, 1.4531216621398926, 1.2548490762710571, 1.1663196086883545, 1.139012336730957, 1.33201003074646, 1.206498146057129, 1.1950596570968628]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0455098152160645, 1.5037696361541748, 1.6497141122817993, 1.7950862646102905, 1.2548539638519287, 1.5872933864593506, 1.295387864112854, 1.5292552709579468, 1.350508689880371, 0.6247636675834656, 1.7022275924682617, 1.953099012374878, 1.7615728378295898, 1.677276372909546, 1.4242957830429077, 1.5303691625595093]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.075437068939209, 3.863607883453369, 3.032956838607788, 2.648651123046875, 3.485772132873535, 2.0798721313476562, 3.662245988845825, 3.1726675033569336, 3.829807758331299, 3.3822951316833496, 3.193575143814087, 3.9293344020843506, 2.1324737071990967, 3.9606449604034424, 3.1559948921203613, 2.801614761352539]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3905155658721924, 2.3699777126312256, 2.848536491394043, 2.4318926334381104, 2.8069069385528564, 2.0638346672058105, 1.5758064985275269, 2.103264570236206, 2.640198230743408, 1.9859528541564941, 1.3433185815811157, 1.9383621215820312, 2.428358316421509, 2.6767337322235107, 2.6435086727142334, 1.99801504611969]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.56756854057312, 3.414123296737671, 3.054049015045166, 3.1155903339385986, 3.03708553314209, 2.8416285514831543, 2.7867519855499268, 2.0561366081237793, 3.279423475265503, 2.9451916217803955, 2.7617759704589844, 1.3694534301757812, 2.9201557636260986, 2.764737844467163, 3.036675214767456, 2.062481164932251]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9251694679260254, 1.8438336849212646, 2.6433284282684326, 2.5974924564361572, 2.6317124366760254, 1.9924302101135254, 1.7805616855621338, 3.0962419509887695, 0.567657470703125, 2.179413080215454, 0.9842591285705566, 2.454439163208008, 2.751305103302002, 2.9304099082946777, 2.9612326622009277, 1.4336085319519043]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6813395023345947, 3.952463150024414, 5.561164855957031, 4.946346759796143, 5.514038562774658, 5.677276134490967, 5.133420467376709, 5.114029407501221, 6.157078742980957, 5.05050802230835, 5.771429061889648, 6.075493335723877, 4.833192825317383, 5.401383399963379, 4.306686878204346, 4.9596710205078125]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.210700035095215, 4.5588765144348145, 5.5533928871154785, 5.172368049621582, 3.703453779220581, 4.4656805992126465, 4.879053592681885, 5.295129299163818, 5.364939212799072, 4.91586446762085, 4.26907205581665, 3.351531982421875, 4.408046722412109, 3.3849122524261475, 4.921681880950928, 4.496345520019531]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.876677989959717, 5.839961051940918, 6.0325212478637695, 5.6252336502075195, 6.071968078613281, 4.8127641677856445, 5.683807849884033, 5.63570499420166, 5.30256986618042, 5.354513645172119, 6.211479663848877, 5.898693561553955, 4.943911075592041, 5.856318473815918, 6.04231595993042, 5.864139556884766]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.992311954498291, 2.9734599590301514, 2.811948776245117, 2.2446959018707275, -0.43756598234176636, 3.040397882461548, 2.7197017669677734, 1.856447458267212, 2.716233015060425, 3.4674489498138428, 2.685338258743286, 3.2454833984375, 2.8593719005584717, 2.948425054550171, 2.3315937519073486, 1.7389805316925049]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.545212268829346, 6.815465450286865, 6.5323381423950195, 6.3017401695251465, 6.455519199371338, 6.475638389587402, 6.741143226623535, 6.630746841430664, 6.636868000030518, 6.32302713394165, 6.920872211456299, 6.2541823387146, 5.278095245361328, 7.196946620941162, 6.4801411628723145, 6.433629512786865]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.008676528930664, 5.35076904296875, 4.95894718170166, 4.046794414520264, 4.9009623527526855, 5.24798583984375, 4.629908561706543, 4.782069206237793, 5.071990966796875, 4.846973419189453, 5.268224716186523, 4.876063823699951, 5.143057346343994, 5.433196067810059, 4.598082065582275, 5.477143287658691]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.144415378570557, 4.666539669036865, 3.3379766941070557, 4.194890975952148, 4.454531669616699, 4.076976776123047, 4.243485927581787, 4.232358455657959, 4.3611907958984375, 4.4691572189331055, 4.055692672729492, 4.428332805633545, 3.6731247901916504, 2.0243701934814453, 3.378920078277588, 4.018746376037598]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  30%|██████████████▉                                  | 61/200 [02:17<04:34,  1.98s/it]Layer: gate_24 - Captured router_logits: [6.284724235534668, 6.23726749420166, 5.911148548126221, 6.082315444946289, 5.428118705749512, 5.850092887878418, 6.177100658416748, 5.7047953605651855, 5.346932888031006, 5.754767417907715, 6.438879489898682, 5.522488594055176, 5.768217086791992, 6.210044860839844, 6.22743034362793, 5.78539514541626]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8347668647766113, 3.67010760307312, 3.784639835357666, 3.9460954666137695, 2.686877965927124, 3.8617067337036133, 3.938908576965332, 3.6545491218566895, 3.9905831813812256, 3.612657308578491, 3.6968421936035156, 3.6198410987854004, 3.6616039276123047, 3.598904609680176, 3.70326566696167, 3.8936116695404053]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4500653743743896, 3.524733304977417, 3.404340982437134, 3.5541744232177734, 3.3839712142944336, 3.6200060844421387, 3.572633981704712, 3.36635684967041, 3.5994958877563477, 3.295039653778076, 3.599425792694092, 3.4473695755004883, 3.4347083568573, 2.7255234718322754, 3.6482865810394287, 3.564026117324829]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.181243896484375, 3.2079834938049316, 2.9842567443847656, 2.8956246376037598, 2.8184502124786377, 3.0853352546691895, 3.1811978816986084, 3.274637460708618, 3.0329654216766357, 3.00537109375, 2.286912202835083, 2.9978554248809814, 3.1555428504943848, 3.062221050262451, 3.082986354827881, 2.885406494140625]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.372440338134766, 4.488753318786621, 4.566287040710449, 4.373246669769287, 4.49159049987793, 4.462629318237305, 4.216815948486328, 4.707367420196533, 4.492211818695068, 4.457551956176758, 4.469242095947266, 4.052515029907227, 4.196576118469238, 4.0555644035339355, 4.57587194442749, 4.373542785644531]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.577990531921387, 8.903786659240723, 8.7831392288208, 8.468840599060059, 8.551508903503418, 8.263616561889648, 8.633387565612793, 8.538376808166504, 8.625481605529785, 8.491228103637695, 8.595865249633789, 8.229456901550293, 8.328948974609375, 8.998242378234863, 8.89559268951416, 8.504809379577637]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.886846542358398, 5.426960468292236, 5.385313034057617, 4.786595344543457, 5.134355068206787, 5.086603164672852, 4.667870998382568, 5.130758285522461, 5.235759258270264, 4.924850940704346, 4.836099147796631, 4.730194091796875, 4.863405227661133, 4.980081558227539, 5.065438747406006, 4.831079006195068]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2332863807678223, 2.999918222427368, 3.1004207134246826, 2.7611942291259766, 3.0224788188934326, 3.309163808822632, 2.7545411586761475, 3.178300142288208, 3.0855329036712646, 3.1828598976135254, 3.1724693775177, 3.2602760791778564, 3.0325369834899902, 2.9793338775634766, 3.1195178031921387, 3.178067445755005]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11813809722661972, 0.13313938677310944, 0.12589548528194427, -0.23521776497364044, -0.23916812241077423, -0.10252626985311508, 0.14027871191501617, -0.16348662972450256, 0.10228496044874191, 0.11257212609052658, 0.11240004748106003, 0.07280708849430084, 0.10536834597587585, 0.12419088184833527, -1.0984649658203125, 0.13403262197971344]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08654002100229263, 0.05335696041584015, 0.04226851835846901, 0.06086435168981552, 0.08395347744226456, 0.03600483015179634, 0.0528375618159771, 0.0806666687130928, 0.021001828834414482, 0.06115703657269478, -0.17549990117549896, 0.05864660441875458, 0.0011667150538414717, -0.0014833988389000297, 0.017155522480607033, 0.029810989275574684]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07086548209190369, 0.0446440652012825, 0.09148470312356949, 0.052601322531700134, 0.09873396158218384, 0.1029917299747467, 0.058681387454271317, -0.14177224040031433, 0.0629797950387001, 0.08340819180011749, -0.0001473384618293494, 0.08052448183298111, -0.20696792006492615, -0.0063520772382617, -0.056223198771476746, 0.12391719222068787]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21509836614131927, 0.14981961250305176, 0.1257467120885849, 0.1477212756872177, 0.04805442690849304, 0.055818475782871246, -0.08691822737455368, 0.17863959074020386, 0.15413908660411835, -0.4423619508743286, 0.029800759628415108, 0.15936629474163055, 0.09729193150997162, -0.2802361845970154, -0.08096618950366974, -0.043823275715112686]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07415056228637695, -0.031116288155317307, -0.06620016694068909, 0.1391894519329071, -0.06971980631351471, -0.15097975730895996, 0.09185279160737991, 0.048843227326869965, 0.06452623754739761, 0.2036857008934021, -0.2772165536880493, 0.026814918965101242, 0.012588131241500378, -0.17514297366142273, 0.0772813931107521, 0.02198946848511696]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21289320290088654, 0.242001011967659, 0.1435561329126358, 0.05500618368387222, -0.308908611536026, -0.05645840987563133, 0.09393675625324249, 0.001679824898019433, 0.1669703722000122, -0.05962027609348297, -0.5110189914703369, 0.052851539105176926, -0.3360389471054077, 0.25880664587020874, 0.15434540808200836, 0.15692999958992004]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.31472721695899963, 0.33408859372138977, 0.19552923738956451, 0.26805630326271057, 0.2718505859375, 0.24160273373126984, 0.1234535202383995, 0.1522456407546997, -0.39533600211143494, 0.39064866304397583, -0.07824219018220901, 0.31369510293006897, 0.09574055671691895, -0.07459293305873871, 0.28631338477134705, 0.17651088535785675]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05326750874519348, 0.19867664575576782, 0.11758195608854294, 0.1585512012243271, -0.3579949140548706, 0.20157958567142487, 0.058635566383600235, 0.20916877686977386, -0.0297673512250185, -0.16970819234848022, 0.0607130266726017, -0.0652487650513649, 0.1724112331867218, 0.10756121575832367, -0.027386756613850594, 0.14572970569133759]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21192783117294312, 0.2827223241329193, -0.0927330031991005, 0.3279283046722412, 0.4609719216823578, 0.3986737132072449, 0.11634264886379242, -0.014049254357814789, 0.6233811378479004, -0.18956269323825836, 0.2565138638019562, 0.05432838946580887, 0.6749981641769409, -0.20736734569072723, -0.03133795037865639, 0.19355431199073792]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8636263012886047, 0.7543891668319702, 0.620028018951416, 0.5776301622390747, 0.8403674364089966, 0.4394570291042328, 1.1038652658462524, 1.0741477012634277, 0.09003646671772003, 0.025939524173736572, 0.4298125207424164, 0.7920902967453003, 0.7432343363761902, 0.821932852268219, 1.1230765581130981, 0.7890962958335876]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1503515243530273, 1.1710981130599976, 0.6558541655540466, 1.0529060363769531, 0.955112636089325, 0.3825934827327728, 0.9873444437980652, 1.425850510597229, 0.7710450291633606, 0.2868109941482544, 1.0984631776809692, 1.1082926988601685, 0.6256753206253052, 1.100195050239563, 0.49693533778190613, 1.0033036470413208]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2423309087753296, 1.6052354574203491, 1.5295847654342651, 1.4124903678894043, 1.4224146604537964, 0.8344742655754089, 1.7519166469573975, 1.227705478668213, 1.4843522310256958, 1.4532983303070068, 1.2556662559509277, 1.1691476106643677, 1.144572138786316, 1.3348486423492432, 1.210797667503357, 1.1962193250656128]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0452839136123657, 1.505914568901062, 1.6501340866088867, 1.7941843271255493, 1.256819486618042, 1.5874583721160889, 1.292493462562561, 1.5299452543258667, 1.3526945114135742, 0.6253081560134888, 1.7024474143981934, 1.9516743421554565, 1.7611713409423828, 1.675455927848816, 1.423558235168457, 1.5301188230514526]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0804426670074463, 3.866487503051758, 3.0334932804107666, 2.6529178619384766, 3.4838693141937256, 2.0842580795288086, 3.662930488586426, 3.174999952316284, 3.8286001682281494, 3.382056951522827, 3.192704200744629, 3.92875075340271, 2.137338638305664, 3.963695764541626, 3.1617343425750732, 2.8015806674957275]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3913872241973877, 2.370556592941284, 2.8498125076293945, 2.4329092502593994, 2.8073525428771973, 2.065850019454956, 1.5779434442520142, 2.099987268447876, 2.6401448249816895, 1.9835551977157593, 1.3482621908187866, 1.9402309656143188, 2.4263672828674316, 2.6737709045410156, 2.642728090286255, 1.9969021081924438]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.570737361907959, 3.4149553775787354, 3.0570075511932373, 3.1173148155212402, 3.043339252471924, 2.843679904937744, 2.788662910461426, 2.0566086769104004, 3.278519630432129, 2.945675849914551, 2.766885280609131, 1.3741947412490845, 2.9217031002044678, 2.764716863632202, 3.038691997528076, 2.065908432006836]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9236164093017578, 1.8490506410598755, 2.6412177085876465, 2.5962319374084473, 2.629788398742676, 1.9910943508148193, 1.7768168449401855, 3.09662127494812, 0.573390007019043, 2.1743719577789307, 0.9889408946037292, 2.4544239044189453, 2.7508997917175293, 2.9307422637939453, 2.959731340408325, 1.4330649375915527]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.682990074157715, 3.952103614807129, 5.556581974029541, 4.945611476898193, 5.512018203735352, 5.67507266998291, 5.129049301147461, 5.112828731536865, 6.152624130249023, 5.047076225280762, 5.766558647155762, 6.072274208068848, 4.82972526550293, 5.399361610412598, 4.306647777557373, 4.956293106079102]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.208728313446045, 4.556126594543457, 5.553780555725098, 5.168858528137207, 3.702883720397949, 4.465555667877197, 4.876284599304199, 5.295083999633789, 5.362257480621338, 4.915765285491943, 4.268918514251709, 3.3506038188934326, 4.409247398376465, 3.3859200477600098, 4.919374465942383, 4.495325565338135]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.876008033752441, 5.838857650756836, 6.030186653137207, 5.624004364013672, 6.070318222045898, 4.812548637390137, 5.682908058166504, 5.634209156036377, 5.299159526824951, 5.354125022888184, 6.209545135498047, 5.896082878112793, 4.942661762237549, 5.855171203613281, 6.040874004364014, 5.860836982727051]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.991140842437744, 2.9723851680755615, 2.810638189315796, 2.244175910949707, -0.43534916639328003, 3.0387821197509766, 2.7189621925354004, 1.854298710823059, 2.717287540435791, 3.466378688812256, 2.684966802597046, 3.2432339191436768, 2.8583104610443115, 2.9481842517852783, 2.3316996097564697, 1.7383545637130737]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.54383659362793, 6.8124680519104, 6.529109477996826, 6.3000993728637695, 6.454186916351318, 6.473681926727295, 6.739034175872803, 6.627832889556885, 6.635873317718506, 6.321333408355713, 6.918462753295898, 6.251763820648193, 5.275471210479736, 7.194684028625488, 6.478025436401367, 6.432016372680664]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.006863594055176, 5.3479437828063965, 4.957382678985596, 4.047058582305908, 4.8988423347473145, 5.24533224105835, 4.628138065338135, 4.7783355712890625, 5.070565700531006, 4.8443989753723145, 5.265148162841797, 4.873080253601074, 5.140526294708252, 5.431924819946289, 4.596047401428223, 5.4749836921691895]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.14326286315918, 4.665936470031738, 3.3379905223846436, 4.1948137283325195, 4.452546119689941, 4.076904296875, 4.24232816696167, 4.232345104217529, 4.360341548919678, 4.467988967895508, 4.055385112762451, 4.428112506866455, 3.6731057167053223, 2.0243804454803467, 3.379018783569336, 4.018620491027832]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.285172462463379, 6.237615585327148, 5.911848068237305, 6.082663536071777, 5.427757263183594, 5.8512115478515625, 6.17840576171875, 5.705796241760254, 5.348150253295898, 5.756606578826904, 6.437038898468018, 5.523442268371582, 5.769185543060303, 6.21036958694458, 6.228395938873291, 5.786468505859375]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.836092233657837, 3.670409917831421, 3.7854950428009033, 3.9458413124084473, 2.687615394592285, 3.860590696334839, 3.93851637840271, 3.6545658111572266, 3.990922212600708, 3.613295078277588, 3.696807622909546, 3.620497703552246, 3.662860631942749, 3.5992014408111572, 3.7032361030578613, 3.8948731422424316]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.450836658477783, 3.5255351066589355, 3.405635118484497, 3.555185079574585, 3.3855018615722656, 3.621006965637207, 3.573859691619873, 3.3662736415863037, 3.600407361984253, 3.2958362102508545, 3.5998823642730713, 3.4475998878479004, 3.436269998550415, 2.7262156009674072, 3.6488804817199707, 3.565019369125366]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1826655864715576, 3.2093231678009033, 2.985239028930664, 2.8970203399658203, 2.819652795791626, 3.087097406387329, 3.182420015335083, 3.276402711868286, 3.034313440322876, 3.006617784500122, 2.2874395847320557, 2.998877763748169, 3.1570873260498047, 3.063962697982788, 3.084038496017456, 2.8867580890655518]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  32%|███████████████▉                                 | 65/200 [02:24<04:23,  1.95s/it]Layer: gate_28 - Captured router_logits: [4.376308441162109, 4.492241382598877, 4.570362091064453, 4.377108097076416, 4.495951175689697, 4.466586589813232, 4.2204155921936035, 4.711210250854492, 4.496110916137695, 4.462096214294434, 4.473549842834473, 4.057621955871582, 4.201611042022705, 4.060147285461426, 4.579480171203613, 4.378151893615723]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.585864067077637, 8.909667015075684, 8.793362617492676, 8.476236343383789, 8.559919357299805, 8.273225784301758, 8.64130687713623, 8.548212051391602, 8.633665084838867, 8.499115943908691, 8.603798866271973, 8.236262321472168, 8.336502075195312, 9.00632381439209, 8.902762413024902, 8.512410163879395]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.890467166900635, 5.430471897125244, 5.388139247894287, 4.789905071258545, 5.13939094543457, 5.093040466308594, 4.671142578125, 5.134014129638672, 5.24083137512207, 4.928703308105469, 4.838969707489014, 4.732313632965088, 4.866831302642822, 4.9852294921875, 5.068734645843506, 4.834129810333252]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.234679937362671, 3.0009427070617676, 3.1014773845672607, 2.764195442199707, 3.023729085922241, 3.3111202716827393, 2.7567899227142334, 3.1809239387512207, 3.087707281112671, 3.1851325035095215, 3.174938201904297, 3.261692762374878, 3.034641742706299, 2.981776714324951, 3.1223113536834717, 3.179858446121216]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12349098920822144, 0.1395110934972763, 0.1313379853963852, -0.24284303188323975, -0.24503101408481598, -0.10470270365476608, 0.14537057280540466, -0.1836516410112381, 0.10101700574159622, 0.11825545877218246, 0.11795768141746521, 0.07735543698072433, 0.10839077830314636, 0.12750674784183502, -1.126878023147583, 0.13947893679141998]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09132112562656403, 0.05349266901612282, 0.042740970849990845, 0.06181025877594948, 0.08566225320100784, 0.03693327307701111, 0.052524589002132416, 0.07698357105255127, 0.01761825568974018, 0.06240065023303032, -0.18109413981437683, 0.062262650579214096, 0.0013126460835337639, -0.005552699789404869, 0.018213875591754913, 0.030499543994665146]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07335714995861053, 0.0430137999355793, 0.08970897644758224, 0.05082856863737106, 0.09319084882736206, 0.10464970022439957, 0.05757000297307968, -0.1452694535255432, 0.06392025202512741, 0.08315370976924896, 0.00024357617076020688, 0.07489714026451111, -0.21309658885002136, -0.008797639049589634, -0.05963265895843506, 0.12419197708368301]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21519358456134796, 0.14854371547698975, 0.12486821413040161, 0.1430455893278122, 0.04313085600733757, 0.051721084862947464, -0.08573804050683975, 0.1766793578863144, 0.1514476090669632, -0.43608716130256653, 0.024736791849136353, 0.1613062024116516, 0.08562957495450974, -0.2747775912284851, -0.07959999889135361, -0.04737922549247742]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07582667469978333, -0.03248047083616257, -0.06773346662521362, 0.1396031677722931, -0.06855282187461853, -0.14874401688575745, 0.08937930315732956, 0.04919863119721413, 0.07119192183017731, 0.20622652769088745, -0.2776392102241516, 0.026474224403500557, 0.012658472172915936, -0.18141038715839386, 0.08086428791284561, 0.028179122135043144]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21123909950256348, 0.24030859768390656, 0.14322137832641602, 0.05514216795563698, -0.30580785870552063, -0.057368870824575424, 0.09240492433309555, 0.005535392090678215, 0.16416138410568237, -0.05843779444694519, -0.5064333081245422, 0.05096951499581337, -0.3368934988975525, 0.258574515581131, 0.15294623374938965, 0.15485350787639618]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3116517961025238, 0.33111533522605896, 0.19448691606521606, 0.2673918604850769, 0.26859432458877563, 0.24142631888389587, 0.12172018736600876, 0.14941264688968658, -0.3916557729244232, 0.3897622525691986, -0.08028212189674377, 0.3135843276977539, 0.09024687111377716, -0.07246250659227371, 0.28253284096717834, 0.1759830266237259]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.052121005952358246, 0.19816042482852936, 0.11734801530838013, 0.15879493951797485, -0.35674601793289185, 0.20191970467567444, 0.05752575770020485, 0.20892244577407837, -0.02996676415205002, -0.17164303362369537, 0.06207865849137306, -0.06756401062011719, 0.17308519780635834, 0.10581545531749725, -0.022126007825136185, 0.1456034928560257]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20983350276947021, 0.28213950991630554, -0.09040369093418121, 0.32585424184799194, 0.4606354534626007, 0.39867448806762695, 0.11536595225334167, -0.011712769977748394, 0.6224336624145508, -0.19087770581245422, 0.25461024045944214, 0.053509823977947235, 0.6725173592567444, -0.20755009353160858, -0.029763342812657356, 0.19121921062469482]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.863057017326355, 0.7539005279541016, 0.6190122961997986, 0.5731755495071411, 0.8400475978851318, 0.4393419921398163, 1.1037707328796387, 1.0723956823349, 0.08911670744419098, 0.025259939953684807, 0.42939502000808716, 0.7910462021827698, 0.7418679594993591, 0.8238785266876221, 1.1177358627319336, 0.7885416150093079]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1485559940338135, 1.1693165302276611, 0.6555838584899902, 1.0527006387710571, 0.951058566570282, 0.38339942693710327, 0.9878422021865845, 1.4228347539901733, 0.7715094685554504, 0.2892262041568756, 1.0976601839065552, 1.1069927215576172, 0.625289261341095, 1.09812593460083, 0.499598890542984, 1.0015913248062134]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2406996488571167, 1.6021445989608765, 1.5283371210098267, 1.413265347480774, 1.4207172393798828, 0.8316242098808289, 1.7499916553497314, 1.2238283157348633, 1.4811770915985107, 1.4509398937225342, 1.254804253578186, 1.1666048765182495, 1.1401158571243286, 1.3319206237792969, 1.207421064376831, 1.1953247785568237]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0461915731430054, 1.5053280591964722, 1.6489282846450806, 1.7940255403518677, 1.2560590505599976, 1.5866363048553467, 1.2940393686294556, 1.5281097888946533, 1.3514198064804077, 0.6255601048469543, 1.7014786005020142, 1.952093243598938, 1.7622722387313843, 1.6759032011032104, 1.423540711402893, 1.5302846431732178]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0756640434265137, 3.8618814945220947, 3.0326569080352783, 2.650603771209717, 3.484424591064453, 2.0816962718963623, 3.661341667175293, 3.170339822769165, 3.8283705711364746, 3.381530284881592, 3.1916661262512207, 3.929098129272461, 2.1339099407196045, 3.960688829421997, 3.157980442047119, 2.8009729385375977]
Running loglikelihood requests:  34%|████████████████▉                                | 69/200 [02:33<04:23,  2.01s/it]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3896963596343994, 2.3682198524475098, 2.8472254276275635, 2.4314119815826416, 2.8056466579437256, 2.0645670890808105, 1.5755431652069092, 2.1012940406799316, 2.639335870742798, 1.9856082201004028, 1.3445475101470947, 1.9390581846237183, 2.42712664604187, 2.675245761871338, 2.642199754714966, 1.998308539390564]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5680689811706543, 3.4140732288360596, 3.0546295642852783, 3.1149184703826904, 3.0384795665740967, 2.8419299125671387, 2.7858593463897705, 2.055924892425537, 3.278862953186035, 2.944770336151123, 2.7637686729431152, 1.3707987070083618, 2.9198966026306152, 2.7652077674865723, 3.0366690158843994, 2.0637285709381104]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9236838817596436, 1.8450068235397339, 2.6416709423065186, 2.594758987426758, 2.6309587955474854, 1.992125153541565, 1.7781201601028442, 3.0941479206085205, 0.5681908130645752, 2.1775009632110596, 0.9852246642112732, 2.4534451961517334, 2.7498395442962646, 2.928400754928589, 2.9595420360565186, 1.432153582572937]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6852362155914307, 3.9537737369537354, 5.560890197753906, 4.94823694229126, 5.514702320098877, 5.678399085998535, 5.133152008056641, 5.115740776062012, 6.15666389465332, 5.049141883850098, 5.7714009284973145, 6.075408458709717, 4.834136962890625, 5.401690483093262, 4.308171272277832, 4.960116863250732]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.211212158203125, 4.559035778045654, 5.552379608154297, 5.171633720397949, 3.703784227371216, 4.465164661407471, 4.879499435424805, 5.295306205749512, 5.364638328552246, 4.916312217712402, 4.271431922912598, 3.3530824184417725, 4.409998893737793, 3.3868777751922607, 4.9220051765441895, 4.497259616851807]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.877931594848633, 5.840195655822754, 6.033353805541992, 5.625881671905518, 6.072204113006592, 4.814860820770264, 5.685669422149658, 5.637052059173584, 5.302842617034912, 5.355935096740723, 6.2121500968933105, 5.899563312530518, 4.946061134338379, 5.857021331787109, 6.042342662811279, 5.864536762237549]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.9930665493011475, 2.9728856086730957, 2.811880111694336, 2.245392322540283, -0.4362524747848511, 3.0404269695281982, 2.719867706298828, 1.8574477434158325, 2.7166998386383057, 3.4673025608062744, 2.686063051223755, 3.2450592517852783, 2.8592100143432617, 2.9481232166290283, 2.3321166038513184, 1.7400983572006226]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.5465474128723145, 6.816226005554199, 6.533137321472168, 6.302877426147461, 6.457480430603027, 6.476874351501465, 6.7422332763671875, 6.631954669952393, 6.63794469833374, 6.324528217315674, 6.9218854904174805, 6.254726409912109, 5.279211521148682, 7.197964191436768, 6.481816291809082, 6.434696197509766]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.009276866912842, 5.351222991943359, 4.959604740142822, 4.047065258026123, 4.901135444641113, 5.24815559387207, 4.6304097175598145, 4.781539440155029, 5.071928024291992, 4.846837043762207, 5.268258571624756, 4.875792980194092, 5.143246173858643, 5.432672023773193, 4.598194599151611, 5.476919174194336]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.143943786621094, 4.6661763191223145, 3.3368747234344482, 4.194639205932617, 4.453360080718994, 4.076197624206543, 4.242126941680908, 4.231621265411377, 4.3606109619140625, 4.46837854385376, 4.054717540740967, 4.427076816558838, 3.6725707054138184, 2.0233073234558105, 3.377941846847534, 4.017730236053467]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.2840447425842285, 6.236319541931152, 5.910680770874023, 6.081907749176025, 5.427492141723633, 5.849064826965332, 6.176603317260742, 5.703623294830322, 5.346235752105713, 5.754323482513428, 6.4375200271606445, 5.5213141441345215, 5.76812744140625, 6.209309101104736, 6.226563930511475, 5.78501033782959]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.834913730621338, 3.6699554920196533, 3.783935308456421, 3.9458165168762207, 2.6868066787719727, 3.8610215187072754, 3.9389045238494873, 3.6543612480163574, 3.9905474185943604, 3.612321376800537, 3.6965949535369873, 3.619750499725342, 3.6610445976257324, 3.5987908840179443, 3.7028191089630127, 3.8939921855926514]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4501938819885254, 3.524937391281128, 3.404630661010742, 3.5540812015533447, 3.38338303565979, 3.6202239990234375, 3.5722806453704834, 3.3656082153320312, 3.599738836288452, 3.2951550483703613, 3.5988752841949463, 3.446960687637329, 3.434656858444214, 2.7256295680999756, 3.648439884185791, 3.564049482345581]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1810965538024902, 3.2074265480041504, 2.9838054180145264, 2.8953959941864014, 2.818255662918091, 3.0850183963775635, 3.1808359622955322, 3.274348735809326, 3.032396078109741, 3.004923105239868, 2.2867696285247803, 2.997821569442749, 3.1552011966705322, 3.061941385269165, 3.0822644233703613, 2.8852100372314453]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.37162971496582, 4.48842191696167, 4.565441131591797, 4.372798919677734, 4.490473747253418, 4.462464332580566, 4.215755939483643, 4.706883430480957, 4.491796016693115, 4.456942081451416, 4.468178749084473, 4.051881313323975, 4.19612979888916, 4.054980278015137, 4.575226306915283, 4.3725409507751465]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.57752799987793, 8.903329849243164, 8.782718658447266, 8.468172073364258, 8.551183700561523, 8.262927055358887, 8.632721900939941, 8.537487030029297, 8.62466049194336, 8.490647315979004, 8.594886779785156, 8.22898006439209, 8.328446388244629, 8.997591018676758, 8.895293235778809, 8.50424575805664]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.887868881225586, 5.427777290344238, 5.385669231414795, 4.787879467010498, 5.134943962097168, 5.0872015953063965, 4.669127464294434, 5.131619453430176, 5.236756324768066, 4.925776481628418, 4.836285591125488, 4.7307024002075195, 4.864233493804932, 4.980067729949951, 5.066300392150879, 4.831834316253662]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.233180046081543, 3.0001630783081055, 3.10101056098938, 2.7608251571655273, 3.023223638534546, 3.309598445892334, 2.754503011703491, 3.1782047748565674, 3.0856873989105225, 3.1828081607818604, 3.172560214996338, 3.260683536529541, 3.032768726348877, 2.979745864868164, 3.1195623874664307, 3.178302764892578]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12178939580917358, 0.13831113278865814, 0.12992586195468903, -0.24244099855422974, -0.24371927976608276, -0.10545618087053299, 0.1437031328678131, -0.17758560180664062, 0.10043609887361526, 0.11680804193019867, 0.11592770367860794, 0.07612378150224686, 0.1070653572678566, 0.12638813257217407, -1.1210154294967651, 0.1381770521402359]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09062176197767258, 0.052039001137018204, 0.04224017262458801, 0.061116497963666916, 0.08524653315544128, 0.036977674812078476, 0.0529191717505455, 0.07783690840005875, 0.01697893813252449, 0.06090061366558075, -0.1807677447795868, 0.06257070600986481, 0.0013568743597716093, -0.005732811987400055, 0.018561700358986855, 0.031166089698672295]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07415667921304703, 0.0429367758333683, 0.0905773788690567, 0.052476830780506134, 0.09282112121582031, 0.1033659502863884, 0.05896908789873123, -0.14467819035053253, 0.06331413239240646, 0.08309514820575714, 0.00010485094389878213, 0.076884925365448, -0.21378886699676514, -0.010822991840541363, -0.060600101947784424, 0.12419489026069641]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21457047760486603, 0.14990392327308655, 0.12469303607940674, 0.14459596574306488, 0.042407844215631485, 0.053084846585989, -0.0865163654088974, 0.1775427758693695, 0.15123936533927917, -0.4352373778820038, 0.024860918521881104, 0.1610361784696579, 0.08975479006767273, -0.2792195975780487, -0.0796869620680809, -0.04778733849525452]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07649892568588257, -0.03177354484796524, -0.06819315254688263, 0.1407627910375595, -0.06899871677160263, -0.1476830393075943, 0.08882295340299606, 0.04850862920284271, 0.07191961258649826, 0.2074923813343048, -0.27934664487838745, 0.02708146534860134, 0.016460834071040154, -0.1838804930448532, 0.08192291855812073, 0.023798061534762383]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21031461656093597, 0.24064192175865173, 0.14255332946777344, 0.056077033281326294, -0.30764642357826233, -0.05824441835284233, 0.09349274635314941, 0.004684981424361467, 0.16553722321987152, -0.057537686079740524, -0.5061295032501221, 0.05069250240921974, -0.33772012591362, 0.25914162397384644, 0.15252535045146942, 0.15472738444805145]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.312127947807312, 0.3304082453250885, 0.19466188549995422, 0.26727887988090515, 0.26869213581085205, 0.24178096652030945, 0.12154649943113327, 0.1490357667207718, -0.3917052745819092, 0.3895520567893982, -0.08033860474824905, 0.31339719891548157, 0.09116021543741226, -0.07251574099063873, 0.2840198278427124, 0.17564764618873596]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05250219255685806, 0.19797486066818237, 0.1167089119553566, 0.15826515853405, -0.35601580142974854, 0.20136897265911102, 0.056987401098012924, 0.20832641422748566, -0.029691914096474648, -0.17100892961025238, 0.06125488877296448, -0.06784847378730774, 0.17342031002044678, 0.10570728033781052, -0.022938763722777367, 0.14561346173286438]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21032316982746124, 0.28104454278945923, -0.09123951196670532, 0.32569360733032227, 0.4611395001411438, 0.39842647314071655, 0.11487191915512085, -0.012085790745913982, 0.6221340894699097, -0.19062760472297668, 0.2544806897640228, 0.05308879166841507, 0.6725199222564697, -0.20769543945789337, -0.030685700476169586, 0.19171477854251862]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8623784184455872, 0.7535573840141296, 0.6185073852539062, 0.5729002356529236, 0.8387429714202881, 0.43824315071105957, 1.1024104356765747, 1.0715758800506592, 0.08844833821058273, 0.0237579345703125, 0.42903757095336914, 0.7910361886024475, 0.7417376637458801, 0.8213794231414795, 1.1180732250213623, 0.7877265214920044]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1483519077301025, 1.169905662536621, 0.6549087166786194, 1.0519847869873047, 0.949595034122467, 0.38301655650138855, 0.9875373244285583, 1.4229477643966675, 0.769773542881012, 0.2884184718132019, 1.0975202322006226, 1.1063013076782227, 0.6245664358139038, 1.0976407527923584, 0.49970394372940063, 1.001035213470459]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2399705648422241, 1.6024441719055176, 1.528124451637268, 1.4126514196395874, 1.4200489521026611, 0.8313265442848206, 1.7491565942764282, 1.2236669063568115, 1.4813545942306519, 1.451473355293274, 1.2537786960601807, 1.1666486263275146, 1.1399739980697632, 1.33174729347229, 1.206522822380066, 1.1941571235656738]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0449472665786743, 1.5044664144515991, 1.6485583782196045, 1.7927954196929932, 1.2553335428237915, 1.585312008857727, 1.2930388450622559, 1.5270920991897583, 1.3504329919815063, 0.6254421472549438, 1.6999824047088623, 1.950764775276184, 1.7604396343231201, 1.674844741821289, 1.422331690788269, 1.5294655561447144]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.076214075088501, 3.861187696456909, 3.031895875930786, 2.6496334075927734, 3.4839370250701904, 2.082077980041504, 3.6599464416503906, 3.1709957122802734, 3.8276867866516113, 3.3815228939056396, 3.190370798110962, 3.9267992973327637, 2.133777618408203, 3.9593045711517334, 3.1560745239257812, 2.7995169162750244]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.389195203781128, 2.367849826812744, 2.8459036350250244, 2.429645538330078, 2.8046462535858154, 2.063040256500244, 1.5758264064788818, 2.1008858680725098, 2.6391379833221436, 1.98495352268219, 1.3448442220687866, 1.9384475946426392, 2.426476240158081, 2.6738266944885254, 2.6411936283111572, 1.9971611499786377]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.567042827606201, 3.4128496646881104, 3.053652763366699, 3.114060401916504, 3.037980794906616, 2.842000961303711, 2.784989595413208, 2.0545544624328613, 3.277327299118042, 2.9439165592193604, 2.7628190517425537, 1.3710007667541504, 2.9185523986816406, 2.7646665573120117, 3.0354764461517334, 2.06278657913208]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.924042820930481, 1.8444359302520752, 2.641406774520874, 2.594705820083618, 2.630169630050659, 1.9913889169692993, 1.7778406143188477, 3.0941481590270996, 0.5702843070030212, 2.1773855686187744, 0.9861883521080017, 2.4531331062316895, 2.7494826316833496, 2.927755355834961, 2.958592653274536, 1.4323396682739258]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6819498538970947, 3.9526398181915283, 5.558870315551758, 4.945711612701416, 5.5119853019714355, 5.675184726715088, 5.130918025970459, 5.113144874572754, 6.153967380523682, 5.047501087188721, 5.76783561706543, 6.071928977966309, 4.83136510848999, 5.399568557739258, 4.306340217590332, 4.957611560821533]
Running loglikelihood requests:  36%|█████████████████▉                               | 73/200 [02:41<04:13,  2.00s/it]Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.2096662521362305, 4.556653022766113, 5.551045894622803, 5.170009136199951, 3.703314781188965, 4.463450908660889, 4.876481056213379, 5.293784141540527, 5.362514495849609, 4.914432048797607, 4.269104480743408, 3.351402997970581, 4.40789794921875, 3.3849880695343018, 4.920166492462158, 4.495278358459473]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.875434875488281, 5.838085651397705, 6.030518531799316, 5.62357234954834, 6.069972991943359, 4.812664985656738, 5.682811737060547, 5.634112358093262, 5.300502777099609, 5.353409290313721, 6.209587574005127, 5.896507740020752, 4.943094730377197, 5.854818344116211, 6.040415287017822, 5.8622002601623535]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.99161958694458, 2.9726145267486572, 2.8115251064300537, 2.2446634769439697, -0.4360371232032776, 3.039820909500122, 2.7189993858337402, 1.8556630611419678, 2.716493844985962, 3.467029333114624, 2.6853909492492676, 3.244598627090454, 2.859337568283081, 2.9480206966400146, 2.331329822540283, 1.73911452293396]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.543881893157959, 6.813414573669434, 6.530313491821289, 6.299679756164551, 6.453922748565674, 6.474228858947754, 6.7396416664123535, 6.628909111022949, 6.635095119476318, 6.32159423828125, 6.919100284576416, 6.251161098480225, 5.276845455169678, 7.194759368896484, 6.478689193725586, 6.432015419006348]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.007643699645996, 5.34988260269165, 4.958440780639648, 4.04658317565918, 4.899665355682373, 5.2466864585876465, 4.629452228546143, 4.7806782722473145, 5.070184707641602, 4.845897674560547, 5.266637325286865, 4.873737335205078, 5.141609191894531, 5.431784152984619, 4.596923828125, 5.475165367126465]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.143636703491211, 4.666277885437012, 3.3373258113861084, 4.194449424743652, 4.453330993652344, 4.07646369934082, 4.242237091064453, 4.231527805328369, 4.3607707023620605, 4.468094825744629, 4.0549163818359375, 4.427682399749756, 3.6730847358703613, 2.0246100425720215, 3.378730058670044, 4.018133640289307]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.282925128936768, 6.235634803771973, 5.909839630126953, 6.080691337585449, 5.426499843597412, 5.848428726196289, 6.175609588623047, 5.70328426361084, 5.3459248542785645, 5.7538228034973145, 6.436317443847656, 5.521374225616455, 5.767012119293213, 6.208471775054932, 6.225790500640869, 5.784050941467285]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.83494234085083, 3.6698086261749268, 3.7840049266815186, 3.9454643726348877, 2.687100887298584, 3.861272096633911, 3.938499689102173, 3.6541292667388916, 3.9903745651245117, 3.612337589263916, 3.6964423656463623, 3.6195313930511475, 3.6611745357513428, 3.5981087684631348, 3.702338695526123, 3.893501043319702]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.449998617172241, 3.5245587825775146, 3.4043893814086914, 3.554316759109497, 3.3834824562072754, 3.6202359199523926, 3.572359323501587, 3.3656039237976074, 3.599494218826294, 3.2951643466949463, 3.5992941856384277, 3.4470231533050537, 3.4348762035369873, 2.7262611389160156, 3.6482150554656982, 3.5641565322875977]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1815695762634277, 3.2078990936279297, 2.984206199645996, 2.895758628845215, 2.818711042404175, 3.0855894088745117, 3.181309938430786, 3.274792194366455, 3.0329408645629883, 3.005312919616699, 2.28690242767334, 2.9980034828186035, 3.155906915664673, 3.0627472400665283, 3.0831172466278076, 2.8853607177734375]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.373178958892822, 4.489645957946777, 4.567233562469482, 4.374094009399414, 4.492128849029541, 4.463536739349365, 4.217044353485107, 4.707940101623535, 4.493402004241943, 4.458547592163086, 4.469886779785156, 4.0535125732421875, 4.197926998138428, 4.056382179260254, 4.57651948928833, 4.3741021156311035]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.58017349243164, 8.905112266540527, 8.78567123413086, 8.470356941223145, 8.554126739501953, 8.265746116638184, 8.634943008422852, 8.540517807006836, 8.627485275268555, 8.49324893951416, 8.597640991210938, 8.231148719787598, 8.33067512512207, 9.000701904296875, 8.897713661193848, 8.506356239318848]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.887138366699219, 5.42716121673584, 5.384989261627197, 4.7865986824035645, 5.134860992431641, 5.087662696838379, 4.667599678039551, 5.130965709686279, 5.236546039581299, 4.925113201141357, 4.835306167602539, 4.729605197906494, 4.863220691680908, 4.980239391326904, 5.065329074859619, 4.831090927124023]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2335638999938965, 3.0006215572357178, 3.101069688796997, 2.7614333629608154, 3.0230536460876465, 3.3099517822265625, 2.755141496658325, 3.1789989471435547, 3.0857856273651123, 3.1832594871520996, 3.173234701156616, 3.2606160640716553, 3.0329689979553223, 2.9800593852996826, 3.1205925941467285, 3.178774833679199]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12338007241487503, 0.13964931666851044, 0.13171814382076263, -0.243478924036026, -0.24356557428836823, -0.10397981107234955, 0.14563065767288208, -0.18429595232009888, 0.10046837478876114, 0.1186322271823883, 0.11830096691846848, 0.0785711258649826, 0.10841944813728333, 0.12763139605522156, -1.1248326301574707, 0.13957005739212036]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09131798893213272, 0.05299990624189377, 0.04281548410654068, 0.062061745673418045, 0.08575523644685745, 0.036750320345163345, 0.05384125933051109, 0.0767616257071495, 0.017371270805597305, 0.06317082792520523, -0.18186353147029877, 0.0636909157037735, 0.0010834194254130125, -0.006071591284126043, 0.01827363856136799, 0.03050420992076397]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07409417629241943, 0.04177211597561836, 0.09029634296894073, 0.05084807425737381, 0.09314487129449844, 0.10473688691854477, 0.058665771037340164, -0.14506222307682037, 0.06416165083646774, 0.08415070176124573, 2.4579923774581403e-05, 0.07523138076066971, -0.2139107882976532, -0.00847021397203207, -0.060284797102212906, 0.1236492395401001]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21596650779247284, 0.14981043338775635, 0.12450921535491943, 0.14324481785297394, 0.041762422770261765, 0.05275695025920868, -0.08643557131290436, 0.17680472135543823, 0.1522427499294281, -0.43515971302986145, 0.024509647861123085, 0.16190080344676971, 0.08558893948793411, -0.2786484658718109, -0.07936342060565948, -0.04638436436653137]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07663073390722275, -0.03274054080247879, -0.06663971394300461, 0.13959303498268127, -0.06915576756000519, -0.14797917008399963, 0.08947690576314926, 0.05066240578889847, 0.0712260976433754, 0.20538079738616943, -0.27852389216423035, 0.027543213218450546, 0.013488356955349445, -0.18321147561073303, 0.08093462884426117, 0.026675887405872345]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21175211668014526, 0.23988653719425201, 0.14223195612430573, 0.055408064275979996, -0.3055275082588196, -0.05820617079734802, 0.0928066149353981, 0.0064067658968269825, 0.16401079297065735, -0.057986900210380554, -0.5052234530448914, 0.050255488604307175, -0.33645376563072205, 0.25843527913093567, 0.15228834748268127, 0.1542527973651886]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3115735650062561, 0.3304506242275238, 0.19333094358444214, 0.26758286356925964, 0.2688199579715729, 0.2412412315607071, 0.12175464630126953, 0.14957597851753235, -0.392120361328125, 0.3892084062099457, -0.08032044768333435, 0.31375637650489807, 0.09008938074111938, -0.07228080928325653, 0.2825544774532318, 0.1753806322813034]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05210619047284126, 0.19867341220378876, 0.11716097593307495, 0.15930205583572388, -0.3560784161090851, 0.20199169218540192, 0.05732499808073044, 0.2074023336172104, -0.03007659502327442, -0.17221638560295105, 0.062163759022951126, -0.06734619289636612, 0.17322024703025818, 0.1054101288318634, -0.022129308432340622, 0.14560574293136597]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21048496663570404, 0.28193894028663635, -0.09004947543144226, 0.3258579671382904, 0.4604668617248535, 0.39808207750320435, 0.11484527587890625, -0.011707991361618042, 0.6218892335891724, -0.19136136770248413, 0.25453445315361023, 0.05355755612254143, 0.6721879839897156, -0.20687414705753326, -0.029251836240291595, 0.19103224575519562]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8630664348602295, 0.7542327046394348, 0.6185799241065979, 0.57293701171875, 0.8400384187698364, 0.43966737389564514, 1.1040751934051514, 1.071640968322754, 0.08920904994010925, 0.025668010115623474, 0.4298737943172455, 0.7912319302558899, 0.7417610883712769, 0.824486255645752, 1.1174253225326538, 0.7881484031677246]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1488134860992432, 1.1691035032272339, 0.6560682654380798, 1.0530660152435303, 0.9508379101753235, 0.38289108872413635, 0.9876837134361267, 1.4232937097549438, 0.7720558047294617, 0.28994306921958923, 1.0982805490493774, 1.107456922531128, 0.6258896589279175, 1.09906005859375, 0.4995764195919037, 1.0015316009521484]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.241288185119629, 1.6024692058563232, 1.5293233394622803, 1.4132908582687378, 1.4213618040084839, 0.8324606418609619, 1.750328779220581, 1.2250815629959106, 1.4818164110183716, 1.4514296054840088, 1.2550246715545654, 1.167136788368225, 1.1411211490631104, 1.3325071334838867, 1.2092573642730713, 1.1969105005264282]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0475492477416992, 1.5055516958236694, 1.6483062505722046, 1.794002890586853, 1.256191611289978, 1.5877304077148438, 1.2933850288391113, 1.5278695821762085, 1.351828932762146, 0.6261147260665894, 1.7012429237365723, 1.952548623085022, 1.7617219686508179, 1.6762899160385132, 1.424058437347412, 1.529905915260315]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.075566530227661, 3.8618078231811523, 3.0325136184692383, 2.6508262157440186, 3.4842734336853027, 2.082477569580078, 3.661005973815918, 3.169557809829712, 3.828228712081909, 3.38153338432312, 3.1918630599975586, 3.928826093673706, 2.1343579292297363, 3.960498332977295, 3.1579246520996094, 2.80112624168396]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3904552459716797, 2.368255138397217, 2.8475687503814697, 2.431300640106201, 2.8057918548583984, 2.0643622875213623, 1.5761414766311646, 2.100470542907715, 2.6400773525238037, 1.9855692386627197, 1.3455742597579956, 1.9380720853805542, 2.427450180053711, 2.6749680042266846, 2.642413377761841, 1.998282790184021]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5686426162719727, 3.4136226177215576, 3.054788827896118, 3.1153786182403564, 3.0383031368255615, 2.842280149459839, 2.785672187805176, 2.055903196334839, 3.2783210277557373, 2.9448599815368652, 2.763333559036255, 1.3712624311447144, 2.9198899269104004, 2.7652289867401123, 3.0363991260528564, 2.0638880729675293]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9230353832244873, 1.8437925577163696, 2.6413767337799072, 2.594477415084839, 2.6303226947784424, 1.991917371749878, 1.7781388759613037, 3.094102144241333, 0.5680215954780579, 2.1772232055664062, 0.9852612614631653, 2.45310640335083, 2.750603199005127, 2.928445816040039, 2.9591305255889893, 1.4320130348205566]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6837046146392822, 3.9530043601989746, 5.559598922729492, 4.94666051864624, 5.51357364654541, 5.6768717765808105, 5.131941795349121, 5.1142096519470215, 6.154989242553711, 5.047804832458496, 5.769837379455566, 6.074060440063477, 4.832569599151611, 5.400310039520264, 4.307285785675049, 4.9592485427856445]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.210205078125, 4.557988166809082, 5.551820278167725, 5.170835971832275, 3.703234910964966, 4.464887619018555, 4.877985000610352, 5.294557094573975, 5.363707065582275, 4.915313243865967, 4.270185470581055, 3.352243423461914, 4.408837795257568, 3.386237621307373, 4.92118501663208, 4.496158599853516]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.876519203186035, 5.839456081390381, 6.032008171081543, 5.625179767608643, 6.071460247039795, 4.813500881195068, 5.684494495391846, 5.63586950302124, 5.301509857177734, 5.354560852050781, 6.211063385009766, 5.898831367492676, 4.94454288482666, 5.856168746948242, 6.041582107543945, 5.863831996917725]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.993067741394043, 2.9735496044158936, 2.8119382858276367, 2.2456247806549072, -0.43698370456695557, 3.040454387664795, 2.7198073863983154, 1.8569843769073486, 2.716728925704956, 3.467240810394287, 2.68591570854187, 3.244805097579956, 2.8596231937408447, 2.9485018253326416, 2.3318333625793457, 1.7395946979522705]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.54571533203125, 6.815385341644287, 6.53211784362793, 6.301928997039795, 6.456416130065918, 6.476142883300781, 6.741514682769775, 6.630895137786865, 6.637082576751709, 6.323623180389404, 6.921072959899902, 6.253469467163086, 5.278314113616943, 7.197261810302734, 6.480851173400879, 6.434046745300293]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  38%|██████████████████▊                              | 77/200 [02:49<04:06,  2.01s/it]Layer: gate_22 - Captured router_logits: [5.008755207061768, 5.35064697265625, 4.959423065185547, 4.046557426452637, 4.900597095489502, 5.247819423675537, 4.6300811767578125, 4.781458854675293, 5.071437835693359, 4.846378803253174, 5.267577171325684, 4.875005722045898, 5.142871379852295, 5.432636737823486, 4.59813928604126, 5.476807594299316]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.1442131996154785, 4.666140079498291, 3.3375232219696045, 4.19458532333374, 4.4539384841918945, 4.076402187347412, 4.24242639541626, 4.231950283050537, 4.3608784675598145, 4.46895170211792, 4.055113315582275, 4.427291393280029, 3.672948122024536, 2.0234198570251465, 3.3784916400909424, 4.0179619789123535]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.283625602722168, 6.235915660858154, 5.910386085510254, 6.081644058227539, 5.427121639251709, 5.84898042678833, 6.175754070281982, 5.703466415405273, 5.345877647399902, 5.75386381149292, 6.437494277954102, 5.521114826202393, 5.767242908477783, 6.208960056304932, 6.226492404937744, 5.784487724304199]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8346805572509766, 3.6696925163269043, 3.7842204570770264, 3.945772171020508, 2.68674373626709, 3.861210823059082, 3.9388489723205566, 3.6542282104492188, 3.9903416633605957, 3.6124379634857178, 3.6964974403381348, 3.6196322441101074, 3.6611557006835938, 3.5985169410705566, 3.702923059463501, 3.893634796142578]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4502053260803223, 3.525094985961914, 3.404453754425049, 3.554292917251587, 3.3835806846618652, 3.6203362941741943, 3.5724596977233887, 3.3657007217407227, 3.599820613861084, 3.2951273918151855, 3.5993058681488037, 3.4471826553344727, 3.4346916675567627, 2.7258670330047607, 3.648505926132202, 3.5642898082733154]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1814844608306885, 3.2079129219055176, 2.9840142726898193, 2.8953683376312256, 2.818610429763794, 3.0854578018188477, 3.181304454803467, 3.2749295234680176, 3.0327699184417725, 3.005413293838501, 2.2868669033050537, 2.998002290725708, 3.155522584915161, 3.0622060298919678, 3.0827314853668213, 2.885496139526367]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.371634483337402, 4.488539695739746, 4.565608501434326, 4.372768402099609, 4.490793228149414, 4.462584972381592, 4.21566915512085, 4.7071051597595215, 4.491962909698486, 4.457150459289551, 4.468662738800049, 4.052067756652832, 4.196417331695557, 4.055360317230225, 4.57552433013916, 4.372825622558594]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.578323364257812, 8.903985023498535, 8.782986640930176, 8.46827507019043, 8.551454544067383, 8.263477325439453, 8.633174896240234, 8.537846565246582, 8.624938011169434, 8.490530967712402, 8.595081329345703, 8.229215621948242, 8.328824996948242, 8.997943878173828, 8.895758628845215, 8.504354476928711]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.886943817138672, 5.427646160125732, 5.385357856750488, 4.787143707275391, 5.134369850158691, 5.087042331695557, 4.66780424118042, 5.130986213684082, 5.235931396484375, 4.924976348876953, 4.83568811416626, 4.7298808097839355, 4.863053798675537, 4.9796857833862305, 5.065667152404785, 4.831048011779785]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.233327627182007, 3.0001583099365234, 3.101191520690918, 2.761256217956543, 3.023087501525879, 3.3096301555633545, 2.754503011703491, 3.1779730319976807, 3.0856220722198486, 3.182509422302246, 3.172701597213745, 3.260481595993042, 3.032726287841797, 2.979543447494507, 3.11978816986084, 3.1781654357910156]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.1228000596165657, 0.13874326646327972, 0.13023903965950012, -0.24080562591552734, -0.2392003834247589, -0.10370342433452606, 0.14544335007667542, -0.18419010937213898, 0.09839995205402374, 0.11752045899629593, 0.11763254553079605, 0.07813780754804611, 0.10861628502607346, 0.1274205595254898, -1.1277484893798828, 0.13831059634685516]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08971277624368668, 0.05456401780247688, 0.04233645275235176, 0.06406234204769135, 0.08515118807554245, 0.037725433707237244, 0.05455132573843002, 0.07716502994298935, 0.018336152657866478, 0.06247684732079506, -0.17563803493976593, 0.05840007960796356, -0.0003119845350738615, -0.001998345833271742, 0.015796104446053505, 0.02806205302476883]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07028978317975998, 0.04157673940062523, 0.0914120227098465, 0.05046689137816429, 0.09291843324899673, 0.10588247328996658, 0.05937443673610687, -0.1415567547082901, 0.061595991253852844, 0.08322242647409439, -0.0035976131912320852, 0.07604273408651352, -0.2057008445262909, -0.009857785888016224, -0.05601499602198601, 0.12354912608861923]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21518492698669434, 0.1504230499267578, 0.12447664141654968, 0.14909687638282776, 0.044234953820705414, 0.05236689746379852, -0.08362103253602982, 0.17923296988010406, 0.15030662715435028, -0.4265550673007965, 0.024456365033984184, 0.15977130830287933, 0.08608100563287735, -0.2689385414123535, -0.08355559408664703, -0.04720209911465645]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07681169360876083, -0.03449064865708351, -0.06714026629924774, 0.1391337364912033, -0.06672359257936478, -0.14531110227108002, 0.09029877930879593, 0.053660910576581955, 0.0673970952630043, 0.19793148338794708, -0.27235251665115356, 0.024935787543654442, 0.00948052667081356, -0.17470034956932068, 0.07881592214107513, 0.022786391898989677]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.2112472504377365, 0.24241992831230164, 0.1445598602294922, 0.055008288472890854, -0.3068121373653412, -0.055963803082704544, 0.09267285466194153, 0.0036315862089395523, 0.1640198975801468, -0.058332763612270355, -0.5085583925247192, 0.05134659260511398, -0.3346823751926422, 0.25884270668029785, 0.15281768143177032, 0.15499302744865417]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.31226658821105957, 0.3324315845966339, 0.19437120854854584, 0.26534536480903625, 0.26975125074386597, 0.24223420023918152, 0.12278036028146744, 0.1514417976140976, -0.39359402656555176, 0.3892533779144287, -0.07718301564455032, 0.31302618980407715, 0.09193979203701019, -0.07121361792087555, 0.28383609652519226, 0.17474769055843353]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05366137996315956, 0.19945645332336426, 0.11684515327215195, 0.15825708210468292, -0.35465890169143677, 0.2012983113527298, 0.05798004940152168, 0.20646651089191437, -0.030877240002155304, -0.17027199268341064, 0.06184902414679527, -0.06728164851665497, 0.17291143536567688, 0.10616552084684372, -0.020903240889310837, 0.1444643884897232]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21355435252189636, 0.2823668122291565, -0.09260469675064087, 0.32552799582481384, 0.46073436737060547, 0.3992842435836792, 0.11555822938680649, -0.012931927107274532, 0.6226512789726257, -0.19020801782608032, 0.25581833720207214, 0.05181969329714775, 0.672657310962677, -0.20689353346824646, -0.030142268165946007, 0.1917119175195694]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8636974096298218, 0.7546512484550476, 0.620922327041626, 0.5738967657089233, 0.8393223285675049, 0.44074857234954834, 1.1054275035858154, 1.0717980861663818, 0.08931764215230942, 0.02815067395567894, 0.4293205738067627, 0.7922646403312683, 0.7402529120445251, 0.8232418894767761, 1.1182680130004883, 0.7884359359741211]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1487276554107666, 1.1675368547439575, 0.6546714901924133, 1.05122709274292, 0.9507192373275757, 0.38394084572792053, 0.9876118302345276, 1.4225260019302368, 0.7703748345375061, 0.2905760407447815, 1.0965343713760376, 1.1060724258422852, 0.6255776882171631, 1.0971946716308594, 0.4989747405052185, 1.0013394355773926]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2396897077560425, 1.6009492874145508, 1.527690052986145, 1.4151465892791748, 1.4198625087738037, 0.8334298729896545, 1.7492320537567139, 1.2245101928710938, 1.4818769693374634, 1.4520150423049927, 1.2564634084701538, 1.169060468673706, 1.141252875328064, 1.333910346031189, 1.2138850688934326, 1.1966395378112793]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.046152114868164, 1.5058331489562988, 1.6493093967437744, 1.792512059211731, 1.2565559148788452, 1.5892529487609863, 1.2948771715164185, 1.5273489952087402, 1.3505237102508545, 0.6258916258811951, 1.701677680015564, 1.9518691301345825, 1.7611137628555298, 1.676418662071228, 1.4254482984542847, 1.5305842161178589]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.077786684036255, 3.863466262817383, 3.035414457321167, 2.651599884033203, 3.4844653606414795, 2.0861475467681885, 3.663058280944824, 3.170210123062134, 3.8275418281555176, 3.3817973136901855, 3.1934282779693604, 3.9300403594970703, 2.141461133956909, 3.963407516479492, 3.1566858291625977, 2.804652690887451]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.392174005508423, 2.3651814460754395, 2.846867084503174, 2.4318103790283203, 2.805361032485962, 2.067596912384033, 1.5804177522659302, 2.102968215942383, 2.6387298107147217, 1.983915090560913, 1.3465641736984253, 1.942033052444458, 2.4266557693481445, 2.6743576526641846, 2.641042709350586, 1.998504638671875]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.572326898574829, 3.41163969039917, 3.05704927444458, 3.114999532699585, 3.043011426925659, 2.8421316146850586, 2.79085111618042, 2.0560879707336426, 3.2797844409942627, 2.945284128189087, 2.7646255493164062, 1.3750211000442505, 2.922739267349243, 2.7656447887420654, 3.0381572246551514, 2.0664222240448]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9208787679672241, 1.8456298112869263, 2.6385092735290527, 2.59338116645813, 2.6276323795318604, 1.9887442588806152, 1.778122901916504, 3.092683792114258, 0.5717176795005798, 2.1728670597076416, 0.9862123131752014, 2.4529542922973633, 2.747901678085327, 2.925156354904175, 2.9572291374206543, 1.431535005569458]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6917428970336914, 3.958131790161133, 5.563214302062988, 4.948815822601318, 5.519484043121338, 5.679315567016602, 5.132836818695068, 5.118781566619873, 6.157857418060303, 5.050084590911865, 5.7735161781311035, 6.079126834869385, 4.83599328994751, 5.406055927276611, 4.3138508796691895, 4.9619269371032715]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.211153030395508, 4.560396671295166, 5.553327560424805, 5.171594142913818, 3.704829692840576, 4.467867851257324, 4.880786895751953, 5.296299457550049, 5.365414142608643, 4.9163737297058105, 4.272660732269287, 3.3529341220855713, 4.41008186340332, 3.3868634700775146, 4.920002460479736, 4.497152328491211]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.8796539306640625, 5.840468406677246, 6.034444808959961, 5.626282215118408, 6.0723066329956055, 4.817653656005859, 5.687629699707031, 5.638869285583496, 5.3043928146362305, 5.357206344604492, 6.212491989135742, 5.900320053100586, 4.945925712585449, 5.8570942878723145, 6.043576717376709, 5.865262985229492]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.9943034648895264, 2.972750663757324, 2.81099009513855, 2.2449989318847656, -0.431725412607193, 3.040165662765503, 2.720264434814453, 1.858416199684143, 2.7167437076568604, 3.4664525985717773, 2.6863226890563965, 3.243305206298828, 2.8572051525115967, 2.9455692768096924, 2.330886125564575, 1.7403616905212402]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.542436122894287, 6.8122148513793945, 6.531113624572754, 6.2993903160095215, 6.455668926239014, 6.474767208099365, 6.739231109619141, 6.627756118774414, 6.633330345153809, 6.321145057678223, 6.918093681335449, 6.252007007598877, 5.277011871337891, 7.194419860839844, 6.47831916809082, 6.43195104598999]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.006872653961182, 5.3482866287231445, 4.9582085609436035, 4.046814918518066, 4.899563789367676, 5.24529504776001, 4.627960681915283, 4.777892589569092, 5.070105075836182, 4.8437886238098145, 5.265859603881836, 4.874032974243164, 5.140628337860107, 5.429969787597656, 4.595724582672119, 5.473807334899902]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.140172481536865, 4.6622748374938965, 3.333832025527954, 4.192081451416016, 4.44949197769165, 4.07234525680542, 4.2388081550598145, 4.22892427444458, 4.357555389404297, 4.464540958404541, 4.050769329071045, 4.424042701721191, 3.6691417694091797, 2.0225934982299805, 3.3737380504608154, 4.01480770111084]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.283690452575684, 6.236207485198975, 5.91038703918457, 6.080933570861816, 5.427217483520508, 5.849151134490967, 6.174778938293457, 5.703060626983643, 5.346817493438721, 5.754175186157227, 6.43749475479126, 5.519387722015381, 5.767275333404541, 6.208545684814453, 6.225554466247559, 5.78400182723999]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8354625701904297, 3.6702775955200195, 3.7841310501098633, 3.946603775024414, 2.6885874271392822, 3.8602261543273926, 3.940157890319824, 3.655947685241699, 3.9911882877349854, 3.6135544776916504, 3.6977341175079346, 3.6203293800354004, 3.6620445251464844, 3.6003475189208984, 3.7037556171417236, 3.894857406616211]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  40%|███████████████████▊                             | 81/200 [02:56<03:54,  1.97s/it]Layer: gate_26 - Captured router_logits: [3.449917793273926, 3.5244240760803223, 3.4039511680603027, 3.553847551345825, 3.383382558822632, 3.619649648666382, 3.5718533992767334, 3.3665974140167236, 3.599069356918335, 3.294893503189087, 3.598991632461548, 3.446460723876953, 3.4345483779907227, 2.726121187210083, 3.6478374004364014, 3.563507318496704]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.18099308013916, 3.2079200744628906, 2.98363995552063, 2.896055221557617, 2.8188064098358154, 3.0847973823547363, 3.180899143218994, 3.2740466594696045, 3.0323684215545654, 3.0046749114990234, 2.2871696949005127, 2.997570037841797, 3.1549887657165527, 3.061073064804077, 3.0823514461517334, 2.885209560394287]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.3741350173950195, 4.489831924438477, 4.56716251373291, 4.374497890472412, 4.490323066711426, 4.46466588973999, 4.218100070953369, 4.709646224975586, 4.493437767028809, 4.459104061126709, 4.470059394836426, 4.054457187652588, 4.198218822479248, 4.05701208114624, 4.577311038970947, 4.374202728271484]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.58499526977539, 8.911596298217773, 8.79150390625, 8.476540565490723, 8.559182167053223, 8.27051830291748, 8.641014099121094, 8.54487419128418, 8.63194751739502, 8.496955871582031, 8.60193920135498, 8.23735523223877, 8.336529731750488, 9.004632949829102, 8.903067588806152, 8.511777877807617]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.8894453048706055, 5.429632186889648, 5.386003017425537, 4.788769245147705, 5.135112285614014, 5.089109420776367, 4.670063018798828, 5.133890151977539, 5.238097667694092, 4.926984786987305, 4.837606430053711, 4.7311930656433105, 4.864809036254883, 4.9812164306640625, 5.067842960357666, 4.832292556762695]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.235568046569824, 3.0019779205322266, 3.1039023399353027, 2.7623190879821777, 3.0263614654541016, 3.3113553524017334, 2.7565245628356934, 3.18013596534729, 3.0873796939849854, 3.1850829124450684, 3.1752536296844482, 3.2626290321350098, 3.035417318344116, 2.9827592372894287, 3.122469663619995, 3.180323600769043]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12370599061250687, 0.13815422356128693, 0.13150864839553833, -0.24664252996444702, -0.2396366149187088, -0.10667623579502106, 0.14394152164459229, -0.17774197459220886, 0.10535162687301636, 0.11774683743715286, 0.11632626503705978, 0.08074390143156052, 0.10856825858354568, 0.12942978739738464, -1.1116185188293457, 0.13988889753818512]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0908646509051323, 0.053216252475976944, 0.042389605194330215, 0.06333710998296738, 0.08389034122228622, 0.039249904453754425, 0.05339478328824043, 0.07801151275634766, 0.022140732035040855, 0.0609920434653759, -0.18224193155765533, 0.061736997216939926, 0.0025982214137911797, -0.0035452889278531075, 0.01803969219326973, 0.026161547750234604]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07031727582216263, 0.043852031230926514, 0.09235896915197372, 0.05450892448425293, 0.09640850126743317, 0.09923405945301056, 0.06067495793104172, -0.14269359409809113, 0.06602891534566879, 0.08377400785684586, 0.0016141183441504836, 0.07490073144435883, -0.2131245881319046, -0.009768808260560036, -0.05331042408943176, 0.12647628784179688]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21226093173027039, 0.14845283329486847, 0.12577643990516663, 0.14479626715183258, 0.045074086636304855, 0.05768023431301117, -0.08319241553544998, 0.17896686494350433, 0.1519158035516739, -0.44185858964920044, 0.02124853990972042, 0.1670435220003128, 0.09425514191389084, -0.2827370762825012, -0.08050315827131271, -0.04483160004019737]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07711572200059891, -0.029965149238705635, -0.06801328808069229, 0.1407317817211151, -0.0663372129201889, -0.14939045906066895, 0.0916997566819191, 0.04906069114804268, 0.06505608558654785, 0.20486626029014587, -0.28076156973838806, 0.021334649994969368, 0.013836401514708996, -0.17776937782764435, 0.08312315493822098, 0.027581440284848213]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.2101997286081314, 0.24078847467899323, 0.14518214762210846, 0.05726661905646324, -0.3078944683074951, -0.057413551956415176, 0.09556896239519119, 0.0054345037788152695, 0.16565759479999542, -0.05837123468518257, -0.5139299035072327, 0.052229925990104675, -0.34031379222869873, 0.2598731219768524, 0.1559184193611145, 0.1539950966835022]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.31481051445007324, 0.3340664803981781, 0.19825151562690735, 0.2651766836643219, 0.2701270282268524, 0.2456187754869461, 0.12019947916269302, 0.15139140188694, -0.39285337924957275, 0.39195016026496887, -0.08780111372470856, 0.3162965178489685, 0.09448816627264023, -0.07426314055919647, 0.2844206392765045, 0.17850130796432495]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05423400551080704, 0.19861648976802826, 0.11834361404180527, 0.1582750529050827, -0.36157140135765076, 0.203348308801651, 0.0598393976688385, 0.2091350108385086, -0.028117934241890907, -0.1685974895954132, 0.059777963906526566, -0.06563190370798111, 0.1696971207857132, 0.10801631957292557, -0.026489723473787308, 0.13847513496875763]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2118701934814453, 0.2818591892719269, -0.09196596592664719, 0.3238646388053894, 0.46166425943374634, 0.40140917897224426, 0.11719117313623428, -0.012318613938987255, 0.6217635869979858, -0.1857653558254242, 0.25580450892448425, 0.049226269125938416, 0.6731172204017639, -0.21241123974323273, -0.029326507821679115, 0.1918548047542572]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8632872700691223, 0.7465700507164001, 0.617847204208374, 0.5769253969192505, 0.8381110429763794, 0.43645521998405457, 1.1045502424240112, 1.0704113245010376, 0.08863062411546707, 0.02176150679588318, 0.4247773587703705, 0.7909187078475952, 0.7391343712806702, 0.8188570737838745, 1.118998646736145, 0.7911112904548645]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1470704078674316, 1.167967438697815, 0.6513912677764893, 1.0509673357009888, 0.9498979449272156, 0.3732418119907379, 0.9855287075042725, 1.4187650680541992, 0.7595384120941162, 0.2790902554988861, 1.0965734720230103, 1.1045609712600708, 0.6141642332077026, 1.0945626497268677, 0.4994708299636841, 0.9985070824623108]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2368725538253784, 1.5985617637634277, 1.5260950326919556, 1.406427025794983, 1.4189339876174927, 0.8307318091392517, 1.7534459829330444, 1.2180018424987793, 1.483803153038025, 1.452124834060669, 1.251383900642395, 1.1635992527008057, 1.1388148069381714, 1.3270362615585327, 1.1947226524353027, 1.1899969577789307]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0389043092727661, 1.5037953853607178, 1.6491650342941284, 1.7910658121109009, 1.2470123767852783, 1.5800292491912842, 1.2926737070083618, 1.527689814567566, 1.3467767238616943, 0.6273288130760193, 1.697890043258667, 1.950879693031311, 1.7605032920837402, 1.6728618144989014, 1.4229800701141357, 1.530456781387329]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0729992389678955, 3.862619400024414, 3.033107042312622, 2.6457297801971436, 3.486905336380005, 2.0824599266052246, 3.6609444618225098, 3.176248073577881, 3.8257415294647217, 3.382230043411255, 3.19260573387146, 3.9274866580963135, 2.12855863571167, 3.9569687843322754, 3.155799150466919, 2.792151927947998]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.395888328552246, 2.37229061126709, 2.847109079360962, 2.430375576019287, 2.8078300952911377, 2.073791265487671, 1.5741376876831055, 2.110788345336914, 2.639944314956665, 1.99088716506958, 1.3478485345840454, 1.9415782690048218, 2.4326834678649902, 2.676812171936035, 2.642855644226074, 1.999315619468689]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.565608263015747, 3.411930561065674, 3.0546534061431885, 3.1151559352874756, 3.033989906311035, 2.8413891792297363, 2.7898197174072266, 2.054508686065674, 3.2802205085754395, 2.9441614151000977, 2.7654318809509277, 1.370159387588501, 2.9172630310058594, 2.7634921073913574, 3.0357847213745117, 2.0663771629333496]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9222570657730103, 1.8381900787353516, 2.6388702392578125, 2.592787742614746, 2.629504442214966, 1.9899067878723145, 1.7811481952667236, 3.093536376953125, 0.5679484009742737, 2.1748270988464355, 0.9819599986076355, 2.4521567821502686, 2.7472102642059326, 2.924656391143799, 2.9567573070526123, 1.4318937063217163]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6831939220428467, 3.951587200164795, 5.5576043128967285, 4.939181327819824, 5.507200717926025, 5.674439430236816, 5.132288932800293, 5.112241268157959, 6.153536796569824, 5.041864395141602, 5.767477989196777, 6.071520805358887, 4.831799030303955, 5.398173809051514, 4.302515983581543, 4.95537805557251]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.204618453979492, 4.556488513946533, 5.545065879821777, 5.1665544509887695, 3.6977531909942627, 4.455938339233398, 4.877286434173584, 5.287681579589844, 5.359060287475586, 4.912225723266602, 4.265021800994873, 3.347550868988037, 4.404204845428467, 3.3832201957702637, 4.917938709259033, 4.494203090667725]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.874181270599365, 5.838089466094971, 6.0306596755981445, 5.621803283691406, 6.068860054016113, 4.812540054321289, 5.682490825653076, 5.633487224578857, 5.303427696228027, 5.35444450378418, 6.208552360534668, 5.897085189819336, 4.944918155670166, 5.85304594039917, 6.039526462554932, 5.861841678619385]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.991122007369995, 2.970305919647217, 2.809946060180664, 2.243643045425415, -0.4344201385974884, 3.0387942790985107, 2.717470645904541, 1.857019305229187, 2.7164671421051025, 3.466646909713745, 2.6849496364593506, 3.244980573654175, 2.859337329864502, 2.9476423263549805, 2.3302934169769287, 1.7419239282608032]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.539124965667725, 6.810494899749756, 6.527630805969238, 6.296329975128174, 6.452484130859375, 6.470199108123779, 6.735506057739258, 6.623693466186523, 6.6322550773620605, 6.318534851074219, 6.915721416473389, 6.250212669372559, 5.273921966552734, 7.190313816070557, 6.474384784698486, 6.428233623504639]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.006651878356934, 5.3479695320129395, 4.955841064453125, 4.04468297958374, 4.898507118225098, 5.244495868682861, 4.62823486328125, 4.780157566070557, 5.067168712615967, 4.843794822692871, 5.266143798828125, 4.87501335144043, 5.139584541320801, 5.429238796234131, 4.595236301422119, 5.472897529602051]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.1440815925598145, 4.662225723266602, 3.3340296745300293, 4.192153453826904, 4.450751781463623, 4.073221206665039, 4.238507270812988, 4.227067470550537, 4.356671333312988, 4.464306354522705, 4.05163049697876, 4.424274921417236, 3.671095132827759, 2.0236480236053467, 3.3737645149230957, 4.015830039978027]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.280175685882568, 6.231289863586426, 5.905445098876953, 6.076977252960205, 5.42462158203125, 5.844719409942627, 6.1714982986450195, 5.698922634124756, 5.3425092697143555, 5.750646114349365, 6.4318952560424805, 5.517104625701904, 5.762884616851807, 6.204005718231201, 6.2210516929626465, 5.780989170074463]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.829899787902832, 3.665822982788086, 3.7788984775543213, 3.941896438598633, 2.684818983078003, 3.8558712005615234, 3.933349609375, 3.6519317626953125, 3.9854161739349365, 3.6077890396118164, 3.6909101009368896, 3.6157305240631104, 3.6578240394592285, 3.5942108631134033, 3.6985855102539062, 3.8890538215637207]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.446739912033081, 3.521275758743286, 3.4014923572540283, 3.5501551628112793, 3.3801681995391846, 3.6162421703338623, 3.5681443214416504, 3.362277030944824, 3.5954902172088623, 3.2918310165405273, 3.5961949825286865, 3.4426674842834473, 3.4310433864593506, 2.724536657333374, 3.645582675933838, 3.5602474212646484]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1772680282592773, 3.203242540359497, 2.9804365634918213, 2.892265796661377, 2.815032958984375, 3.081010103225708, 3.1779723167419434, 3.2707085609436035, 3.0283117294311523, 3.001809597015381, 2.284665107727051, 2.993704319000244, 3.1510586738586426, 3.057651996612549, 3.0776686668395996, 2.8812031745910645]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.368525505065918, 4.485036373138428, 4.563036918640137, 4.369469165802002, 4.4863810539245605, 4.459259986877441, 4.213015556335449, 4.703629493713379, 4.488566875457764, 4.453385829925537, 4.464953422546387, 4.049302577972412, 4.1926188468933105, 4.0519633293151855, 4.571759223937988, 4.3683905601501465]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.568486213684082, 8.89033031463623, 8.773978233337402, 8.457334518432617, 8.539959907531738, 8.253857612609863, 8.620180130004883, 8.528623580932617, 8.615700721740723, 8.48034381866455, 8.585593223571777, 8.218605041503906, 8.319265365600586, 8.987109184265137, 8.883262634277344, 8.492255210876465]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  42%|████████████████████▊                            | 85/200 [03:04<03:46,  1.97s/it]Layer: gate_30 - Captured router_logits: [4.878220558166504, 5.417654991149902, 5.3761749267578125, 4.779397010803223, 5.1263885498046875, 5.076972007751465, 4.658998966217041, 5.121683597564697, 5.227051258087158, 4.915285110473633, 4.824607849121094, 4.7203688621521, 4.854285717010498, 4.971800327301025, 5.055761814117432, 4.8229241371154785]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2265172004699707, 2.994069814682007, 3.09574818611145, 2.7543089389801025, 3.0171902179718018, 3.304744243621826, 2.7478487491607666, 3.1724157333374023, 3.078613758087158, 3.1766128540039062, 3.1684763431549072, 3.253894090652466, 3.0260517597198486, 2.973813772201538, 3.1147420406341553, 3.171455144882202]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12236232310533524, 0.1383485645055771, 0.13087503612041473, -0.24486929178237915, -0.24661804735660553, -0.10074520856142044, 0.1445903331041336, -0.1760619431734085, 0.10001399368047714, 0.11702315509319305, 0.11673461645841599, 0.07727974653244019, 0.10819506645202637, 0.12674173712730408, -1.1248899698257446, 0.13791409134864807]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08984146267175674, 0.05212068185210228, 0.042360689491033554, 0.06174859404563904, 0.08519510179758072, 0.0369577556848526, 0.05381419137120247, 0.07911375164985657, 0.01797575317323208, 0.06207768991589546, -0.18280236423015594, 0.06131284683942795, 0.0016875573201104999, -0.006770982872694731, 0.019837014377117157, 0.030693313106894493]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07231371104717255, 0.040303800255060196, 0.08978788554668427, 0.05225761607289314, 0.09267892688512802, 0.1034100204706192, 0.05776841938495636, -0.14467011392116547, 0.06330236792564392, 0.08633901923894882, 0.00048237916780635715, 0.07475012540817261, -0.21585042774677277, -0.01023914199322462, -0.05962381884455681, 0.1241876631975174]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.2134384959936142, 0.1475917100906372, 0.12368935346603394, 0.14163373410701752, 0.04263623431324959, 0.05131576955318451, -0.08787862956523895, 0.17721284925937653, 0.15124574303627014, -0.4389227032661438, 0.027315551415085793, 0.16034628450870514, 0.08888371288776398, -0.28189617395401, -0.07600283622741699, -0.046777985990047455]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07586824893951416, -0.03238454461097717, -0.06704434752464294, 0.14269985258579254, -0.06776905059814453, -0.15006528794765472, 0.08753734827041626, 0.049024876207113266, 0.06795977801084518, 0.20383580029010773, -0.2796148359775543, 0.028052780777215958, 0.011874493211507797, -0.181819349527359, 0.08256642520427704, 0.023417504504323006]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21015311777591705, 0.23923814296722412, 0.14296802878379822, 0.05564655736088753, -0.306417852640152, -0.060367077589035034, 0.0930854007601738, 0.004392184317111969, 0.1647130697965622, -0.05959365516901016, -0.5039052367210388, 0.050352901220321655, -0.3390425741672516, 0.259053111076355, 0.15320472419261932, 0.1538088172674179]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3121933341026306, 0.3301433324813843, 0.1932685822248459, 0.26664337515830994, 0.26870396733283997, 0.24391677975654602, 0.12021858990192413, 0.14879463613033295, -0.3921464681625366, 0.3891333341598511, -0.08354423195123672, 0.3144776225090027, 0.09042701870203018, -0.07362867146730423, 0.28455042839050293, 0.17654556035995483]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05151110142469406, 0.19765008985996246, 0.11702622473239899, 0.15868186950683594, -0.3580990731716156, 0.2027047574520111, 0.05731205642223358, 0.20662015676498413, -0.02774432674050331, -0.17152747511863708, 0.0611211396753788, -0.06763628125190735, 0.17219431698322296, 0.10710335522890091, -0.023874105885624886, 0.14564435184001923]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21046623587608337, 0.2821265161037445, -0.09295354783535004, 0.32542580366134644, 0.461428701877594, 0.39837872982025146, 0.11517571657896042, -0.01072443462908268, 0.6216496825218201, -0.1896902620792389, 0.2549242675304413, 0.05304670333862305, 0.6733406782150269, -0.2093931883573532, -0.030818888917565346, 0.19148311018943787]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8628371953964233, 0.7517474889755249, 0.6202635169029236, 0.5753812789916992, 0.8393176794052124, 0.4379488527774811, 1.103940486907959, 1.07230806350708, 0.0886046290397644, 0.02303251251578331, 0.4265844225883484, 0.7924323678016663, 0.7416918277740479, 0.8206945657730103, 1.118377447128296, 0.7899254560470581]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1516430377960205, 1.1716893911361694, 0.655076265335083, 1.0538556575775146, 0.9509928822517395, 0.3837907612323761, 0.9894629716873169, 1.424828052520752, 0.7676705121994019, 0.2871595621109009, 1.0996118783950806, 1.1086750030517578, 0.6224860548973083, 1.0968586206436157, 0.5017217397689819, 0.999790608882904]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2413297891616821, 1.6038944721221924, 1.5304633378982544, 1.4138728380203247, 1.4233686923980713, 0.8315500020980835, 1.7521828413009644, 1.222996711730957, 1.4858602285385132, 1.4547510147094727, 1.25677490234375, 1.1658504009246826, 1.1404813528060913, 1.3306214809417725, 1.2042882442474365, 1.194795846939087]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0446959733963013, 1.5058268308639526, 1.6512413024902344, 1.7951303720474243, 1.2525497674942017, 1.5870424509048462, 1.2947543859481812, 1.528172492980957, 1.3501702547073364, 0.6271860599517822, 1.7043975591659546, 1.9539347887039185, 1.762001395225525, 1.6787372827529907, 1.4244064092636108, 1.5322465896606445]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0792601108551025, 3.8685946464538574, 3.0382752418518066, 2.6511809825897217, 3.489187479019165, 2.084739923477173, 3.6668050289154053, 3.173649787902832, 3.83190655708313, 3.3861677646636963, 3.196143865585327, 3.933767318725586, 2.134671211242676, 3.9625442028045654, 3.1612846851348877, 2.801924467086792]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.39180850982666, 2.369218349456787, 2.8494012355804443, 2.432358503341675, 2.807807207107544, 2.06522798538208, 1.571865200996399, 2.103353500366211, 2.6404778957366943, 1.9865398406982422, 1.3418911695480347, 1.9400038719177246, 2.429704189300537, 2.6789796352386475, 2.6447854042053223, 1.9973613023757935]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5695438385009766, 3.4154865741729736, 3.0566117763519287, 3.1190357208251953, 3.039673328399658, 2.844169855117798, 2.7884106636047363, 2.0565741062164307, 3.281639337539673, 2.9477498531341553, 2.7648754119873047, 1.3695237636566162, 2.9236602783203125, 2.7668211460113525, 3.038378953933716, 2.064554452896118]
Running loglikelihood requests:  44%|█████████████████████▊                           | 89/200 [03:12<03:39,  1.98s/it]Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9228473901748657, 1.8421404361724854, 2.641955614089966, 2.596489191055298, 2.631845712661743, 1.9908732175827026, 1.7776836156845093, 3.0952768325805664, 0.5644401907920837, 2.1766045093536377, 0.980495035648346, 2.4526302814483643, 2.7505829334259033, 2.9278781414031982, 2.959488868713379, 1.4295074939727783]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.685222864151001, 3.954545259475708, 5.563684463500977, 4.948575496673584, 5.519217014312744, 5.681241035461426, 5.135787010192871, 5.118117332458496, 6.1605544090271, 5.049124240875244, 5.774487495422363, 6.079474449157715, 4.836087226867676, 5.4065961837768555, 4.308877944946289, 4.962011337280273]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.213319778442383, 4.561192989349365, 5.553235054016113, 5.174550533294678, 3.7027580738067627, 4.464784145355225, 4.880814552307129, 5.296360492706299, 5.366988658905029, 4.917369365692139, 4.271378517150879, 3.352184295654297, 4.410045146942139, 3.386807441711426, 4.921655178070068, 4.498666286468506]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.87924337387085, 5.8415422439575195, 6.0353875160217285, 5.627218723297119, 6.073864459991455, 4.814506530761719, 5.686362266540527, 5.637712478637695, 5.304698467254639, 5.358364105224609, 6.214430332183838, 5.902080535888672, 4.9476213455200195, 5.858244895935059, 6.044351577758789, 5.866365909576416]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.9950506687164307, 2.975266933441162, 2.81351375579834, 2.2456023693084717, -0.43944141268730164, 3.0434999465942383, 2.721102714538574, 1.8582979440689087, 2.7185585498809814, 3.4690780639648438, 2.6884307861328125, 3.24770188331604, 2.8606841564178467, 2.9496397972106934, 2.3323163986206055, 1.7410434484481812]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.548187255859375, 6.817994117736816, 6.535424709320068, 6.303401947021484, 6.4595489501953125, 6.479349613189697, 6.743842601776123, 6.6332316398620605, 6.63958215713501, 6.325883865356445, 6.923492431640625, 6.257359504699707, 5.279706954956055, 7.19945764541626, 6.48323917388916, 6.436062335968018]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.0098724365234375, 5.352313041687012, 4.960179328918457, 4.0469889640808105, 4.902134895324707, 5.249054908752441, 4.63083553314209, 4.780622482299805, 5.072383403778076, 4.847768783569336, 5.269728660583496, 4.8774213790893555, 5.1439690589904785, 5.433055400848389, 4.598449230194092, 5.47761344909668]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.145748615264893, 4.666804790496826, 3.336845636367798, 4.195241928100586, 4.453584671020508, 4.076931953430176, 4.242177963256836, 4.2317304611206055, 4.361473560333252, 4.469542026519775, 4.055014133453369, 4.428616046905518, 3.6731014251708984, 2.0223312377929688, 3.377788782119751, 4.018272399902344]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.286026954650879, 6.238073825836182, 5.911764621734619, 6.0835371017456055, 5.4297943115234375, 5.850677967071533, 6.177653789520264, 5.704474925994873, 5.347729206085205, 5.755739212036133, 6.440014839172363, 5.522486209869385, 5.770411014556885, 6.211130619049072, 6.227725028991699, 5.786073207855225]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.83686900138855, 3.6723384857177734, 3.7851321697235107, 3.9483134746551514, 2.6882059574127197, 3.8626341819763184, 3.940615177154541, 3.6575241088867188, 3.9924514293670654, 3.6141934394836426, 3.698906898498535, 3.621875047683716, 3.662848711013794, 3.600301742553711, 3.7051336765289307, 3.8958945274353027]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4527194499969482, 3.5269734859466553, 3.406224012374878, 3.5561704635620117, 3.3859078884124756, 3.6226162910461426, 3.5740113258361816, 3.3685216903686523, 3.6021227836608887, 3.297333002090454, 3.601808786392212, 3.4488277435302734, 3.436691999435425, 2.7277867794036865, 3.650460958480835, 3.5666706562042236]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1831789016723633, 3.2093307971954346, 2.9855048656463623, 2.8976731300354004, 2.820453405380249, 3.086939573287964, 3.1827080249786377, 3.276435613632202, 3.034019708633423, 3.0069587230682373, 2.2881698608398438, 2.9992635250091553, 3.1572892665863037, 3.063666343688965, 3.0836739540100098, 2.8870391845703125]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.3743367195129395, 4.490511417388916, 4.568171977996826, 4.37529182434082, 4.493238925933838, 4.465219974517822, 4.219247817993164, 4.70994234085083, 4.494040012359619, 4.45974063873291, 4.470555782318115, 4.054475784301758, 4.198845863342285, 4.057115077972412, 4.5777740478515625, 4.3748064041137695]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.584362983703613, 8.910755157470703, 8.7894868850708, 8.474346160888672, 8.557160377502441, 8.269352912902832, 8.639472961425781, 8.543478965759277, 8.631269454956055, 8.49608325958252, 8.601648330688477, 8.23528003692627, 8.33542251586914, 9.004690170288086, 8.902098655700684, 8.511516571044922]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.890296459197998, 5.4306182861328125, 5.388521194458008, 4.789937496185303, 5.137946128845215, 5.092286586761475, 4.67138147354126, 5.135036468505859, 5.240009784698486, 4.92832612991333, 4.838863372802734, 4.733037948608398, 4.866677284240723, 4.984135150909424, 5.069247722625732, 4.83428430557251]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.234750986099243, 3.001676321029663, 3.10292911529541, 2.7635393142700195, 3.0249218940734863, 3.310969591140747, 2.756866931915283, 3.1796932220458984, 3.087048053741455, 3.1845908164978027, 3.1749415397644043, 3.2619330883026123, 3.034503698348999, 2.9818968772888184, 3.12162446975708, 3.179669141769409]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12208408117294312, 0.1384604126214981, 0.13037411868572235, -0.24096345901489258, -0.24207550287246704, -0.10424359887838364, 0.14423033595085144, -0.18015800416469574, 0.10108467191457748, 0.11727769672870636, 0.11630918085575104, 0.07672835141420364, 0.10786422342061996, 0.12617358565330505, -1.115388035774231, 0.1382998675107956]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09109679609537125, 0.05279497429728508, 0.042727939784526825, 0.061186470091342926, 0.08553269505500793, 0.03726420924067497, 0.053599629551172256, 0.07745902240276337, 0.016279572620987892, 0.06207936629652977, -0.1799408495426178, 0.06154688447713852, 0.0015704211546108127, -0.005963523872196674, 0.017831623554229736, 0.030734911561012268]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07324276119470596, 0.041826289147138596, 0.09041910618543625, 0.05239935219287872, 0.09258301556110382, 0.10387738049030304, 0.05791430547833443, -0.14408020675182343, 0.06363335996866226, 0.08451716601848602, -0.00046233346802182496, 0.07620590180158615, -0.2127392739057541, -0.010117619298398495, -0.059443727135658264, 0.12349818646907806]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21474163234233856, 0.14917166531085968, 0.1250002384185791, 0.14490307867527008, 0.04276129975914955, 0.051984645426273346, -0.08799037337303162, 0.17752693593502045, 0.15044866502285004, -0.4348923861980438, 0.024302899837493896, 0.1634947657585144, 0.08604255318641663, -0.2797955870628357, -0.07957468926906586, -0.046262748539447784]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07603402435779572, -0.03197591379284859, -0.06681957840919495, 0.13930068910121918, -0.06998427957296371, -0.14806042611598969, 0.08782602101564407, 0.04984019324183464, 0.06983069330453873, 0.2045043557882309, -0.27624428272247314, 0.027821622788906097, 0.014191243797540665, -0.1813904494047165, 0.08159384876489639, 0.023761732503771782]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21025247871875763, 0.2396569848060608, 0.14232169091701508, 0.055182084441185, -0.3053899109363556, -0.05846957489848137, 0.09327071160078049, 0.006330395117402077, 0.1636781394481659, -0.05825984105467796, -0.504256010055542, 0.04982743039727211, -0.3363809585571289, 0.25767982006073, 0.15256264805793762, 0.15396004915237427]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3118921220302582, 0.33073750138282776, 0.1934063732624054, 0.26797282695770264, 0.26723286509513855, 0.24177837371826172, 0.12288950383663177, 0.15027789771556854, -0.39001351594924927, 0.3893084228038788, -0.07791450619697571, 0.31403952836990356, 0.08957898616790771, -0.0704197809100151, 0.2814909815788269, 0.17444275319576263]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05217090621590614, 0.1982669234275818, 0.11750617623329163, 0.15859007835388184, -0.3553507626056671, 0.2021329551935196, 0.05723379924893379, 0.2101346254348755, -0.029939519241452217, -0.1712627112865448, 0.06258286535739899, -0.06776194274425507, 0.1743994653224945, 0.10574610531330109, -0.01979607716202736, 0.1454857885837555]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20962989330291748, 0.2811269462108612, -0.0891771912574768, 0.32588592171669006, 0.4606058597564697, 0.39825430512428284, 0.11497975885868073, -0.009832214564085007, 0.6203927397727966, -0.19223834574222565, 0.25473088026046753, 0.053609397262334824, 0.6714516282081604, -0.20509737730026245, -0.02927476353943348, 0.19118249416351318]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8633012175559998, 0.7546586990356445, 0.6182891726493835, 0.5711890459060669, 0.8409054279327393, 0.4400201439857483, 1.1044315099716187, 1.0705223083496094, 0.09018499404191971, 0.026142437011003494, 0.4314800798892975, 0.7908952832221985, 0.7436513304710388, 0.8247166872024536, 1.1170599460601807, 0.7886398434638977]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1497228145599365, 1.170159101486206, 0.6560461521148682, 1.0549654960632324, 0.9508309364318848, 0.3823297619819641, 0.9877247214317322, 1.4221621751785278, 0.7730743885040283, 0.2906469404697418, 1.09805166721344, 1.1074591875076294, 0.6292669177055359, 1.0980595350265503, 0.4989088475704193, 1.0031938552856445]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.241123080253601, 1.6020780801773071, 1.5288450717926025, 1.4124321937561035, 1.4225432872772217, 0.8354828953742981, 1.7490745782852173, 1.225475788116455, 1.4811294078826904, 1.4503833055496216, 1.2569032907485962, 1.1675384044647217, 1.144101619720459, 1.3337361812591553, 1.209503412246704, 1.196306824684143]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0481101274490356, 1.50663423538208, 1.6483513116836548, 1.7936776876449585, 1.2572978734970093, 1.5867371559143066, 1.2923002243041992, 1.5258915424346924, 1.35162353515625, 0.6295794248580933, 1.7009003162384033, 1.9514682292938232, 1.762697696685791, 1.6761330366134644, 1.423649549484253, 1.530070424079895]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0783886909484863, 3.861696481704712, 3.034433126449585, 2.6535916328430176, 3.4844887256622314, 2.0858139991760254, 3.6610584259033203, 3.1684982776641846, 3.8277993202209473, 3.3827123641967773, 3.191417694091797, 3.930695056915283, 2.1367478370666504, 3.9618630409240723, 3.157533645629883, 2.801570415496826]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3903133869171143, 2.3676199913024902, 2.8471155166625977, 2.4310340881347656, 2.8047449588775635, 2.064953327178955, 1.5767593383789062, 2.097468137741089, 2.6387102603912354, 1.986512541770935, 1.3458470106124878, 1.938247799873352, 2.427300453186035, 2.674372673034668, 2.6414570808410645, 1.9975779056549072]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5692317485809326, 3.4143264293670654, 3.0548410415649414, 3.115354299545288, 3.039137363433838, 2.843325614929199, 2.7854907512664795, 2.0540366172790527, 3.27912974357605, 2.945241689682007, 2.7654263973236084, 1.3725531101226807, 2.9196410179138184, 2.765501022338867, 3.0369150638580322, 2.0641467571258545]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9233225584030151, 1.8449327945709229, 2.641296863555908, 2.5946884155273438, 2.6308255195617676, 1.9915883541107178, 1.7771129608154297, 3.092633008956909, 0.5683541297912598, 2.1757044792175293, 0.9838000535964966, 2.4525763988494873, 2.750175714492798, 2.927952289581299, 2.9586596488952637, 1.4313523769378662]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.684974193572998, 3.9533188343048096, 5.559231281280518, 4.94635009765625, 5.514074325561523, 5.6768479347229, 5.131944179534912, 5.114756107330322, 6.155871391296387, 5.047415256500244, 5.769981384277344, 6.074287414550781, 4.830644607543945, 5.4000678062438965, 4.307699680328369, 4.958895683288574]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.210434913635254, 4.558589935302734, 5.549628257751465, 5.170668601989746, 3.7030928134918213, 4.46469259262085, 4.8770623207092285, 5.293481826782227, 5.363713264465332, 4.914411544799805, 4.269631385803223, 3.3510634899139404, 4.40883207321167, 3.386322498321533, 4.919883728027344, 4.495412826538086]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.877107620239258, 5.839371204376221, 6.033448696136475, 5.62502908706665, 6.072349548339844, 4.81395959854126, 5.684845924377441, 5.635624885559082, 5.301509857177734, 5.354976654052734, 6.212307929992676, 5.899708271026611, 4.946225166320801, 5.856698989868164, 6.041957378387451, 5.864458084106445]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Running loglikelihood requests:  46%|██████████████████████▊                          | 93/200 [03:20<03:30,  1.97s/it]Layer: gate_20 - Captured router_logits: [2.9939510822296143, 2.972686529159546, 2.8117430210113525, 2.2435593605041504, -0.4391081631183624, 3.0403928756713867, 2.7196855545043945, 1.8564982414245605, 2.716437578201294, 3.467639923095703, 2.686408281326294, 3.2454819679260254, 2.8589532375335693, 2.9484710693359375, 2.3303074836730957, 1.7390130758285522]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.546451568603516, 6.816009044647217, 6.533252239227295, 6.30185604095459, 6.456845760345459, 6.476471424102783, 6.741971492767334, 6.632258415222168, 6.637404918670654, 6.324106693267822, 6.921748638153076, 6.254056453704834, 5.278417587280273, 7.198429107666016, 6.4815168380737305, 6.434793472290039]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.008728981018066, 5.350841045379639, 4.95989990234375, 4.0465407371521, 4.900160312652588, 5.247819423675537, 4.629481792449951, 4.781295299530029, 5.071301460266113, 4.8460893630981445, 5.268109321594238, 4.87461519241333, 5.142848491668701, 5.43202018737793, 4.597867488861084, 5.475632190704346]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.145103454589844, 4.666576862335205, 3.33720326423645, 4.1950531005859375, 4.45430850982666, 4.07677698135376, 4.242054462432861, 4.23168420791626, 4.360943794250488, 4.469357967376709, 4.055142402648926, 4.428318023681641, 3.672757625579834, 2.0232768058776855, 3.378143072128296, 4.018221855163574]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.283507823944092, 6.235466957092285, 5.910396575927734, 6.081060409545898, 5.42716646194458, 5.848382472991943, 6.175137519836426, 5.702696800231934, 5.345048904418945, 5.753236293792725, 6.436313152313232, 5.520553112030029, 5.767429828643799, 6.208705425262451, 6.225688457489014, 5.783637046813965]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8348641395568848, 3.670006275177002, 3.7835707664489746, 3.9460854530334473, 2.6862878799438477, 3.86124587059021, 3.938957929611206, 3.6550824642181396, 3.9905598163604736, 3.6124463081359863, 3.6966018676757812, 3.6197736263275146, 3.661452054977417, 3.5981249809265137, 3.703524112701416, 3.893707036972046]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4503090381622314, 3.5258166790008545, 3.404348134994507, 3.5543248653411865, 3.3838915824890137, 3.621173143386841, 3.5728228092193604, 3.365880012512207, 3.600212574005127, 3.2955708503723145, 3.599576950073242, 3.4470107555389404, 3.4348931312561035, 2.7255516052246094, 3.648653507232666, 3.5650670528411865]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1817786693573, 3.208136796951294, 2.9839470386505127, 2.8955578804016113, 2.818560838699341, 3.085527181625366, 3.1816952228546143, 3.2745115756988525, 3.03261661529541, 3.005056619644165, 2.286966323852539, 2.99764084815979, 3.1559839248657227, 3.0624747276306152, 3.0821785926818848, 2.8855133056640625]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.371481418609619, 4.48834753036499, 4.564684867858887, 4.371632099151611, 4.489871978759766, 4.462039947509766, 4.215284824371338, 4.70638370513916, 4.491459369659424, 4.456200122833252, 4.467805862426758, 4.051346302032471, 4.195850849151611, 4.0543904304504395, 4.574214935302734, 4.3718695640563965]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.576931953430176, 8.903233528137207, 8.781628608703613, 8.4672212600708, 8.55074405670166, 8.262701034545898, 8.632078170776367, 8.536569595336914, 8.62310791015625, 8.48946475982666, 8.594156265258789, 8.228216171264648, 8.328593254089355, 8.996663093566895, 8.894794464111328, 8.503159523010254]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.886695861816406, 5.42746114730835, 5.384931564331055, 4.786925315856934, 5.134161949157715, 5.086257457733154, 4.668082237243652, 5.131565570831299, 5.236023902893066, 4.925090789794922, 4.834761142730713, 4.730254650115967, 4.8631134033203125, 4.978673934936523, 5.065483570098877, 4.830521583557129]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.233055353164673, 2.9997775554656982, 3.1016361713409424, 2.7613532543182373, 3.023432970046997, 3.3092246055603027, 2.7548696994781494, 3.1778650283813477, 3.0854899883270264, 3.1826236248016357, 3.1729938983917236, 3.2609870433807373, 3.0329689979553223, 2.979511260986328, 3.1204659938812256, 3.178022623062134]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12249554693698883, 0.1389024555683136, 0.1312078833580017, -0.2471173107624054, -0.2476041615009308, -0.10488098114728928, 0.14503076672554016, -0.17526336014270782, 0.1010957732796669, 0.11755296587944031, 0.11705806851387024, 0.07863280922174454, 0.10773998498916626, 0.12724114954471588, -1.1267880201339722, 0.1390884965658188]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08886763453483582, 0.051784832030534744, 0.04191102460026741, 0.06096681207418442, 0.08641914278268814, 0.03628253564238548, 0.054003506898880005, 0.07841082662343979, 0.018907690420746803, 0.061436302959918976, -0.1833154857158661, 0.06105753779411316, 0.0028742242138832808, -0.007872682996094227, 0.019377287477254868, 0.030179712921380997]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07298478484153748, 0.04293442517518997, 0.09025940299034119, 0.05257134139537811, 0.09250785410404205, 0.10350817441940308, 0.05942375585436821, -0.14641347527503967, 0.06384862959384918, 0.08342710882425308, 0.0005986546166241169, 0.07509348541498184, -0.21763664484024048, -0.010161825455725193, -0.06012997776269913, 0.1243165135383606]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.2145761400461197, 0.14908520877361298, 0.12521450221538544, 0.14319030940532684, 0.04276511073112488, 0.05405021831393242, -0.08613667637109756, 0.17837926745414734, 0.15174245834350586, -0.4417111873626709, 0.025376038625836372, 0.16089020669460297, 0.08899044245481491, -0.28019967675209045, -0.08051759749650955, -0.049216918647289276]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07671678811311722, -0.03143944963812828, -0.06695591658353806, 0.14223137497901917, -0.06856021285057068, -0.1521405130624771, 0.08932438492774963, 0.049410492181777954, 0.0702052116394043, 0.20513403415679932, -0.2797453701496124, 0.028685618191957474, 0.013134236447513103, -0.1835200935602188, 0.08117944747209549, 0.02440001629292965]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21016564965248108, 0.23980975151062012, 0.14289811253547668, 0.05430416762828827, -0.30575528740882874, -0.05960295721888542, 0.0942213386297226, 0.006119520869106054, 0.16378504037857056, -0.059782739728689194, -0.5013339519500732, 0.04930192232131958, -0.33979812264442444, 0.2580086886882782, 0.1525702327489853, 0.15259286761283875]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.31121811270713806, 0.3318399488925934, 0.19233909249305725, 0.2672547698020935, 0.267413467168808, 0.2428676187992096, 0.11989929527044296, 0.1481369137763977, -0.3909406065940857, 0.38855189085006714, -0.08370868861675262, 0.3148978352546692, 0.08852303773164749, -0.07345858961343765, 0.2837050259113312, 0.17629174888134003]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05200924351811409, 0.19908320903778076, 0.11674762517213821, 0.1582152396440506, -0.35701262950897217, 0.20154409110546112, 0.056729163974523544, 0.20749546587467194, -0.028690120205283165, -0.1717677116394043, 0.06096319481730461, -0.06807351857423782, 0.17351263761520386, 0.10579812526702881, -0.023503847420215607, 0.1479010283946991]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2095523327589035, 0.2813813388347626, -0.09134428948163986, 0.3248242735862732, 0.4603603780269623, 0.3985856771469116, 0.11454497277736664, -0.009738088585436344, 0.6213095188140869, -0.19090767204761505, 0.25469425320625305, 0.054318446666002274, 0.6723841428756714, -0.20863251388072968, -0.029169240966439247, 0.19045661389827728]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8629477024078369, 0.7523218393325806, 0.6185582280158997, 0.5740872025489807, 0.8391839861869812, 0.43906691670417786, 1.1046591997146606, 1.0714970827102661, 0.08804934471845627, 0.0229241531342268, 0.42870286107063293, 0.791854977607727, 0.742406964302063, 0.8197539448738098, 1.1183648109436035, 0.7892168164253235]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1512974500656128, 1.171043038368225, 0.655237078666687, 1.0543091297149658, 0.9505359530448914, 0.3835718333721161, 0.9896987080574036, 1.4242202043533325, 0.7689877152442932, 0.28718301653862, 1.0984300374984741, 1.1080509424209595, 0.6241675615310669, 1.0966113805770874, 0.5014516711235046, 1.0019279718399048]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.24093759059906, 1.6027129888534546, 1.5294032096862793, 1.4134104251861572, 1.4223861694335938, 0.8326636552810669, 1.751510500907898, 1.2236967086791992, 1.4851930141448975, 1.4530372619628906, 1.255283236503601, 1.16708242893219, 1.1417104005813599, 1.3313506841659546, 1.2048519849777222, 1.1947306394577026]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0446048974990845, 1.5058636665344238, 1.650524616241455, 1.7940818071365356, 1.2543668746948242, 1.5862168073654175, 1.2936738729476929, 1.5275030136108398, 1.350790023803711, 0.6284322142601013, 1.7017117738723755, 1.9535431861877441, 1.7609961032867432, 1.6774022579193115, 1.4232465028762817, 1.5319381952285767]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0794670581817627, 3.8673441410064697, 3.0372705459594727, 2.6519968509674072, 3.4882748126983643, 2.086876153945923, 3.6651015281677246, 3.172262191772461, 3.8305230140686035, 3.3852951526641846, 3.194892168045044, 3.9329118728637695, 2.1358118057250977, 3.960510730743408, 3.1613306999206543, 2.8012962341308594]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3899192810058594, 2.36936354637146, 2.8477699756622314, 2.430434465408325, 2.8070359230041504, 2.0644967555999756, 1.5717737674713135, 2.1018002033233643, 2.6392290592193604, 1.9853304624557495, 1.341176986694336, 1.940121054649353, 2.428130626678467, 2.6769964694976807, 2.642760753631592, 1.9969762563705444]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.568389892578125, 3.413985252380371, 3.0558807849884033, 3.1171228885650635, 3.039224624633789, 2.8431222438812256, 2.7869489192962646, 2.0554211139678955, 3.2809665203094482, 2.9464709758758545, 2.764169216156006, 1.3698146343231201, 2.9218292236328125, 2.7667620182037354, 3.03737473487854, 2.0637760162353516]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9221655130386353, 1.8410848379135132, 2.641099214553833, 2.594905138015747, 2.6308138370513916, 1.990293025970459, 1.7761398553848267, 3.093714952468872, 0.5658506751060486, 2.1757729053497314, 0.9803143739700317, 2.4518353939056396, 2.750321626663208, 2.9259676933288574, 2.957641839981079, 1.4287681579589844]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.683767557144165, 3.9534528255462646, 5.561268329620361, 4.947647571563721, 5.516071319580078, 5.6787190437316895, 5.133167743682861, 5.117264270782471, 6.158114910125732, 5.046653747558594, 5.772087097167969, 6.076358795166016, 4.83423376083374, 5.404752731323242, 4.306597709655762, 4.95967435836792]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.212981700897217, 4.560283660888672, 5.5516037940979, 5.173280715942383, 3.70316481590271, 4.463963031768799, 4.879268646240234, 5.29586935043335, 5.3658061027526855, 4.916912078857422, 4.271607875823975, 3.3516628742218018, 4.409230709075928, 3.386744737625122, 4.921224117279053, 4.4983439445495605]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.877255916595459, 5.839902400970459, 6.0337042808532715, 5.625463008880615, 6.071903228759766, 4.8140387535095215, 5.6849446296691895, 5.636384010314941, 5.302525997161865, 5.35703706741333, 6.212463855743408, 5.900656223297119, 4.947617530822754, 5.8565449714660645, 6.042167663574219, 5.864799976348877]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.9948623180389404, 2.9742496013641357, 2.8124005794525146, 2.2451674938201904, -0.43924766778945923, 3.0421295166015625, 2.7205753326416016, 1.8573888540267944, 2.717585325241089, 3.4681646823883057, 2.6880600452423096, 3.246748924255371, 2.8601512908935547, 2.949542999267578, 2.3322694301605225, 1.7412421703338623]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.548503875732422, 6.817624568939209, 6.5351481437683105, 6.303262233734131, 6.459419250488281, 6.478704929351807, 6.743720054626465, 6.63250207901001, 6.639857292175293, 6.326322078704834, 6.923244476318359, 6.256196022033691, 5.279938220977783, 7.1992716789245605, 6.483417987823486, 6.435679912567139]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.008832931518555, 5.351034641265869, 4.959531784057617, 4.04621696472168, 4.901062488555908, 5.2479023933410645, 4.630367279052734, 4.7797112464904785, 5.07079553604126, 4.846567153930664, 5.268529415130615, 4.876115798950195, 5.142938613891602, 5.43192720413208, 4.597474575042725, 5.476153373718262]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.145772933959961, 4.667133331298828, 3.3371849060058594, 4.195777893066406, 4.453344345092773, 4.076794147491455, 4.241893291473389, 4.231592178344727, 4.361401557922363, 4.4695634841918945, 4.054769992828369, 4.428427696228027, 3.6736013889312744, 2.0230743885040283, 3.378354072570801, 4.01832914352417]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  48%|███████████████████████▊                         | 97/200 [03:27<03:17,  1.92s/it]Layer: gate_24 - Captured router_logits: [6.28463888168335, 6.236724853515625, 5.911496639251709, 6.082066535949707, 5.428680419921875, 5.849647045135498, 6.176775932312012, 5.7030134201049805, 5.346752643585205, 5.75446891784668, 6.437881946563721, 5.521568298339844, 5.768934726715088, 6.2097649574279785, 6.226316928863525, 5.7852020263671875]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.836599588394165, 3.6716794967651367, 3.7839088439941406, 3.9473745822906494, 2.68826961517334, 3.8620409965515137, 3.939852476119995, 3.656942367553711, 3.9920248985290527, 3.613666296005249, 3.6980435848236084, 3.6214094161987305, 3.662182331085205, 3.5993103981018066, 3.704167366027832, 3.8954124450683594]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4526171684265137, 3.5267796516418457, 3.406381845474243, 3.555960178375244, 3.3853325843811035, 3.6223270893096924, 3.573861837387085, 3.367732048034668, 3.602118730545044, 3.297612190246582, 3.6017932891845703, 3.4485714435577393, 3.4368484020233154, 2.7281672954559326, 3.6502742767333984, 3.5663061141967773]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1830008029937744, 3.2095839977264404, 2.9855690002441406, 2.8978946208953857, 2.820641040802002, 3.086793899536133, 3.1828830242156982, 3.2762396335601807, 3.0341427326202393, 3.0070059299468994, 2.2883033752441406, 2.9996793270111084, 3.157097339630127, 3.0643680095672607, 3.083655834197998, 2.887079954147339]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.374226093292236, 4.490165710449219, 4.567899703979492, 4.375139236450195, 4.49260950088501, 4.464724540710449, 4.218505382537842, 4.709017276763916, 4.493679523468018, 4.459516525268555, 4.469940185546875, 4.054131507873535, 4.198421955108643, 4.0567145347595215, 4.577226638793945, 4.374536514282227]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.582158088684082, 8.90864086151123, 8.787348747253418, 8.472026824951172, 8.555240631103516, 8.267590522766113, 8.637127876281738, 8.542095184326172, 8.629084587097168, 8.494385719299316, 8.599264144897461, 8.233498573303223, 8.333950996398926, 9.003202438354492, 8.900052070617676, 8.50876522064209]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.8889570236206055, 5.428646087646484, 5.3868818283081055, 4.788144588470459, 5.1368818283081055, 5.089602470397949, 4.669850826263428, 5.133058071136475, 5.239041328430176, 4.927285671234131, 4.836901664733887, 4.731761932373047, 4.86545991897583, 4.982388973236084, 5.067519664764404, 4.832479476928711]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2338593006134033, 3.0008625984191895, 3.1019678115844727, 2.762880563735962, 3.024071455001831, 3.3100552558898926, 2.7565183639526367, 3.1791913509368896, 3.086052417755127, 3.1836798191070557, 3.1737284660339355, 3.261063575744629, 3.0335681438446045, 2.981241464614868, 3.1213197708129883, 3.178954601287842]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12335126847028732, 0.13852600753307343, 0.13156694173812866, -0.24876165390014648, -0.24351057410240173, -0.10550107806921005, 0.14409351348876953, -0.17581059038639069, 0.10483627766370773, 0.1171785220503807, 0.11612647771835327, 0.07773575186729431, 0.1085132509469986, 0.1286306530237198, -1.113069772720337, 0.13969159126281738]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09096339344978333, 0.05086052045226097, 0.042264629155397415, 0.06297898292541504, 0.08398957550525665, 0.039200007915496826, 0.052171170711517334, 0.07864418625831604, 0.02146589383482933, 0.06120942533016205, -0.18477872014045715, 0.06113080307841301, 0.002806050004437566, -0.0045629385858774185, 0.017169635742902756, 0.025323400273919106]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06958478689193726, 0.04227936267852783, 0.09194561094045639, 0.054075006395578384, 0.0964946448802948, 0.10025037825107574, 0.05904027447104454, -0.14384429156780243, 0.0644671693444252, 0.08477044850587845, 0.0008758408948779106, 0.07585906237363815, -0.21524742245674133, -0.0105164535343647, -0.054251234978437424, 0.12660114467144012]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.2118397206068039, 0.1466549038887024, 0.12444180995225906, 0.14781655371189117, 0.0428139753639698, 0.0577581562101841, -0.08783213049173355, 0.17838086187839508, 0.1530197113752365, -0.4422575533390045, 0.021168500185012817, 0.16638551652431488, 0.09314606338739395, -0.2828662693500519, -0.08144692331552505, -0.04657357558608055]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0776422768831253, -0.03082813322544098, -0.06663981825113297, 0.14205093681812286, -0.06579255312681198, -0.14992573857307434, 0.09118915349245071, 0.04966850206255913, 0.06631635129451752, 0.20334996283054352, -0.2840227484703064, 0.022187866270542145, 0.013815676793456078, -0.1788046956062317, 0.08311673253774643, 0.0287319365888834]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21014226973056793, 0.23975886404514313, 0.14388982951641083, 0.056657180190086365, -0.3053637146949768, -0.058726370334625244, 0.09498919546604156, 0.00798039324581623, 0.16363555192947388, -0.0591411218047142, -0.5086721777915955, 0.050441350787878036, -0.3411124646663666, 0.25854915380477905, 0.15503470599651337, 0.1523774415254593]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3146611154079437, 0.3343871235847473, 0.1962425410747528, 0.26578715443611145, 0.26860755681991577, 0.24549487233161926, 0.12119125574827194, 0.15216416120529175, -0.3914602994918823, 0.3918328583240509, -0.08610326051712036, 0.3162747621536255, 0.09267893433570862, -0.07414914667606354, 0.28339970111846924, 0.17688995599746704]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.053943391889333725, 0.19927221536636353, 0.1182355061173439, 0.15825200080871582, -0.35938727855682373, 0.2032354176044464, 0.05880443751811981, 0.21114295721054077, -0.028126414865255356, -0.16922889649868011, 0.06193552538752556, -0.06616367399692535, 0.1711467206478119, 0.10769578814506531, -0.024991778656840324, 0.14026491343975067]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21143263578414917, 0.2815060615539551, -0.0906122624874115, 0.3231031894683838, 0.46126291155815125, 0.40118053555488586, 0.1165284812450409, -0.011259512044489384, 0.620094895362854, -0.18794837594032288, 0.25554269552230835, 0.04945848137140274, 0.6721435189247131, -0.21149906516075134, -0.028856659308075905, 0.1905364990234375]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8629392385482788, 0.7463173270225525, 0.6176896095275879, 0.5743178725242615, 0.8388174176216125, 0.43641597032546997, 1.1041653156280518, 1.0685582160949707, 0.08685484528541565, 0.022266097366809845, 0.42525461316108704, 0.7907455563545227, 0.7396063208580017, 0.8187530040740967, 1.1175107955932617, 0.7901475429534912]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1492486000061035, 1.1682952642440796, 0.6522436141967773, 1.0517023801803589, 0.949844241142273, 0.3725510537624359, 0.9859136939048767, 1.4179604053497314, 0.7609527707099915, 0.27927878499031067, 1.0965429544448853, 1.1046767234802246, 0.6162283420562744, 1.0950400829315186, 0.4990130066871643, 0.9992648363113403]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2358616590499878, 1.5969786643981934, 1.5253409147262573, 1.4049969911575317, 1.419208288192749, 0.832566499710083, 1.7531821727752686, 1.2183709144592285, 1.4824568033218384, 1.450608491897583, 1.2516037225723267, 1.162292242050171, 1.1383785009384155, 1.3271667957305908, 1.1954846382141113, 1.191532850265503]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0407551527023315, 1.5036842823028564, 1.6481647491455078, 1.7908633947372437, 1.247930884361267, 1.5792165994644165, 1.290557861328125, 1.5256719589233398, 1.3471612930297852, 0.629224419593811, 1.6978284120559692, 1.949386477470398, 1.7597899436950684, 1.6726988554000854, 1.422742247581482, 1.5304226875305176]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0719635486602783, 3.860318899154663, 3.0332791805267334, 2.6454951763153076, 3.4864308834075928, 2.0815834999084473, 3.658656120300293, 3.1734392642974854, 3.8237152099609375, 3.3811821937561035, 3.1899547576904297, 3.9259090423583984, 2.1289820671081543, 3.9547505378723145, 3.155513286590576, 2.791826009750366]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3947908878326416, 2.3716371059417725, 2.8466193675994873, 2.42891001701355, 2.806365728378296, 2.071965217590332, 1.5730332136154175, 2.108396291732788, 2.6382086277008057, 1.9911518096923828, 1.344361662864685, 1.9408855438232422, 2.432420253753662, 2.675874948501587, 2.641427516937256, 1.999342441558838]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5643551349639893, 3.4099314212799072, 3.052809715270996, 3.1146836280822754, 3.031099319458008, 2.8406753540039062, 2.7874698638916016, 2.052591562271118, 3.2791099548339844, 2.942775011062622, 2.764033317565918, 1.37032151222229, 2.9162769317626953, 2.7623255252838135, 3.03356671333313, 2.06600022315979]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.920699954032898, 1.8358078002929688, 2.6383345127105713, 2.591555595397949, 2.628767967224121, 1.9896775484085083, 1.7784500122070312, 3.0917489528656006, 0.566349983215332, 2.173508644104004, 0.9782125353813171, 2.4500622749328613, 2.7469699382781982, 2.92390513420105, 2.9556634426116943, 1.429992437362671]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.677881956100464, 3.949718952178955, 5.554623126983643, 4.93699836730957, 5.506014823913574, 5.671687602996826, 5.131319999694824, 5.110145568847656, 6.150660514831543, 5.038702011108398, 5.765162467956543, 6.068041801452637, 4.828561782836914, 5.3951735496521, 4.299039840698242, 4.951718807220459]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.203661918640137, 4.553962230682373, 5.541968822479248, 5.165369987487793, 3.6965577602386475, 4.453832626342773, 4.8749237060546875, 5.285706043243408, 5.357734203338623, 4.909958839416504, 4.263755798339844, 3.3449344635009766, 4.401937007904053, 3.381296396255493, 4.917011737823486, 4.491937160491943]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.870758056640625, 5.8352155685424805, 6.028193473815918, 5.618871688842773, 6.066632270812988, 4.809576988220215, 5.6791815757751465, 5.630312442779541, 5.300460338592529, 5.351407051086426, 6.206366539001465, 5.895236492156982, 4.943253040313721, 5.850473880767822, 6.036843776702881, 5.859141826629639]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.9917471408843994, 2.9703993797302246, 2.809805393218994, 2.2420880794525146, -0.437440425157547, 3.039870262145996, 2.7170565128326416, 1.8573265075683594, 2.716294527053833, 3.466705799102783, 2.685088634490967, 3.245335817337036, 2.85923433303833, 2.9485433101654053, 2.3297228813171387, 1.7415374517440796]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.537769794464111, 6.808745384216309, 6.525730133056641, 6.293827056884766, 6.450558185577393, 6.468758583068848, 6.7330641746521, 6.622551918029785, 6.629828929901123, 6.316695690155029, 6.914098262786865, 6.24845552444458, 5.272061347961426, 7.18944787979126, 6.473145484924316, 6.426793575286865]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.006531715393066, 5.348453521728516, 4.955961227416992, 4.044153690338135, 4.897716999053955, 5.244637489318848, 4.627774715423584, 4.780182838439941, 5.067167282104492, 4.843973636627197, 5.266165733337402, 4.8747124671936035, 5.139638900756836, 5.429188251495361, 4.594751358032227, 5.472479343414307]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.145261764526367, 4.663411617279053, 3.3349151611328125, 4.193628787994385, 4.451785564422607, 4.0748186111450195, 4.238837242126465, 4.227390289306641, 4.357989311218262, 4.466196060180664, 4.052855491638184, 4.4256911277771, 3.671996593475342, 2.0241360664367676, 3.3753395080566406, 4.016885757446289]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.278751373291016, 6.230264186859131, 5.904386043548584, 6.076031684875488, 5.42460823059082, 5.843550682067871, 6.1704630851745605, 5.697635650634766, 5.340906620025635, 5.749023914337158, 6.430349349975586, 5.516082763671875, 5.762196063995361, 6.203110694885254, 6.220114231109619, 5.779521465301514]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8292219638824463, 3.665541172027588, 3.7777256965637207, 3.941218614578247, 2.684340000152588, 3.8552420139312744, 3.9325735569000244, 3.651482582092285, 3.9849562644958496, 3.6071743965148926, 3.6905581951141357, 3.6152024269104004, 3.657161235809326, 3.5926146507263184, 3.69848895072937, 3.888474225997925]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.446767807006836, 3.5217013359069824, 3.4006175994873047, 3.5500876903533936, 3.380190372467041, 3.616795063018799, 3.567873954772949, 3.361945867538452, 3.5958616733551025, 3.292367935180664, 3.596616506576538, 3.442268133163452, 3.430994987487793, 2.7236826419830322, 3.645557165145874, 3.56070876121521]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.177097797393799, 3.2032406330108643, 2.980184555053711, 2.8922317028045654, 2.814692258834839, 3.081207036972046, 3.1779797077178955, 3.2701969146728516, 3.0278968811035156, 3.0016252994537354, 2.2848458290100098, 2.993642568588257, 3.151186943054199, 3.057931661605835, 3.0770726203918457, 2.881152629852295]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  50%|████████████████████████▏                       | 101/200 [03:35<03:07,  1.89s/it]Layer: gate_28 - Captured router_logits: [4.366711616516113, 4.483342170715332, 4.560326099395752, 4.366883277893066, 4.4833478927612305, 4.456857681274414, 4.210671901702881, 4.700784206390381, 4.4863481521606445, 4.4507670402526855, 4.462327480316162, 4.04699182510376, 4.1898722648620605, 4.049213409423828, 4.568904876708984, 4.365955352783203]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.56599235534668, 8.888219833374023, 8.770760536193848, 8.454090118408203, 8.537629127502441, 8.251232147216797, 8.61766242980957, 8.526013374328613, 8.612056732177734, 8.477377891540527, 8.582925796508789, 8.216766357421875, 8.31799602508545, 8.984539985656738, 8.880800247192383, 8.48918342590332]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.876498699188232, 5.4160542488098145, 5.3741888999938965, 4.777674674987793, 5.124204158782959, 5.073800086975098, 4.657155990600586, 5.12007999420166, 5.225500106811523, 4.914103031158447, 4.822846412658691, 4.719432353973389, 4.852921962738037, 4.969830513000488, 5.054135322570801, 4.821104049682617]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2255210876464844, 2.993133544921875, 3.094827890396118, 2.753782033920288, 3.0163943767547607, 3.3032963275909424, 2.7472481727600098, 3.171062469482422, 3.077711343765259, 3.1754398345947266, 3.1669211387634277, 3.2539620399475098, 3.025294065475464, 2.9727845191955566, 3.114121675491333, 3.170234203338623]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11831708997488022, 0.13264872133731842, 0.12945710122585297, -0.24952130019664764, -0.25390273332595825, -0.0947536826133728, 0.1400330811738968, -0.16580210626125336, 0.09908735752105713, 0.11267800629138947, 0.11224379390478134, 0.06649620085954666, 0.10483729094266891, 0.12547138333320618, -1.1167919635772705, 0.13488346338272095]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08742967247962952, 0.049884598702192307, 0.04054751619696617, 0.06178085878491402, 0.08205606043338776, 0.03759235888719559, 0.056130725890398026, 0.07998622953891754, 0.027536513283848763, 0.06068732216954231, -0.1839107722043991, 0.0606393925845623, 0.0014804252423346043, -0.0037207675632089376, 0.02279761992394924, 0.02806221693754196]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0714196190237999, 0.041225261986255646, 0.09140218049287796, 0.05311736837029457, 0.0985260009765625, 0.1015811637043953, 0.056966859847307205, -0.14330491423606873, 0.06792136281728745, 0.0885968878865242, 0.004009977448731661, 0.07817938178777695, -0.21934662759304047, -0.009788315743207932, -0.05613026022911072, 0.1249975860118866]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21530365943908691, 0.14699417352676392, 0.1246013417840004, 0.14400914311408997, 0.04567514359951019, 0.054530736058950424, -0.0961647778749466, 0.17766311764717102, 0.154135599732399, -0.45626938343048096, 0.03283201903104782, 0.15911731123924255, 0.10433001071214676, -0.29714611172676086, -0.06335517019033432, -0.04788823425769806]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07624337077140808, -0.026948701590299606, -0.06858498603105545, 0.15233732759952545, -0.06272570788860321, -0.14745038747787476, 0.08905306458473206, 0.04463397338986397, 0.059498824179172516, 0.20193953812122345, -0.2928785979747772, 0.02298547700047493, 0.02066250704228878, -0.18527066707611084, 0.08615495264530182, 0.014730817638337612]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.20728076994419098, 0.23833802342414856, 0.14202755689620972, 0.05710217356681824, -0.309867799282074, -0.06429658085107803, 0.09739833325147629, 0.0048065572045743465, 0.1665731519460678, -0.059068463742733, -0.5006572008132935, 0.04864531755447388, -0.3431780934333801, 0.25976699590682983, 0.15489469468593597, 0.15251368284225464]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.31738612055778503, 0.3327329158782959, 0.1958782970905304, 0.2670164704322815, 0.2679097652435303, 0.24644634127616882, 0.11605503410100937, 0.14999976754188538, -0.3930746018886566, 0.3928973972797394, -0.08911892026662827, 0.3166588246822357, 0.09309615194797516, -0.08008316159248352, 0.28594157099723816, 0.17244857549667358]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.04863734543323517, 0.1999824047088623, 0.11765482276678085, 0.16044336557388306, -0.3627963662147522, 0.20433463156223297, 0.060535185039043427, 0.20102646946907043, -0.020648278295993805, -0.1680745780467987, 0.056747544556856155, -0.06414903700351715, 0.16955581307411194, 0.11289031058549881, -0.03297482430934906, 0.14182163774967194]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21004487574100494, 0.2814718186855316, -0.09584558010101318, 0.325199693441391, 0.4641701877117157, 0.4000343978404999, 0.11663269251585007, -0.009170242585241795, 0.6186057329177856, -0.18745142221450806, 0.2585175037384033, 0.05089171603322029, 0.6746507883071899, -0.2141904979944229, -0.0374351367354393, 0.19293977320194244]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8621185421943665, 0.7449607849121094, 0.6181411743164062, 0.5799643993377686, 0.837800145149231, 0.4331168830394745, 1.1022762060165405, 1.0677520036697388, 0.08934270590543747, 0.021217653527855873, 0.42294153571128845, 0.7926586866378784, 0.7411692142486572, 0.8089214563369751, 1.1189312934875488, 0.791115939617157]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.154405117034912, 1.1760401725769043, 0.6572726964950562, 1.0565478801727295, 0.9497694373130798, 0.3690628707408905, 0.9892983436584473, 1.422090768814087, 0.7623995542526245, 0.2755125164985657, 1.1018989086151123, 1.1091711521148682, 0.6203518509864807, 1.092965841293335, 0.5033376216888428, 1.0005470514297485]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2409676313400269, 1.6022486686706543, 1.5316232442855835, 1.3988580703735352, 1.4249821901321411, 0.8293803334236145, 1.7514195442199707, 1.2157992124557495, 1.4880633354187012, 1.458215594291687, 1.257825255393982, 1.1646640300750732, 1.1386746168136597, 1.3278402090072632, 1.1954237222671509, 1.1901590824127197]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0377075672149658, 1.5001884698867798, 1.6532208919525146, 1.7927496433258057, 1.2456703186035156, 1.5806833505630493, 1.2880619764328003, 1.5261826515197754, 1.3450392484664917, 0.6298679113388062, 1.7007005214691162, 1.952954888343811, 1.7546592950820923, 1.6770509481430054, 1.4236626625061035, 1.5336713790893555]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0737850666046143, 3.865288496017456, 3.0401194095611572, 2.644763708114624, 3.490995168685913, 2.081437587738037, 3.6626861095428467, 3.175593852996826, 3.826634407043457, 3.3863165378570557, 3.1921935081481934, 3.9312522411346436, 2.1293351650238037, 3.9516565799713135, 3.158623218536377, 2.7927489280700684]
Running loglikelihood requests:  52%|█████████████████████████▏                      | 105/200 [03:42<02:59,  1.89s/it]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.39560866355896, 2.371525764465332, 2.8502581119537354, 2.4315943717956543, 2.809072732925415, 2.0765771865844727, 1.5698319673538208, 2.109231472015381, 2.6393163204193115, 1.9930081367492676, 1.3387150764465332, 1.94248628616333, 2.435673713684082, 2.681018829345703, 2.644925594329834, 1.9966683387756348]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5644948482513428, 3.4111335277557373, 3.0541181564331055, 3.120112419128418, 3.0323126316070557, 2.8428280353546143, 2.789876937866211, 2.0527501106262207, 3.279066324234009, 2.9447622299194336, 2.7634477615356445, 1.3689357042312622, 2.920114755630493, 2.7650742530822754, 3.0321202278137207, 2.0648231506347656]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9223841428756714, 1.833882212638855, 2.6396327018737793, 2.596160411834717, 2.6305010318756104, 1.988755226135254, 1.7700554132461548, 3.0904977321624756, 0.5626577138900757, 2.177309513092041, 0.9705837368965149, 2.448768377304077, 2.75091814994812, 2.9256410598754883, 2.957319498062134, 1.4248740673065186]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.666987657546997, 3.9454505443573, 5.551159858703613, 4.933460235595703, 5.505504608154297, 5.666787624359131, 5.129377841949463, 5.1068620681762695, 6.152218341827393, 5.030824661254883, 5.7641215324401855, 6.064210891723633, 4.822660446166992, 5.3921003341674805, 4.292062282562256, 4.946115016937256]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.203523635864258, 4.550914764404297, 5.539788722991943, 5.168904781341553, 3.695268392562866, 4.45191764831543, 4.872015476226807, 5.28452205657959, 5.361285209655762, 4.907951831817627, 4.261678218841553, 3.339324951171875, 4.398185729980469, 3.3755173683166504, 4.912989139556885, 4.489675045013428]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.8700737953186035, 5.834754943847656, 6.030623435974121, 5.619503498077393, 6.068132400512695, 4.806463718414307, 5.677343845367432, 5.627841472625732, 5.299449920654297, 5.354590892791748, 6.209815979003906, 5.896409511566162, 4.942704677581787, 5.851999759674072, 6.037434101104736, 5.860716342926025]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.9958629608154297, 2.9715564250946045, 2.8104894161224365, 2.24021053314209, -0.4478342831134796, 3.0434041023254395, 2.7181832790374756, 1.8544353246688843, 2.7158844470977783, 3.467836856842041, 2.6850781440734863, 3.2485008239746094, 2.8591811656951904, 2.9483895301818848, 2.3249990940093994, 1.7363635301589966]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.546566963195801, 6.817972183227539, 6.532421112060547, 6.301230430603027, 6.4571661949157715, 6.476973533630371, 6.7410054206848145, 6.6334547996521, 6.638182640075684, 6.324902057647705, 6.922532558441162, 6.255337715148926, 5.27791166305542, 7.199677467346191, 6.480855941772461, 6.4344987869262695]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.010041236877441, 5.353464126586914, 4.962217330932617, 4.048148155212402, 4.902279376983643, 5.250082015991211, 4.63065242767334, 4.7829790115356445, 5.072399139404297, 4.847927093505859, 5.273927211761475, 4.878212928771973, 5.1454339027404785, 5.4342041015625, 4.598082065582275, 5.476093769073486]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.149959564208984, 4.67065954208374, 3.339088201522827, 4.199942111968994, 4.4590678215026855, 4.081315040588379, 4.244095802307129, 4.233591556549072, 4.363936901092529, 4.474287033081055, 4.058565616607666, 4.432601451873779, 3.6766879558563232, 2.0277211666107178, 3.379284381866455, 4.023471355438232]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.287722110748291, 6.236953258514404, 5.911311149597168, 6.082220554351807, 5.430304050445557, 5.850465297698975, 6.1772613525390625, 5.703646659851074, 5.346527099609375, 5.754335880279541, 6.43778133392334, 5.522491931915283, 5.7700395584106445, 6.210501670837402, 6.226865768432617, 5.786757946014404]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8362977504730225, 3.672713041305542, 3.784452199935913, 3.948294162750244, 2.6890547275543213, 3.863229751586914, 3.93906307220459, 3.657212972640991, 3.9920570850372314, 3.613771677017212, 3.6985771656036377, 3.6229441165924072, 3.664297103881836, 3.598893165588379, 3.7076425552368164, 3.8958351612091064]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.452695608139038, 3.5276987552642822, 3.405404567718506, 3.5559635162353516, 3.3859996795654297, 3.623772382736206, 3.5741705894470215, 3.3673593997955322, 3.6018426418304443, 3.2977352142333984, 3.6036548614501953, 3.448474168777466, 3.4369852542877197, 2.7268221378326416, 3.652181625366211, 3.5676920413970947]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1835012435913086, 3.2099499702453613, 2.9851415157318115, 2.8981316089630127, 2.819345235824585, 3.0867538452148438, 3.1838278770446777, 3.2756130695343018, 3.033243179321289, 3.006863594055176, 2.288281202316284, 2.999378204345703, 3.1585845947265625, 3.0673792362213135, 3.0817580223083496, 2.8869009017944336]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.373697757720947, 4.489868640899658, 4.566402912139893, 4.372957229614258, 4.492650508880615, 4.463326454162598, 4.218522548675537, 4.707489967346191, 4.49287748336792, 4.4573187828063965, 4.468871116638184, 4.054279327392578, 4.196771621704102, 4.055548667907715, 4.5754313468933105, 4.373651504516602]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.577666282653809, 8.904342651367188, 8.781778335571289, 8.466568946838379, 8.550342559814453, 8.263553619384766, 8.632412910461426, 8.539114952087402, 8.623627662658691, 8.48884391784668, 8.595649719238281, 8.228350639343262, 8.334040641784668, 8.99697208404541, 8.894206047058105, 8.501824378967285]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.883556842803955, 5.423874855041504, 5.380871295928955, 4.7831878662109375, 5.132519245147705, 5.0809855461120605, 4.664041042327881, 5.127921104431152, 5.235500335693359, 4.921934604644775, 4.831089496612549, 4.7255425453186035, 4.860329627990723, 4.9770426750183105, 5.061127662658691, 4.827746391296387]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2285561561584473, 2.996584892272949, 3.0984253883361816, 2.757059097290039, 3.020125389099121, 3.30564022064209, 2.750023365020752, 3.1733622550964355, 3.0815415382385254, 3.178210973739624, 3.170510768890381, 3.257073402404785, 3.0292763710021973, 2.977287530899048, 3.1196587085723877, 3.1731882095336914]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.120383121073246, 0.1337762475013733, 0.12804926931858063, -0.2364388257265091, -0.23571284115314484, -0.09089187532663345, 0.14206157624721527, -0.18356381356716156, 0.10586734116077423, 0.11419660598039627, 0.11325740814208984, 0.07327347248792648, 0.10511308908462524, 0.12544746696949005, -1.0937756299972534, 0.1351373940706253]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09066949784755707, 0.04926013574004173, 0.038464438170194626, 0.06358063220977783, 0.08109549432992935, 0.04142516851425171, 0.05132266506552696, 0.07406094670295715, 0.012948817573487759, 0.060599204152822495, -0.17429929971694946, 0.060083888471126556, 0.0007520020590163767, -0.00028975444729439914, 0.01670161262154579, 0.02223401702940464]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06649193912744522, 0.04495915398001671, 0.09318019449710846, 0.05077822506427765, 0.09136094897985458, 0.0986262783408165, 0.054158251732587814, -0.1413670927286148, 0.06155429035425186, 0.08154502511024475, -0.001059354399330914, 0.07526663690805435, -0.20502224564552307, -0.011136684566736221, -0.06244826689362526, 0.12369915843009949]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.20870773494243622, 0.14536646008491516, 0.1261347383260727, 0.14697884023189545, 0.04649876430630684, 0.05218146741390228, -0.09527359902858734, 0.17511412501335144, 0.14923787117004395, -0.43730300664901733, 0.03072413243353367, 0.16683419048786163, 0.0804578885436058, -0.284310907125473, -0.07825734466314316, -0.03764163702726364]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07828245311975479, -0.02819112502038479, -0.06784091889858246, 0.13691435754299164, -0.06690195202827454, -0.14750586450099945, 0.08964725583791733, 0.048751238733530045, 0.07053204625844955, 0.2020319551229477, -0.2742084562778473, 0.026934340596199036, 0.0028120132628828287, -0.17453670501708984, 0.0813031867146492, 0.02229357697069645]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.2114153951406479, 0.240469291806221, 0.14519381523132324, 0.05402522534132004, -0.3057394027709961, -0.05917179584503174, 0.09097572416067123, 0.008921770378947258, 0.16101369261741638, -0.056263942271471024, -0.5070095658302307, 0.052209530025720596, -0.3349909782409668, 0.2574816048145294, 0.15605388581752777, 0.15426476299762726]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3157242238521576, 0.33821916580200195, 0.193431094288826, 0.26651865243911743, 0.2656271755695343, 0.2437048703432083, 0.12184455990791321, 0.15181367099285126, -0.39138466119766235, 0.3922348916530609, -0.0761827677488327, 0.3180391788482666, 0.08921639621257782, -0.06993365287780762, 0.27728378772735596, 0.17297178506851196]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05413069948554039, 0.20186470448970795, 0.11840847134590149, 0.15761999785900116, -0.35153892636299133, 0.20229049026966095, 0.057853709906339645, 0.21418960392475128, -0.032192230224609375, -0.16913968324661255, 0.061526957899332047, -0.06710626929998398, 0.17494434118270874, 0.10540729016065598, -0.021857179701328278, 0.14601676166057587]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2078544944524765, 0.2799985408782959, -0.08651834726333618, 0.3241519033908844, 0.4605523347854614, 0.4004722833633423, 0.11499084532260895, -0.006775332614779472, 0.6182063221931458, -0.19296368956565857, 0.2566312849521637, 0.05047048255801201, 0.6708407998085022, -0.20659783482551575, -0.030704788863658905, 0.1900903284549713]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8656989932060242, 0.7484368085861206, 0.6210184693336487, 0.5680340528488159, 0.8416146039962769, 0.43829861283302307, 1.10792076587677, 1.0688652992248535, 0.08664163947105408, 0.02675742655992508, 0.42748433351516724, 0.7923081517219543, 0.7436663508415222, 0.8185626268386841, 1.1173092126846313, 0.7893482446670532]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.152123212814331, 1.168567180633545, 0.6564853191375732, 1.05620276927948, 0.9532873630523682, 0.37791907787323, 0.9908692240715027, 1.4228012561798096, 0.7695343494415283, 0.28611502051353455, 1.0955926179885864, 1.1048965454101562, 0.6306195855140686, 1.0958304405212402, 0.4990027844905853, 1.004752516746521]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.243085265159607, 1.5979984998703003, 1.5315546989440918, 1.409664511680603, 1.4232600927352905, 0.8404999375343323, 1.7524441480636597, 1.2241427898406982, 1.4837112426757812, 1.4498820304870605, 1.2601280212402344, 1.1708412170410156, 1.1488896608352661, 1.3387786149978638, 1.207472324371338, 1.1983104944229126]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0475716590881348, 1.5086604356765747, 1.649076223373413, 1.7926491498947144, 1.258613109588623, 1.5845706462860107, 1.2915821075439453, 1.52263605594635, 1.352237343788147, 0.6383630037307739, 1.7005523443222046, 1.9520306587219238, 1.7601841688156128, 1.6770498752593994, 1.4264605045318604, 1.5345571041107178]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0819718837738037, 3.8600614070892334, 3.0394744873046875, 2.653852701187134, 3.484480381011963, 2.093111991882324, 3.660158395767212, 3.165287971496582, 3.8241779804229736, 3.3857014179229736, 3.191162109375, 3.933424472808838, 2.1427829265594482, 3.9585163593292236, 3.156285285949707, 2.798771858215332]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.392524480819702, 2.3686347007751465, 2.8456201553344727, 2.4296464920043945, 2.8048129081726074, 2.073850631713867, 1.5768282413482666, 2.0996789932250977, 2.6344220638275146, 1.9890968799591064, 1.346579909324646, 1.9433785676956177, 2.431410074234009, 2.67560076713562, 2.6402945518493652, 2.0003042221069336]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5730173587799072, 3.4137370586395264, 3.056267499923706, 3.1183290481567383, 3.03991436958313, 2.8451812267303467, 2.7910711765289307, 2.0511550903320312, 3.2778913974761963, 2.947707176208496, 2.7702701091766357, 1.3798094987869263, 2.9229259490966797, 2.768679618835449, 3.0363667011260986, 2.0685126781463623]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.925513505935669, 1.8407678604125977, 2.641266107559204, 2.5934808254241943, 2.627929210662842, 1.9922313690185547, 1.7742469310760498, 3.0875279903411865, 0.5701695680618286, 2.1724767684936523, 0.9806423783302307, 2.4509758949279785, 2.752645969390869, 2.9265952110290527, 2.958866834640503, 1.427686095237732]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.68216609954834, 3.956885814666748, 5.558298110961914, 4.945968151092529, 5.516272068023682, 5.675880432128906, 5.131790637969971, 5.11942195892334, 6.153973579406738, 5.042512893676758, 5.770196914672852, 6.074779510498047, 4.829090118408203, 5.399007320404053, 4.309845447540283, 4.9591803550720215]
Running loglikelihood requests:  55%|██████████████████████████▏                     | 109/200 [03:50<02:52,  1.90s/it]Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.2121734619140625, 4.557398796081543, 5.550245761871338, 5.1711530685424805, 3.705658435821533, 4.462795734405518, 4.879560470581055, 5.2938361167907715, 5.366262435913086, 4.917379379272461, 4.274005889892578, 3.3494606018066406, 4.410067081451416, 3.386381149291992, 4.919246673583984, 4.496132850646973]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.879650592803955, 5.841558456420898, 6.036638259887695, 5.626816272735596, 6.074633598327637, 4.8185296058654785, 5.687858581542969, 5.637605667114258, 5.303886890411377, 5.358648777008057, 6.214742183685303, 5.902462482452393, 4.953502655029297, 5.860061168670654, 6.044551372528076, 5.867414474487305]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.9987003803253174, 2.97456693649292, 2.8147153854370117, 2.243248462677002, -0.4398830533027649, 3.044910192489624, 2.7243170738220215, 1.8618011474609375, 2.7179527282714844, 3.471681833267212, 2.690452814102173, 3.248242139816284, 2.8623552322387695, 2.9517741203308105, 2.331329584121704, 1.7434372901916504]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.552093029022217, 6.821321487426758, 6.536525249481201, 6.305543422698975, 6.463028430938721, 6.481412887573242, 6.74652099609375, 6.638021469116211, 6.644237995147705, 6.330981254577637, 6.92719030380249, 6.2606706619262695, 5.28335428237915, 7.206137180328369, 6.4891533851623535, 6.441359043121338]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.011813163757324, 5.353018283843994, 4.963940143585205, 4.04904842376709, 4.903497695922852, 5.250729560852051, 4.633291244506836, 4.7836408615112305, 5.073497295379639, 4.848873615264893, 5.272045612335205, 4.8795952796936035, 5.146766662597656, 5.4352617263793945, 4.599978923797607, 5.477738857269287]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.1482253074646, 4.668062210083008, 3.33674693107605, 4.199246883392334, 4.457458019256592, 4.079123020172119, 4.242933750152588, 4.23349142074585, 4.363402366638184, 4.472335338592529, 4.057189464569092, 4.4295783042907715, 3.675354480743408, 2.024085521697998, 3.3798179626464844, 4.0210795402526855]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.287487030029297, 6.238340377807617, 5.912357807159424, 6.083957672119141, 5.429647922515869, 5.852074146270752, 6.1773247718811035, 5.704705238342285, 5.346304893493652, 5.755112171173096, 6.440778732299805, 5.523941516876221, 5.769044876098633, 6.211267948150635, 6.2276177406311035, 5.785962104797363]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.838308095932007, 3.6730282306671143, 3.7838938236236572, 3.948986768722534, 2.687473773956299, 3.862291097640991, 3.9412434101104736, 3.6589860916137695, 3.992115020751953, 3.614461898803711, 3.6978893280029297, 3.622962236404419, 3.6643126010894775, 3.5990021228790283, 3.707970142364502, 3.8964242935180664]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.45249080657959, 3.5288171768188477, 3.4058406352996826, 3.5564866065979004, 3.386429786682129, 3.6234893798828125, 3.574554681777954, 3.367136240005493, 3.602511167526245, 3.298306703567505, 3.602275848388672, 3.4480462074279785, 3.4376370906829834, 2.727816104888916, 3.6510465145111084, 3.5668153762817383]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1828255653381348, 3.20912504196167, 2.9846692085266113, 2.8975558280944824, 2.8200743198394775, 3.0861623287200928, 3.183701276779175, 3.2753419876098633, 3.033085346221924, 3.006054639816284, 2.288158416748047, 2.998591184616089, 3.156780242919922, 3.0632545948028564, 3.0814003944396973, 2.887782096862793]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.373584270477295, 4.489607334136963, 4.565176486968994, 4.373132705688477, 4.490693092346191, 4.4640913009643555, 4.217475891113281, 4.7082200050354, 4.492265701293945, 4.458060264587402, 4.469114780426025, 4.053613662719727, 4.196996212005615, 4.056099891662598, 4.57657527923584, 4.373778343200684]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.58286190032959, 8.909026145935059, 8.789265632629395, 8.471145629882812, 8.5551176071167, 8.268436431884766, 8.637635231018066, 8.541415214538574, 8.628024101257324, 8.493600845336914, 8.600191116333008, 8.232937812805176, 8.33706283569336, 9.002862930297852, 8.900226593017578, 8.510031700134277]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.888803482055664, 5.428170204162598, 5.384884834289551, 4.788238525390625, 5.132861137390137, 5.084007263183594, 4.669520854949951, 5.1333208084106445, 5.2366814613342285, 4.927038192749023, 4.834963321685791, 4.728999614715576, 4.863898277282715, 4.978643417358398, 5.065985202789307, 4.831527233123779]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.232553005218506, 3.000606060028076, 3.1020190715789795, 2.7593462467193604, 3.0242700576782227, 3.3112826347351074, 2.753558397293091, 3.1766176223754883, 3.0844690799713135, 3.183098077774048, 3.1730246543884277, 3.2601118087768555, 3.0336291790008545, 2.9819271564483643, 3.1212141513824463, 3.177304267883301]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11909562349319458, 0.13447999954223633, 0.12727345526218414, -0.2315032035112381, -0.2348758429288864, -0.10198226571083069, 0.1420254111289978, -0.1755836457014084, 0.1024320051074028, 0.11399795114994049, 0.11358264833688736, 0.07461003214120865, 0.10644178092479706, 0.12462298572063446, -1.0928051471710205, 0.13530908524990082]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0880037471652031, 0.054456088691949844, 0.043186742812395096, 0.06247660517692566, 0.08502277731895447, 0.03799780085682869, 0.05460767820477486, 0.07796066254377365, 0.020457953214645386, 0.061744846403598785, -0.17553815245628357, 0.05917210504412651, 0.0010328671196475625, -0.0013031205162405968, 0.015543513931334019, 0.02912645787000656]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07123272120952606, 0.044638246297836304, 0.09219261258840561, 0.0525427870452404, 0.0983409583568573, 0.10443612933158875, 0.05685875564813614, -0.1401229202747345, 0.06217524781823158, 0.08480393141508102, -0.002181785646826029, 0.08011803776025772, -0.20548725128173828, -0.005449292715638876, -0.0546809546649456, 0.12447315454483032]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.2151690274477005, 0.15079164505004883, 0.12686774134635925, 0.1492595076560974, 0.04642574116587639, 0.055499691516160965, -0.08481660485267639, 0.17932726442813873, 0.15239456295967102, -0.4349808394908905, 0.027446720749139786, 0.16205963492393494, 0.08926908671855927, -0.2770605981349945, -0.08357066661119461, -0.041554007679224014]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07544161379337311, -0.031559258699417114, -0.0653114840388298, 0.13634732365608215, -0.07027757167816162, -0.1493377387523651, 0.09246502071619034, 0.05009434372186661, 0.0669458732008934, 0.2011846899986267, -0.2713077664375305, 0.028326649218797684, 0.008386177942156792, -0.17205648124217987, 0.07767021656036377, 0.022234182804822922]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.2122572809457779, 0.24037759006023407, 0.14178991317749023, 0.05117553099989891, -0.3041251301765442, -0.05651962384581566, 0.0925646722316742, 0.0049604992382228374, 0.16254161298274994, -0.05971452593803406, -0.5045661926269531, 0.050596192479133606, -0.3357737958431244, 0.2553064227104187, 0.15302056074142456, 0.15480126440525055]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.313239723443985, 0.3339676558971405, 0.19262605905532837, 0.26774895191192627, 0.26807600259780884, 0.24038507044315338, 0.12750113010406494, 0.15396013855934143, -0.3895868957042694, 0.3904292583465576, -0.07130755484104156, 0.3163078725337982, 0.09280029684305191, -0.06873873621225357, 0.2792350947856903, 0.17089003324508667]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.053920112550258636, 0.20010368525981903, 0.11950135231018066, 0.15999673306941986, -0.3550646901130676, 0.202643021941185, 0.05871499329805374, 0.21352481842041016, -0.031805869191884995, -0.16904963552951813, 0.06373997777700424, -0.06644412875175476, 0.17635327577590942, 0.10631252825260162, -0.021029077470302582, 0.14794984459877014]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21112097799777985, 0.28233015537261963, -0.08670993894338608, 0.32627126574516296, 0.4583384692668915, 0.39851313829421997, 0.11561473459005356, -0.009155941195786, 0.6178561449050903, -0.19546490907669067, 0.25654444098472595, 0.05365621671080589, 0.6703774929046631, -0.19948263466358185, -0.02504376508295536, 0.1919965147972107]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8654753565788269, 0.7570326924324036, 0.6186933517456055, 0.5696614384651184, 0.8434953093528748, 0.44561371207237244, 1.1082360744476318, 1.068198561668396, 0.09369177371263504, 0.03322256729006767, 0.437919020652771, 0.7903099656105042, 0.7491416931152344, 0.8301793336868286, 1.118898868560791, 0.7909727692604065]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1533987522125244, 1.1700445413589478, 0.6599405407905579, 1.060720682144165, 0.9576889872550964, 0.378556489944458, 0.989039421081543, 1.4231789112091064, 0.7790137529373169, 0.2908681631088257, 1.0997806787490845, 1.109081506729126, 0.6393313407897949, 1.0997939109802246, 0.4955087900161743, 1.009692668914795]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2446249723434448, 1.6018638610839844, 1.5312752723693848, 1.4096108675003052, 1.4262524843215942, 0.848030149936676, 1.7495102882385254, 1.2332003116607666, 1.4834421873092651, 1.4499648809432983, 1.263496994972229, 1.174072504043579, 1.1545186042785645, 1.342063307762146, 1.2194327116012573, 1.2029948234558105]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0545626878738403, 1.5109272003173828, 1.6482881307601929, 1.7936400175094604, 1.264405369758606, 1.5891352891921997, 1.289059042930603, 1.5236668586730957, 1.356764793395996, 0.6369499564170837, 1.7005615234375, 1.9518156051635742, 1.7637758255004883, 1.6774193048477173, 1.4270943403244019, 1.5327303409576416]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0865185260772705, 3.8644495010375977, 3.0406768321990967, 2.665297508239746, 3.4855055809020996, 2.0958855152130127, 3.6632559299468994, 3.166874647140503, 3.8273515701293945, 3.386234998703003, 3.1917903423309326, 3.935382127761841, 2.1467883586883545, 3.9679300785064697, 3.1636650562286377, 2.808574914932251]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.394784450531006, 2.369231939315796, 2.8513543605804443, 2.4352357387542725, 2.806955337524414, 2.0697686672210693, 1.5841081142425537, 2.0906617641448975, 2.6391139030456543, 1.9907586574554443, 1.3519126176834106, 1.942444920539856, 2.4303390979766846, 2.674971342086792, 2.6420226097106934, 1.9997477531433105]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.576080322265625, 3.4167346954345703, 3.059281587600708, 3.118636131286621, 3.0451767444610596, 2.8483734130859375, 2.785771131515503, 2.0549423694610596, 3.2801759243011475, 2.9488542079925537, 2.7738754749298096, 1.3785748481750488, 2.921553611755371, 2.7685439586639404, 3.039569139480591, 2.0692126750946045]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9246132373809814, 1.8490511178970337, 2.6418046951293945, 2.596856117248535, 2.6307356357574463, 1.9932833909988403, 1.7723482847213745, 3.0925180912017822, 0.5707883238792419, 2.1702969074249268, 0.9848752617835999, 2.4542722702026367, 2.752061605453491, 2.9298973083496094, 2.9612832069396973, 1.4305846691131592]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.690342426300049, 3.9541425704956055, 5.5575852394104, 4.946963310241699, 5.515820503234863, 5.676542282104492, 5.129835605621338, 5.117229461669922, 6.154755115509033, 5.045737266540527, 5.767582416534424, 6.0746331214904785, 4.830168724060059, 5.398825645446777, 4.309105396270752, 4.957338333129883]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.2106781005859375, 4.558747291564941, 5.548550605773926, 5.1703619956970215, 3.7052531242370605, 4.4676103591918945, 4.875304222106934, 5.29209566116333, 5.36288595199585, 4.915144920349121, 4.2708516120910645, 3.3511312007904053, 4.410782814025879, 3.3886780738830566, 4.918703079223633, 4.495796203613281]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.880787372589111, 5.8422932624816895, 6.036319732666016, 5.627391338348389, 6.076428413391113, 4.817983150482178, 5.6895928382873535, 5.637872695922852, 5.300501346588135, 5.359836578369141, 6.2155985832214355, 5.903071880340576, 4.9525017738342285, 5.8604512214660645, 6.045022487640381, 5.866502285003662]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.995079517364502, 2.97206449508667, 2.8103654384613037, 2.2421233654022217, -0.44213664531707764, 3.03997802734375, 2.7206499576568604, 1.8551957607269287, 2.7176389694213867, 3.4681761264801025, 2.687425136566162, 3.2448267936706543, 2.85797119140625, 2.947421073913574, 2.328660488128662, 1.7377301454544067]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.552976608276367, 6.821779727935791, 6.537350177764893, 6.308069229125977, 6.46388053894043, 6.481282711029053, 6.747780799865723, 6.6389689445495605, 6.643319129943848, 6.330693244934082, 6.927949905395508, 6.260303497314453, 5.282742023468018, 7.205960750579834, 6.487910270690918, 6.441377639770508]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  56%|███████████████████████████                     | 113/200 [03:57<02:43,  1.88s/it]Layer: gate_22 - Captured router_logits: [5.010759353637695, 5.351792812347412, 4.961103916168213, 4.050381660461426, 4.901461124420166, 5.249592304229736, 4.6309990882873535, 4.781742572784424, 5.074610233306885, 4.846588134765625, 5.270034313201904, 4.875436782836914, 5.1452531814575195, 5.434293270111084, 4.600683212280273, 5.476376533508301]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.148215293884277, 4.667983531951904, 3.3396217823028564, 4.1973161697387695, 4.457900524139404, 4.079533576965332, 4.244442939758301, 4.234053134918213, 4.363463401794434, 4.4716691970825195, 4.057214736938477, 4.429702281951904, 3.6744656562805176, 2.0252952575683594, 3.379509449005127, 4.019888877868652]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.28830099105835, 6.238936424255371, 5.914334297180176, 6.085328102111816, 5.429383754730225, 5.853194713592529, 6.179013252258301, 5.7055983543396, 5.348424911499023, 5.7566962242126465, 6.437595844268799, 5.523778915405273, 5.771487712860107, 6.212450981140137, 6.229435920715332, 5.7869062423706055]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8377459049224854, 3.6714870929718018, 3.7851953506469727, 3.9484951496124268, 2.688201427459717, 3.8629488945007324, 3.9415993690490723, 3.657449722290039, 3.99306321144104, 3.6155054569244385, 3.698923349380493, 3.622814893722534, 3.6650640964508057, 3.6008388996124268, 3.706747531890869, 3.8970015048980713]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.452451229095459, 3.5288286209106445, 3.406376600265503, 3.5566890239715576, 3.3856029510498047, 3.624647617340088, 3.5762135982513428, 3.3673906326293945, 3.603048801422119, 3.2981486320495605, 3.601562023162842, 3.4485974311828613, 3.436985969543457, 2.7263295650482178, 3.651745319366455, 3.5686662197113037]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.184324264526367, 3.210660934448242, 2.984926700592041, 2.8973915576934814, 2.819941997528076, 3.087646007537842, 3.1842448711395264, 3.2763233184814453, 3.034105062484741, 3.006377935409546, 2.288097381591797, 2.999131202697754, 3.1583786010742188, 3.063775062561035, 3.082508087158203, 2.887415647506714]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.373818874359131, 4.4904561042785645, 4.566047191619873, 4.372636318206787, 4.491470813751221, 4.464613437652588, 4.217842102050781, 4.70920467376709, 4.493361473083496, 4.457917213439941, 4.469935417175293, 4.054091930389404, 4.197774410247803, 4.0566606521606445, 4.57460880279541, 4.374395847320557]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.581830024719238, 8.90941047668457, 8.788556098937988, 8.471546173095703, 8.555935859680176, 8.26993465423584, 8.63788890838623, 8.54155445098877, 8.627418518066406, 8.493352890014648, 8.598231315612793, 8.232683181762695, 8.335126876831055, 9.000589370727539, 8.899940490722656, 8.508563995361328]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.891964435577393, 5.433932304382324, 5.389035224914551, 4.793397426605225, 5.139918804168701, 5.091647148132324, 4.6740827560424805, 5.1373724937438965, 5.242281913757324, 4.930846691131592, 4.839582443237305, 4.735078811645508, 4.868094444274902, 4.98368501663208, 5.070647716522217, 4.835626602172852]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2346670627593994, 2.9997079372406006, 3.1036758422851562, 2.762861728668213, 3.0255024433135986, 3.3113527297973633, 2.755117654800415, 3.1785261631011963, 3.0873773097991943, 3.1841344833374023, 3.1752991676330566, 3.26358699798584, 3.034808397293091, 2.981077194213867, 3.122364044189453, 3.178844690322876]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11890440434217453, 0.1343887597322464, 0.12722887098789215, -0.23002079129219055, -0.23396733403205872, -0.1016489788889885, 0.14191260933876038, -0.17701801657676697, 0.10266552120447159, 0.11386549472808838, 0.11345279961824417, 0.07468868046998978, 0.10665851831436157, 0.12446761131286621, -1.0906304121017456, 0.13526803255081177]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08786527812480927, 0.05453653633594513, 0.04317210987210274, 0.06183682754635811, 0.08463408797979355, 0.037944354116916656, 0.05366397649049759, 0.07729627937078476, 0.019507819786667824, 0.06144406646490097, -0.17438340187072754, 0.05879117548465729, 0.0011793997837230563, -0.0007729008793830872, 0.016048861667513847, 0.028794217854738235]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07058602571487427, 0.04433511570096016, 0.09167882055044174, 0.05230866000056267, 0.09753911197185516, 0.10401712357997894, 0.0563184879720211, -0.13953396677970886, 0.06227680668234825, 0.08414276689291, -0.0020825890824198723, 0.08009506016969681, -0.20533885061740875, -0.005303120706230402, -0.05548664554953575, 0.12440147250890732]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.2151980996131897, 0.1498267948627472, 0.126189723610878, 0.14828330278396606, 0.04521889239549637, 0.05489962548017502, -0.08401288092136383, 0.17884089052677155, 0.15194521844387054, -0.43310877680778503, 0.02752763032913208, 0.16200441122055054, 0.08804246783256531, -0.2747132480144501, -0.08425895869731903, -0.04052675887942314]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0756683349609375, -0.032401055097579956, -0.0649903416633606, 0.13616825640201569, -0.07078579813241959, -0.14895117282867432, 0.09242026507854462, 0.049737561494112015, 0.06769907474517822, 0.20077744126319885, -0.26925042271614075, 0.028386404737830162, 0.007363391108810902, -0.17071014642715454, 0.0768798440694809, 0.02378559485077858]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.2117847204208374, 0.2402586191892624, 0.14136886596679688, 0.051263585686683655, -0.3027604818344116, -0.055792298167943954, 0.09225654602050781, 0.005354949273169041, 0.16194814443588257, -0.05927307531237602, -0.5053120255470276, 0.05051451548933983, -0.3346266746520996, 0.25460249185562134, 0.15314602851867676, 0.1546834260225296]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3130400776863098, 0.3343579173088074, 0.19189123809337616, 0.26819515228271484, 0.2670591473579407, 0.24022309482097626, 0.1283453404903412, 0.15375863015651703, -0.38805559277534485, 0.39033013582229614, -0.06968055665493011, 0.3161225914955139, 0.09237636625766754, -0.06753149628639221, 0.2777424156665802, 0.16979801654815674]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.053714148700237274, 0.20052608847618103, 0.1195472925901413, 0.16006061434745789, -0.35444971919059753, 0.2030261754989624, 0.05857262760400772, 0.21456016600131989, -0.03218427300453186, -0.16931873559951782, 0.06482553482055664, -0.06634864956140518, 0.17699328064918518, 0.10604365915060043, -0.019155709072947502, 0.14775888621807098]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21077384054660797, 0.28218504786491394, -0.08585670590400696, 0.32599616050720215, 0.45812439918518066, 0.39868247509002686, 0.11558955162763596, -0.00818550679832697, 0.6168395280838013, -0.19650022685527802, 0.25649020075798035, 0.05310690402984619, 0.6696821451187134, -0.19786116480827332, -0.023603899404406548, 0.19142401218414307]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.865734338760376, 0.757335901260376, 0.618218719959259, 0.5686178803443909, 0.844683825969696, 0.4464036226272583, 1.1090642213821411, 1.0671154260635376, 0.09388689696788788, 0.034928686916828156, 0.4392993748188019, 0.7901845574378967, 0.7499814033508301, 0.8318570852279663, 1.118128776550293, 0.7911302447319031]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1539299488067627, 1.1702289581298828, 0.6607761979103088, 1.062294363975525, 0.9585431218147278, 0.37859347462654114, 0.9892064332962036, 1.4223724603652954, 0.7799997329711914, 0.2928052842617035, 1.0998499393463135, 1.1096407175064087, 0.6412596106529236, 1.1002081632614136, 0.49501243233680725, 1.0108200311660767]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2450952529907227, 1.601136326789856, 1.5311927795410156, 1.4090625047683716, 1.426720380783081, 0.850210964679718, 1.7488853931427002, 1.2338873147964478, 1.4830212593078613, 1.448553442955017, 1.2648565769195557, 1.1748532056808472, 1.1564863920211792, 1.3431590795516968, 1.220622181892395, 1.2038519382476807]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.056357502937317, 1.5115506649017334, 1.647581696510315, 1.7940022945404053, 1.2660140991210938, 1.589271068572998, 1.288360595703125, 1.5233147144317627, 1.3574920892715454, 0.6397998929023743, 1.7001875638961792, 1.9516797065734863, 1.7647322416305542, 1.6777875423431396, 1.427655816078186, 1.5333234071731567]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0871493816375732, 3.8630776405334473, 3.0413010120391846, 2.666613817214966, 3.4860033988952637, 2.097538948059082, 3.662954092025757, 3.165055513381958, 3.827335834503174, 3.3865132331848145, 3.1914572715759277, 3.9359939098358154, 2.147780656814575, 3.968651056289673, 3.163348913192749, 2.809521198272705]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.39473819732666, 2.3691213130950928, 2.8513941764831543, 2.435304880142212, 2.8064465522766113, 2.0701935291290283, 1.5854707956314087, 2.089495897293091, 2.6387805938720703, 1.9919991493225098, 1.352282166481018, 1.9435549974441528, 2.4312782287597656, 2.6748464107513428, 2.642158031463623, 2.0002849102020264]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5767452716827393, 3.4173214435577393, 3.0594091415405273, 3.1189165115356445, 3.045206308364868, 2.848768472671509, 2.7850003242492676, 2.0541701316833496, 3.280735969543457, 2.9489994049072266, 2.7749624252319336, 1.379123568534851, 2.920987606048584, 2.7688727378845215, 3.039647102355957, 2.0698182582855225]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9259475469589233, 1.8493448495864868, 2.6425349712371826, 2.597252607345581, 2.631695032119751, 1.9945839643478394, 1.772338628768921, 3.092851400375366, 0.5710379481315613, 2.170522689819336, 0.9848587512969971, 2.454606533050537, 2.752816915512085, 2.9303500652313232, 2.962010383605957, 1.4312341213226318]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6913013458251953, 3.9540610313415527, 5.557767391204834, 4.94762659072876, 5.516010761260986, 5.677004814147949, 5.130637168884277, 5.117910385131836, 6.155351638793945, 5.045926094055176, 5.768198490142822, 6.075250148773193, 4.8296589851379395, 5.398385524749756, 4.309752941131592, 4.957764148712158]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.210732936859131, 4.558633327484131, 5.547562122344971, 5.17027473449707, 3.7052013874053955, 4.467628002166748, 4.875608444213867, 5.291316986083984, 5.362613201141357, 4.914997577667236, 4.271468639373779, 3.3514533042907715, 4.411043167114258, 3.3891475200653076, 4.918819904327393, 4.496048927307129]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.882250785827637, 5.84387731552124, 6.038100242614746, 5.628396034240723, 6.07777738571167, 4.819540023803711, 5.691075325012207, 5.639232158660889, 5.301290512084961, 5.361209392547607, 6.217242240905762, 5.904934883117676, 4.954828262329102, 5.862014293670654, 6.046151638031006, 5.867847442626953]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.996140956878662, 2.972193717956543, 2.8108270168304443, 2.241847038269043, -0.4423955976963043, 3.0398285388946533, 2.7207858562469482, 1.8553895950317383, 2.717632293701172, 3.46824049949646, 2.6876003742218018, 3.24532413482666, 2.8578271865844727, 2.9474008083343506, 2.3282978534698486, 1.7379661798477173]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.554119110107422, 6.823080062866211, 6.53815221786499, 6.309265613555908, 6.465084552764893, 6.481966972351074, 6.748815536499023, 6.640626430511475, 6.644200325012207, 6.33197021484375, 6.929069995880127, 6.261775970458984, 5.284084320068359, 7.207277774810791, 6.489216327667236, 6.442378520965576]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.011992931365967, 5.35306453704834, 4.962000846862793, 4.051494598388672, 4.902692794799805, 5.250848770141602, 4.632021427154541, 4.783393383026123, 5.075921535491943, 4.847773551940918, 5.271627426147461, 4.876798152923584, 5.146653175354004, 5.435242176055908, 4.602193355560303, 5.477353096008301]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.149210453033447, 4.668336391448975, 3.3403587341308594, 4.197654724121094, 4.459168910980225, 4.079908847808838, 4.244788646697998, 4.234411239624023, 4.3640594482421875, 4.472568035125732, 4.05779504776001, 4.430126190185547, 3.674915313720703, 2.0260818004608154, 3.3798961639404297, 4.020371913909912]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.2893195152282715, 6.239533424377441, 5.91507625579834, 6.086120128631592, 5.430148124694824, 5.853982925415039, 6.179740905761719, 5.706360340118408, 5.3489670753479, 5.757143020629883, 6.43803596496582, 5.524408340454102, 5.772249698638916, 6.2132110595703125, 6.230293273925781, 5.78750467300415]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.83782696723938, 3.6715099811553955, 3.7850935459136963, 3.9486920833587646, 2.688425064086914, 3.8633530139923096, 3.941662311553955, 3.657561779022217, 3.9932098388671875, 3.615449905395508, 3.698906898498535, 3.623016834259033, 3.6650125980377197, 3.6009037494659424, 3.7072174549102783, 3.8969879150390625]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  58%|████████████████████████████                    | 117/200 [04:04<02:33,  1.85s/it]Layer: gate_26 - Captured router_logits: [3.452349901199341, 3.5289204120635986, 3.4060442447662354, 3.556411027908325, 3.385352849960327, 3.624621629714966, 3.576042890548706, 3.3673365116119385, 3.6030094623565674, 3.297987937927246, 3.601077079772949, 3.448310136795044, 3.4367282390594482, 2.725996971130371, 3.6515872478485107, 3.5687484741210938]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.183835744857788, 3.210289478302002, 2.984365224838257, 2.8969080448150635, 2.81938099861145, 3.0871198177337646, 3.1838290691375732, 3.2757346630096436, 3.0335216522216797, 3.0058934688568115, 2.2880172729492188, 2.998518705368042, 3.157855272293091, 3.0630640983581543, 3.081620216369629, 2.886852979660034]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.3724684715271, 4.489254951477051, 4.564342498779297, 4.371068000793457, 4.489931583404541, 4.463260173797607, 4.216507434844971, 4.707932949066162, 4.4920549392700195, 4.456243515014648, 4.468250751495361, 4.0527544021606445, 4.196224212646484, 4.055183410644531, 4.57291316986084, 4.372847080230713]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.578672409057617, 8.906801223754883, 8.785350799560547, 8.468311309814453, 8.55256462097168, 8.26681900024414, 8.634937286376953, 8.538634300231934, 8.624351501464844, 8.490082740783691, 8.59499740600586, 8.23013973236084, 8.33273696899414, 8.997179985046387, 8.89684772491455, 8.505683898925781]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.891207695007324, 5.433712482452393, 5.388364315032959, 4.793290138244629, 5.138810634613037, 5.089927673339844, 4.6739044189453125, 5.136791706085205, 5.241281032562256, 4.930111885070801, 4.8388848304748535, 4.734919548034668, 4.8675737380981445, 4.982241153717041, 5.06998348236084, 4.834894180297852]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2344954013824463, 2.9992520809173584, 3.1038496494293213, 2.762402296066284, 3.0257177352905273, 3.311340570449829, 2.754589319229126, 3.17804217338562, 3.0871620178222656, 3.1838765144348145, 3.174881935119629, 3.2639153003692627, 3.034419298171997, 2.980586528778076, 3.121973752975464, 3.178274631500244]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11723709851503372, 0.13206295669078827, 0.1265123188495636, -0.24656422436237335, -0.24916863441467285, -0.08984142541885376, 0.13971786201000214, -0.154275581240654, 0.09978686273097992, 0.11204475164413452, 0.11144819855690002, 0.07058286666870117, 0.10496903955936432, 0.12326844036579132, -1.1120381355285645, 0.1335223913192749]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08448785543441772, 0.04786157235503197, 0.0391443595290184, 0.0604499988257885, 0.08101657778024673, 0.03466980904340744, 0.05222012847661972, 0.08059275150299072, 0.019115719944238663, 0.06044755131006241, -0.18253563344478607, 0.05928075686097145, 0.0025512075517326593, -0.00512561434879899, 0.02424447424709797, 0.026903029531240463]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06835764646530151, 0.043390195816755295, 0.09050234407186508, 0.05504024401307106, 0.09440164268016815, 0.09833046048879623, 0.05493210628628731, -0.14495731890201569, 0.06136132776737213, 0.08790609240531921, 0.0076149762608110905, 0.07744605094194412, -0.22203388810157776, -0.010163561441004276, -0.06316007673740387, 0.12247400730848312]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21604710817337036, 0.14511679112911224, 0.12273655831813812, 0.14430765807628632, 0.04368167370557785, 0.05695277079939842, -0.09910056740045547, 0.17665810883045197, 0.15507851541042328, -0.45873546600341797, 0.030332673341035843, 0.15751060843467712, 0.10386199504137039, -0.29741987586021423, -0.07461957633495331, -0.048572126775979996]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07766874879598618, -0.030750589445233345, -0.061686377972364426, 0.15044733881950378, -0.06684790551662445, -0.1517031490802765, 0.09120046347379684, 0.047089122235774994, 0.06547745317220688, 0.20487868785858154, -0.28958821296691895, 0.026398999616503716, 0.014795169234275818, -0.18668778240680695, 0.08114422112703323, 0.018415095284581184]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21059204638004303, 0.2368701994419098, 0.13978168368339539, 0.05355450510978699, -0.30619916319847107, -0.06466611474752426, 0.09886196255683899, 0.005381095223128796, 0.16304749250411987, -0.06308404356241226, -0.49210324883461, 0.046915072947740555, -0.34673741459846497, 0.2565641403198242, 0.15582381188869476, 0.14859600365161896]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3141244351863861, 0.33082371950149536, 0.18867217004299164, 0.26661762595176697, 0.2633039355278015, 0.24592845141887665, 0.1215711161494255, 0.15202367305755615, -0.3918900191783905, 0.38934072852134705, -0.08329959213733673, 0.3184601664543152, 0.0902833491563797, -0.0742650032043457, 0.2861306071281433, 0.17477895319461823]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05014289170503616, 0.199518084526062, 0.1173216849565506, 0.15946191549301147, -0.3620424270629883, 0.2054223269224167, 0.056770458817481995, 0.20601677894592285, -0.025964368134737015, -0.17011456191539764, 0.06120365858078003, -0.06696794927120209, 0.17219042778015137, 0.10775481909513474, -0.025257840752601624, 0.14865513145923615]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20947760343551636, 0.2786363363265991, -0.08964166045188904, 0.32248178124427795, 0.4634324312210083, 0.39783522486686707, 0.11451190710067749, -0.006521831266582012, 0.6166717410087585, -0.19466181099414825, 0.2549748420715332, 0.05655394494533539, 0.6720435619354248, -0.20806659758090973, -0.029442280530929565, 0.19003741443157196]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8624445199966431, 0.7477005124092102, 0.6168709397315979, 0.5755226612091064, 0.8414211273193359, 0.44034790992736816, 1.105350136756897, 1.0663257837295532, 0.09145178645849228, 0.023369068279862404, 0.42894527316093445, 0.7907111048698425, 0.7471598386764526, 0.8181912899017334, 1.1188515424728394, 0.7900610566139221]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.158714771270752, 1.1728147268295288, 0.6559600234031677, 1.0594557523727417, 0.950961172580719, 0.37722644209861755, 0.9885343909263611, 1.4222280979156494, 0.7662815451622009, 0.28575950860977173, 1.0995579957962036, 1.1119493246078491, 0.6283169984817505, 1.0966858863830566, 0.49882322549819946, 1.0034379959106445]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2403628826141357, 1.601626992225647, 1.531050205230713, 1.405945062637329, 1.4298738241195679, 0.8410573601722717, 1.7502609491348267, 1.2255887985229492, 1.4851346015930176, 1.4540679454803467, 1.260416030883789, 1.1669827699661255, 1.1481515169143677, 1.330687403678894, 1.205145001411438, 1.1971439123153687]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0476481914520264, 1.506217122077942, 1.6510381698608398, 1.7947403192520142, 1.252524733543396, 1.5856103897094727, 1.2877005338668823, 1.525667428970337, 1.3501801490783691, 0.6413333415985107, 1.70209801197052, 1.9519046545028687, 1.7591263055801392, 1.6779683828353882, 1.4240264892578125, 1.5330859422683716]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.081300735473633, 3.8639543056488037, 3.0394930839538574, 2.6538045406341553, 3.4885544776916504, 2.094393014907837, 3.6612257957458496, 3.167490243911743, 3.827392101287842, 3.3850760459899902, 3.19419527053833, 3.934617757797241, 2.1371920108795166, 3.957227945327759, 3.1595804691314697, 2.799152374267578]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3929874897003174, 2.3677875995635986, 2.8489277362823486, 2.4286725521087646, 2.803628444671631, 2.075537919998169, 1.5741618871688843, 2.099745750427246, 2.6369597911834717, 1.989094853401184, 1.3451764583587646, 1.9419984817504883, 2.4329493045806885, 2.675114631652832, 2.6427721977233887, 1.9966046810150146]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.572532892227173, 3.4150099754333496, 3.0565037727355957, 3.121305465698242, 3.039384603500366, 2.8465230464935303, 2.791182279586792, 2.049823045730591, 3.2814955711364746, 2.9471898078918457, 2.7695868015289307, 1.372283697128296, 2.922614812850952, 2.7669718265533447, 3.0375142097473145, 2.071925401687622]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9235655069351196, 1.8375270366668701, 2.641017436981201, 2.5966365337371826, 2.6307427883148193, 1.9916627407073975, 1.774435043334961, 3.093010663986206, 0.5631483197212219, 2.1740565299987793, 0.9783470034599304, 2.450504779815674, 2.7512428760528564, 2.9282379150390625, 2.9567956924438477, 1.429796814918518]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6728320121765137, 3.9462392330169678, 5.550333023071289, 4.938005447387695, 5.510943412780762, 5.668090343475342, 5.1264166831970215, 5.109071254730225, 6.149714469909668, 5.035711288452148, 5.765021800994873, 6.0677008628845215, 4.816927433013916, 5.394348621368408, 4.298445701599121, 4.950901031494141]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.207217216491699, 4.551116466522217, 5.544280529022217, 5.169704437255859, 3.697624444961548, 4.4582061767578125, 4.8729095458984375, 5.287084102630615, 5.364858627319336, 4.910798072814941, 4.264736652374268, 3.3427140712738037, 4.402825832366943, 3.3799004554748535, 4.914635181427002, 4.49124813079834]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.872513294219971, 5.836843967437744, 6.0336995124816895, 5.621402740478516, 6.072391033172607, 4.807590961456299, 5.6807966232299805, 5.6297760009765625, 5.298985958099365, 5.352609634399414, 6.213194370269775, 5.900537490844727, 4.944540500640869, 5.8556928634643555, 6.040612697601318, 5.86232852935791]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.9996376037597656, 2.9750101566314697, 2.8135900497436523, 2.2424631118774414, -0.44969624280929565, 3.047085762023926, 2.7216014862060547, 1.8564127683639526, 2.7176523208618164, 3.4719398021698, 2.687809705734253, 3.2511985301971436, 2.8621749877929688, 2.952972888946533, 2.328908920288086, 1.739227294921875]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.547255039215088, 6.81819486618042, 6.532583236694336, 6.300941467285156, 6.45719575881958, 6.479099750518799, 6.742328643798828, 6.6375885009765625, 6.638265132904053, 6.325319290161133, 6.92385196685791, 6.254417896270752, 5.275857448577881, 7.200902462005615, 6.481471538543701, 6.4365763664245605]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.010143756866455, 5.353941917419434, 4.963111400604248, 4.0459418296813965, 4.900154113769531, 5.250091075897217, 4.629587650299072, 4.7816033363342285, 5.072187900543213, 4.8482441902160645, 5.2729597091674805, 4.876564979553223, 5.145823001861572, 5.435617923736572, 4.5988030433654785, 5.477536678314209]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.153188705444336, 4.6720194816589355, 3.339902877807617, 4.200888633728027, 4.460010051727295, 4.0819783210754395, 4.24634313583374, 4.2345476150512695, 4.365409851074219, 4.476876735687256, 4.060728073120117, 4.435540199279785, 3.676358461380005, 2.023921251296997, 3.3806495666503906, 4.023016452789307]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.286290645599365, 6.237458229064941, 5.910930156707764, 6.0809431076049805, 5.428167343139648, 5.848362922668457, 6.176689624786377, 5.702676773071289, 5.343286037445068, 5.7530741691589355, 6.435314178466797, 5.521904945373535, 5.768311500549316, 6.210280895233154, 6.226818084716797, 5.783152103424072]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8365278244018555, 3.672691822052002, 3.7846672534942627, 3.9478235244750977, 2.6866917610168457, 3.862753391265869, 3.940186023712158, 3.6582164764404297, 3.9921703338623047, 3.614193916320801, 3.697913885116577, 3.622746229171753, 3.6641685962677, 3.5977284908294678, 3.7083675861358643, 3.8953089714050293]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4531736373901367, 3.530524969100952, 3.4059717655181885, 3.557387351989746, 3.3872294425964355, 3.626052141189575, 3.5761218070983887, 3.368180513381958, 3.603940010070801, 3.3001177310943604, 3.606090784072876, 3.4495537281036377, 3.4383463859558105, 2.728339195251465, 3.6524345874786377, 3.5698111057281494]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1848466396331787, 3.2116336822509766, 2.9859225749969482, 2.898261785507202, 2.819948434829712, 3.0883867740631104, 3.1856493949890137, 3.277467966079712, 3.034156084060669, 3.0078511238098145, 2.2884700298309326, 2.9995977878570557, 3.159430980682373, 3.066225290298462, 3.0840771198272705, 2.8878514766693115]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.373446464538574, 4.490403652191162, 4.565374374389648, 4.371984481811523, 4.4938483238220215, 4.462929725646973, 4.217545032501221, 4.707184791564941, 4.4922990798950195, 4.455926418304443, 4.469526290893555, 4.053155899047852, 4.197394847869873, 4.054823398590088, 4.5759735107421875, 4.373993396759033]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.582254409790039, 8.909430503845215, 8.785646438598633, 8.468415260314941, 8.555362701416016, 8.267753601074219, 8.637401580810547, 8.542993545532227, 8.627071380615234, 8.491150856018066, 8.59975528717041, 8.233912467956543, 8.336960792541504, 9.00119686126709, 8.8998441696167, 8.505303382873535]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  60%|█████████████████████████████                   | 121/200 [04:12<02:26,  1.86s/it]Layer: gate_30 - Captured router_logits: [4.886838436126709, 5.428493976593018, 5.384507656097412, 4.785378456115723, 5.136533260345459, 5.087642192840576, 4.666348934173584, 5.1319684982299805, 5.237754821777344, 4.925384521484375, 4.833416938781738, 4.7270636558532715, 4.862371921539307, 4.981460094451904, 5.065155982971191, 4.830791473388672]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2309820652008057, 2.999593496322632, 3.102660655975342, 2.765406370162964, 3.0235841274261475, 3.3098788261413574, 2.7567648887634277, 3.17716383934021, 3.0853564739227295, 3.1817715167999268, 3.174879789352417, 3.2591562271118164, 3.032996416091919, 2.979703903198242, 3.122929334640503, 3.1755764484405518]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11805198341608047, 0.1331687718629837, 0.12749408185482025, -0.24926228821277618, -0.2543160319328308, -0.08931726217269897, 0.1399182826280594, -0.1682438999414444, 0.09661248326301575, 0.11312849819660187, 0.11229047179222107, 0.06998804211616516, 0.10476546734571457, 0.1249334067106247, -1.1202131509780884, 0.13487450778484344]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08533424884080887, 0.048024971038103104, 0.039350610226392746, 0.0638338103890419, 0.08234670758247375, 0.03602292761206627, 0.05070764198899269, 0.07790496200323105, 0.017814088612794876, 0.060769982635974884, -0.17993229627609253, 0.06032203137874603, 0.0025737457908689976, -0.0025050523690879345, 0.021583721041679382, 0.025946257635951042]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06853346526622772, 0.044379010796546936, 0.08989431709051132, 0.05377808213233948, 0.09407544136047363, 0.09959682077169418, 0.05276167020201683, -0.14463584125041962, 0.06516347825527191, 0.08359456807374954, 0.0038561851251870394, 0.07630855590105057, -0.2177519053220749, -0.006585036404430866, -0.05837077647447586, 0.12318705767393112]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.2146589756011963, 0.14656418561935425, 0.12738707661628723, 0.1424599587917328, 0.04452477768063545, 0.053697019815444946, -0.09483018517494202, 0.17684268951416016, 0.15401071310043335, -0.45749178528785706, 0.038654789328575134, 0.1594538539648056, 0.09444566816091537, -0.29383304715156555, -0.07165946066379547, -0.04365958645939827]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08005165308713913, -0.029760664328932762, -0.06777478754520416, 0.1483655422925949, -0.058080531656742096, -0.15137287974357605, 0.08852840960025787, 0.04604126140475273, 0.0666680857539177, 0.20403744280338287, -0.28739604353904724, 0.025662602856755257, 0.012645140290260315, -0.18085455894470215, 0.08434788882732391, 0.01800123229622841]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.20941630005836487, 0.2371189445257187, 0.1427737921476364, 0.05398418381810188, -0.3052879869937897, -0.06340602785348892, 0.09456862509250641, 0.00574919069185853, 0.1622801125049591, -0.059973299503326416, -0.4974091053009033, 0.047788653522729874, -0.3439946174621582, 0.25775668025016785, 0.15384462475776672, 0.1521487534046173]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.31492045521736145, 0.3318505585193634, 0.19371196627616882, 0.26566752791404724, 0.2651030719280243, 0.24341972172260284, 0.12149924039840698, 0.15073415637016296, -0.3877384066581726, 0.39333122968673706, -0.08239804953336716, 0.31719353795051575, 0.08961527794599533, -0.07427850365638733, 0.28283074498176575, 0.1715179681777954]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05059519410133362, 0.19922946393489838, 0.11753901094198227, 0.15997031331062317, -0.35803699493408203, 0.20352093875408173, 0.05877836048603058, 0.20824947953224182, -0.026011327281594276, -0.16860733926296234, 0.060261018574237823, -0.0655738040804863, 0.1724282056093216, 0.11067550629377365, -0.025445744395256042, 0.14768856763839722]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2085508406162262, 0.27920427918434143, -0.09224817901849747, 0.3233630955219269, 0.46230146288871765, 0.3990755081176758, 0.11515964567661285, -0.005872408859431744, 0.6172789931297302, -0.19354550540447235, 0.2584157884120941, 0.053377315402030945, 0.6735767722129822, -0.20864179730415344, -0.03016708232462406, 0.19208835065364838]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8629335761070251, 0.749495267868042, 0.618152379989624, 0.574080228805542, 0.8409368395805359, 0.4406813979148865, 1.1054972410202026, 1.0672191381454468, 0.09160691499710083, 0.0269803274422884, 0.42812681198120117, 0.7915161848068237, 0.7446093559265137, 0.8180829286575317, 1.1159402132034302, 0.791914701461792]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1571334600448608, 1.1731230020523071, 0.6573123931884766, 1.059578776359558, 0.9519323706626892, 0.372926265001297, 0.9896541833877563, 1.4233336448669434, 0.7669079899787903, 0.28167617321014404, 1.1013946533203125, 1.108725666999817, 0.6297211647033691, 1.0966449975967407, 0.5001097321510315, 1.0024229288101196]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.243113398551941, 1.6000481843948364, 1.5322651863098145, 1.405065655708313, 1.428309679031372, 0.843764066696167, 1.7478653192520142, 1.224023699760437, 1.4852815866470337, 1.4547470808029175, 1.2606892585754395, 1.1685266494750977, 1.1478999853134155, 1.334672451019287, 1.204168438911438, 1.196684718132019]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0459332466125488, 1.5075035095214844, 1.6511552333831787, 1.793472409248352, 1.2513713836669922, 1.5859522819519043, 1.2880691289901733, 1.5247893333435059, 1.3518314361572266, 0.641659677028656, 1.7040648460388184, 1.9537327289581299, 1.7591073513031006, 1.6798535585403442, 1.4262278079986572, 1.5360132455825806]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.079397678375244, 3.8648149967193604, 3.044416666030884, 2.654167413711548, 3.492267608642578, 2.0924808979034424, 3.6623220443725586, 3.167438268661499, 3.8278539180755615, 3.3889572620391846, 3.1943233013153076, 3.936300754547119, 2.137582540512085, 3.957488775253296, 3.161358118057251, 2.8003110885620117]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3974509239196777, 2.371211051940918, 2.852313995361328, 2.4323654174804688, 2.8072609901428223, 2.077928066253662, 1.5747053623199463, 2.1029365062713623, 2.6386899948120117, 1.9942327737808228, 1.3445243835449219, 1.9445993900299072, 2.4379119873046875, 2.681547164916992, 2.6442596912384033, 1.9990893602371216]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.570765972137451, 3.413217782974243, 3.0560102462768555, 3.120345115661621, 3.0353567600250244, 2.8463385105133057, 2.7919695377349854, 2.0516746044158936, 3.2805886268615723, 2.949237823486328, 2.7691428661346436, 1.3730064630508423, 2.9229896068573, 2.768326997756958, 3.0360729694366455, 2.069851875305176]
Running loglikelihood requests:  62%|██████████████████████████████                  | 125/200 [04:19<02:18,  1.85s/it]Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9271808862686157, 1.8385294675827026, 2.644336462020874, 2.5984816551208496, 2.631983757019043, 1.9921904802322388, 1.7747578620910645, 3.090301036834717, 0.5619556307792664, 2.175492286682129, 0.9765464663505554, 2.451566219329834, 2.752718448638916, 2.928095817565918, 2.961923360824585, 1.4290223121643066]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6794164180755615, 3.951629877090454, 5.558465480804443, 4.942353248596191, 5.51939058303833, 5.677585124969482, 5.13587760925293, 5.11735200881958, 6.158524036407471, 5.037093639373779, 5.772496700286865, 6.077056407928467, 4.830023288726807, 5.3997697830200195, 4.303964614868164, 4.955357551574707]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.21349573135376, 4.559962272644043, 5.547062397003174, 5.174921035766602, 3.701725959777832, 4.459909439086914, 4.880195140838623, 5.2906012535095215, 5.368491172790527, 4.91636848449707, 4.271961688995361, 3.3475894927978516, 4.407907962799072, 3.384706735610962, 4.91994047164917, 4.497073173522949]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.881681442260742, 5.844282627105713, 6.041922092437744, 5.629302978515625, 6.079257488250732, 4.8164381980896, 5.6897783279418945, 5.638465881347656, 5.305428981781006, 5.3648681640625, 6.220062255859375, 5.908111095428467, 4.955466270446777, 5.863242149353027, 6.048476219177246, 5.869988441467285]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.0034444332122803, 2.9768385887145996, 2.8158645629882812, 2.245770215988159, -0.45018383860588074, 3.0502471923828125, 2.725161075592041, 1.8594937324523926, 2.721503734588623, 3.474276065826416, 2.691857099533081, 3.2528345584869385, 2.8645567893981934, 2.953143835067749, 2.3297009468078613, 1.7405829429626465]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.559122562408447, 6.829892635345459, 6.5442094802856445, 6.3133111000061035, 6.471755504608154, 6.488950729370117, 6.753203868865967, 6.6467084884643555, 6.649043560028076, 6.337192535400391, 6.9351887702941895, 6.268345355987549, 5.28700590133667, 7.21303129196167, 6.494451999664307, 6.447070121765137]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.015965938568115, 5.3596296310424805, 4.9669647216796875, 4.051781177520752, 4.907092571258545, 5.256098747253418, 4.6359734535217285, 4.786465644836426, 5.078700542449951, 4.852837085723877, 5.278633117675781, 4.883756160736084, 5.151308059692383, 5.439420700073242, 4.604226112365723, 5.481579780578613]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.155061721801758, 4.673567295074463, 3.3405697345733643, 4.202704906463623, 4.461966037750244, 4.084135055541992, 4.246912479400635, 4.236609935760498, 4.368027210235596, 4.478385925292969, 4.062285900115967, 4.434778690338135, 3.679198741912842, 2.026595115661621, 3.381441593170166, 4.024636268615723]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.2941765785217285, 6.243869304656982, 5.9167609214782715, 6.088984489440918, 5.434142589569092, 5.856229782104492, 6.18228006362915, 5.707565784454346, 5.350581645965576, 5.758769512176514, 6.4434919357299805, 5.527487754821777, 5.775787353515625, 6.217134475708008, 6.232146263122559, 5.79063081741333]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.841928720474243, 3.6773831844329834, 3.7887659072875977, 3.9540162086486816, 2.6910667419433594, 3.86737060546875, 3.9461143016815186, 3.6633989810943604, 3.997361183166504, 3.6186892986297607, 3.703211545944214, 3.628042459487915, 3.6685750484466553, 3.603978395462036, 3.712986946105957, 3.901254177093506]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.456817150115967, 3.5334458351135254, 3.410015821456909, 3.561102867126465, 3.3912017345428467, 3.6296181678771973, 3.5787391662597656, 3.3722903728485107, 3.6076366901397705, 3.3029425144195557, 3.6081554889678955, 3.452617883682251, 3.440356969833374, 2.730968952178955, 3.6566154956817627, 3.5737593173980713]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1876304149627686, 3.2136499881744385, 2.987961769104004, 2.901918411254883, 2.822890281677246, 3.0903353691101074, 3.1877939701080322, 3.279978036880493, 3.0362439155578613, 3.0100467205047607, 2.2909481525421143, 3.0026278495788574, 3.1621768474578857, 3.0682125091552734, 3.0840442180633545, 2.8913347721099854]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.375515460968018, 4.4924635887146, 4.567483425140381, 4.3745951652526855, 4.4942307472229, 4.466638565063477, 4.220857620239258, 4.7114386558532715, 4.494898319244385, 4.459010601043701, 4.470852375030518, 4.055481910705566, 4.198464393615723, 4.0573554039001465, 4.577417850494385, 4.375278472900391]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.586901664733887, 8.91549301147461, 8.790806770324707, 8.473899841308594, 8.55846118927002, 8.27043628692627, 8.641783714294434, 8.543824195861816, 8.631402015686035, 8.495491981506348, 8.603515625, 8.237709999084473, 8.342108726501465, 9.004733085632324, 8.904046058654785, 8.512372970581055]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.892185688018799, 5.4340667724609375, 5.389465808868408, 4.792019367218018, 5.139595985412598, 5.0905046463012695, 4.6724467277526855, 5.137094497680664, 5.242140769958496, 4.930359363555908, 4.837954044342041, 4.732208251953125, 4.867591381072998, 4.983346939086914, 5.070634841918945, 4.836472988128662]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2336044311523438, 3.0015013217926025, 3.1048595905303955, 2.7615814208984375, 3.026262044906616, 3.3120813369750977, 2.754246711730957, 3.1770756244659424, 3.086374044418335, 3.1831581592559814, 3.176349401473999, 3.2620763778686523, 3.034247398376465, 2.981096029281616, 3.122814416885376, 3.1767005920410156]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.1200297623872757, 0.13462431728839874, 0.12903663516044617, -0.25294819474220276, -0.25394323468208313, -0.09154786169528961, 0.1416654735803604, -0.1688505858182907, 0.09925349801778793, 0.11438575387001038, 0.11454351246356964, 0.07157638669013977, 0.10582564026117325, 0.12628011405467987, -1.128512978553772, 0.13641446828842163]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08664454519748688, 0.04962752386927605, 0.04098907485604286, 0.06455712020397186, 0.0836322158575058, 0.03648605942726135, 0.05354852229356766, 0.07989580184221268, 0.018415987491607666, 0.06156951189041138, -0.1853843480348587, 0.061217475682497025, 0.0015577381709590554, -0.0024726339615881443, 0.02281750738620758, 0.025465307757258415]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07054158300161362, 0.045685186982154846, 0.09055806696414948, 0.05356207117438316, 0.09323766827583313, 0.09979581832885742, 0.05299386754631996, -0.14421720802783966, 0.06447439640760422, 0.08792034536600113, 0.0030084860045462847, 0.07655788958072662, -0.2213585525751114, -0.006105007138103247, -0.06321316957473755, 0.12453215569257736]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21570049226284027, 0.1479817032814026, 0.12774114310741425, 0.14509965479373932, 0.045378684997558594, 0.053985271602869034, -0.09850243479013443, 0.17745444178581238, 0.15450920164585114, -0.45954039692878723, 0.03572013974189758, 0.1600208580493927, 0.09637460857629776, -0.2986955940723419, -0.07091163098812103, -0.04922961816191673]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07827460765838623, -0.028297461569309235, -0.06773250550031662, 0.1493634283542633, -0.06233552098274231, -0.15103988349437714, 0.08760077506303787, 0.0471816249191761, 0.06410033255815506, 0.20540264248847961, -0.2887538969516754, 0.026343926787376404, 0.012789388187229633, -0.18352752923965454, 0.0832667425274849, 0.018504509702324867]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.2100992053747177, 0.23750202357769012, 0.14125977456569672, 0.0546957328915596, -0.3073788583278656, -0.06575126200914383, 0.09538350254297256, 0.005405012983828783, 0.16387730836868286, -0.061233632266521454, -0.49949750304222107, 0.047995127737522125, -0.34489893913269043, 0.25826695561408997, 0.15406669676303864, 0.15222389996051788]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3157823085784912, 0.33105167746543884, 0.19189615547657013, 0.2651885151863098, 0.2653406262397766, 0.24588514864444733, 0.121161550283432, 0.15244640409946442, -0.3885166645050049, 0.39123815298080444, -0.0865907222032547, 0.31876903772354126, 0.09185265749692917, -0.07590410858392715, 0.2836330533027649, 0.17211483418941498]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.04990530014038086, 0.20007842779159546, 0.11924579739570618, 0.16041319072246552, -0.3623482286930084, 0.20490430295467377, 0.05822576954960823, 0.20705370604991913, -0.02462911605834961, -0.16923227906227112, 0.059583649039268494, -0.06678501516580582, 0.17120660841464996, 0.11098286509513855, -0.02782815881073475, 0.14665168523788452]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2102425992488861, 0.28130003809928894, -0.09117115288972855, 0.32353445887565613, 0.4638029634952545, 0.39890551567077637, 0.114934042096138, -0.006218938622623682, 0.6172968149185181, -0.1924636960029602, 0.25656139850616455, 0.051925476640462875, 0.6733304858207703, -0.2104739397764206, -0.03183027356863022, 0.19122937321662903]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8629598617553711, 0.7483348250389099, 0.6188614368438721, 0.5761940479278564, 0.8414892554283142, 0.43956366181373596, 1.1075968742370605, 1.067283272743225, 0.09280819445848465, 0.024772537872195244, 0.428055077791214, 0.7928813695907593, 0.7467822432518005, 0.8185210824012756, 1.1181578636169434, 0.7923097610473633]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1587457656860352, 1.1743170022964478, 0.6577281355857849, 1.061118245124817, 0.9537222385406494, 0.3729621171951294, 0.9909242391586304, 1.4240847826004028, 0.7666992545127869, 0.28380003571510315, 1.102503776550293, 1.11086905002594, 0.6277976036071777, 1.0969196557998657, 0.5029386878013611, 1.0031861066818237]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2443969249725342, 1.6023191213607788, 1.534835934638977, 1.4049406051635742, 1.430238962173462, 0.8427366614341736, 1.7526260614395142, 1.224080204963684, 1.4890433549880981, 1.4584519863128662, 1.263548731803894, 1.1691445112228394, 1.1487208604812622, 1.3331096172332764, 1.2024483680725098, 1.1986889839172363]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0444252490997314, 1.5081106424331665, 1.6532678604125977, 1.795265793800354, 1.2484575510025024, 1.5854038000106812, 1.2881618738174438, 1.5247727632522583, 1.3504148721694946, 0.6384721994400024, 1.7027603387832642, 1.9538604021072388, 1.759893536567688, 1.683192491531372, 1.4260956048965454, 1.5355796813964844]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.080936908721924, 3.8656983375549316, 3.0436296463012695, 2.654425621032715, 3.4931137561798096, 2.0914254188537598, 3.6638739109039307, 3.171889305114746, 3.8289222717285156, 3.3898708820343018, 3.1948444843292236, 3.9365744590759277, 2.1346981525421143, 3.958376169204712, 3.161940574645996, 2.799730062484741]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3972175121307373, 2.3692593574523926, 2.850994825363159, 2.4327876567840576, 2.8071892261505127, 2.0769333839416504, 1.5713410377502441, 2.1033029556274414, 2.6389780044555664, 1.994128942489624, 1.3462083339691162, 1.9426367282867432, 2.436833620071411, 2.6809632778167725, 2.644059181213379, 1.9986811876296997]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5684218406677246, 3.4127745628356934, 3.0553741455078125, 3.1213419437408447, 3.0353920459747314, 2.8453593254089355, 2.789325714111328, 2.0519957542419434, 3.2814269065856934, 2.948065996170044, 2.768552541732788, 1.3681931495666504, 2.9214117527008057, 2.766012191772461, 3.035395860671997, 2.069082498550415]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9252926111221313, 1.8363378047943115, 2.6428029537200928, 2.598341703414917, 2.6317837238311768, 1.990777611732483, 1.7737798690795898, 3.0891330242156982, 0.5571972727775574, 2.175809144973755, 0.9744040369987488, 2.450529098510742, 2.7523484230041504, 2.9270682334899902, 2.960820198059082, 1.4277708530426025]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6789865493774414, 3.949040174484253, 5.558787822723389, 4.941077709197998, 5.518552780151367, 5.675999641418457, 5.136270523071289, 5.116311550140381, 6.159080505371094, 5.034502029418945, 5.7718729972839355, 6.075634956359863, 4.831107139587402, 5.401847839355469, 4.302908897399902, 4.95395565032959]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.210572242736816, 4.557718276977539, 5.544363021850586, 5.174371242523193, 3.6980392932891846, 4.45730447769165, 4.878625869750977, 5.288404941558838, 5.366097927093506, 4.914181232452393, 4.268928050994873, 3.344972610473633, 4.406506538391113, 3.3833487033843994, 4.9182024002075195, 4.494710445404053]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.879978179931641, 5.8431501388549805, 6.039944648742676, 5.627549648284912, 6.07774543762207, 4.814321994781494, 5.687539100646973, 5.637157440185547, 5.3041911125183105, 5.363841533660889, 6.219221591949463, 5.906991004943848, 4.953097820281982, 5.861425399780273, 6.046960353851318, 5.868557929992676]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Running loglikelihood requests:  64%|██████████████████████████████▉                 | 129/200 [04:26<02:09,  1.83s/it]Layer: gate_20 - Captured router_logits: [3.0013315677642822, 2.975674867630005, 2.8143792152404785, 2.2441837787628174, -0.45248693227767944, 3.048692464828491, 2.7231857776641846, 1.858277678489685, 2.719978094100952, 3.4726030826568604, 2.691047430038452, 3.2524197101593018, 2.8623528480529785, 2.9510016441345215, 2.3277289867401123, 1.7396677732467651]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.556805610656738, 6.828187942504883, 6.542236804962158, 6.311422824859619, 6.46953821182251, 6.487028121948242, 6.751028537750244, 6.645158290863037, 6.646377086639404, 6.334840297698975, 6.933515548706055, 6.267642498016357, 5.284379005432129, 7.211249351501465, 6.492228984832764, 6.444930076599121]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.014167785644531, 5.357791900634766, 4.964068412780762, 4.049797058105469, 4.90565299987793, 5.254464626312256, 4.634241104125977, 4.786099910736084, 5.0768232345581055, 4.850053310394287, 5.277669429779053, 4.882203578948975, 5.14939022064209, 5.4370903968811035, 4.602062225341797, 5.479635715484619]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.152962684631348, 4.67169189453125, 3.338388681411743, 4.20090913772583, 4.460725784301758, 4.082172870635986, 4.245237350463867, 4.234774112701416, 4.365840435028076, 4.476508617401123, 4.059919834136963, 4.43264102935791, 3.677426815032959, 2.024115800857544, 3.378208637237549, 4.022768974304199]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.29298210144043, 6.242532730102539, 5.9150800704956055, 6.08791446685791, 5.433364391326904, 5.854347229003906, 6.180627822875977, 5.706183910369873, 5.349687576293945, 5.757541656494141, 6.443267822265625, 5.525397777557373, 5.7756428718566895, 6.216091632843018, 6.231024742126465, 5.789644718170166]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8398678302764893, 3.6753180027008057, 3.787074565887451, 3.9525392055511475, 2.6891188621520996, 3.866567850112915, 3.9442496299743652, 3.661755084991455, 3.995605945587158, 3.6172969341278076, 3.702077627182007, 3.6262705326080322, 3.6665544509887695, 3.6024861335754395, 3.7114861011505127, 3.8993706703186035]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.456258773803711, 3.531785249710083, 3.408726692199707, 3.5594727993011475, 3.3893160820007324, 3.6290225982666016, 3.57769513130188, 3.3710925579071045, 3.606255531311035, 3.3019700050354004, 3.607123374938965, 3.451301336288452, 3.439070224761963, 2.729231834411621, 3.6558244228363037, 3.573103427886963]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.187389850616455, 3.212679386138916, 2.9871487617492676, 2.9012789726257324, 2.822082996368408, 3.0897998809814453, 3.186748743057251, 3.278737783432007, 3.035522222518921, 3.0087764263153076, 2.28985595703125, 3.0013773441314697, 3.1619746685028076, 3.0673317909240723, 3.0829734802246094, 2.8900864124298096]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.374856948852539, 4.491392135620117, 4.566864013671875, 4.373569011688232, 4.4933881759643555, 4.466014385223389, 4.2198944091796875, 4.710501670837402, 4.494312286376953, 4.457977771759033, 4.469574928283691, 4.054139614105225, 4.197394371032715, 4.055808067321777, 4.576007843017578, 4.3739333152771]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.587120056152344, 8.915760040283203, 8.789759635925293, 8.475711822509766, 8.55872631072998, 8.271324157714844, 8.642314910888672, 8.544164657592773, 8.631367683410645, 8.49638843536377, 8.604562759399414, 8.237298965454102, 8.342491149902344, 9.005241394042969, 8.905085563659668, 8.513686180114746]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.891251087188721, 5.433104515075684, 5.389166355133057, 4.791104793548584, 5.13845157623291, 5.088785171508789, 4.672099590301514, 5.137053966522217, 5.241250038146973, 4.929555892944336, 4.837393760681152, 4.732027530670166, 4.867099761962891, 4.983335971832275, 5.069670677185059, 4.834924221038818]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.233621120452881, 3.000837564468384, 3.104858160018921, 2.7616770267486572, 3.0263590812683105, 3.31109356880188, 2.7545862197875977, 3.1765475273132324, 3.086561441421509, 3.1829216480255127, 3.1764743328094482, 3.262052536010742, 3.0342516899108887, 2.981156587600708, 3.122915744781494, 3.1764843463897705]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12140771746635437, 0.13607753813266754, 0.1295377016067505, -0.2504306137561798, -0.2500615119934082, -0.09885844588279724, 0.14337259531021118, -0.16588228940963745, 0.10042835772037506, 0.11600934714078903, 0.11576522141695023, 0.07187533378601074, 0.10657413303852081, 0.1266988217830658, -1.1255775690078735, 0.13778115808963776]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08830089122056961, 0.04937686771154404, 0.04136152192950249, 0.062276531010866165, 0.08414872735738754, 0.03701009228825569, 0.05303409695625305, 0.08028767257928848, 0.01790662296116352, 0.06220763549208641, -0.18580462038516998, 0.05902206152677536, 0.004334789235144854, -0.002810451667755842, 0.021088091656565666, 0.02457490935921669]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06971395015716553, 0.04413602128624916, 0.09069295972585678, 0.05558302626013756, 0.09266939759254456, 0.1020750179886818, 0.05806056410074234, -0.14722558856010437, 0.06338335573673248, 0.08597253262996674, 0.00283768936060369, 0.07717273384332657, -0.21883508563041687, -0.007669141981750727, -0.06050548702478409, 0.12463374435901642]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21596403419971466, 0.14894166588783264, 0.12572266161441803, 0.14656126499176025, 0.04481992870569229, 0.05539983510971069, -0.09682454913854599, 0.18208447098731995, 0.1556341052055359, -0.4598971903324127, 0.02777835540473461, 0.1606506109237671, 0.09427258372306824, -0.2887411415576935, -0.07951772212982178, -0.0505552738904953]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07816116511821747, -0.02948400378227234, -0.0645570233464241, 0.14589662849903107, -0.06639956682920456, -0.15174289047718048, 0.08980605006217957, 0.04790016636252403, 0.06954000145196915, 0.206607386469841, -0.28487688302993774, 0.02798505499958992, 0.011664176359772682, -0.18750129640102386, 0.0809919610619545, 0.01878191903233528]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21006454527378082, 0.2365948110818863, 0.14026741683483124, 0.052781134843826294, -0.3046593964099884, -0.06475022435188293, 0.0964418351650238, 0.006042069755494595, 0.16206751763820648, -0.06272966414690018, -0.4942668676376343, 0.047389574348926544, -0.3432537317276001, 0.2560604214668274, 0.1550561636686325, 0.15017418563365936]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3136177361011505, 0.33254584670066833, 0.18978852033615112, 0.2665643095970154, 0.26429468393325806, 0.24373239278793335, 0.12195119261741638, 0.154609814286232, -0.3887781500816345, 0.3899777829647064, -0.08250980824232101, 0.31904155015945435, 0.0901627168059349, -0.07183713465929031, 0.2841302752494812, 0.1723797619342804]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05070656165480614, 0.19992944598197937, 0.11874938011169434, 0.1594662368297577, -0.36305710673332214, 0.20490753650665283, 0.0568961575627327, 0.20716656744480133, -0.0268297977745533, -0.17055460810661316, 0.06290595233440399, -0.06714818626642227, 0.17189115285873413, 0.10753662139177322, -0.0229537021368742, 0.14882367849349976]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20867489278316498, 0.280230313539505, -0.08824046701192856, 0.32240185141563416, 0.46207743883132935, 0.39900490641593933, 0.11480019241571426, -0.004935207311064005, 0.6154959797859192, -0.19432522356510162, 0.25407129526138306, 0.05437179282307625, 0.6712985634803772, -0.20651613175868988, -0.02839239127933979, 0.18988613784313202]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8634205460548401, 0.7488218545913696, 0.6180949807167053, 0.5738906264305115, 0.8446106910705566, 0.4419376254081726, 1.107258915901184, 1.0657614469528198, 0.09264636039733887, 0.024678360670804977, 0.43091318011283875, 0.7920133471488953, 0.7486340403556824, 0.8191862106323242, 1.1170130968093872, 0.790971577167511]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1581716537475586, 1.1738288402557373, 0.6586136221885681, 1.0615249872207642, 0.9534186720848083, 0.37549951672554016, 0.990126371383667, 1.4222898483276367, 0.7699030637741089, 0.2854476273059845, 1.100576639175415, 1.1117123365402222, 0.6320674419403076, 1.097425937652588, 0.49901819229125977, 1.0051647424697876]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2419637441635132, 1.6015346050262451, 1.5311237573623657, 1.4047150611877441, 1.4296013116836548, 0.8438133597373962, 1.7496883869171143, 1.2273201942443848, 1.4849611520767212, 1.454119324684143, 1.2637821435928345, 1.1697407960891724, 1.1499419212341309, 1.3330787420272827, 1.2086676359176636, 1.1994115114212036]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0502071380615234, 1.5089880228042603, 1.6515235900878906, 1.7957252264022827, 1.2524601221084595, 1.587381362915039, 1.2865856885910034, 1.5252705812454224, 1.3539201021194458, 0.6422054171562195, 1.7014210224151611, 1.951887607574463, 1.7603213787078857, 1.6802399158477783, 1.4272537231445312, 1.5350582599639893]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.083697557449341, 3.8632309436798096, 3.0399749279022217, 2.6580348014831543, 3.4887261390686035, 2.096794605255127, 3.662288188934326, 3.165698289871216, 3.8276493549346924, 3.386711597442627, 3.1934471130371094, 3.9366838932037354, 2.1392712593078613, 3.95908784866333, 3.158824920654297, 2.8035058975219727]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3945353031158447, 2.368771553039551, 2.84950852394104, 2.4309401512145996, 2.8040242195129395, 2.076917886734009, 1.5726972818374634, 2.0976338386535645, 2.6376192569732666, 1.9920393228530884, 1.3480950593948364, 1.9450628757476807, 2.4333972930908203, 2.6767609119415283, 2.64324951171875, 1.99958336353302]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5743439197540283, 3.4156041145324707, 3.0573630332946777, 3.1204416751861572, 3.0415914058685303, 2.8476390838623047, 2.7895283699035645, 2.050710916519165, 3.281709671020508, 2.9480576515197754, 2.771496295928955, 1.3717401027679443, 2.9218692779541016, 2.767930030822754, 3.03730845451355, 2.07255482673645]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9246374368667603, 1.8371143341064453, 2.6411242485046387, 2.5972776412963867, 2.6322402954101562, 1.9921010732650757, 1.7738803625106812, 3.090444803237915, 0.561013400554657, 2.172700881958008, 0.9789181351661682, 2.4515957832336426, 2.752568483352661, 2.92807674407959, 2.9581634998321533, 1.4284881353378296]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.678131103515625, 3.947295904159546, 5.554444313049316, 4.940677642822266, 5.514350891113281, 5.670845031738281, 5.129549026489258, 5.113521099090576, 6.1543426513671875, 5.035447120666504, 5.768167018890381, 6.0727972984313965, 4.822778701782227, 5.396803379058838, 4.302114963531494, 4.95407772064209]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.2080769538879395, 4.554977893829346, 5.543354511260986, 5.170492172241211, 3.6980490684509277, 4.4602885246276855, 4.873787879943848, 5.287503719329834, 5.3651509284973145, 4.911386489868164, 4.268138885498047, 3.344329357147217, 4.404576778411865, 3.3820629119873047, 4.915621280670166, 4.492246627807617]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.876677989959717, 5.839712619781494, 6.037509918212891, 5.624205112457275, 6.075494766235352, 4.8126373291015625, 5.686246871948242, 5.634552478790283, 5.301537990570068, 5.357285499572754, 6.2166361808776855, 5.904093265533447, 4.949431896209717, 5.859237194061279, 6.043859004974365, 5.866082668304443]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [2.999829053878784, 2.974066972732544, 2.8127851486206055, 2.242182731628418, -0.45137354731559753, 3.0459113121032715, 2.722428321838379, 1.8570506572723389, 2.71692156791687, 3.4713480472564697, 2.6890580654144287, 3.250297784805298, 2.861478805541992, 2.951728582382202, 2.3279497623443604, 1.7397408485412598]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.550390243530273, 6.821563720703125, 6.536211013793945, 6.304272651672363, 6.461731910705566, 6.481374263763428, 6.745153427124023, 6.640209674835205, 6.640847206115723, 6.3290324211120605, 6.927175521850586, 6.257654190063477, 5.2788567543029785, 7.204884052276611, 6.48513126373291, 6.439127445220947]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.011874675750732, 5.355702877044678, 4.964022159576416, 4.0478997230529785, 4.902761936187744, 5.2522993087768555, 4.631717205047607, 4.784422874450684, 5.0746965408325195, 4.849453926086426, 5.274703502655029, 4.878203392028809, 5.147571086883545, 5.43649959564209, 4.601107597351074, 5.478472709655762]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.152781009674072, 4.6716837882995605, 3.339271068572998, 4.20082950592041, 4.4607462882995605, 4.0816731452941895, 4.246109962463379, 4.2345476150512695, 4.365690231323242, 4.475128173828125, 4.060333728790283, 4.43332576751709, 3.6763744354248047, 2.023998737335205, 3.3797504901885986, 4.0230817794799805]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  66%|███████████████████████████████▉                | 133/200 [04:33<02:01,  1.81s/it]Layer: gate_24 - Captured router_logits: [6.28767204284668, 6.238349437713623, 5.9122843742370605, 6.083456516265869, 5.429222106933594, 5.850508689880371, 6.1770243644714355, 5.7039408683776855, 5.345202922821045, 5.753586769104004, 6.437404632568359, 5.522489070892334, 5.769923686981201, 6.21178674697876, 6.227865219116211, 5.7850022315979]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8385097980499268, 3.674196720123291, 3.785210609436035, 3.9503839015960693, 2.6879279613494873, 3.8650715351104736, 3.94266414642334, 3.660335063934326, 3.9939920902252197, 3.6161015033721924, 3.7000226974487305, 3.6250627040863037, 3.6660308837890625, 3.6003527641296387, 3.7097830772399902, 3.8977091312408447]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4534850120544434, 3.530946969985962, 3.406468629837036, 3.557758092880249, 3.3877201080322266, 3.6267685890197754, 3.576477527618408, 3.36840558052063, 3.604360580444336, 3.3012139797210693, 3.606597900390625, 3.4496703147888184, 3.4384353160858154, 2.727670907974243, 3.6530473232269287, 3.570464849472046]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.185835599899292, 3.212029457092285, 2.9863314628601074, 2.898946762084961, 2.8205251693725586, 3.0888185501098633, 3.1862051486968994, 3.277735948562622, 3.0345075130462646, 3.008155584335327, 2.288846015930176, 2.999877691268921, 3.1602976322174072, 3.067066192626953, 3.0834295749664307, 2.889033555984497]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.373323917388916, 4.490177154541016, 4.56492805480957, 4.371066093444824, 4.492604732513428, 4.463277816772461, 4.217035293579102, 4.707825660705566, 4.492367744445801, 4.455723285675049, 4.468580722808838, 4.052809238433838, 4.196244239807129, 4.054487228393555, 4.575558185577393, 4.373071670532227]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.581995964050293, 8.91049861907959, 8.786588668823242, 8.469281196594238, 8.554779052734375, 8.267478942871094, 8.637727737426758, 8.54205322265625, 8.626431465148926, 8.491357803344727, 8.599365234375, 8.232972145080566, 8.337752342224121, 9.001625061035156, 8.900542259216309, 8.50622844696045]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.888433456420898, 5.429858207702637, 5.38591194152832, 4.786994457244873, 5.136406898498535, 5.086764812469482, 4.668861389160156, 5.133411884307861, 5.238735198974609, 4.926255226135254, 4.834053993225098, 4.728179454803467, 4.863466739654541, 4.9807610511779785, 5.066538333892822, 4.831777572631836]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2321929931640625, 3.000325918197632, 3.104008436203003, 2.7633771896362305, 3.0251598358154297, 3.3111164569854736, 2.7558467388153076, 3.1766037940979004, 3.085641622543335, 3.182101249694824, 3.1753909587860107, 3.2602977752685547, 3.0336670875549316, 2.9800314903259277, 3.123883008956909, 3.1758100986480713]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.1177561804652214, 0.13246677815914154, 0.12679646909236908, -0.24601203203201294, -0.24831447005271912, -0.08830269426107407, 0.14006100594997406, -0.1579941064119339, 0.1000915989279747, 0.11189551651477814, 0.11166908591985703, 0.07235457003116608, 0.10574542731046677, 0.12349767982959747, -1.1139951944351196, 0.13403832912445068]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08526932448148727, 0.04857926815748215, 0.04094742238521576, 0.06104351207613945, 0.08314993977546692, 0.035733506083488464, 0.05305781587958336, 0.08126165717840195, 0.016461903229355812, 0.06200065463781357, -0.18265759944915771, 0.05908751115202904, 0.003315029200166464, -0.004166424740105867, 0.02403523400425911, 0.02601814828813076]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06793149560689926, 0.04371441900730133, 0.09098062664270401, 0.0557977631688118, 0.09350863844156265, 0.10110887140035629, 0.05398596450686455, -0.14490735530853271, 0.06504885852336884, 0.09046877175569534, 0.004143205936998129, 0.07789986580610275, -0.2241218388080597, -0.008949130773544312, -0.06386789679527283, 0.12407782673835754]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21537438035011292, 0.14559490978717804, 0.12409044802188873, 0.14469049870967865, 0.04459020122885704, 0.05582841858267784, -0.09824655205011368, 0.17881783843040466, 0.15431241691112518, -0.46095144748687744, 0.03273695707321167, 0.1583368182182312, 0.09913647919893265, -0.298029363155365, -0.07225353270769119, -0.047830455005168915]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07627420127391815, -0.030499842017889023, -0.06182296201586723, 0.14989537000656128, -0.06531719863414764, -0.15486930310726166, 0.08851102739572525, 0.04844820499420166, 0.0659252405166626, 0.20216357707977295, -0.286447137594223, 0.02924034371972084, 0.009388141334056854, -0.1848868727684021, 0.08283083140850067, 0.018117420375347137]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21085284650325775, 0.2366642951965332, 0.13979530334472656, 0.051713328808546066, -0.30563053488731384, -0.06508053094148636, 0.09910132735967636, 0.005089540965855122, 0.16261087357997894, -0.06431685388088226, -0.4919012784957886, 0.047596581280231476, -0.3449374735355377, 0.2565317153930664, 0.15596900880336761, 0.1497289389371872]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.315446138381958, 0.3312855362892151, 0.19009774923324585, 0.2679485082626343, 0.26485615968704224, 0.2455998808145523, 0.12309671193361282, 0.15333686769008636, -0.39068323373794556, 0.39097273349761963, -0.08344320207834244, 0.3197365999221802, 0.09280526638031006, -0.07523488998413086, 0.2851867079734802, 0.17389898002147675]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05022091045975685, 0.19924527406692505, 0.11820484697818756, 0.16059403121471405, -0.36351674795150757, 0.20685900747776031, 0.05783025175333023, 0.20742471516132355, -0.025792153552174568, -0.16999787092208862, 0.06075568497180939, -0.06628681719303131, 0.17085587978363037, 0.10962720215320587, -0.02475154586136341, 0.14665791392326355]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2099137008190155, 0.279217004776001, -0.09006769955158234, 0.3237234354019165, 0.46407175064086914, 0.39864417910575867, 0.11522550135850906, -0.005429917946457863, 0.6162753701210022, -0.19540424644947052, 0.25561395287513733, 0.05656306818127632, 0.6729865670204163, -0.2073071300983429, -0.03063909150660038, 0.19010740518569946]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.863267183303833, 0.7484356164932251, 0.6172164082527161, 0.5745865106582642, 0.8430731892585754, 0.44056057929992676, 1.1072593927383423, 1.0672247409820557, 0.09234150499105453, 0.023557564243674278, 0.42875009775161743, 0.7911551594734192, 0.7495259642601013, 0.8187212944030762, 1.1188057661056519, 0.7915617227554321]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1609176397323608, 1.1743769645690918, 0.658149778842926, 1.0627540349960327, 0.953437328338623, 0.37570247054100037, 0.9897139668464661, 1.4240849018096924, 0.7674216032028198, 0.2862064242362976, 1.1014736890792847, 1.113146185874939, 0.631742537021637, 1.0973405838012695, 0.4990021586418152, 1.0041298866271973]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2423416376113892, 1.6014361381530762, 1.5330405235290527, 1.404408574104309, 1.4323580265045166, 0.8424343466758728, 1.7491928339004517, 1.2266989946365356, 1.4856599569320679, 1.4544540643692017, 1.2639539241790771, 1.1677719354629517, 1.1494176387786865, 1.3316594362258911, 1.2064095735549927, 1.198426604270935]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.048424482345581, 1.5086919069290161, 1.6517585515975952, 1.7958993911743164, 1.2518714666366577, 1.5872269868850708, 1.2865040302276611, 1.5244545936584473, 1.3502556085586548, 0.6431487798690796, 1.7049200534820557, 1.9534215927124023, 1.760819911956787, 1.6806563138961792, 1.4265908002853394, 1.534217119216919]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0840914249420166, 3.8661794662475586, 3.0433921813964844, 2.6566061973571777, 3.4912068843841553, 2.096055030822754, 3.6638071537017822, 3.1656272411346436, 3.828782558441162, 3.3879611492156982, 3.195338010787964, 3.9390177726745605, 2.138960838317871, 3.960019111633301, 3.1617112159729004, 2.8021175861358643]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.394434928894043, 2.3678054809570312, 2.851433753967285, 2.431488037109375, 2.804729700088501, 2.07647967338562, 1.5717147588729858, 2.0954389572143555, 2.6379218101501465, 1.9903018474578857, 1.3446986675262451, 1.9424999952316284, 2.433797597885132, 2.676400899887085, 2.6434853076934814, 1.9966349601745605]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.574064254760742, 3.417015552520752, 3.057644844055176, 3.1225943565368652, 3.0412769317626953, 2.8482744693756104, 2.7917253971099854, 2.0495989322662354, 3.2825002670288086, 2.949693441390991, 2.772040367126465, 1.3718345165252686, 2.9248831272125244, 2.768404960632324, 3.0395193099975586, 2.0718753337860107]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9230406284332275, 1.8384817838668823, 2.6411783695220947, 2.597801923751831, 2.631732940673828, 1.9913803339004517, 1.7726906538009644, 3.0918753147125244, 0.5594965815544128, 2.171755313873291, 0.975598156452179, 2.4499638080596924, 2.752065420150757, 2.928802490234375, 2.9586963653564453, 1.4271920919418335]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6766278743743896, 3.9476065635681152, 5.5538506507873535, 4.939854145050049, 5.5162577629089355, 5.67166805267334, 5.128629207611084, 5.113138675689697, 6.1533942222595215, 5.0354461669921875, 5.768441200256348, 6.072795391082764, 4.819645404815674, 5.397154331207275, 4.301640033721924, 4.95435094833374]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.208146095275879, 4.552561283111572, 5.5441741943359375, 5.170929431915283, 3.696531057357788, 4.458744525909424, 4.873799800872803, 5.287068843841553, 5.3655924797058105, 4.911124229431152, 4.2657976150512695, 3.3419535160064697, 4.404013633728027, 3.3815462589263916, 4.914061546325684, 4.491147518157959]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.876163005828857, 5.839641094207764, 6.036921977996826, 5.624897003173828, 6.07619571685791, 4.809776306152344, 5.6848368644714355, 5.633037090301514, 5.300414085388184, 5.356712818145752, 6.217227458953857, 5.904598712921143, 4.948747634887695, 5.858865737915039, 6.044400691986084, 5.865583896636963]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.0001652240753174, 2.9757802486419678, 2.8140785694122314, 2.242339849472046, -0.45337364077568054, 3.049006462097168, 2.7229080200195312, 1.8563780784606934, 2.7188284397125244, 3.4732449054718018, 2.6894967555999756, 3.251966714859009, 2.861640214920044, 2.953131675720215, 2.327817916870117, 1.7394429445266724]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.5506062507629395, 6.822012424468994, 6.536341667175293, 6.303776264190674, 6.461889266967773, 6.482578277587891, 6.745965480804443, 6.641509056091309, 6.641707420349121, 6.328430652618408, 6.927771091461182, 6.258141040802002, 5.2773566246032715, 7.205334186553955, 6.48504114151001, 6.440317630767822]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.011654376983643, 5.355993270874023, 4.964370250701904, 4.046502113342285, 4.901968002319336, 5.251932621002197, 4.630629539489746, 4.782864570617676, 5.0742034912109375, 4.849000453948975, 5.274847507476807, 4.877710342407227, 5.1475138664245605, 5.436365604400635, 4.599929332733154, 5.478263854980469]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.153406143188477, 4.671513557434082, 3.338305950164795, 4.200540065765381, 4.461030006408691, 4.08175802230835, 4.246234893798828, 4.234489440917969, 4.365440845489502, 4.477017879486084, 4.0601606369018555, 4.435313701629639, 3.675663948059082, 2.0220165252685547, 3.379171371459961, 4.022481441497803]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.2876152992248535, 6.238569736480713, 5.911463737487793, 6.0817766189575195, 5.428840160369873, 5.8488640785217285, 6.177085876464844, 5.702682018280029, 5.343301296234131, 5.752951622009277, 6.43658447265625, 5.521041393280029, 5.7698187828063965, 6.211813449859619, 6.227024555206299, 5.783366680145264]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8373522758483887, 3.6736602783203125, 3.7854747772216797, 3.94946551322937, 2.686058521270752, 3.863947629928589, 3.9420597553253174, 3.659787893295288, 3.993598222732544, 3.6150736808776855, 3.699268341064453, 3.6239588260650635, 3.6651768684387207, 3.5990047454833984, 3.7096145153045654, 3.8964734077453613]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.454003095626831, 3.5316598415374756, 3.4067115783691406, 3.557969808578491, 3.3881938457489014, 3.627777099609375, 3.577096700668335, 3.368993043899536, 3.604998826980591, 3.300652503967285, 3.6064603328704834, 3.449981927871704, 3.438607931137085, 2.7283987998962402, 3.653358221054077, 3.571582078933716]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1856603622436523, 3.2118172645568848, 2.986149787902832, 2.89860200881958, 2.8201494216918945, 3.088184356689453, 3.185920238494873, 3.277618408203125, 3.034245491027832, 3.007498264312744, 2.2882959842681885, 2.999422311782837, 3.1602494716644287, 3.065467357635498, 3.083293914794922, 2.8884105682373047]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  68%|████████████████████████████████▉               | 137/200 [04:41<01:54,  1.81s/it]Layer: gate_28 - Captured router_logits: [4.3730902671813965, 4.490314483642578, 4.564637660980225, 4.371010780334473, 4.492493152618408, 4.4634270668029785, 4.217357158660889, 4.707800388336182, 4.492292881011963, 4.45572566986084, 4.46892786026001, 4.052262783050537, 4.197176456451416, 4.054283142089844, 4.575359344482422, 4.373083114624023]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.582777976989746, 8.911297798156738, 8.78677749633789, 8.469834327697754, 8.556317329406738, 8.268266677856445, 8.63845157623291, 8.54194450378418, 8.627219200134277, 8.49156665802002, 8.600334167480469, 8.234357833862305, 8.33768367767334, 9.001376152038574, 8.901602745056152, 8.506529808044434]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.888884544372559, 5.431488037109375, 5.386751174926758, 4.787994861602783, 5.137333393096924, 5.088898181915283, 4.6692399978637695, 5.13563346862793, 5.238954067230225, 4.927239418029785, 4.834960460662842, 4.7286858558654785, 4.86403751373291, 4.9820556640625, 5.067732334136963, 4.8321146965026855]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.232767343521118, 3.000694513320923, 3.1049091815948486, 2.7653326988220215, 3.0259084701538086, 3.3113603591918945, 2.757310152053833, 3.1775641441345215, 3.086679697036743, 3.183171510696411, 3.1767566204071045, 3.260972023010254, 3.0345609188079834, 2.980638027191162, 3.123800754547119, 3.1766486167907715]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12229957431554794, 0.13653364777565002, 0.12986868619918823, -0.2367975264787674, -0.23329880833625793, -0.10289648920297623, 0.14408619701862335, -0.18105033040046692, 0.10057157278060913, 0.11586160957813263, 0.11606993526220322, 0.07056999206542969, 0.10719532519578934, 0.12639252841472626, -1.0851905345916748, 0.13949300348758698]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08818675577640533, 0.050987642258405685, 0.040753111243247986, 0.06338385492563248, 0.08400540053844452, 0.04147714748978615, 0.05111350491642952, 0.07633185386657715, 0.015051078982651234, 0.06188769266009331, -0.17786742746829987, 0.058986712247133255, 0.0030226942617446184, -0.0013791221426799893, 0.017496569082140923, 0.024775808677077293]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0683823749423027, 0.044929128140211105, 0.09368298202753067, 0.053772833198308945, 0.09116123616695404, 0.10302916914224625, 0.05483724921941757, -0.14061233401298523, 0.0595400296151638, 0.0791955217719078, -0.00016231430345214903, 0.07909666746854782, -0.21189215779304504, -0.00606404198333621, -0.05851916968822479, 0.1264832317829132]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21041305363178253, 0.14690116047859192, 0.1272290050983429, 0.14699631929397583, 0.04378819838166237, 0.05232628062367439, -0.0904611274600029, 0.181904599070549, 0.15210507810115814, -0.43506038188934326, 0.021312177181243896, 0.16255320608615875, 0.08195934444665909, -0.2737232446670532, -0.08914146572351456, -0.04434022307395935]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0788516253232956, -0.030626676976680756, -0.06641653180122375, 0.13788126409053802, -0.0729326456785202, -0.1496439278125763, 0.09014500677585602, 0.05041571706533432, 0.07612548768520355, 0.20192015171051025, -0.26888078451156616, 0.02833455801010132, 0.006069143768399954, -0.17664283514022827, 0.07691550254821777, 0.025863759219646454]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21032196283340454, 0.2399453967809677, 0.14092595875263214, 0.05274675041437149, -0.3024904131889343, -0.05649992823600769, 0.09098339080810547, 0.010334339924156666, 0.15906284749507904, -0.05597614869475365, -0.5036078691482544, 0.050741348415613174, -0.3322763442993164, 0.2544032037258148, 0.1573253720998764, 0.15333667397499084]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3116067945957184, 0.33400338888168335, 0.1944805383682251, 0.26440879702568054, 0.26245078444480896, 0.23996861279010773, 0.12396715581417084, 0.15211988985538483, -0.38371285796165466, 0.38845571875572205, -0.0701170489192009, 0.3161897659301758, 0.08690768480300903, -0.06525721400976181, 0.2746390402317047, 0.16821235418319702]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05285749211907387, 0.2011697143316269, 0.11788688600063324, 0.15820454061031342, -0.3485206067562103, 0.20232419669628143, 0.055760305374860764, 0.2125345915555954, -0.03234235197305679, -0.16698713600635529, 0.06478120386600494, -0.06922609359025955, 0.17660227417945862, 0.10355381667613983, -0.01424420066177845, 0.1491580307483673]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2075469195842743, 0.2786121666431427, -0.08522365987300873, 0.3220505714416504, 0.4578351080417633, 0.39901211857795715, 0.11410875618457794, -0.004419501405209303, 0.6153630614280701, -0.19638703763484955, 0.2557012438774109, 0.053604453802108765, 0.6668638586997986, -0.19778458774089813, -0.023785753175616264, 0.1901843547821045]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8633412718772888, 0.7513730525970459, 0.6186534762382507, 0.5664200186729431, 0.8444939255714417, 0.4449177384376526, 1.1088371276855469, 1.0640943050384521, 0.09020930528640747, 0.034669429063797, 0.43655896186828613, 0.7906150221824646, 0.7514222264289856, 0.8269810676574707, 1.1139781475067139, 0.7894710302352905]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1537922620773315, 1.1682732105255127, 0.658046305179596, 1.0622026920318604, 0.9538418650627136, 0.3785346746444702, 0.9900062680244446, 1.419825553894043, 0.7749102115631104, 0.2930343449115753, 1.0959174633026123, 1.107513666152954, 0.6404430270195007, 1.0976101160049438, 0.4970938265323639, 1.0078017711639404]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2425892353057861, 1.5974162817001343, 1.531187891960144, 1.408203363418579, 1.425961971282959, 0.8526381254196167, 1.7486542463302612, 1.2280423641204834, 1.4829778671264648, 1.44647216796875, 1.2646526098251343, 1.1728774309158325, 1.1546409130096436, 1.3419848680496216, 1.2152996063232422, 1.2020422220230103]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0562403202056885, 1.5110695362091064, 1.6465071439743042, 1.7928330898284912, 1.2649614810943604, 1.5874747037887573, 1.2877551317214966, 1.5207631587982178, 1.3566246032714844, 0.6469911932945251, 1.6975327730178833, 1.9513704776763916, 1.761359691619873, 1.6776113510131836, 1.4278610944747925, 1.535449743270874]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0867810249328613, 3.8592989444732666, 3.0430617332458496, 2.663673162460327, 3.48696231842041, 2.1033105850219727, 3.660174608230591, 3.1605401039123535, 3.8254990577697754, 3.386592149734497, 3.1913599967956543, 3.9374935626983643, 2.149165630340576, 3.963116407394409, 3.1559603214263916, 2.8084380626678467]
Running loglikelihood requests:  70%|█████████████████████████████████▊              | 141/200 [04:48<01:47,  1.82s/it]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.394808053970337, 2.3695907592773438, 2.848649501800537, 2.4318816661834717, 2.8052847385406494, 2.07366943359375, 1.5842409133911133, 2.095083713531494, 2.6358299255371094, 1.9944581985473633, 1.3491908311843872, 1.9465532302856445, 2.4358363151550293, 2.6764934062957764, 2.642681837081909, 2.002370834350586]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.577573776245117, 3.417268991470337, 3.0570273399353027, 3.1173458099365234, 3.0405924320220947, 2.8484160900115967, 2.7873945236206055, 2.050333023071289, 3.280827760696411, 2.9492783546447754, 2.7725865840911865, 1.3809906244277954, 2.9196109771728516, 2.769115924835205, 3.037881851196289, 2.0705373287200928]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9295287132263184, 1.840869665145874, 2.6433331966400146, 2.595629930496216, 2.6320769786834717, 1.995580792427063, 1.7743923664093018, 3.089278221130371, 0.5706124901771545, 2.172459840774536, 0.9823543429374695, 2.4537031650543213, 2.7546536922454834, 2.9271883964538574, 2.961259365081787, 1.4301289319992065]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.693951368331909, 3.9599149227142334, 5.562158107757568, 4.949991226196289, 5.521016597747803, 5.679652690887451, 5.138128757476807, 5.125307083129883, 6.160701751708984, 5.045625686645508, 5.775586128234863, 6.081393718719482, 4.833320140838623, 5.403480052947998, 4.315169334411621, 4.9629435539245605]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.216167449951172, 4.5625834465026855, 5.548192501068115, 5.1745524406433105, 3.7096877098083496, 4.469728946685791, 4.881454944610596, 5.29276180267334, 5.369389057159424, 4.918285369873047, 4.278403282165527, 3.3546459674835205, 4.413742542266846, 3.3931565284729004, 4.923064708709717, 4.5003790855407715]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.887974262237549, 5.848826885223389, 6.045947074890137, 5.632587432861328, 6.082983493804932, 4.827613830566406, 5.697366714477539, 5.646091938018799, 5.3095221519470215, 5.367415428161621, 6.223377227783203, 5.912253379821777, 4.964935779571533, 5.867819309234619, 6.051319122314453, 5.8747429847717285]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.0019168853759766, 2.9743311405181885, 2.8142096996307373, 2.2430214881896973, -0.44111835956573486, 3.0429487228393555, 2.725316286087036, 1.8617693185806274, 2.7166593074798584, 3.471189498901367, 2.6922495365142822, 3.249838352203369, 2.861163377761841, 2.949395179748535, 2.330174684524536, 1.7439435720443726]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.559493541717529, 6.82963752746582, 6.543267726898193, 6.314143657684326, 6.4727067947387695, 6.486982345581055, 6.753968715667725, 6.6472930908203125, 6.650651931762695, 6.338835716247559, 6.9347028732299805, 6.268285274505615, 5.291858673095703, 7.213268756866455, 6.497037887573242, 6.447238445281982]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.015583038330078, 5.35748815536499, 4.967387676239014, 4.053539752960205, 4.908200263977051, 5.25557279586792, 4.637585639953613, 4.787707805633545, 5.078675746917725, 4.852478981018066, 5.277594089508057, 4.883947849273682, 5.151148319244385, 5.437978267669678, 4.606158256530762, 5.480422019958496]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.152912139892578, 4.670645236968994, 3.3402059078216553, 4.199930667877197, 4.459762096405029, 4.081397533416748, 4.244367599487305, 4.234857559204102, 4.365579128265381, 4.4749226570129395, 4.058918476104736, 4.431219577789307, 3.6774630546569824, 2.0268354415893555, 3.3802027702331543, 4.02227783203125]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.293093681335449, 6.242282390594482, 5.918283939361572, 6.0895915031433105, 5.433859348297119, 5.857343673706055, 6.180791854858398, 5.707417011260986, 5.351993560791016, 5.758627414703369, 6.443394184112549, 5.527287006378174, 5.775136947631836, 6.216215133666992, 6.232839107513428, 5.790725231170654]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.840487241744995, 3.674243688583374, 3.784898519515991, 3.9521522521972656, 2.690720796585083, 3.865375280380249, 3.94384503364563, 3.6625776290893555, 3.994852066040039, 3.6173505783081055, 3.700815439224243, 3.626281261444092, 3.666266918182373, 3.603097438812256, 3.7108659744262695, 3.899491310119629]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.455092430114746, 3.5311033725738525, 3.408513307571411, 3.558575391769409, 3.3871042728424072, 3.6261205673217773, 3.5768768787384033, 3.3691048622131348, 3.6052653789520264, 3.3009681701660156, 3.604081869125366, 3.4496543407440186, 3.4383890628814697, 2.7281901836395264, 3.654020071029663, 3.570390224456787]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1840755939483643, 3.2108614444732666, 2.9854652881622314, 2.898366928100586, 2.8208301067352295, 3.0872933864593506, 3.1851067543029785, 3.275832176208496, 3.033661365509033, 3.0067315101623535, 2.289513111114502, 2.999436855316162, 3.1577062606811523, 3.064171314239502, 3.0809571743011475, 2.8883259296417236]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.37266731262207, 4.48905086517334, 4.563957214355469, 4.371342182159424, 4.488656997680664, 4.463590621948242, 4.217234134674072, 4.70820426940918, 4.4919843673706055, 4.456347942352295, 4.467284202575684, 4.052472114562988, 4.195288181304932, 4.0544281005859375, 4.572612762451172, 4.372212886810303]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.582182884216309, 8.909960746765137, 8.78506088256836, 8.470927238464355, 8.553983688354492, 8.267024993896484, 8.637528419494629, 8.539398193359375, 8.62582015991211, 8.49224853515625, 8.597477912902832, 8.232429504394531, 8.337320327758789, 9.000391006469727, 8.89893627166748, 8.508954048156738]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.888962268829346, 5.431853771209717, 5.387381076812744, 4.7928619384765625, 5.134958267211914, 5.084168434143066, 4.673970699310303, 5.136359214782715, 5.240082740783691, 4.929230213165283, 4.836874961853027, 4.734005451202393, 4.867003440856934, 4.978545188903809, 5.068427085876465, 4.833028316497803]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2336862087249756, 2.9992923736572266, 3.1042098999023438, 2.7586445808410645, 3.0260605812072754, 3.311748743057251, 2.752453327178955, 3.1765239238739014, 3.085078001022339, 3.1838321685791016, 3.1738131046295166, 3.263648509979248, 3.0341150760650635, 2.9810805320739746, 3.1215476989746094, 3.177919387817383]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12207134813070297, 0.13663598895072937, 0.12992949783802032, -0.2392372190952301, -0.2358580231666565, -0.10403505712747574, 0.14449797570705414, -0.1792064756155014, 0.10132662206888199, 0.11591418832540512, 0.11623969674110413, 0.07147388160228729, 0.10766460746526718, 0.1263185441493988, -1.095262050628662, 0.1391974836587906]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08802909404039383, 0.050635676831007004, 0.04163447394967079, 0.06234961375594139, 0.08392395824193954, 0.04024578630924225, 0.05074179917573929, 0.07642120867967606, 0.016678374260663986, 0.06177680939435959, -0.17976219952106476, 0.05848933011293411, 0.004029922652989626, -0.002913822652772069, 0.017103472724556923, 0.024843670427799225]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06802792102098465, 0.043251823633909225, 0.09273245930671692, 0.054246269166469574, 0.09124630689620972, 0.10274491459131241, 0.05496470630168915, -0.14336560666561127, 0.059314485639333725, 0.07880071550607681, -0.00038607759051956236, 0.07873588055372238, -0.21186649799346924, -0.007026284001767635, -0.05673537403345108, 0.12662899494171143]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21112996339797974, 0.14762745797634125, 0.12594656646251678, 0.14730258285999298, 0.04321477189660072, 0.05352943763136864, -0.09245005995035172, 0.18220511078834534, 0.15242551267147064, -0.43804264068603516, 0.020167140290141106, 0.16149580478668213, 0.08301357924938202, -0.2720463275909424, -0.09040316939353943, -0.046233873814344406]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07875774055719376, -0.030243581160902977, -0.06602710485458374, 0.13826853036880493, -0.07338128983974457, -0.15094546973705292, 0.0916898101568222, 0.05010363087058067, 0.07663004100322723, 0.20392251014709473, -0.2707805335521698, 0.028496377170085907, 0.006445607170462608, -0.17584949731826782, 0.07724414020776749, 0.02771025337278843]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21086658537387848, 0.23879900574684143, 0.1401013731956482, 0.05167989805340767, -0.30004578828811646, -0.057019613683223724, 0.09221915900707245, 0.010775775648653507, 0.15870735049247742, -0.05805238336324692, -0.5001581907272339, 0.04917319864034653, -0.33497944474220276, 0.25321540236473083, 0.1563127338886261, 0.1516522765159607]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.31129106879234314, 0.33409416675567627, 0.1924847662448883, 0.26468586921691895, 0.2615616023540497, 0.24043568968772888, 0.1245611235499382, 0.15255840122699738, -0.38330569863319397, 0.38828834891319275, -0.07112051546573639, 0.31693023443222046, 0.08569725602865219, -0.06428550183773041, 0.2738160490989685, 0.1678103506565094]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.052301302552223206, 0.20101003348827362, 0.11844061315059662, 0.15861308574676514, -0.35108378529548645, 0.20296570658683777, 0.05611702427268028, 0.21397048234939575, -0.03157458081841469, -0.16682636737823486, 0.06549414992332458, -0.06884876638650894, 0.17759616672992706, 0.1038416177034378, -0.015614524483680725, 0.15051184594631195]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20767877995967865, 0.2786247134208679, -0.0838252380490303, 0.3212408721446991, 0.45724648237228394, 0.39867109060287476, 0.1140379086136818, -0.0027737566269934177, 0.6136584877967834, -0.19767363369464874, 0.25502529740333557, 0.054135944694280624, 0.6661558747291565, -0.19843940436840057, -0.021745119243860245, 0.18994709849357605]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.863369882106781, 0.7510012984275818, 0.6179012060165405, 0.565667450428009, 0.8449522256851196, 0.44643130898475647, 1.108736515045166, 1.0621408224105835, 0.09186524152755737, 0.03448105603456497, 0.43818485736846924, 0.7895930409431458, 0.7516595721244812, 0.8280728459358215, 1.1136962175369263, 0.7895491719245911]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.154791235923767, 1.168534755706787, 0.6578140258789062, 1.0634498596191406, 0.9545640349388123, 0.37812983989715576, 0.9888038039207458, 1.4187928438186646, 0.7754507064819336, 0.29252487421035767, 1.0968940258026123, 1.1078964471817017, 0.6409674286842346, 1.0971750020980835, 0.496764600276947, 1.0091267824172974]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2421725988388062, 1.5960954427719116, 1.5296776294708252, 1.4060667753219604, 1.425837516784668, 0.8552805185317993, 1.747800588607788, 1.2287476062774658, 1.4820151329040527, 1.4450268745422363, 1.2645816802978516, 1.1724042892456055, 1.1569583415985107, 1.3409401178359985, 1.213541865348816, 1.2019612789154053]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.05731201171875, 1.5111961364746094, 1.6454826593399048, 1.7922159433364868, 1.264281153678894, 1.5864567756652832, 1.28553307056427, 1.519596815109253, 1.3579293489456177, 0.6500096917152405, 1.6965807676315308, 1.9497159719467163, 1.7624412775039673, 1.6773053407669067, 1.427085041999817, 1.5349029302597046]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0866973400115967, 3.8568947315216064, 3.0424857139587402, 2.664612054824829, 3.4866654872894287, 2.105757236480713, 3.657904863357544, 3.15771484375, 3.824026346206665, 3.3861801624298096, 3.1890153884887695, 3.9365949630737305, 2.1477224826812744, 3.962926149368286, 3.1565639972686768, 2.806861162185669]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3942484855651855, 2.368199110031128, 2.847775936126709, 2.430413007736206, 2.803353786468506, 2.0736846923828125, 1.5834639072418213, 2.0923006534576416, 2.634582042694092, 1.9953186511993408, 1.3506858348846436, 1.9461028575897217, 2.4354724884033203, 2.6752736568450928, 2.64164662361145, 2.0020008087158203]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.576716899871826, 3.416274309158325, 3.0563950538635254, 3.116795301437378, 3.0398809909820557, 2.8479766845703125, 2.7844550609588623, 2.048332691192627, 3.280043363571167, 2.947730779647827, 2.773848533630371, 1.3802493810653687, 2.918179750442505, 2.7683331966400146, 3.03661847114563, 2.071021318435669]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9293177127838135, 1.8411928415298462, 2.642834186553955, 2.594974994659424, 2.6316936016082764, 1.9961029291152954, 1.7732179164886475, 3.088996648788452, 0.5698501467704773, 2.1714117527008057, 0.9825431704521179, 2.453782320022583, 2.7540299892425537, 2.9262752532958984, 2.9601330757141113, 1.4307399988174438]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6925442218780518, 3.9570090770721436, 5.559302806854248, 4.948894023895264, 5.518789291381836, 5.676698207855225, 5.1355180740356445, 5.122547626495361, 6.157585620880127, 5.042506694793701, 5.7721333503723145, 6.078176975250244, 4.830191612243652, 5.400259494781494, 4.31276798248291, 4.960113525390625]
Running loglikelihood requests:  72%|██████████████████████████████████▊             | 145/200 [04:55<01:39,  1.81s/it]Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.214426517486572, 4.560674667358398, 5.545256614685059, 5.173180103302002, 3.7081997394561768, 4.467708110809326, 4.879232883453369, 5.29080867767334, 5.367456912994385, 4.9166412353515625, 4.277276039123535, 3.354461669921875, 4.413324356079102, 3.392387628555298, 4.921690464019775, 4.498722553253174]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.88698673248291, 5.847375869750977, 6.045234680175781, 5.630991458892822, 6.081943511962891, 4.826087951660156, 5.696223735809326, 5.644350051879883, 5.307370662689209, 5.36646842956543, 6.222649574279785, 5.91192102432251, 4.9641499519348145, 5.866957187652588, 6.049937725067139, 5.873286724090576]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.002751111984253, 2.973916530609131, 2.814204454421997, 2.2422826290130615, -0.4430638253688812, 3.0432040691375732, 2.7244157791137695, 1.86150062084198, 2.71659779548645, 3.470963716506958, 2.6921496391296387, 3.249962329864502, 2.8611888885498047, 2.9495368003845215, 2.3301713466644287, 1.7438480854034424]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.558865547180176, 6.828786373138428, 6.542346954345703, 6.313129425048828, 6.471474647521973, 6.4863762855529785, 6.753313064575195, 6.64762544631958, 6.649872779846191, 6.338410377502441, 6.9340033531188965, 6.267178058624268, 5.2906341552734375, 7.212698459625244, 6.496333599090576, 6.446724891662598]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.015543460845947, 5.3573317527771, 4.966905117034912, 4.053264617919922, 4.907252788543701, 5.25495719909668, 4.636959075927734, 4.787125587463379, 5.078059673309326, 4.852301597595215, 5.277355194091797, 4.8824567794799805, 5.15100622177124, 5.437360763549805, 4.606091499328613, 5.479427814483643]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.153625011444092, 4.670955657958984, 3.3409337997436523, 4.200569152832031, 4.460555076599121, 4.081877708435059, 4.244917392730713, 4.2348246574401855, 4.36594295501709, 4.4752349853515625, 4.059324264526367, 4.4318366050720215, 3.677830457687378, 2.026968240737915, 3.380873918533325, 4.022491931915283]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.292297840118408, 6.241247653961182, 5.917538166046143, 6.088901519775391, 5.433634281158447, 5.856196403503418, 6.180105209350586, 5.706420421600342, 5.350679397583008, 5.7577924728393555, 6.44110631942749, 5.526425838470459, 5.774727821350098, 6.215247631072998, 6.2318854331970215, 5.789691925048828]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8401942253112793, 3.6742775440216064, 3.784545421600342, 3.951916217803955, 2.6904122829437256, 3.865243911743164, 3.94378662109375, 3.662621259689331, 3.9948277473449707, 3.6172122955322266, 3.700895309448242, 3.6263067722320557, 3.6663432121276855, 3.6025991439819336, 3.710719347000122, 3.8993234634399414]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.454859495162964, 3.5314011573791504, 3.407851219177246, 3.558142900466919, 3.3864400386810303, 3.6263623237609863, 3.5767672061920166, 3.368696928024292, 3.6053876876831055, 3.3008735179901123, 3.6042685508728027, 3.449242353439331, 3.4380900859832764, 2.7281205654144287, 3.6541261672973633, 3.570566177368164]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.184225559234619, 3.211038112640381, 2.985133409500122, 2.8983001708984375, 2.820535182952881, 3.0873148441314697, 3.1851754188537598, 3.275707483291626, 3.033405303955078, 3.0067484378814697, 2.2893869876861572, 2.99945068359375, 3.1578941345214844, 3.064384937286377, 3.080456256866455, 2.888169765472412]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.372379779815674, 4.488955974578857, 4.563297748565674, 4.37058162689209, 4.488312244415283, 4.463229179382324, 4.216431617736816, 4.70774507522583, 4.49156379699707, 4.455327033996582, 4.466761589050293, 4.052135467529297, 4.195009708404541, 4.053598403930664, 4.57167387008667, 4.371739864349365]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.580952644348145, 8.90945053100586, 8.783599853515625, 8.469219207763672, 8.552849769592285, 8.266474723815918, 8.636338233947754, 8.538314819335938, 8.624625205993652, 8.490507125854492, 8.596397399902344, 8.2317476272583, 8.33665657043457, 8.998896598815918, 8.897871971130371, 8.506860733032227]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.889622211456299, 5.43223762512207, 5.38731050491333, 4.793032646179199, 5.135777950286865, 5.084095001220703, 4.67409610748291, 5.136861801147461, 5.240476608276367, 4.929615497589111, 4.836808681488037, 4.734200954437256, 4.867326259613037, 4.978774547576904, 5.068795204162598, 4.8335347175598145]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2326323986053467, 2.998631477355957, 3.104177713394165, 2.7584493160247803, 3.0259294509887695, 3.311256170272827, 2.752148151397705, 3.175563097000122, 3.0845437049865723, 3.182847261428833, 3.173649549484253, 3.26316237449646, 3.0334830284118652, 2.9803688526153564, 3.1215627193450928, 3.1764848232269287]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12173296511173248, 0.13681411743164062, 0.12988798320293427, -0.23265856504440308, -0.23184746503829956, -0.10434625297784805, 0.14451606571674347, -0.18035981059074402, 0.1024809405207634, 0.1165495216846466, 0.11559871584177017, 0.07390888780355453, 0.10711458325386047, 0.12605459988117218, -1.088630199432373, 0.1392972767353058]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0884454995393753, 0.05000913515686989, 0.039472393691539764, 0.06155453994870186, 0.08500039577484131, 0.042058125138282776, 0.053258590400218964, 0.07645970582962036, 0.0161427091807127, 0.06115441769361496, -0.1753445267677307, 0.0593712218105793, 0.0041918521746993065, -0.002245470881462097, 0.017659375444054604, 0.024542823433876038]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06910625845193863, 0.04256444051861763, 0.09274417161941528, 0.05553852394223213, 0.09243208169937134, 0.10279661417007446, 0.0574650801718235, -0.14319704473018646, 0.06058347597718239, 0.07948160171508789, -0.0003800915728788823, 0.07857756316661835, -0.21012544631958008, -0.004629509523510933, -0.058607086539268494, 0.12577392160892487]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21171028912067413, 0.14795847237110138, 0.12616084516048431, 0.1472632884979248, 0.043434400111436844, 0.053995344787836075, -0.0911058709025383, 0.18172132968902588, 0.1516515612602234, -0.43453270196914673, 0.01955697126686573, 0.16324001550674438, 0.08511362224817276, -0.2748173475265503, -0.0905623733997345, -0.04611299932003021]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07804811000823975, -0.029247747734189034, -0.06435280293226242, 0.1374317705631256, -0.07416790723800659, -0.14930665493011475, 0.09047000110149384, 0.05156321078538895, 0.07329288125038147, 0.20292197167873383, -0.2689342796802521, 0.031388502568006516, 0.00589769659563899, -0.1759634017944336, 0.0773521214723587, 0.024605978280305862]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21112112700939178, 0.23897817730903625, 0.14019735157489777, 0.05154868960380554, -0.29959607124328613, -0.05557069554924965, 0.09162675589323044, 0.01049018744379282, 0.15925262868404388, -0.05799971520900726, -0.5015240907669067, 0.04913649335503578, -0.3323346972465515, 0.25355997681617737, 0.15586990118026733, 0.152047261595726]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.31059834361076355, 0.33377498388290405, 0.19264750182628632, 0.26536446809768677, 0.2617034316062927, 0.2393229901790619, 0.12561237812042236, 0.1515047401189804, -0.38153159618377686, 0.38803258538246155, -0.06785184890031815, 0.3164384961128235, 0.08576105535030365, -0.06245255097746849, 0.2746560275554657, 0.16781184077262878]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05212296545505524, 0.20209906995296478, 0.11792348325252533, 0.15830369293689728, -0.3493601381778717, 0.20330527424812317, 0.05649098381400108, 0.21477216482162476, -0.03208776190876961, -0.16783694922924042, 0.06661304831504822, -0.06809688359498978, 0.17794886231422424, 0.10350167751312256, -0.012515072710812092, 0.14926613867282867]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20849500596523285, 0.27869120240211487, -0.08406946808099747, 0.3215835392475128, 0.456955224275589, 0.39827120304107666, 0.11405189335346222, -0.004237664397805929, 0.6140169501304626, -0.19729633629322052, 0.2552137076854706, 0.05335301533341408, 0.6656931042671204, -0.19439156353473663, -0.020842961966991425, 0.1897183358669281]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8638152480125427, 0.7542862892150879, 0.6177650094032288, 0.5658098459243774, 0.8460137248039246, 0.4476213753223419, 1.110003113746643, 1.062909722328186, 0.09297341108322144, 0.03485310822725296, 0.4402763843536377, 0.789622962474823, 0.7514996528625488, 0.8328983783721924, 1.112755298614502, 0.789540708065033]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1552196741104126, 1.1684260368347168, 0.6581554412841797, 1.0642510652542114, 0.9542211294174194, 0.37784314155578613, 0.9895094633102417, 1.418738842010498, 0.780012309551239, 0.2946639358997345, 1.096814751625061, 1.1084933280944824, 0.641547679901123, 1.0975985527038574, 0.4965723156929016, 1.0097947120666504]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.242148518562317, 1.5950508117675781, 1.529172658920288, 1.4047110080718994, 1.4253385066986084, 0.8546013832092285, 1.746742606163025, 1.230777382850647, 1.480893611907959, 1.4437764883041382, 1.267249584197998, 1.172816514968872, 1.1573331356048584, 1.3406730890274048, 1.2165720462799072, 1.202237844467163]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0599148273468018, 1.5123416185379028, 1.6455681324005127, 1.792384147644043, 1.2663638591766357, 1.588598370552063, 1.285070538520813, 1.5196782350540161, 1.3591960668563843, 0.6496878862380981, 1.6958297491073608, 1.9502345323562622, 1.7633639574050903, 1.6781827211380005, 1.4272470474243164, 1.5351957082748413]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0859923362731934, 3.856922149658203, 3.0422956943511963, 2.6660914421081543, 3.4861278533935547, 2.106179714202881, 3.6592071056365967, 3.1571502685546875, 3.823949098587036, 3.3869566917419434, 3.1886417865753174, 3.9374897480010986, 2.149271011352539, 3.962974786758423, 3.16011643409729, 2.809242010116577]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.395270586013794, 2.367365837097168, 2.848263740539551, 2.4325482845306396, 2.803853750228882, 2.0739002227783203, 1.585837721824646, 2.0903308391571045, 2.6372792720794678, 1.9955576658248901, 1.3526356220245361, 1.9471555948257446, 2.4358928203582764, 2.675593376159668, 2.6417744159698486, 2.0034561157226562]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5795345306396484, 3.4168686866760254, 3.058732032775879, 3.117443561553955, 3.042593002319336, 2.8489139080047607, 2.7840158939361572, 2.050111770629883, 3.2808032035827637, 2.948800802230835, 2.7747294902801514, 1.3819992542266846, 2.918606758117676, 2.7702205181121826, 3.0380971431732178, 2.071478843688965]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9295434951782227, 1.8431421518325806, 2.6433627605438232, 2.5971667766571045, 2.6313257217407227, 1.9958531856536865, 1.77163827419281, 3.0894968509674072, 0.5711517333984375, 2.1700472831726074, 0.9834021329879761, 2.4536147117614746, 2.7548604011535645, 2.9266703128814697, 2.9605653285980225, 1.4292078018188477]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6921870708465576, 3.9567601680755615, 5.558285713195801, 4.948324680328369, 5.5175580978393555, 5.676562309265137, 5.133938312530518, 5.121572494506836, 6.157769680023193, 5.041809558868408, 5.770176410675049, 6.077117443084717, 4.829131126403809, 5.3981475830078125, 4.313776016235352, 4.959588527679443]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.214270114898682, 4.561053276062012, 5.545382022857666, 5.173182487487793, 3.7083687782287598, 4.469575881958008, 4.878889083862305, 5.291051387786865, 5.366349220275879, 4.916943550109863, 4.27734899520874, 3.353801965713501, 4.412693023681641, 3.392585039138794, 4.921054363250732, 4.4987592697143555]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.885707855224609, 5.846768856048584, 6.044290542602539, 5.6299896240234375, 6.081258773803711, 4.825335502624512, 5.695396900177002, 5.643761157989502, 5.306626319885254, 5.365576267242432, 6.221139907836914, 5.91083288192749, 4.962255954742432, 5.866275787353516, 6.049083709716797, 5.872751712799072]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.0025815963745117, 2.973593235015869, 2.8135898113250732, 2.2415201663970947, -0.44308724999427795, 3.042412042617798, 2.7241628170013428, 1.8603432178497314, 2.717278003692627, 3.470907211303711, 2.6923036575317383, 3.2497425079345703, 2.8610241413116455, 2.9502036571502686, 2.3293094635009766, 1.7419772148132324]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.557986736297607, 6.827692985534668, 6.54176139831543, 6.311235427856445, 6.469790458679199, 6.485167980194092, 6.752148151397705, 6.64553165435791, 6.648678779602051, 6.336954593658447, 6.93277645111084, 6.265690326690674, 5.289037227630615, 7.211318492889404, 6.494455814361572, 6.445790767669678]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  74%|███████████████████████████████████▊            | 149/200 [05:02<01:31,  1.79s/it]Layer: gate_22 - Captured router_logits: [5.014918804168701, 5.356691360473633, 4.966862201690674, 4.0534234046936035, 4.906185150146484, 5.254624366760254, 4.6363067626953125, 4.786635398864746, 5.078039646148682, 4.851474285125732, 5.276578426361084, 4.881527423858643, 5.150518417358398, 5.437285900115967, 4.60599422454834, 5.47904109954834]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.154160499572754, 4.670949935913086, 3.341634511947632, 4.200496196746826, 4.461641311645508, 4.082207202911377, 4.245837688446045, 4.235298156738281, 4.366525173187256, 4.475570201873779, 4.059327125549316, 4.4324846267700195, 3.6776552200317383, 2.0275931358337402, 3.3807008266448975, 4.0220866203308105]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.290721416473389, 6.239277362823486, 5.916112899780273, 6.087213039398193, 5.432095527648926, 5.8547186851501465, 6.178555011749268, 5.704909801483154, 5.348940849304199, 5.755981922149658, 6.44003963470459, 5.524925231933594, 5.773110389709473, 6.213868141174316, 6.230679035186768, 5.787826061248779]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.839341402053833, 3.6736292839050293, 3.783817768096924, 3.9512317180633545, 2.6900877952575684, 3.865518093109131, 3.9431533813476562, 3.661763906478882, 3.994088888168335, 3.616685152053833, 3.7002768516540527, 3.6252505779266357, 3.665412664413452, 3.6012864112854004, 3.710343360900879, 3.89819598197937]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.454068899154663, 3.5304882526397705, 3.4068875312805176, 3.557680368423462, 3.385380268096924, 3.6258459091186523, 3.5765838623046875, 3.3681511878967285, 3.6051342487335205, 3.3002026081085205, 3.602727174758911, 3.449016809463501, 3.4371910095214844, 2.7274365425109863, 3.653437614440918, 3.5699567794799805]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1838979721069336, 3.2108044624328613, 2.9846625328063965, 2.898155689239502, 2.820418119430542, 3.0871870517730713, 3.184741497039795, 3.275571823120117, 3.032987117767334, 3.0060551166534424, 2.289407968521118, 2.9989261627197266, 3.157392740249634, 3.0636749267578125, 3.080522298812866, 2.8876793384552]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.371481895446777, 4.488095283508301, 4.56244421005249, 4.369570255279541, 4.487137317657471, 4.462378025054932, 4.216331958770752, 4.7069525718688965, 4.490912914276123, 4.454653263092041, 4.465810298919678, 4.051505088806152, 4.194169044494629, 4.052978992462158, 4.571098804473877, 4.370662689208984]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.57870864868164, 8.907642364501953, 8.782113075256348, 8.466455459594727, 8.55069637298584, 8.264673233032227, 8.634329795837402, 8.536212921142578, 8.622000694274902, 8.487926483154297, 8.593976020812988, 8.229991912841797, 8.333626747131348, 8.99667739868164, 8.895613670349121, 8.505126953125]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.890388011932373, 5.433496475219727, 5.387956619262695, 4.794340133666992, 5.1363525390625, 5.086039066314697, 4.67415189743042, 5.137131214141846, 5.240772247314453, 4.9299798011779785, 4.836772441864014, 4.735102653503418, 4.867523193359375, 4.979129791259766, 5.069154262542725, 4.834192276000977]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2336041927337646, 2.9991955757141113, 3.104788064956665, 2.7596280574798584, 3.02645206451416, 3.311980962753296, 2.752872943878174, 3.176347017288208, 3.085355043411255, 3.183316946029663, 3.1742241382598877, 3.2642476558685303, 3.0339195728302, 2.9806973934173584, 3.1218037605285645, 3.17724347114563]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12107288092374802, 0.1358545422554016, 0.12990334630012512, -0.2440565526485443, -0.2480195164680481, -0.09407088905572891, 0.14322242140769958, -0.16939449310302734, 0.1012519896030426, 0.11525945365428925, 0.11552119255065918, 0.07281045615673065, 0.1075173020362854, 0.1266501247882843, -1.1163036823272705, 0.13751228153705597]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08705396205186844, 0.05009490251541138, 0.0415014773607254, 0.06254752725362778, 0.08388040959835052, 0.03733772039413452, 0.05231805890798569, 0.0774165540933609, 0.01768774911761284, 0.06271162629127502, -0.18310488760471344, 0.060299959033727646, 0.0041523100808262825, -0.0019802243914455175, 0.021578703075647354, 0.0251179039478302]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07101079821586609, 0.045051176100969315, 0.0910584107041359, 0.05520344153046608, 0.09321483969688416, 0.10392162203788757, 0.05700927600264549, -0.14542797207832336, 0.06409977376461029, 0.08336150646209717, 0.0035080253146588802, 0.07747848331928253, -0.22039905190467834, -0.0044344160705804825, -0.06332110613584518, 0.1261136382818222]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21453478932380676, 0.1470399647951126, 0.12547625601291656, 0.146814227104187, 0.04463042691349983, 0.05296143889427185, -0.09424809366464615, 0.1813362091779709, 0.1550607532262802, -0.4530872702598572, 0.028695566579699516, 0.15908828377723694, 0.09129885584115982, -0.2854566276073456, -0.08143085241317749, -0.04804816469550133]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07956960797309875, -0.03019682690501213, -0.0637621134519577, 0.14698009192943573, -0.06654193997383118, -0.15104836225509644, 0.08914367854595184, 0.048805076628923416, 0.07083317637443542, 0.20454047620296478, -0.2825607359409332, 0.027740756049752235, 0.007602454163134098, -0.18369032442569733, 0.08079297840595245, 0.021889526396989822]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21138045191764832, 0.23759518563747406, 0.1412651240825653, 0.052645597606897354, -0.3025433421134949, -0.06416312605142593, 0.09408092498779297, 0.007601037621498108, 0.16096080839633942, -0.061581578105688095, -0.49616971611976624, 0.04812229797244072, -0.3438684046268463, 0.2549270987510681, 0.15486319363117218, 0.15082529187202454]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3139741122722626, 0.3317868709564209, 0.18955180048942566, 0.2670845687389374, 0.2625206410884857, 0.2436133325099945, 0.12449992448091507, 0.1525101214647293, -0.38391023874282837, 0.38991764187812805, -0.07825108617544174, 0.3196001648902893, 0.08956313133239746, -0.07077203691005707, 0.27885115146636963, 0.17147308588027954]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05013898015022278, 0.2007906436920166, 0.12001905590295792, 0.16056029498577118, -0.35685423016548157, 0.20502504706382751, 0.05635911598801613, 0.21112369000911713, -0.027049444615840912, -0.16813483834266663, 0.062130142003297806, -0.06770695000886917, 0.17450954020023346, 0.10733524709939957, -0.02197190560400486, 0.14994269609451294]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.208367258310318, 0.2795226573944092, -0.08549034595489502, 0.3219681680202484, 0.4597839415073395, 0.3987509608268738, 0.11360737681388855, -0.005046756472438574, 0.6139626502990723, -0.1968909054994583, 0.25487491488456726, 0.05419231206178665, 0.6690830588340759, -0.20310215651988983, -0.026866432279348373, 0.19001570343971252]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8635344505310059, 0.7509682774543762, 0.6175739169120789, 0.5704323649406433, 0.8451811671257019, 0.44492658972740173, 1.1084063053131104, 1.0632517337799072, 0.09367777407169342, 0.029042299836874008, 0.43409404158592224, 0.7901880741119385, 0.7517694234848022, 0.8214395642280579, 1.1151155233383179, 0.7903334498405457]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1585743427276611, 1.1712889671325684, 0.658609926700592, 1.0644230842590332, 0.9550121426582336, 0.3723154366016388, 0.9897342324256897, 1.4208558797836304, 0.7731185555458069, 0.2873740494251251, 1.0995075702667236, 1.1110789775848389, 0.6387724280357361, 1.0969303846359253, 0.49852249026298523, 1.0068082809448242]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2409436702728271, 1.5988060235977173, 1.5304664373397827, 1.401910424232483, 1.4303607940673828, 0.8511180281639099, 1.747944951057434, 1.2293572425842285, 1.4831836223602295, 1.450240135192871, 1.2660311460494995, 1.1713534593582153, 1.153559684753418, 1.3359371423721313, 1.2118613719940186, 1.2007439136505127]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0514392852783203, 1.5094987154006958, 1.6477357149124146, 1.7938894033432007, 1.257250428199768, 1.5852659940719604, 1.2831586599349976, 1.522194743156433, 1.354992389678955, 0.6482264399528503, 1.6994850635528564, 1.9503265619277954, 1.759714961051941, 1.6797211170196533, 1.4272211790084839, 1.5344256162643433]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0851216316223145, 3.8600826263427734, 3.042776346206665, 2.661808967590332, 3.487652063369751, 2.103120803833008, 3.6605212688446045, 3.160137891769409, 3.8260915279388428, 3.3856940269470215, 3.1914968490600586, 3.937802791595459, 2.1438374519348145, 3.9583067893981934, 3.160568952560425, 2.805833578109741]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3938050270080566, 2.367997646331787, 2.850552558898926, 2.43119740486145, 2.8042707443237305, 2.07712984085083, 1.5756340026855469, 2.092226505279541, 2.6359400749206543, 1.9930800199508667, 1.3485475778579712, 1.9471672773361206, 2.435106039047241, 2.6766390800476074, 2.642089605331421, 1.999719500541687]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5753610134124756, 3.416621685028076, 3.0579636096954346, 3.1208486557006836, 3.0438790321350098, 2.8491427898406982, 2.7878100872039795, 2.049914598464966, 3.2808589935302734, 2.9489517211914062, 2.774346351623535, 1.3743551969528198, 2.9212806224823, 2.768294334411621, 3.037639856338501, 2.073643207550049]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.924155354499817, 1.8369680643081665, 2.640472650527954, 2.59641170501709, 2.6309547424316406, 1.9917924404144287, 1.768097996711731, 3.087460517883301, 0.5603533387184143, 2.1693501472473145, 0.9760634303092957, 2.450850248336792, 2.752556324005127, 2.9255621433258057, 2.958609104156494, 1.425866723060608]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6820883750915527, 3.9493086338043213, 5.555044174194336, 4.9431633949279785, 5.516524314880371, 5.672613143920898, 5.13032341003418, 5.1168904304504395, 6.155272483825684, 5.031589984893799, 5.769097328186035, 6.074746131896973, 4.827729225158691, 5.397887229919434, 4.303138256072998, 4.954317569732666]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.209789752960205, 4.555912971496582, 5.541642665863037, 5.172076225280762, 3.699732780456543, 4.459603309631348, 4.875639915466309, 5.287459373474121, 5.366278171539307, 4.91311502456665, 4.273088455200195, 3.345468521118164, 4.406570911407471, 3.385892152786255, 4.916415691375732, 4.493824005126953]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.879802703857422, 5.842067718505859, 6.040994167327881, 5.626016139984131, 6.07803201675415, 4.8166584968566895, 5.690104961395264, 5.637350082397461, 5.301565647125244, 5.363020420074463, 6.219390392303467, 5.906976222991943, 4.957132339477539, 5.861836910247803, 6.046111583709717, 5.868597984313965]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.002532958984375, 2.973111629486084, 2.8125522136688232, 2.24141526222229, -0.452846884727478, 3.045776128768921, 2.723407506942749, 1.8567410707473755, 2.7170917987823486, 3.4711086750030518, 2.691281318664551, 3.250509023666382, 2.860428810119629, 2.9494338035583496, 2.3258934020996094, 1.7394675016403198]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.555566787719727, 6.8271379470825195, 6.54000997543335, 6.308859348297119, 6.468482971191406, 6.485279083251953, 6.75007438659668, 6.645732879638672, 6.646418571472168, 6.33442497253418, 6.932182312011719, 6.263606071472168, 5.28371000289917, 7.210620880126953, 6.491054058074951, 6.443888187408447]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.014028549194336, 5.358153343200684, 4.965939044952393, 4.050893783569336, 4.905308246612549, 5.254698276519775, 4.634413719177246, 4.785984039306641, 5.076934337615967, 4.851309776306152, 5.2776265144348145, 4.880202770233154, 5.149872303009033, 5.437101364135742, 4.603392601013184, 5.478848934173584]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.155867099761963, 4.672624111175537, 3.339115858078003, 4.202252388000488, 4.462866306304932, 4.0827555656433105, 4.246971607208252, 4.235048770904541, 4.367282390594482, 4.477236747741699, 4.06049919128418, 4.43372917175293, 3.677147388458252, 2.0248992443084717, 3.3795812129974365, 4.023359298706055]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.290916442871094, 6.240349769592285, 5.915018558502197, 6.0861992835998535, 5.430387496948242, 5.85269021987915, 6.178610801696777, 5.704395294189453, 5.347087383270264, 5.754824638366699, 6.43934965133667, 5.523719787597656, 5.773430824279785, 6.21437931060791, 6.229135513305664, 5.7872724533081055]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.839888095855713, 3.674741506576538, 3.784811019897461, 3.952174425125122, 2.688929557800293, 3.8667898178100586, 3.9440393447875977, 3.662102460861206, 3.995429754257202, 3.6174094676971436, 3.701591968536377, 3.626689910888672, 3.6668620109558105, 3.601870536804199, 3.712144136428833, 3.8991377353668213]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  76%|████████████████████████████████████▋           | 153/200 [05:09<01:24,  1.79s/it]Layer: gate_26 - Captured router_logits: [3.454867362976074, 3.532973051071167, 3.4080114364624023, 3.558835506439209, 3.3877155780792236, 3.628824472427368, 3.577787399291992, 3.3692822456359863, 3.606368064880371, 3.302281379699707, 3.6068451404571533, 3.4502933025360107, 3.4391160011291504, 2.7288596630096436, 3.655534505844116, 3.572965621948242]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1867032051086426, 3.213050603866577, 2.986281156539917, 2.8998758792877197, 2.8211240768432617, 3.0888969898223877, 3.186933755874634, 3.2779881954193115, 3.0342886447906494, 3.008288621902466, 2.2893099784851074, 3.000807285308838, 3.1609487533569336, 3.066789388656616, 3.082308292388916, 2.8895626068115234]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.373863697052002, 4.490791320800781, 4.565096378326416, 4.371208667755127, 4.491967678070068, 4.464930057525635, 4.217774391174316, 4.709336280822754, 4.493130207061768, 4.456051826477051, 4.468329906463623, 4.053229808807373, 4.1963653564453125, 4.054656028747559, 4.574324131011963, 4.373182773590088]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.585104942321777, 8.914897918701172, 8.789628982543945, 8.471977233886719, 8.557311058044434, 8.271077156066895, 8.641212463378906, 8.542752265930176, 8.629470825195312, 8.493769645690918, 8.601083755493164, 8.236560821533203, 8.341346740722656, 9.00365924835205, 8.903539657592773, 8.509943008422852]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.891221523284912, 5.4334821701049805, 5.388249397277832, 4.791501998901367, 5.137660980224609, 5.086382865905762, 4.672175407409668, 5.136688709259033, 5.240630149841309, 4.9296040534973145, 4.836791038513184, 4.731462001800537, 4.866384506225586, 4.981102466583252, 5.069074630737305, 4.834651947021484]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.233853816986084, 3.0017917156219482, 3.1063625812530518, 2.7615625858306885, 3.0277788639068604, 3.3133888244628906, 2.7549784183502197, 3.1771960258483887, 3.0865466594696045, 3.1834800243377686, 3.1768810749053955, 3.2639052867889404, 3.0352447032928467, 2.9815778732299805, 3.124309778213501, 3.177079916000366]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.1177714541554451, 0.13260653614997864, 0.12675656378269196, -0.24507583677768707, -0.246900737285614, -0.0897337794303894, 0.14022688567638397, -0.16051070392131805, 0.09963518381118774, 0.11203807592391968, 0.11155984550714493, 0.07162865996360779, 0.10569895058870316, 0.12358136475086212, -1.1118444204330444, 0.1341821551322937]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08605830371379852, 0.04893108829855919, 0.04106215760111809, 0.06129073351621628, 0.08318360894918442, 0.03642219677567482, 0.053077805787324905, 0.08057713508605957, 0.016057370230555534, 0.062193602323532104, -0.18303091824054718, 0.059187740087509155, 0.0030528493225574493, -0.0034435472916811705, 0.02334798313677311, 0.026046717539429665]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06809765100479126, 0.042829498648643494, 0.09082464873790741, 0.055981360375881195, 0.09352230280637741, 0.10097495466470718, 0.054050445556640625, -0.14420904219150543, 0.06372370570898056, 0.08917957544326782, 0.0033817219082266092, 0.07791447639465332, -0.221403107047081, -0.008240378461778164, -0.06263117492198944, 0.12411051243543625]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21501286327838898, 0.14576557278633118, 0.12405329942703247, 0.14491507411003113, 0.044179368764162064, 0.054422035813331604, -0.09802907705307007, 0.1789124459028244, 0.1535959094762802, -0.4564428925514221, 0.03135772421956062, 0.15902094542980194, 0.0971822515130043, -0.29517093300819397, -0.0738953948020935, -0.047757331281900406]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07730656117200851, -0.03041025437414646, -0.06229478493332863, 0.14916537702083588, -0.066073939204216, -0.15259197354316711, 0.08915834128856659, 0.04791361093521118, 0.06675010919570923, 0.20215317606925964, -0.2838168740272522, 0.0288580060005188, 0.00917166005820036, -0.18338456749916077, 0.08228357881307602, 0.018404865637421608]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21034111082553864, 0.23568059504032135, 0.1391473114490509, 0.05115123465657234, -0.303587943315506, -0.06473639607429504, 0.09815433621406555, 0.006604752968996763, 0.16187146306037903, -0.06367936730384827, -0.4923287332057953, 0.046970829367637634, -0.3443301320075989, 0.2550615668296814, 0.15580594539642334, 0.1492944061756134]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3143431842327118, 0.33099618554115295, 0.18958106637001038, 0.2670678496360779, 0.26291966438293457, 0.24498452246189117, 0.1238241121172905, 0.15361452102661133, -0.3877122104167938, 0.39091697335243225, -0.08021388202905655, 0.3201059103012085, 0.09163074940443039, -0.0720999464392662, 0.2820092439651489, 0.17126238346099854]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.0497070737183094, 0.19973434507846832, 0.11919361352920532, 0.16054515540599823, -0.36226949095726013, 0.2073611468076706, 0.05777200311422348, 0.20938196778297424, -0.02590964175760746, -0.16893954575061798, 0.06248698756098747, -0.06658542901277542, 0.17270873486995697, 0.1091257780790329, -0.022377271205186844, 0.14685562252998352]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20889262855052948, 0.2791271507740021, -0.08737275749444962, 0.32261934876441956, 0.46291279792785645, 0.39847126603126526, 0.11496555805206299, -0.003362959483638406, 0.6127755641937256, -0.19731652736663818, 0.2552809417247772, 0.054693955928087234, 0.6704317331314087, -0.2036508470773697, -0.027804747223854065, 0.18994759023189545]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8638973236083984, 0.7488174438476562, 0.6167075634002686, 0.5712584257125854, 0.844741702079773, 0.44335323572158813, 1.1082738637924194, 1.0634263753890991, 0.09378907829523087, 0.02708466164767742, 0.43267548084259033, 0.7900319695472717, 0.7511466145515442, 0.8228611946105957, 1.1157974004745483, 0.7917457222938538]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1607599258422852, 1.1735254526138306, 0.659269392490387, 1.065496563911438, 0.9543720483779907, 0.3732430636882782, 0.9899334907531738, 1.4211195707321167, 0.7715091705322266, 0.2868027687072754, 1.1018377542495728, 1.1125447750091553, 0.6373951435089111, 1.0957130193710327, 0.49798282980918884, 1.0063434839248657]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.241977334022522, 1.599124550819397, 1.5319398641586304, 1.4014942646026611, 1.4325242042541504, 0.8500906825065613, 1.7470908164978027, 1.2282990217208862, 1.4839686155319214, 1.45145583152771, 1.2669578790664673, 1.1701574325561523, 1.1547094583511353, 1.3349066972732544, 1.209968090057373, 1.2005786895751953]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0527580976486206, 1.5098280906677246, 1.6492586135864258, 1.7939484119415283, 1.2552345991134644, 1.585891842842102, 1.2836246490478516, 1.5211032629013062, 1.352439045906067, 0.6492757797241211, 1.7025272846221924, 1.9512639045715332, 1.7615957260131836, 1.6795951128005981, 1.4271384477615356, 1.53513503074646]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0861568450927734, 3.86173939704895, 3.044724941253662, 2.661320209503174, 3.4899086952209473, 2.1026856899261475, 3.66127610206604, 3.159018039703369, 3.8263351917266846, 3.3881471157073975, 3.1920766830444336, 3.9391748905181885, 2.1425838470458984, 3.9607741832733154, 3.1604321002960205, 2.80462384223938]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.394205331802368, 2.3655574321746826, 2.8496851921081543, 2.430710554122925, 2.8024673461914062, 2.077241897583008, 1.5750603675842285, 2.0910563468933105, 2.634910821914673, 1.9930672645568848, 1.3478020429611206, 1.9441123008728027, 2.4348716735839844, 2.6755542755126953, 2.6411702632904053, 1.9973551034927368]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5759692192077637, 3.4163365364074707, 3.0574326515197754, 3.121488094329834, 3.041546106338501, 2.8491902351379395, 2.788804292678833, 2.0478763580322266, 3.2812082767486572, 2.949049234390259, 2.7745728492736816, 1.3744701147079468, 2.922670364379883, 2.768092155456543, 3.037949800491333, 2.0737900733947754]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9239501953125, 1.8388035297393799, 2.6405210494995117, 2.596210479736328, 2.630941152572632, 1.9925481081008911, 1.7705403566360474, 3.0882949829101562, 0.5609802007675171, 2.170078992843628, 0.9767049551010132, 2.4502086639404297, 2.751239776611328, 2.9270033836364746, 2.9580280780792236, 1.4274905920028687]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6812539100646973, 3.9480082988739014, 5.552434921264648, 4.940090656280518, 5.515644073486328, 5.6702046394348145, 5.127676963806152, 5.113065719604492, 6.151916027069092, 5.033326625823975, 5.766456127166748, 6.071109771728516, 4.818570613861084, 5.394114017486572, 4.302764415740967, 4.953139305114746]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.207283973693848, 4.552365779876709, 5.539353847503662, 5.169612407684326, 3.6973955631256104, 4.4583916664123535, 4.871847629547119, 5.2841081619262695, 5.364500999450684, 4.909137725830078, 4.26701021194458, 3.3435044288635254, 4.404440402984619, 3.3832199573516846, 4.912452697753906, 4.490233421325684]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.877988815307617, 5.840356349945068, 6.038858413696289, 5.624749660491943, 6.077489376068115, 4.8127336502075195, 5.687530040740967, 5.634376049041748, 5.300435543060303, 5.358953952789307, 6.218726634979248, 5.906368732452393, 4.953151226043701, 5.860518455505371, 6.0449724197387695, 5.8667097091674805]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.0014262199401855, 2.97407603263855, 2.812990427017212, 2.240290641784668, -0.4536290764808655, 3.0478460788726807, 2.722487688064575, 1.857042670249939, 2.7171273231506348, 3.471768617630005, 2.68984317779541, 3.2510316371917725, 2.860527753829956, 2.950734853744507, 2.3260409832000732, 1.7394613027572632]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.55219841003418, 6.823450565338135, 6.5372772216796875, 6.305225372314453, 6.463936805725098, 6.483128070831299, 6.747175216674805, 6.644039154052734, 6.64275598526001, 6.3305277824401855, 6.929074287414551, 6.259729385375977, 5.279971599578857, 7.207402229309082, 6.48711633682251, 6.44181489944458]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.012518405914307, 5.356663227081299, 4.965357780456543, 4.048529148101807, 4.902712345123291, 5.252654075622559, 4.6316680908203125, 4.783444404602051, 5.075245380401611, 4.849817276000977, 5.2759246826171875, 4.877804756164551, 5.148501396179199, 5.435760974884033, 4.601803779602051, 5.4773688316345215]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.154602527618408, 4.670968055725098, 3.3386175632476807, 4.200157165527344, 4.46134090423584, 4.081826686859131, 4.2457685470581055, 4.233651161193848, 4.365363597869873, 4.476905345916748, 4.059637546539307, 4.434229850769043, 3.675499200820923, 2.0230846405029297, 3.378615379333496, 4.0216569900512695]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.287309646606445, 6.237337112426758, 5.911568641662598, 6.082180500030518, 5.428510665893555, 5.848881721496582, 6.1758036613464355, 5.701320171356201, 5.342907905578613, 5.752135276794434, 6.434952259063721, 5.520387172698975, 5.769979000091553, 6.21114444732666, 6.226297378540039, 5.782752990722656]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8371031284332275, 3.673211097717285, 3.7841129302978516, 3.9496610164642334, 2.686497449874878, 3.8639702796936035, 3.9419174194335938, 3.6601216793060303, 3.993344306945801, 3.6151883602142334, 3.6994810104370117, 3.6241507530212402, 3.6650068759918213, 3.5992751121520996, 3.7098424434661865, 3.89658784866333]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.453671932220459, 3.5316102504730225, 3.4058315753936768, 3.5574886798858643, 3.3865201473236084, 3.627833843231201, 3.5768234729766846, 3.368237257003784, 3.6049585342407227, 3.3003087043762207, 3.6054258346557617, 3.449146270751953, 3.437487840652466, 2.7276997566223145, 3.6532957553863525, 3.571744680404663]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.185220718383789, 3.2112345695495605, 2.984877347946167, 2.8979830741882324, 2.8193137645721436, 3.0874266624450684, 3.1854379177093506, 3.2762253284454346, 3.033025026321411, 3.006572723388672, 2.2882328033447266, 2.9986069202423096, 3.1596529483795166, 3.064929962158203, 3.0813486576080322, 2.887836456298828]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.370912551879883, 4.488325595855713, 4.5618367195129395, 4.368181228637695, 4.489387512207031, 4.461645603179932, 4.215200901031494, 4.705862522125244, 4.490075588226318, 4.452798843383789, 4.466240882873535, 4.050394058227539, 4.194662570953369, 4.05190896987915, 4.571988582611084, 4.37059211730957]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.579056739807129, 8.908968925476074, 8.782249450683594, 8.465784072875977, 8.552263259887695, 8.265239715576172, 8.635169982910156, 8.53726863861084, 8.622880935668945, 8.487142562866211, 8.596033096313477, 8.231147766113281, 8.335384368896484, 8.996552467346191, 8.897934913635254, 8.50318717956543]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  78%|█████████████████████████████████████▋          | 157/200 [05:16<01:17,  1.80s/it]Layer: gate_30 - Captured router_logits: [4.888981819152832, 5.431784629821777, 5.385989189147949, 4.789045810699463, 5.1362104415893555, 5.086740016937256, 4.670306205749512, 5.136094570159912, 5.238790988922119, 4.927402019500732, 4.834606647491455, 4.729640007019043, 4.864303112030029, 4.979563236236572, 5.06751823425293, 4.832645893096924]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.231661081314087, 2.9995336532592773, 3.1049540042877197, 2.7625112533569336, 3.025961399078369, 3.3107216358184814, 2.754932403564453, 3.1757607460021973, 3.0856173038482666, 3.181793689727783, 3.1757524013519287, 3.2611889839172363, 3.0338306427001953, 2.979889154434204, 3.123363971710205, 3.17513108253479]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11679089069366455, 0.13258393108844757, 0.12687961757183075, -0.24614694714546204, -0.24946776032447815, -0.09238312393426895, 0.13992081582546234, -0.16986317932605743, 0.09723839908838272, 0.1125505343079567, 0.11132790893316269, 0.07098697125911713, 0.10451877117156982, 0.12337855249643326, -1.103543758392334, 0.13434693217277527]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0849548876285553, 0.04987826570868492, 0.03817733749747276, 0.06116935983300209, 0.08325007557868958, 0.0378725528717041, 0.05248919501900673, 0.07503732293844223, 0.01767001859843731, 0.061159245669841766, -0.18301311135292053, 0.06175444275140762, 0.006625079084187746, -0.0045226276852190495, 0.021269291639328003, 0.02646651305258274]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06959697604179382, 0.04712605103850365, 0.09001599997282028, 0.05388793349266052, 0.09017468243837357, 0.09963084757328033, 0.05465752258896828, -0.14438393712043762, 0.06595467031002045, 0.08339107036590576, 0.0020312871783971786, 0.07653281092643738, -0.22191686928272247, -0.004662604071199894, -0.061837874352931976, 0.12725327908992767]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21353763341903687, 0.14644619822502136, 0.12603996694087982, 0.13988114893436432, 0.04411138594150543, 0.053629156202077866, -0.09344374388456345, 0.1806308478116989, 0.1504494696855545, -0.4511142373085022, 0.03693755716085434, 0.1596045345067978, 0.08808636665344238, -0.28612416982650757, -0.07492990791797638, -0.045888036489486694]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07995457947254181, -0.028112858533859253, -0.06529926508665085, 0.14754603803157806, -0.05949908122420311, -0.14962518215179443, 0.08801277726888657, 0.046805281192064285, 0.06778102368116379, 0.20083022117614746, -0.27889370918273926, 0.027822379022836685, 0.010764299891889095, -0.18142922222614288, 0.08277358114719391, 0.018740743398666382]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.20971046388149261, 0.23675891757011414, 0.13974592089653015, 0.05212581902742386, -0.3017784357070923, -0.062473930418491364, 0.09646071493625641, 0.010579981841146946, 0.15938904881477356, -0.06305105984210968, -0.49404269456863403, 0.04705441743135452, -0.3411490023136139, 0.2548719048500061, 0.1526741087436676, 0.1489344984292984]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3130587935447693, 0.3316996395587921, 0.19222861528396606, 0.26464638113975525, 0.261471688747406, 0.24322505295276642, 0.12236901372671127, 0.14917027950286865, -0.3835027515888214, 0.391356885433197, -0.0776902511715889, 0.3196793794631958, 0.08842046558856964, -0.06986245512962341, 0.27610453963279724, 0.1672222763299942]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.04988399147987366, 0.20297642052173615, 0.11915691941976547, 0.1595400869846344, -0.3559010922908783, 0.2045639455318451, 0.05791009962558746, 0.2088695615530014, -0.027254359796643257, -0.16668552160263062, 0.0635559931397438, -0.0667901262640953, 0.17563921213150024, 0.10856232047080994, -0.018759531900286674, 0.14987893402576447]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2070540338754654, 0.27911177277565, -0.0872376412153244, 0.32101374864578247, 0.4572860598564148, 0.3986262083053589, 0.11353237181901932, -0.0012761392863467336, 0.6094123721122742, -0.19841402769088745, 0.2560761570930481, 0.05150672420859337, 0.6675617694854736, -0.19910135865211487, -0.023412460461258888, 0.18978455662727356]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.864388644695282, 0.7494404315948486, 0.6142063736915588, 0.5673614144325256, 0.8445671796798706, 0.4446457028388977, 1.1089776754379272, 1.0603599548339844, 0.09502577036619186, 0.03037988394498825, 0.43732497096061707, 0.7883121371269226, 0.7479334473609924, 0.8221859931945801, 1.112061858177185, 0.7912552952766418]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1579903364181519, 1.1709673404693604, 0.6606422662734985, 1.0660003423690796, 0.9531664848327637, 0.3700156509876251, 0.989739179611206, 1.4184658527374268, 0.7746861577033997, 0.2819974720478058, 1.1023969650268555, 1.1087336540222168, 0.6403676271438599, 1.0934871435165405, 0.5004240870475769, 1.0079259872436523]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2396941184997559, 1.5939918756484985, 1.5298945903778076, 1.3964636325836182, 1.426633358001709, 0.8532933592796326, 1.7440767288208008, 1.2255922555923462, 1.4821195602416992, 1.4463961124420166, 1.264749526977539, 1.1710270643234253, 1.156335711479187, 1.3362929821014404, 1.20857834815979, 1.199133276939392]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0535811185836792, 1.5094475746154785, 1.6451680660247803, 1.7895667552947998, 1.2573187351226807, 1.5832921266555786, 1.2809348106384277, 1.517661690711975, 1.3539544343948364, 0.6515191197395325, 1.6969225406646729, 1.9495625495910645, 1.7580000162124634, 1.6767019033432007, 1.4255914688110352, 1.5345211029052734]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0862960815429688, 3.859828233718872, 3.0467967987060547, 2.6642537117004395, 3.4893407821655273, 2.1071770191192627, 3.659360647201538, 3.155135154724121, 3.823972702026367, 3.3889646530151367, 3.1884944438934326, 3.937302589416504, 2.1451125144958496, 3.9572057723999023, 3.161712169647217, 2.8060195446014404]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.394374132156372, 2.3679986000061035, 2.8485422134399414, 2.4289889335632324, 2.804354429244995, 2.0767807960510254, 1.5748904943466187, 2.089251756668091, 2.632413148880005, 1.9947386980056763, 1.3468743562698364, 1.9469486474990845, 2.435922145843506, 2.6765153408050537, 2.638258695602417, 1.998327374458313]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5738563537597656, 3.411628007888794, 3.055838108062744, 3.1187753677368164, 3.039950370788574, 2.8473167419433594, 2.783851146697998, 2.0474040508270264, 3.278632164001465, 2.9476630687713623, 2.7733426094055176, 1.3753552436828613, 2.918774127960205, 2.7681634426116943, 3.034299612045288, 2.0702996253967285]
Running loglikelihood requests:  80%|██████████████████████████████████████▋         | 161/200 [05:24<01:09,  1.79s/it]Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.923357605934143, 1.8358370065689087, 2.6397929191589355, 2.5921683311462402, 2.628429412841797, 1.9911798238754272, 1.7668250799179077, 3.0820775032043457, 0.5637626647949219, 2.1662397384643555, 0.9748761057853699, 2.449871063232422, 2.751533269882202, 2.9220292568206787, 2.955737352371216, 1.4230492115020752]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6811740398406982, 3.9480152130126953, 5.549272060394287, 4.939759254455566, 5.511084079742432, 5.6682915687561035, 5.1255269050598145, 5.112483501434326, 6.1485490798950195, 5.0272216796875, 5.761299133300781, 6.065867900848389, 4.820006370544434, 5.390099048614502, 4.299644947052002, 4.947937965393066]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.209182262420654, 4.554052352905273, 5.535150051116943, 5.168863773345947, 3.6985037326812744, 4.457474231719971, 4.870736122131348, 5.284294128417969, 5.361840724945068, 4.909178733825684, 4.270988941192627, 3.344987392425537, 4.403255462646484, 3.384158134460449, 4.912979602813721, 4.491659641265869]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.8790411949157715, 5.840125560760498, 6.04008674621582, 5.62496280670166, 6.077121257781982, 4.817577362060547, 5.689999580383301, 5.636506080627441, 5.299785614013672, 5.36200475692749, 6.218667507171631, 5.9074811935424805, 4.9579997062683105, 5.8611907958984375, 6.0448527336120605, 5.867464065551758]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.002678155899048, 2.972262382507324, 2.8105952739715576, 2.2404801845550537, -0.4521316885948181, 3.0451653003692627, 2.7217774391174316, 1.8555985689163208, 2.7156670093536377, 3.468844175338745, 2.688844919204712, 3.247779369354248, 2.859621524810791, 2.9470527172088623, 2.3250317573547363, 1.738728642463684]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.557425498962402, 6.82658052444458, 6.542106628417969, 6.309921741485596, 6.468836307525635, 6.485498905181885, 6.750842571258545, 6.646617412567139, 6.645829677581787, 6.337474346160889, 6.932480335235596, 6.262168884277344, 5.286510467529297, 7.210638999938965, 6.4925713539123535, 6.445072174072266]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.0138421058654785, 5.357976913452148, 4.966637134552002, 4.051495552062988, 4.905307292938232, 5.2539873123168945, 4.6350998878479, 4.784163475036621, 5.0760498046875, 4.850762844085693, 5.276480197906494, 4.879204750061035, 5.149403095245361, 5.435880184173584, 4.604681491851807, 5.476580619812012]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.156128883361816, 4.671909332275391, 3.341799259185791, 4.202118396759033, 4.463110446929932, 4.083397388458252, 4.2454047203063965, 4.233891487121582, 4.367390155792236, 4.477765083312988, 4.060637950897217, 4.4354753494262695, 3.6783854961395264, 2.028221845626831, 3.3803789615631104, 4.023129940032959]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.288369655609131, 6.237858772277832, 5.913902282714844, 6.084138870239258, 5.430535793304443, 5.852198123931885, 6.177033424377441, 5.701301574707031, 5.344732761383057, 5.753316879272461, 6.434753894805908, 5.522963523864746, 5.772124290466309, 6.211821556091309, 6.226964950561523, 5.785602569580078]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.838475465774536, 3.6739635467529297, 3.7840731143951416, 3.950751781463623, 2.689131021499634, 3.8644027709960938, 3.942131280899048, 3.6614551544189453, 3.994658946990967, 3.6161532402038574, 3.7007925510406494, 3.62571382522583, 3.665692090988159, 3.600743293762207, 3.7099406719207764, 3.8985888957977295]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.45408296585083, 3.5318803787231445, 3.406735897064209, 3.557375431060791, 3.3851706981658936, 3.6277379989624023, 3.5760185718536377, 3.3675012588500977, 3.60536789894104, 3.300217866897583, 3.6040358543395996, 3.4485785961151123, 3.4374866485595703, 2.728041648864746, 3.653299331665039, 3.5719962120056152]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.183887004852295, 3.2095510959625244, 2.982497453689575, 2.897132635116577, 2.8182711601257324, 3.0857884883880615, 3.1837735176086426, 3.274299144744873, 3.0309810638427734, 3.0053482055664062, 2.2881994247436523, 2.997650146484375, 3.1581406593322754, 3.0641112327575684, 3.078643321990967, 2.8859612941741943]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.368361949920654, 4.485310077667236, 4.558448314666748, 4.36490535736084, 4.484193801879883, 4.458375453948975, 4.211397647857666, 4.702068328857422, 4.486382961273193, 4.4494171142578125, 4.462027549743652, 4.047694683074951, 4.189725875854492, 4.048184871673584, 4.5667033195495605, 4.367103576660156]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.570201873779297, 8.900538444519043, 8.77324104309082, 8.456084251403809, 8.541938781738281, 8.255636215209961, 8.62501335144043, 8.527314186096191, 8.612828254699707, 8.478662490844727, 8.585897445678711, 8.223343849182129, 8.328873634338379, 8.987591743469238, 8.888086318969727, 8.49364185333252]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.884021759033203, 5.426637649536133, 5.380151271820068, 4.785798072814941, 5.131433486938477, 5.079006671905518, 4.6658172607421875, 5.130657196044922, 5.2348551750183105, 4.923215866088867, 4.828614711761475, 4.726174831390381, 4.860053062438965, 4.972191333770752, 5.062371730804443, 4.828474521636963]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2289257049560547, 2.996288537979126, 3.102466583251953, 2.756667375564575, 3.023515462875366, 3.3077144622802734, 2.7498412132263184, 3.1720049381256104, 3.0818042755126953, 3.1777470111846924, 3.1709790229797363, 3.259549617767334, 3.0301547050476074, 2.9772541522979736, 3.120095729827881, 3.172046422958374]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.1211157739162445, 0.1355958878993988, 0.1287623792886734, -0.23541010916233063, -0.23725475370883942, -0.09767064452171326, 0.14382977783679962, -0.17297419905662537, 0.103155717253685, 0.11540072411298752, 0.11486352980136871, 0.07519308477640152, 0.10709501802921295, 0.1263536810874939, -1.094944953918457, 0.1373925656080246]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08934012800455093, 0.04993593320250511, 0.04032331332564354, 0.06176867336034775, 0.08540485054254532, 0.04265712574124336, 0.050878845155239105, 0.07565899193286896, 0.014499637298285961, 0.06154104322195053, -0.180023655295372, 0.0591450072824955, 0.0047153932973742485, 0.0003857049741782248, 0.017049921676516533, 0.024640893563628197]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06852567195892334, 0.04458313807845116, 0.09468135982751846, 0.05617114529013634, 0.09119417518377304, 0.10262500494718552, 0.0593610517680645, -0.14523030817508698, 0.060712143778800964, 0.0805271714925766, 0.00060839211801067, 0.07726031541824341, -0.21465852856636047, -0.008099231868982315, -0.06249571591615677, 0.12701354920864105]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.2133539915084839, 0.14728735387325287, 0.12639139592647552, 0.1472192257642746, 0.04452439025044441, 0.053614359349012375, -0.08921381086111069, 0.17969292402267456, 0.15135157108306885, -0.44162502884864807, 0.023265892639756203, 0.16324229538440704, 0.08681318163871765, -0.2744937837123871, -0.09046967327594757, -0.046938057988882065]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07918855547904968, -0.03112855553627014, -0.06365934014320374, 0.1396784633398056, -0.07096672058105469, -0.1496741622686386, 0.09228197485208511, 0.05140063539147377, 0.07055383920669556, 0.202601820230484, -0.2716217339038849, 0.029425762593746185, 0.007753124926239252, -0.17685061693191528, 0.07888155430555344, 0.02461702562868595]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21103569865226746, 0.2379329949617386, 0.14139673113822937, 0.04943428561091423, -0.2976924479007721, -0.059906527400016785, 0.09317094087600708, 0.01166604645550251, 0.15822260081768036, -0.06081187352538109, -0.49733665585517883, 0.04817122593522072, -0.33640554547309875, 0.2522087097167969, 0.15387582778930664, 0.14961518347263336]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3123050630092621, 0.3342432975769043, 0.18914683163166046, 0.264719158411026, 0.25870802998542786, 0.24087294936180115, 0.12521465122699738, 0.1509365439414978, -0.38138532638549805, 0.38824787735939026, -0.07067293673753738, 0.31864672899246216, 0.08482958376407623, -0.061422381550073624, 0.27252256870269775, 0.16770713031291962]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05110928788781166, 0.20306961238384247, 0.11897528171539307, 0.15891557931900024, -0.35178759694099426, 0.2042297124862671, 0.05627185106277466, 0.21442699432373047, -0.031056931242346764, -0.16785036027431488, 0.06572970747947693, -0.06821603327989578, 0.17774738371372223, 0.10423960536718369, -0.013224306516349316, 0.15128234028816223]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20705358684062958, 0.2780015468597412, -0.08225910365581512, 0.3195328116416931, 0.4560307264328003, 0.3982764780521393, 0.11294732987880707, 0.0004016389138996601, 0.6104506254196167, -0.20079420506954193, 0.2533459961414337, 0.053957823663949966, 0.6639158725738525, -0.1955120861530304, -0.018762808293104172, 0.1884930580854416]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8639601469039917, 0.7512296438217163, 0.6165774464607239, 0.5628461837768555, 0.8458771705627441, 0.44848546385765076, 1.1103432178497314, 1.058004379272461, 0.09353728592395782, 0.033994726836681366, 0.440917044878006, 0.7871046662330627, 0.7521317005157471, 0.8303722143173218, 1.1102169752120972, 0.7885019183158875]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1564116477966309, 1.1667495965957642, 0.6593868136405945, 1.0662999153137207, 0.9542004466056824, 0.37338003516197205, 0.9887405037879944, 1.4161697626113892, 0.7776849865913391, 0.29226022958755493, 1.0966062545776367, 1.1088697910308838, 0.6433249711990356, 1.0954760313034058, 0.494687020778656, 1.0103580951690674]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2392096519470215, 1.593393325805664, 1.5282148122787476, 1.4002845287322998, 1.4275474548339844, 0.8604886531829834, 1.7453871965408325, 1.2303333282470703, 1.4794155359268188, 1.4420020580291748, 1.2676697969436646, 1.1736152172088623, 1.1610279083251953, 1.3400541543960571, 1.2150107622146606, 1.2022160291671753]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0577505826950073, 1.5126034021377563, 1.6443978548049927, 1.790696144104004, 1.2628226280212402, 1.5841292142868042, 1.280377745628357, 1.5165599584579468, 1.357886552810669, 0.6553483009338379, 1.6940193176269531, 1.9482402801513672, 1.7615066766738892, 1.6778994798660278, 1.4270144701004028, 1.5343530178070068]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0899901390075684, 3.8552651405334473, 3.0428709983825684, 2.6673920154571533, 3.485786199569702, 2.113023519515991, 3.657463312149048, 3.1521425247192383, 3.8220431804656982, 3.386343479156494, 3.1873230934143066, 3.937516689300537, 2.14884090423584, 3.9613540172576904, 3.158743143081665, 2.807774543762207]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3905718326568604, 2.3654563426971436, 2.845716714859009, 2.4277455806732178, 2.7997193336486816, 2.074849843978882, 1.5791648626327515, 2.0836238861083984, 2.630927562713623, 1.9937913417816162, 1.3505916595458984, 1.944636583328247, 2.4330153465270996, 2.6719164848327637, 2.636964797973633, 1.9982162714004517]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.577221155166626, 3.4155051708221436, 3.0571415424346924, 3.1171014308929443, 3.0411128997802734, 2.8491666316986084, 2.782550096511841, 2.0453224182128906, 3.2798569202423096, 2.9473330974578857, 2.7762362957000732, 1.3787461519241333, 2.9172849655151367, 2.7686967849731445, 3.035324811935425, 2.0735692977905273]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9264856576919556, 1.8391765356063843, 2.6405205726623535, 2.5932273864746094, 2.63010311126709, 1.9940751791000366, 1.7711752653121948, 3.084745168685913, 0.5663809180259705, 2.1677541732788086, 0.9792661666870117, 2.4514143466949463, 2.751938819885254, 2.9234237670898438, 2.9577102661132812, 1.4283342361450195]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.690633773803711, 3.9535837173461914, 5.554964542388916, 4.945960521697998, 5.516562461853027, 5.673107147216797, 5.130496025085449, 5.1194071769714355, 6.153776168823242, 5.035609722137451, 5.7687506675720215, 6.074038028717041, 4.824980735778809, 5.396628379821777, 4.309408187866211, 4.956797122955322]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.211380958557129, 4.557013034820557, 5.538558483123779, 5.170097351074219, 3.7045695781707764, 4.463720321655273, 4.874965190887451, 5.286874771118164, 5.3660149574279785, 4.912944793701172, 4.274505138397217, 3.3505685329437256, 4.40929651260376, 3.390188455581665, 4.916738510131836, 4.494383811950684]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.8840436935424805, 5.844691276550293, 6.044503211975098, 5.627678394317627, 6.080398082733154, 4.823426723480225, 5.694483757019043, 5.6408891677856445, 5.303513526916504, 5.364455699920654, 6.221629619598389, 5.910858631134033, 4.962512016296387, 5.865074634552002, 6.047956943511963, 5.871230602264404]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Running loglikelihood requests:  82%|███████████████████████████████████████▌        | 165/200 [05:30<01:01,  1.77s/it]Layer: gate_20 - Captured router_logits: [3.00549054145813, 2.972738027572632, 2.8129541873931885, 2.239623546600342, -0.4468154013156891, 3.0438356399536133, 2.723536252975464, 1.860724925994873, 2.715123414993286, 3.47050142288208, 2.692517042160034, 3.249645709991455, 2.860032081604004, 2.947730541229248, 2.3267807960510254, 1.7420547008514404]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.5568389892578125, 6.827277660369873, 6.541390895843506, 6.310346603393555, 6.469207286834717, 6.485533237457275, 6.751823902130127, 6.647854328155518, 6.647975921630859, 6.336662292480469, 6.932607173919678, 6.263816833496094, 5.288294315338135, 7.212303638458252, 6.494256496429443, 6.445849418640137]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.014617443084717, 5.3575119972229, 4.967471599578857, 4.052450180053711, 4.905257225036621, 5.254319190979004, 4.635451316833496, 4.7856292724609375, 5.077142238616943, 4.851725101470947, 5.277215957641602, 4.880283355712891, 5.150695323944092, 5.436542987823486, 4.605616569519043, 5.477483749389648]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.155939102172852, 4.672428131103516, 3.341715097427368, 4.20170783996582, 4.462956428527832, 4.082961082458496, 4.246551990509033, 4.234792709350586, 4.367177963256836, 4.477445602416992, 4.0603790283203125, 4.433591365814209, 3.6778364181518555, 2.027754068374634, 3.3807308673858643, 4.022213935852051]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.288931846618652, 6.238158226013184, 5.914523124694824, 6.084893226623535, 5.430285930633545, 5.852536678314209, 6.177109241485596, 5.70244026184082, 5.345961570739746, 5.753544330596924, 6.436370849609375, 5.522765636444092, 5.772017002105713, 6.211809158325195, 6.227798938751221, 5.784942150115967]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8386948108673096, 3.673563241958618, 3.782498359680176, 3.950629949569702, 2.6887474060058594, 3.8643007278442383, 3.943035840988159, 3.6612796783447266, 3.9937472343444824, 3.6159164905548096, 3.699969530105591, 3.6250393390655518, 3.6652474403381348, 3.6003546714782715, 3.710862636566162, 3.8976993560791016]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.454679012298584, 3.5325093269348145, 3.4063875675201416, 3.5580999851226807, 3.3854305744171143, 3.6277685165405273, 3.5772476196289062, 3.367729902267456, 3.605992078781128, 3.301243782043457, 3.604633092880249, 3.44903564453125, 3.437687635421753, 2.7281012535095215, 3.654125452041626, 3.571718454360962]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1844747066497803, 3.2109715938568115, 2.9837729930877686, 2.897552490234375, 2.819357395172119, 3.0864720344543457, 3.185127019882202, 3.2749054431915283, 3.03214955329895, 3.005859851837158, 2.289057970046997, 2.998347043991089, 3.1581614017486572, 3.0637059211730957, 3.079364061355591, 2.8871846199035645]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.369649887084961, 4.486687183380127, 4.559467792510986, 4.3666229248046875, 4.484993934631348, 4.460546016693115, 4.2132978439331055, 4.704878807067871, 4.488157272338867, 4.451326370239258, 4.463792324066162, 4.049070358276367, 4.191303253173828, 4.0504326820373535, 4.568260192871094, 4.36851167678833]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.57607364654541, 8.906293869018555, 8.77838134765625, 8.462075233459473, 8.547773361206055, 8.261878967285156, 8.630997657775879, 8.531878471374512, 8.618620872497559, 8.48361587524414, 8.5911283493042, 8.227621078491211, 8.332616806030273, 8.99295425415039, 8.893492698669434, 8.500580787658691]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.888535022735596, 5.431485652923584, 5.385088920593262, 4.791654109954834, 5.134522438049316, 5.082060813903809, 4.671533107757568, 5.13598108291626, 5.238571643829346, 4.927840232849121, 4.833241939544678, 4.731878280639648, 4.865060806274414, 4.97613525390625, 5.0676069259643555, 4.832230091094971]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2321324348449707, 2.999011993408203, 3.1054883003234863, 2.759188652038574, 3.026850700378418, 3.311633348464966, 2.7528088092803955, 3.1751978397369385, 3.0849053859710693, 3.1821210384368896, 3.17443585395813, 3.2630765438079834, 3.033766031265259, 2.979928731918335, 3.122170925140381, 3.1757540702819824]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.1195315271615982, 0.13486123085021973, 0.12862935662269592, -0.2454661875963211, -0.24471431970596313, -0.09323438256978989, 0.1427771896123886, -0.1721092164516449, 0.10083005577325821, 0.11447969824075699, 0.11362532526254654, 0.07403642684221268, 0.10624738782644272, 0.12520618736743927, -1.1141009330749512, 0.1358899176120758]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08785334974527359, 0.049863144755363464, 0.04075467959046364, 0.06136435270309448, 0.0839446485042572, 0.037864819169044495, 0.05211108550429344, 0.07702057808637619, 0.016023823991417885, 0.06193448603153229, -0.1814885288476944, 0.060368865728378296, 0.0032745953649282455, -0.0029584611766040325, 0.021632438525557518, 0.02505630813539028]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06922268867492676, 0.04409186542034149, 0.0914033055305481, 0.0541936419904232, 0.09254154562950134, 0.1012437492609024, 0.05678243935108185, -0.14440032839775085, 0.06355906277894974, 0.08552110195159912, 0.0016780858859419823, 0.07702960819005966, -0.21843716502189636, -0.00628625275567174, -0.062158726155757904, 0.12405084818601608]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21381039917469025, 0.14645904302597046, 0.12456241995096207, 0.1449389010667801, 0.04229966923594475, 0.05249053239822388, -0.09224753081798553, 0.17974407970905304, 0.15138298273086548, -0.4487822353839874, 0.029620718210935593, 0.16116027534008026, 0.09021454304456711, -0.2867523729801178, -0.07926913350820541, -0.04570093750953674]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07986123859882355, -0.030478285625576973, -0.06356512755155563, 0.14585618674755096, -0.06449731439352036, -0.1508694589138031, 0.09049536287784576, 0.048687271773815155, 0.06886234134435654, 0.2020476907491684, -0.2781044542789459, 0.028461551293730736, 0.00711103156208992, -0.18126505613327026, 0.08042914420366287, 0.020290428772568703]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.21051983535289764, 0.23644192516803741, 0.13963551819324493, 0.05170913413167, -0.3014068603515625, -0.06290610134601593, 0.09471787512302399, 0.009727852419018745, 0.15968427062034607, -0.061772722750902176, -0.49639132618904114, 0.04727508872747421, -0.3424679934978485, 0.25434499979019165, 0.15530329942703247, 0.14947333931922913]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.31263935565948486, 0.332572340965271, 0.18961256742477417, 0.26651322841644287, 0.2606836259365082, 0.2426057755947113, 0.12576048076152802, 0.15311388671398163, -0.38355958461761475, 0.3906860947608948, -0.07557827979326248, 0.320322722196579, 0.08917336910963058, -0.0678466185927391, 0.2767898142337799, 0.16870908439159393]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.04957720637321472, 0.202344611287117, 0.12016621977090836, 0.1603354662656784, -0.35807284712791443, 0.20583941042423248, 0.05711279436945915, 0.21259576082229614, -0.027205688878893852, -0.16765940189361572, 0.06384797394275665, -0.06658187508583069, 0.17563727498054504, 0.10829248279333115, -0.020145297050476074, 0.1473264843225479]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20778077840805054, 0.27861282229423523, -0.08267588168382645, 0.32131943106651306, 0.45846250653266907, 0.3983021378517151, 0.11377362906932831, -0.0020157250110059977, 0.6099897623062134, -0.19941140711307526, 0.25447067618370056, 0.051790524274110794, 0.6672144532203674, -0.1985640674829483, -0.02276221290230751, 0.18936870992183685]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8645363450050354, 0.7512229681015015, 0.6155005097389221, 0.5662243366241455, 0.8458819389343262, 0.44673952460289, 1.109519124031067, 1.0591274499893188, 0.09508635848760605, 0.03283966705203056, 0.43774428963661194, 0.7872725129127502, 0.751747190952301, 0.8274853825569153, 1.1116217374801636, 0.790418267250061]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1589486598968506, 1.1693679094314575, 0.6596462726593018, 1.0658255815505981, 0.9537976384162903, 0.37050291895866394, 0.9885322451591492, 1.4172823429107666, 0.7755786776542664, 0.2883807420730591, 1.0998479127883911, 1.1095019578933716, 0.6439816355705261, 1.0945731401443481, 0.4969996213912964, 1.0094674825668335]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2400670051574707, 1.5937267541885376, 1.5300801992416382, 1.3987573385238647, 1.4289008378982544, 0.8557412624359131, 1.7442127466201782, 1.2302111387252808, 1.4807968139648438, 1.445871114730835, 1.2674298286437988, 1.1714754104614258, 1.157859206199646, 1.3386752605438232, 1.2141835689544678, 1.2020994424819946]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.055679440498352, 1.5103936195373535, 1.6447358131408691, 1.7908986806869507, 1.2606862783432007, 1.5838905572891235, 1.2799216508865356, 1.5160595178604126, 1.3544589281082153, 0.6534612774848938, 1.697698950767517, 1.9474313259124756, 1.7606884241104126, 1.6775530576705933, 1.4268944263458252, 1.5341912508010864]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0866031646728516, 3.8555474281311035, 3.044415235519409, 2.666024684906006, 3.485987663269043, 2.1083478927612305, 3.656736373901367, 3.151975154876709, 3.821347951889038, 3.386101722717285, 3.187645435333252, 3.9367282390594482, 2.147623062133789, 3.9594547748565674, 3.1573052406311035, 2.8067171573638916]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3924591541290283, 2.3647496700286865, 2.8473572731018066, 2.42861270904541, 2.8005363941192627, 2.0776102542877197, 1.5793770551681519, 2.085357904434204, 2.6317408084869385, 1.9944000244140625, 1.3505562543869019, 1.9453518390655518, 2.434119939804077, 2.6733734607696533, 2.6378135681152344, 1.998574137687683]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5768933296203613, 3.4138553142547607, 3.056243419647217, 3.1186089515686035, 3.0420022010803223, 2.8491199016571045, 2.784085512161255, 2.0459225177764893, 3.2787563800811768, 2.947366952896118, 2.7759459018707275, 1.3782637119293213, 2.9194016456604004, 2.7673158645629883, 3.0355279445648193, 2.0735418796539307]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9252156019210815, 1.8385424613952637, 2.639475107192993, 2.5938446521759033, 2.6289002895355225, 1.9925991296768188, 1.7680186033248901, 3.0843987464904785, 0.5648353099822998, 2.1667592525482178, 0.9780486226081848, 2.4500341415405273, 2.751122236251831, 2.924112319946289, 2.9567763805389404, 1.4266141653060913]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6842825412750244, 3.9491934776306152, 5.550143718719482, 4.940183162689209, 5.5138115882873535, 5.668008327484131, 5.125587463378906, 5.113758563995361, 6.1493306159973145, 5.029619216918945, 5.7632551193237305, 6.069029331207275, 4.81930685043335, 5.391253471374512, 4.3024396896362305, 4.950983047485352]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.2077860832214355, 4.553411960601807, 5.5357794761657715, 5.16840124130249, 3.6997241973876953, 4.458777904510498, 4.87017822265625, 5.283165454864502, 5.362619876861572, 4.9089860916137695, 4.270011901855469, 3.34472918510437, 4.405076026916504, 3.385357141494751, 4.912423133850098, 4.490736961364746]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.879227161407471, 5.840888023376465, 6.040509223937988, 5.6245927810668945, 6.077920913696289, 4.8174943923950195, 5.690351486206055, 5.6360368728637695, 5.299983501434326, 5.361016750335693, 6.219193935394287, 5.907616138458252, 4.95785665512085, 5.861717224121094, 6.045195579528809, 5.867345809936523]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.002938985824585, 2.9727349281311035, 2.811824321746826, 2.239246368408203, -0.4520191550254822, 3.0459864139556885, 2.7226362228393555, 1.857886791229248, 2.7159645557403564, 3.470391035079956, 2.6907217502593994, 3.2496843338012695, 2.8600223064422607, 2.9484305381774902, 2.32476806640625, 1.7397387027740479]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.554260730743408, 6.824977874755859, 6.538337707519531, 6.306816101074219, 6.466207027435303, 6.483187675476074, 6.7481770515441895, 6.645514965057373, 6.643190383911133, 6.333164691925049, 6.930430889129639, 6.260673522949219, 5.283112049102783, 7.209429740905762, 6.490060806274414, 6.443221569061279]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.013723850250244, 5.357896327972412, 4.966305255889893, 4.050992488861084, 4.903904438018799, 5.253940582275391, 4.633717060089111, 4.785048007965088, 5.076226711273193, 4.850304126739502, 5.276974678039551, 4.879324913024902, 5.149757385253906, 5.43627405166626, 4.603947162628174, 5.476858139038086]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.156822204589844, 4.671457290649414, 3.3401527404785156, 4.201634883880615, 4.4632768630981445, 4.082610607147217, 4.2462382316589355, 4.233946800231934, 4.3669281005859375, 4.477884769439697, 4.060110092163086, 4.433768272399902, 3.6769964694976807, 2.0260305404663086, 3.3799245357513428, 4.021820545196533]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  84%|████████████████████████████████████████▌       | 169/200 [05:37<00:54,  1.77s/it]Layer: gate_24 - Captured router_logits: [6.288295745849609, 6.23722505569458, 5.912528038024902, 6.083251953125, 5.428439140319824, 5.850400924682617, 6.175671100616455, 5.700442790985107, 5.343513011932373, 5.7519755363464355, 6.434597492218018, 5.5214152336120605, 5.770434379577637, 6.2114129066467285, 6.226236343383789, 5.78331184387207]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.83786678314209, 3.672883987426758, 3.7826008796691895, 3.9502248764038086, 2.6876375675201416, 3.8647994995117188, 3.942305564880371, 3.6609511375427246, 3.9934628009796143, 3.6156368255615234, 3.699820041656494, 3.624666213989258, 3.6651201248168945, 3.599635124206543, 3.7100491523742676, 3.897306442260742]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.453385591506958, 3.5321743488311768, 3.405653238296509, 3.557175636291504, 3.3848841190338135, 3.6279988288879395, 3.5760650634765625, 3.3676071166992188, 3.605114698410034, 3.3005518913269043, 3.6049234867095947, 3.4484941959381104, 3.4366953372955322, 2.7273905277252197, 3.653841257095337, 3.5719234943389893]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1853301525115967, 3.2112860679626465, 2.983835458755493, 2.8979127407073975, 2.8192460536956787, 3.0873219966888428, 3.185542106628418, 3.2757489681243896, 3.032243013381958, 3.0062644481658936, 2.288623571395874, 2.9986770153045654, 3.1594526767730713, 3.064607620239258, 3.079803466796875, 2.887641429901123]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.369543552398682, 4.486896991729736, 4.559852123260498, 4.366241931915283, 4.486633777618408, 4.460620403289795, 4.213274955749512, 4.704620838165283, 4.488337516784668, 4.450900077819824, 4.463949203491211, 4.048882961273193, 4.191946506500244, 4.05005407333374, 4.568869590759277, 4.3687825202941895]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.575922012329102, 8.906883239746094, 8.778943061828613, 8.461925506591797, 8.54770565032959, 8.26183795928955, 8.631837844848633, 8.533072471618652, 8.618904113769531, 8.483673095703125, 8.591869354248047, 8.228362083435059, 8.3341703414917, 8.992887496948242, 8.894475936889648, 8.500795364379883]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.8873748779296875, 5.429959774017334, 5.383890628814697, 4.787856578826904, 5.133879661560059, 5.082722187042236, 4.668972015380859, 5.1336188316345215, 5.236373424530029, 4.925736427307129, 4.831911087036133, 4.728212356567383, 4.8622145652771, 4.975793361663818, 5.065319538116455, 4.831651210784912]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2305748462677, 2.9980881214141846, 3.104287624359131, 2.7598764896392822, 3.025402307510376, 3.30968976020813, 2.7524168491363525, 3.1733951568603516, 3.0836877822875977, 3.1794939041137695, 3.1740365028381348, 3.260563850402832, 3.032186985015869, 2.9779622554779053, 3.1216399669647217, 3.173346757888794]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11900945007801056, 0.13461750745773315, 0.12796202301979065, -0.24618230760097504, -0.2511773705482483, -0.08821295201778412, 0.14117488265037537, -0.17366506159305573, 0.09854408353567123, 0.11399632692337036, 0.11344481259584427, 0.07127613574266434, 0.1059630885720253, 0.1258985698223114, -1.118328332901001, 0.1362425684928894]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08676190674304962, 0.049971938133239746, 0.041449517011642456, 0.06290993839502335, 0.08375126123428345, 0.03819624334573746, 0.05364760383963585, 0.07529538124799728, 0.01671130396425724, 0.06064675748348236, -0.1814306378364563, 0.06318473070859909, 0.003251215210184455, -0.0013034066651016474, 0.0213477723300457, 0.026719825342297554]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0703238919377327, 0.04396699741482735, 0.09141940623521805, 0.053838182240724564, 0.09502240270376205, 0.10094977170228958, 0.05276303365826607, -0.14406819641590118, 0.06515847891569138, 0.08523368090391159, 0.0017042991239577532, 0.07783589512109756, -0.22055475413799286, -0.0053649828769266605, -0.062568299472332, 0.12444797158241272]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21494106948375702, 0.14727529883384705, 0.126449316740036, 0.1434975266456604, 0.04260527342557907, 0.05423232913017273, -0.09699147939682007, 0.178189218044281, 0.15261705219745636, -0.4507761299610138, 0.034311458468437195, 0.16320829093456268, 0.08916764706373215, -0.29208943247795105, -0.07408055663108826, -0.04503002390265465]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0798693299293518, -0.029953401535749435, -0.065980926156044, 0.14758995175361633, -0.0612613819539547, -0.1501917690038681, 0.0866101011633873, 0.04823678359389305, 0.06576993316411972, 0.20097076892852783, -0.27956780791282654, 0.02834477089345455, 0.011397921480238438, -0.17740482091903687, 0.0839478150010109, 0.0156970527023077]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.20977270603179932, 0.23717623949050903, 0.1409052610397339, 0.05211704596877098, -0.3028685748577118, -0.06282994151115417, 0.09623117744922638, 0.008975043892860413, 0.160508394241333, -0.06226753070950508, -0.4960438907146454, 0.04739478975534439, -0.34235647320747375, 0.25481781363487244, 0.15198180079460144, 0.1504155993461609]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3147173821926117, 0.33064767718315125, 0.19236114621162415, 0.26529961824417114, 0.26258501410484314, 0.24332678318023682, 0.12339983880519867, 0.15172307193279266, -0.3832007944583893, 0.3927267789840698, -0.07892515510320663, 0.3206615746021271, 0.08930332958698273, -0.0717424601316452, 0.27618804574012756, 0.1669030338525772]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.049508944153785706, 0.20265960693359375, 0.11998254805803299, 0.1617092490196228, -0.3573986291885376, 0.2062256783246994, 0.058628614991903305, 0.21083447337150574, -0.026397284120321274, -0.16629734635353088, 0.0626952052116394, -0.06641557067632675, 0.1761089414358139, 0.10998886823654175, -0.021288752555847168, 0.1475500911474228]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2079695463180542, 0.27960389852523804, -0.08620534837245941, 0.32116273045539856, 0.4580419063568115, 0.39877399802207947, 0.11302592605352402, -2.339271486562211e-05, 0.6103145480155945, -0.19914494454860687, 0.2562271058559418, 0.05161716416478157, 0.6671883463859558, -0.1990254670381546, -0.022192291915416718, 0.19021882116794586]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8647160530090332, 0.7506007552146912, 0.6149364709854126, 0.5671039819717407, 0.8449028134346008, 0.44612476229667664, 1.1096049547195435, 1.0582727193832397, 0.09551132470369339, 0.032391153275966644, 0.4379670023918152, 0.7874321341514587, 0.7499926686286926, 0.8255952596664429, 1.111545443534851, 0.7918375134468079]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1594594717025757, 1.1701987981796265, 0.6608183979988098, 1.0672153234481812, 0.952964186668396, 0.3666851222515106, 0.9893513321876526, 1.4182403087615967, 0.7762869000434875, 0.2848563492298126, 1.1026149988174438, 1.1089606285095215, 0.6417672634124756, 1.0950647592544556, 0.4989883005619049, 1.0096375942230225]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2411249876022339, 1.5944116115570068, 1.5314140319824219, 1.3965768814086914, 1.4297420978546143, 0.8579649925231934, 1.7464613914489746, 1.228493094444275, 1.4826031923294067, 1.4479106664657593, 1.2680543661117554, 1.173426866531372, 1.1625008583068848, 1.3393065929412842, 1.2130628824234009, 1.2021492719650269]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0543618202209473, 1.5102072954177856, 1.6457734107971191, 1.7899491786956787, 1.2575267553329468, 1.584172010421753, 1.2795515060424805, 1.5164899826049805, 1.354557991027832, 0.6550032496452332, 1.697770357131958, 1.9485948085784912, 1.759588599205017, 1.6791245937347412, 1.4264346361160278, 1.534799337387085]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0871076583862305, 3.857260227203369, 3.047372341156006, 2.664539337158203, 3.4893438816070557, 2.1101646423339844, 3.656848907470703, 3.1546242237091064, 3.8225436210632324, 3.3893704414367676, 3.187227249145508, 3.9382972717285156, 2.146000385284424, 3.9591903686523438, 3.1619911193847656, 2.80544114112854]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3943722248077393, 2.366105794906616, 2.8471839427948, 2.428842544555664, 2.801508665084839, 2.0777454376220703, 1.5775362253189087, 2.0868802070617676, 2.6316590309143066, 1.9958699941635132, 1.3512624502182007, 1.9451645612716675, 2.4363818168640137, 2.6753664016723633, 2.638054370880127, 1.9980380535125732]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5733304023742676, 3.41176700592041, 3.0555458068847656, 3.118075132369995, 3.0386013984680176, 2.8474225997924805, 2.7828807830810547, 2.046311855316162, 3.27811598777771, 2.9467639923095703, 2.7746458053588867, 1.3760305643081665, 2.9182276725769043, 2.76750111579895, 3.0335066318511963, 2.070507287979126]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9247777462005615, 1.8377255201339722, 2.639606237411499, 2.5929081439971924, 2.6292762756347656, 1.9921005964279175, 1.7667832374572754, 3.0826492309570312, 0.5638382434844971, 2.1689646244049072, 0.9758920669555664, 2.448951244354248, 2.750504732131958, 2.92214035987854, 2.958000659942627, 1.4251192808151245]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.685882806777954, 3.9506521224975586, 5.5517778396606445, 4.941420555114746, 5.514140605926514, 5.671241283416748, 5.1296491622924805, 5.115057945251465, 6.152674674987793, 5.028012752532959, 5.764848232269287, 6.069485187530518, 4.82437801361084, 5.392370223999023, 4.303083896636963, 4.951063632965088]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.209893226623535, 4.555818557739258, 5.534030914306641, 5.17135763168335, 3.7003166675567627, 4.457938194274902, 4.872791767120361, 5.283810138702393, 5.364589214324951, 4.9109416007995605, 4.272559642791748, 3.3469254970550537, 4.406357765197754, 3.3871195316314697, 4.915501594543457, 4.4922990798950195]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.88288688659668, 5.843663692474365, 6.042959213256836, 5.627573013305664, 6.079905986785889, 4.820138454437256, 5.692967414855957, 5.638892650604248, 5.302146911621094, 5.365720748901367, 6.221995830535889, 5.910107612609863, 4.962065696716309, 5.863972187042236, 6.047737121582031, 5.8695878982543945]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.0030055046081543, 2.9715094566345215, 2.81113338470459, 2.2397663593292236, -0.45261433720588684, 3.045789957046509, 2.7212326526641846, 1.856732964515686, 2.7160391807556152, 3.4684462547302246, 2.690340280532837, 3.248619556427002, 2.858625888824463, 2.945964813232422, 2.3230836391448975, 1.7389214038848877]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.558896541595459, 6.829497337341309, 6.5431647300720215, 6.312436103820801, 6.4718499183654785, 6.487732887268066, 6.75337028503418, 6.649163722991943, 6.648067951202393, 6.338961601257324, 6.934924125671387, 6.266801834106445, 5.288931369781494, 7.213254928588867, 6.495406150817871, 6.448000431060791]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.015913009643555, 5.3597822189331055, 4.96815299987793, 4.054103374481201, 4.907345771789551, 5.255742073059082, 4.636589050292969, 4.786443710327148, 5.078160285949707, 4.852142333984375, 5.278939723968506, 4.881503582000732, 5.151199817657471, 5.436515808105469, 4.606438159942627, 5.477113723754883]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.156297206878662, 4.671640872955322, 3.340960741043091, 4.200870513916016, 4.4629411697387695, 4.083229064941406, 4.245228290557861, 4.234038829803467, 4.366669178009033, 4.4776740074157715, 4.059868812561035, 4.433427333831787, 3.6779534816741943, 2.0286128520965576, 3.379409074783325, 4.022160053253174]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.290374755859375, 6.239561080932617, 5.914707183837891, 6.08668851852417, 5.431400775909424, 5.85368537902832, 6.1774749755859375, 5.702269554138184, 5.347090244293213, 5.754560947418213, 6.435452461242676, 5.5234785079956055, 5.774129390716553, 6.213619232177734, 6.228049278259277, 5.7869553565979]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8385465145111084, 3.6738274097442627, 3.783618688583374, 3.951447010040283, 2.689251184463501, 3.8650035858154297, 3.9425086975097656, 3.661731719970703, 3.9946393966674805, 3.6165528297424316, 3.701235055923462, 3.625761032104492, 3.6657683849334717, 3.6013238430023193, 3.7107105255126953, 3.898515224456787]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4545860290527344, 3.531935691833496, 3.406717538833618, 3.55755352973938, 3.384856700897217, 3.628164291381836, 3.576061487197876, 3.3679862022399902, 3.6055781841278076, 3.30009388923645, 3.604487180709839, 3.4488890171051025, 3.436849594116211, 2.7289011478424072, 3.6538991928100586, 3.5729432106018066]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1844143867492676, 3.2101023197174072, 2.9826600551605225, 2.897610902786255, 2.818718910217285, 3.0858500003814697, 3.1842799186706543, 3.274066209793091, 3.031273603439331, 3.0054867267608643, 2.2887461185455322, 2.9980411529541016, 3.1588551998138428, 3.06398344039917, 3.0779409408569336, 2.8865673542022705]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  86%|█████████████████████████████████████████▌      | 173/200 [05:44<00:47,  1.74s/it]Layer: gate_28 - Captured router_logits: [4.367865085601807, 4.485426902770996, 4.558226108551025, 4.364867210388184, 4.483670711517334, 4.4594855308532715, 4.212022304534912, 4.70313024520874, 4.486790657043457, 4.449625492095947, 4.462098598480225, 4.048414707183838, 4.190243721008301, 4.04865026473999, 4.566521644592285, 4.366750240325928]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.572426795959473, 8.902780532836914, 8.771974563598633, 8.458855628967285, 8.543646812438965, 8.25766658782959, 8.627776145935059, 8.527334213256836, 8.61520004272461, 8.479775428771973, 8.587611198425293, 8.225419044494629, 8.33043098449707, 8.987447738647461, 8.889881134033203, 8.497010231018066]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.885317325592041, 5.427582263946533, 5.381110191345215, 4.787886619567871, 5.1310014724731445, 5.078378677368164, 4.667854309082031, 5.132164001464844, 5.234689235687256, 4.9245405197143555, 4.8299736976623535, 4.728592395782471, 4.861492156982422, 4.972135066986084, 5.063342094421387, 4.829785346984863]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.229966640472412, 2.9972734451293945, 3.1036059856414795, 2.755587100982666, 3.0249600410461426, 3.308206796646118, 2.7493700981140137, 3.172292709350586, 3.082676887512207, 3.1788086891174316, 3.1726207733154297, 3.2614123821258545, 3.03117299079895, 2.9778547286987305, 3.1206247806549072, 3.1728386878967285]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11812739074230194, 0.1335935890674591, 0.12677612900733948, -0.24399951100349426, -0.24892902374267578, -0.0949527770280838, 0.14097358286380768, -0.17084655165672302, 0.0980471670627594, 0.11319179087877274, 0.11263343691825867, 0.07165718078613281, 0.10551737248897552, 0.12418472766876221, -1.0992592573165894, 0.13612939417362213]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08625984191894531, 0.04910532385110855, 0.040270090103149414, 0.06209047883749008, 0.08266186714172363, 0.0397610142827034, 0.05214768648147583, 0.07511714845895767, 0.016968978568911552, 0.061378415673971176, -0.1819605976343155, 0.05999896675348282, 0.00564661156386137, -0.00238794949837029, 0.019872641190886497, 0.02716972306370735]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06859306991100311, 0.04383998364210129, 0.09120216220617294, 0.055232975631952286, 0.09185332804918289, 0.1009625792503357, 0.0544385127723217, -0.14362090826034546, 0.061610955744981766, 0.08298631757497787, 0.0010698962723836303, 0.07621090859174728, -0.21974284946918488, -0.005575686227530241, -0.06349398195743561, 0.12932845950126648]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.2138645350933075, 0.14664597809314728, 0.12772519886493683, 0.1412573605775833, 0.043012164533138275, 0.05493546277284622, -0.09789492189884186, 0.17936064302921295, 0.15037676692008972, -0.4480203092098236, 0.03150780126452446, 0.1623375415802002, 0.08908147364854813, -0.2832458019256592, -0.07845047116279602, -0.04868027940392494]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07991898059844971, -0.028669625520706177, -0.06552700698375702, 0.14730477333068848, -0.06242428347468376, -0.14730876684188843, 0.09023366123437881, 0.04618450254201889, 0.06832315027713776, 0.2007073611021042, -0.2793925106525421, 0.025922929868102074, 0.011913630180060863, -0.1815217137336731, 0.08289718627929688, 0.018916724249720573]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.20858924090862274, 0.23748916387557983, 0.13957379758358002, 0.051469586789608, -0.30044025182724, -0.06286953389644623, 0.09738494455814362, 0.010299088433384895, 0.1590721160173416, -0.062275443226099014, -0.4955284297466278, 0.046118639409542084, -0.34267517924308777, 0.2534633278846741, 0.1527373045682907, 0.1485014706850052]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.31319135427474976, 0.330918550491333, 0.19201436638832092, 0.2637470066547394, 0.26113468408584595, 0.2441994994878769, 0.12306713312864304, 0.15151196718215942, -0.3816721737384796, 0.3917625844478607, -0.07670661062002182, 0.32113662362098694, 0.08733464777469635, -0.06914281845092773, 0.27549606561660767, 0.16784176230430603]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.04924393072724342, 0.20241722464561462, 0.11966732144355774, 0.1599213182926178, -0.35578984022140503, 0.20685313642024994, 0.05808020383119583, 0.20893912017345428, -0.026372162625193596, -0.16650402545928955, 0.06426414102315903, -0.0667901560664177, 0.17727349698543549, 0.10872485488653183, -0.019675103947520256, 0.1489144116640091]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20750437676906586, 0.27870190143585205, -0.08471222221851349, 0.31983381509780884, 0.45800358057022095, 0.3988005816936493, 0.1128905788064003, -0.00019736126705538481, 0.6086390018463135, -0.19944187998771667, 0.25590282678604126, 0.051828064024448395, 0.6660900712013245, -0.19853511452674866, -0.022514425218105316, 0.18963326513767242]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8648644089698792, 0.7500265836715698, 0.6143440008163452, 0.5672582387924194, 0.8449954986572266, 0.44703689217567444, 1.109043002128601, 1.0573241710662842, 0.09660433232784271, 0.03360826522111893, 0.43812501430511475, 0.7879056930541992, 0.7501850724220276, 0.8245624303817749, 1.1114258766174316, 0.7918519973754883]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.157577395439148, 1.1703343391418457, 0.660401463508606, 1.0675841569900513, 0.9529542922973633, 0.3681928217411041, 0.9894270300865173, 1.4162423610687256, 0.7747671008110046, 0.28493279218673706, 1.1022170782089233, 1.109398365020752, 0.6447311639785767, 1.092626929283142, 0.49856317043304443, 1.0090265274047852]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2384661436080933, 1.591354489326477, 1.5279054641723633, 1.3944555521011353, 1.4270257949829102, 0.8571923971176147, 1.743691086769104, 1.2270588874816895, 1.4794389009475708, 1.4443373680114746, 1.2668215036392212, 1.1709704399108887, 1.1571135520935059, 1.3378808498382568, 1.2100411653518677, 1.2005051374435425]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0554286241531372, 1.5084078311920166, 1.6445608139038086, 1.7894643545150757, 1.2585269212722778, 1.581638216972351, 1.2793693542480469, 1.5159618854522705, 1.3535076379776, 0.6542962193489075, 1.694671630859375, 1.9481536149978638, 1.758305549621582, 1.6775219440460205, 1.4255245923995972, 1.5349655151367188]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.086520195007324, 3.855161666870117, 3.047598361968994, 2.666821002960205, 3.4873719215393066, 2.110369920730591, 3.6565091609954834, 3.1529998779296875, 3.8215627670288086, 3.388477325439453, 3.186302423477173, 3.9367196559906006, 2.1478328704833984, 3.9580252170562744, 3.1571919918060303, 2.8045966625213623]
Running loglikelihood requests:  88%|██████████████████████████████████████████▍     | 177/200 [05:51<00:39,  1.72s/it]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.393704891204834, 2.3624987602233887, 2.844686269760132, 2.4265692234039307, 2.801037311553955, 2.07674503326416, 1.5775885581970215, 2.0871551036834717, 2.6299617290496826, 1.9968383312225342, 1.347968339920044, 1.9448691606521606, 2.4355621337890625, 2.674360513687134, 2.636084794998169, 1.9970422983169556]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5740561485290527, 3.4111745357513428, 3.054572105407715, 3.1160168647766113, 3.0379135608673096, 2.847418785095215, 2.781517744064331, 2.0439844131469727, 3.276888608932495, 2.94551420211792, 2.7745959758758545, 1.3786474466323853, 2.916796922683716, 2.7666449546813965, 3.0317270755767822, 2.0719869136810303]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.92289137840271, 1.83669114112854, 2.6387875080108643, 2.5911474227905273, 2.627143621444702, 1.9916841983795166, 1.7664729356765747, 3.0800328254699707, 0.5663405656814575, 2.1669023036956787, 0.9757031798362732, 2.4490249156951904, 2.7480173110961914, 2.9191503524780273, 2.9544928073883057, 1.4242053031921387]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6868057250976562, 3.9504103660583496, 5.548490047454834, 4.938434600830078, 5.510415554046631, 5.667271137237549, 5.12485408782959, 5.111525535583496, 6.147275924682617, 5.026613712310791, 5.760903835296631, 6.065088748931885, 4.819623947143555, 5.387362003326416, 4.300631046295166, 4.94783878326416]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.208363056182861, 4.553705215454102, 5.5310468673706055, 5.168172836303711, 3.700448989868164, 4.457371234893799, 4.870165824890137, 5.281790733337402, 5.361697673797607, 4.907987117767334, 4.270651817321777, 3.3469717502593994, 4.4042534828186035, 3.3857462406158447, 4.913552284240723, 4.491384029388428]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.879391670227051, 5.839845180511475, 6.041041374206543, 5.624471664428711, 6.077417373657227, 4.818676948547363, 5.691307067871094, 5.635891914367676, 5.2998127937316895, 5.362641334533691, 6.2191362380981445, 5.907765865325928, 4.960044860839844, 5.861692428588867, 6.0445027351379395, 5.867485523223877]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.0039823055267334, 2.970695972442627, 2.8100838661193848, 2.2396323680877686, -0.4510212540626526, 3.0444605350494385, 2.7205801010131836, 1.8568052053451538, 2.7153406143188477, 3.467874526977539, 2.6898560523986816, 3.2476210594177246, 2.858952045440674, 2.9450345039367676, 2.323465585708618, 1.739092469215393]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.557478427886963, 6.827159404754639, 6.541680335998535, 6.310030460357666, 6.470051288604736, 6.485503196716309, 6.751361846923828, 6.647464275360107, 6.645992755889893, 6.337343692779541, 6.932547092437744, 6.26310920715332, 5.287875652313232, 7.210975170135498, 6.493189811706543, 6.4456658363342285]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.014764308929443, 5.359200954437256, 4.967113018035889, 4.05336856842041, 4.905429363250732, 5.254857540130615, 4.635804176330566, 4.784562110900879, 5.07684850692749, 4.851156711578369, 5.277679443359375, 4.880334854125977, 5.150352954864502, 5.435708999633789, 4.606325626373291, 5.476039886474609]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.1564836502075195, 4.671969890594482, 3.3417317867279053, 4.200700759887695, 4.4632039070129395, 4.083298206329346, 4.2449212074279785, 4.233273029327393, 4.36704683303833, 4.477153778076172, 4.0595574378967285, 4.433646202087402, 3.677910089492798, 2.0280590057373047, 3.379207134246826, 4.021708965301514]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.288259029388428, 6.237369537353516, 5.913512229919434, 6.084784507751465, 5.429671287536621, 5.851395130157471, 6.17568302154541, 5.700549125671387, 5.345026016235352, 5.752932071685791, 6.43398380279541, 5.522119522094727, 5.772357940673828, 6.211563587188721, 6.22658634185791, 5.784745693206787]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.837874174118042, 3.6728708744049072, 3.7828550338745117, 3.950493335723877, 2.6891801357269287, 3.8643617630004883, 3.9418811798095703, 3.661630630493164, 3.993945598602295, 3.6161506175994873, 3.7007627487182617, 3.6252224445343018, 3.6647839546203613, 3.600856065750122, 3.710312604904175, 3.8979105949401855]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.453338384628296, 3.5314531326293945, 3.4058821201324463, 3.5565598011016846, 3.3835976123809814, 3.6274311542510986, 3.5751359462738037, 3.3668761253356934, 3.604837417602539, 3.2991080284118652, 3.603386402130127, 3.447514772415161, 3.4360194206237793, 2.7274839878082275, 3.6531922817230225, 3.571685552597046]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1836724281311035, 3.2093021869659424, 2.9816555976867676, 2.8964452743530273, 2.817887544631958, 3.08522629737854, 3.1832361221313477, 3.27313232421875, 3.0302345752716064, 3.0042178630828857, 2.2882001399993896, 2.997356414794922, 3.1576883792877197, 3.063783645629883, 3.077255964279175, 2.8855042457580566]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.367252349853516, 4.484320163726807, 4.557291030883789, 4.364014625549316, 4.4827680587768555, 4.458324909210205, 4.21146297454834, 4.7018842697143555, 4.485864639282227, 4.448379039764404, 4.461208343505859, 4.047234058380127, 4.188670635223389, 4.04791784286499, 4.564977645874023, 4.365659713745117]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.56813907623291, 8.899145126342773, 8.768684387207031, 8.454345703125, 8.539934158325195, 8.25365924835205, 8.623540878295898, 8.523933410644531, 8.610307693481445, 8.475886344909668, 8.583277702331543, 8.221758842468262, 8.326642036437988, 8.984676361083984, 8.885689735412598, 8.492671012878418]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.884541988372803, 5.427269458770752, 5.380516529083252, 4.787684917449951, 5.13179874420166, 5.078942775726318, 4.667458534240723, 5.131731986999512, 5.2351179122924805, 4.92354679107666, 4.829057216644287, 4.728150844573975, 4.860438823699951, 4.971721649169922, 5.062924861907959, 4.829826354980469]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.228658676147461, 2.995866537094116, 3.102816581726074, 2.7562334537506104, 3.0240304470062256, 3.307483196258545, 2.74936580657959, 3.1716995239257812, 3.081993818283081, 3.1774847507476807, 3.171368360519409, 3.2603790760040283, 3.0305864810943604, 2.9768478870391846, 3.1199560165405273, 3.1717135906219482]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11781061440706253, 0.13320912420749664, 0.12672755122184753, -0.24344876408576965, -0.24671414494514465, -0.08898194134235382, 0.14081531763076782, -0.1710447072982788, 0.09947320073843002, 0.11267658323049545, 0.11166452616453171, 0.07008039951324463, 0.1048750951886177, 0.12558917701244354, -1.1126636266708374, 0.13515213131904602]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08535099774599075, 0.04948590323328972, 0.0387188196182251, 0.06270050257444382, 0.08319827169179916, 0.03822888061404228, 0.05295689031481743, 0.07519924640655518, 0.016179651021957397, 0.060665711760520935, -0.18110902607440948, 0.061166971921920776, 0.0050577083602547646, -0.0028361098375171423, 0.023238172754645348, 0.026822051033377647]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0684216320514679, 0.04458117485046387, 0.0901743620634079, 0.0543743371963501, 0.09230122715234756, 0.09991011023521423, 0.053243108093738556, -0.1434745043516159, 0.0667276382446289, 0.08508195728063583, 0.002568635158240795, 0.07657154649496078, -0.21698087453842163, -0.007522497791796923, -0.061917051672935486, 0.125184565782547]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21401673555374146, 0.14706701040267944, 0.1265202909708023, 0.14188899099826813, 0.042804934084415436, 0.05253925174474716, -0.09578090906143188, 0.17877542972564697, 0.1513296514749527, -0.4497968554496765, 0.035303130745887756, 0.16298292577266693, 0.0872221440076828, -0.2884061634540558, -0.07561740279197693, -0.04177200794219971]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08002124726772308, -0.02894318662583828, -0.06631185114383698, 0.14649970829486847, -0.06117043271660805, -0.14998607337474823, 0.08820682764053345, 0.04814703017473221, 0.06784284859895706, 0.1980217546224594, -0.27568989992141724, 0.027292171493172646, 0.009782413020730019, -0.17594201862812042, 0.08390555530786514, 0.016615917906165123]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.20859016478061676, 0.23738610744476318, 0.14094555377960205, 0.05219155177474022, -0.3020784854888916, -0.06146270036697388, 0.0957411453127861, 0.0088711678981781, 0.16002878546714783, -0.06348814070224762, -0.4969378411769867, 0.047460563480854034, -0.34147322177886963, 0.2542559802532196, 0.15270964801311493, 0.1498018205165863]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.31471407413482666, 0.33160853385925293, 0.19140681624412537, 0.26530659198760986, 0.2607002854347229, 0.2427431046962738, 0.12513773143291473, 0.15055124461650848, -0.38009485602378845, 0.3920566737651825, -0.07661742717027664, 0.31958824396133423, 0.08944078534841537, -0.0704696774482727, 0.2746562361717224, 0.16639180481433868]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.04906890541315079, 0.20248056948184967, 0.12041708081960678, 0.16168007254600525, -0.35732370615005493, 0.2069307565689087, 0.05869809910655022, 0.21327145397663116, -0.02675074152648449, -0.1669131964445114, 0.06309576332569122, -0.06601668894290924, 0.1764589548110962, 0.10992742329835892, -0.019929008558392525, 0.14713259041309357]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.208137646317482, 0.27938222885131836, -0.08451960235834122, 0.3210330009460449, 0.4578762352466583, 0.3986337184906006, 0.11352135986089706, -0.000559682201128453, 0.6091910004615784, -0.20020580291748047, 0.25590118765830994, 0.05138838663697243, 0.6667945384979248, -0.19745828211307526, -0.02252955548465252, 0.1892537772655487]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8651336431503296, 0.7518131136894226, 0.6141963005065918, 0.5653380155563354, 0.8451331257820129, 0.4457935690879822, 1.1096240282058716, 1.057564616203308, 0.09521930664777756, 0.03203434869647026, 0.4392146170139313, 0.7865684628486633, 0.7517372369766235, 0.827235758304596, 1.1109156608581543, 0.7915303111076355]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1600565910339355, 1.169954538345337, 0.6604987382888794, 1.0684008598327637, 0.9537416100502014, 0.36559954285621643, 0.9893434047698975, 1.41732919216156, 0.7776988744735718, 0.2853149175643921, 1.1022813320159912, 1.1090104579925537, 0.6452881693840027, 1.094115972518921, 0.4975545406341553, 1.0112396478652954]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2400550842285156, 1.5925105810165405, 1.5314770936965942, 1.3932647705078125, 1.4291030168533325, 0.8587042093276978, 1.7434015274047852, 1.2285213470458984, 1.4809762239456177, 1.4458082914352417, 1.269760012626648, 1.1728895902633667, 1.1620032787322998, 1.3389097452163696, 1.2139049768447876, 1.2008005380630493]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0549267530441284, 1.5103023052215576, 1.6443679332733154, 1.7895511388778687, 1.2580616474151611, 1.5841412544250488, 1.2768038511276245, 1.514177918434143, 1.3546611070632935, 0.6568352580070496, 1.6964056491851807, 1.9484257698059082, 1.7594854831695557, 1.6779547929763794, 1.4259824752807617, 1.535512924194336]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0886282920837402, 3.8558785915374756, 3.0485005378723145, 2.667080879211426, 3.4881703853607178, 2.1123459339141846, 3.6566853523254395, 3.1516287326812744, 3.8208577632904053, 3.3894293308258057, 3.185407876968384, 3.938870668411255, 2.148751974105835, 3.9593634605407715, 3.1611764430999756, 2.805396556854248]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3934853076934814, 2.364189386367798, 2.8462233543395996, 2.427793025970459, 2.8009848594665527, 2.078209400177002, 1.576357364654541, 2.083707094192505, 2.6303131580352783, 1.9957458972930908, 1.349924087524414, 1.9464609622955322, 2.4351773262023926, 2.6742947101593018, 2.636197328567505, 1.9971975088119507]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.575369119644165, 3.4119808673858643, 3.056203842163086, 3.118236780166626, 3.041745185852051, 2.8487586975097656, 2.78208065032959, 2.043551206588745, 3.277729034423828, 2.9471940994262695, 2.7767105102539062, 1.378233790397644, 2.91865611076355, 2.7679238319396973, 3.032980442047119, 2.0718090534210205]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9219038486480713, 1.8383558988571167, 2.6377615928649902, 2.5909805297851562, 2.627016067504883, 1.9899601936340332, 1.763342022895813, 3.0794105529785156, 0.5640085935592651, 2.1656267642974854, 0.9755301475524902, 2.4475080966949463, 2.7485783100128174, 2.919422149658203, 2.9549787044525146, 1.4225646257400513]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6865146160125732, 3.9490137100219727, 5.548043251037598, 4.939549446105957, 5.511754989624023, 5.667448997497559, 5.124643325805664, 5.113204002380371, 6.148571968078613, 5.0234479904174805, 5.760503768920898, 6.065676689147949, 4.818136692047119, 5.387927532196045, 4.301726341247559, 4.94815731048584]
Running loglikelihood requests:  90%|███████████████████████████████████████████▍    | 181/200 [05:58<00:32,  1.72s/it]Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.208334445953369, 4.552729606628418, 5.530712127685547, 5.168270111083984, 3.698237895965576, 4.456081390380859, 4.870218753814697, 5.282269477844238, 5.36155366897583, 4.908094882965088, 4.27192497253418, 3.3454463481903076, 4.405115604400635, 3.3858482837677, 4.912313938140869, 4.49041223526001]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.881096839904785, 5.841326713562012, 6.0415544509887695, 5.625708103179932, 6.07830810546875, 4.819544315338135, 5.692725658416748, 5.637336254119873, 5.299663543701172, 5.364149570465088, 6.220285892486572, 5.908351421356201, 4.961909770965576, 5.862811088562012, 6.046054840087891, 5.867628574371338]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.0022807121276855, 2.970414400100708, 2.809469223022461, 2.2381467819213867, -0.45391547679901123, 3.0447399616241455, 2.720496416091919, 1.8558980226516724, 2.7158193588256836, 3.4672462940216064, 2.689246654510498, 3.2469284534454346, 2.8577187061309814, 2.9450905323028564, 2.3216867446899414, 1.7384768724441528]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.557284832000732, 6.827149868011475, 6.541017532348633, 6.309284210205078, 6.469900131225586, 6.485446453094482, 6.751199722290039, 6.646577835083008, 6.645566463470459, 6.3372483253479, 6.9325714111328125, 6.262651443481445, 5.286401748657227, 7.210877418518066, 6.493189811706543, 6.445655345916748]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.01384973526001, 5.358141899108887, 4.966370105743408, 4.0523552894592285, 4.905100345611572, 5.253744125366211, 4.634564399719238, 4.783578872680664, 5.075845241546631, 4.8501763343811035, 5.2766032218933105, 4.878836631774902, 5.149242877960205, 5.43410587310791, 4.605081558227539, 5.4742608070373535]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.155063152313232, 4.669488430023193, 3.339578628540039, 4.1989593505859375, 4.46234655380249, 4.081287860870361, 4.242986679077148, 4.2317352294921875, 4.365494728088379, 4.475835800170898, 4.05817174911499, 4.432651996612549, 3.676321029663086, 2.0262398719787598, 3.377596139907837, 4.020142078399658]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.287111759185791, 6.23599100112915, 5.911773204803467, 6.083610534667969, 5.428074836730957, 5.849889278411865, 6.1741180419921875, 5.698304653167725, 5.343316555023193, 5.751415252685547, 6.432422637939453, 5.520458698272705, 5.770834922790527, 6.210526466369629, 6.224471569061279, 5.783662796020508]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8370778560638428, 3.672121047973633, 3.7812674045562744, 3.949652910232544, 2.686959981918335, 3.8634910583496094, 3.9406042098999023, 3.6603105068206787, 3.992831230163574, 3.6149051189422607, 3.6995067596435547, 3.624332904815674, 3.663815975189209, 3.5995705127716064, 3.7095487117767334, 3.896773338317871]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4527087211608887, 3.530963182449341, 3.405311346054077, 3.555814743041992, 3.3831653594970703, 3.6272056102752686, 3.5742456912994385, 3.366039276123047, 3.604365110397339, 3.2984910011291504, 3.602459192276001, 3.4468612670898438, 3.435495376586914, 2.7272965908050537, 3.6521542072296143, 3.5715386867523193]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1830220222473145, 3.208827018737793, 2.9810104370117188, 2.896183490753174, 2.81735897064209, 3.084569215774536, 3.183032512664795, 3.2724082469940186, 3.029801368713379, 3.00386381149292, 2.287431001663208, 2.9965920448303223, 3.1574149131774902, 3.062782049179077, 3.0762875080108643, 2.885206699371338]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.366301536560059, 4.483421325683594, 4.555846691131592, 4.362210750579834, 4.480846881866455, 4.457208156585693, 4.209327220916748, 4.7003865242004395, 4.484405517578125, 4.447026252746582, 4.459804534912109, 4.045856475830078, 4.188368320465088, 4.046462059020996, 4.563770294189453, 4.364585876464844]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.567076683044434, 8.897669792175293, 8.767597198486328, 8.453523635864258, 8.538779258728027, 8.252524375915527, 8.622344970703125, 8.522100448608398, 8.608856201171875, 8.47476577758789, 8.582098960876465, 8.220256805419922, 8.32610034942627, 8.982261657714844, 8.884956359863281, 8.490617752075195]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.8840413093566895, 5.426536560058594, 5.379270553588867, 4.785929203033447, 5.1296515464782715, 5.0771708488464355, 4.666470527648926, 5.1312336921691895, 5.233252048492432, 4.922700881958008, 4.827556610107422, 4.726134300231934, 4.859519004821777, 4.969356536865234, 5.061893463134766, 4.828259468078613]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.228933334350586, 2.9962289333343506, 3.103154182434082, 2.7549848556518555, 3.024362325668335, 3.3073229789733887, 2.7488725185394287, 3.171076774597168, 3.0817337036132812, 3.177657127380371, 3.171581506729126, 3.260084629058838, 3.030667781829834, 2.9767000675201416, 3.120144844055176, 3.1717467308044434]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11713041365146637, 0.13213667273521423, 0.12659476697444916, -0.2417791336774826, -0.24619382619857788, -0.0928499773144722, 0.139308899641037, -0.16017264127731323, 0.0997297391295433, 0.11154717206954956, 0.11082867532968521, 0.07091058790683746, 0.10440672934055328, 0.12393494695425034, -1.0951857566833496, 0.13454025983810425]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0868089571595192, 0.04955526441335678, 0.04058576375246048, 0.061326250433921814, 0.08279381692409515, 0.03799838945269585, 0.05336857587099075, 0.07926532626152039, 0.020839834585785866, 0.06000044569373131, -0.17951704561710358, 0.06070468947291374, 0.002311635995283723, -0.004491959232836962, 0.021627705544233322, 0.028524240478873253]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07012654840946198, 0.04238361120223999, 0.09164624661207199, 0.05549627169966698, 0.09616361558437347, 0.10125824064016342, 0.0558403879404068, -0.14339356124401093, 0.0646851509809494, 0.08552949130535126, 0.0022388019133359194, 0.07771742343902588, -0.21891282498836517, -0.007951131090521812, -0.05835361033678055, 0.12581300735473633]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21439504623413086, 0.1458931714296341, 0.1245199665427208, 0.14417840540409088, 0.043638307601213455, 0.054052118211984634, -0.09477696567773819, 0.17709827423095703, 0.1519404500722885, -0.44779592752456665, 0.029454723000526428, 0.16155685484409332, 0.09989592432975769, -0.2912888526916504, -0.07538791745901108, -0.04782600700855255]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0775759220123291, -0.030072787776589394, -0.06495585292577744, 0.14834532141685486, -0.06530296057462692, -0.14886638522148132, 0.0913810282945633, 0.04675610736012459, 0.06566639244556427, 0.2020541876554489, -0.2817187011241913, 0.02708280272781849, 0.014227410778403282, -0.18001344799995422, 0.08244479447603226, 0.017433928325772285]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.20948347449302673, 0.23629654943943024, 0.13754621148109436, 0.0506770946085453, -0.30187907814979553, -0.06343461573123932, 0.09581830352544785, 0.00979006476700306, 0.16160425543785095, -0.062444210052490234, -0.49323663115501404, 0.046527523547410965, -0.34357792139053345, 0.2538565695285797, 0.15474148094654083, 0.14839360117912292]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3132811188697815, 0.33192262053489685, 0.18932248651981354, 0.2658505439758301, 0.2603946328163147, 0.24365511536598206, 0.12405073642730713, 0.153036430478096, -0.38274580240249634, 0.39109575748443604, -0.07581225037574768, 0.3198882043361664, 0.08901169896125793, -0.06870466470718384, 0.2762243449687958, 0.16613592207431793]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.04861123859882355, 0.2019949108362198, 0.12049192935228348, 0.1610921323299408, -0.35802727937698364, 0.206525057554245, 0.05794983357191086, 0.21394841372966766, -0.025411106646060944, -0.1674443781375885, 0.06405461579561234, -0.06629916280508041, 0.17704951763153076, 0.10892987996339798, -0.021752217784523964, 0.1480361819267273]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2076403647661209, 0.2786965072154999, -0.0828530415892601, 0.3207693099975586, 0.4573929011821747, 0.3973475396633148, 0.11413256078958511, -0.0015403348952531815, 0.6074033975601196, -0.20059484243392944, 0.2556818127632141, 0.050829969346523285, 0.6655902862548828, -0.1980094313621521, -0.022587936371564865, 0.1895037740468979]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8636613488197327, 0.7504636645317078, 0.612892210483551, 0.5652199387550354, 0.844812273979187, 0.4462343752384186, 1.107954502105713, 1.0555909872055054, 0.09581568837165833, 0.03370499983429909, 0.4396686255931854, 0.7862805724143982, 0.7519408464431763, 0.8288771510124207, 1.110931634902954, 0.7910302877426147]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1592175960540771, 1.1698235273361206, 0.6619966626167297, 1.068751335144043, 0.952467679977417, 0.3658495545387268, 0.9885584712028503, 1.4154633283615112, 0.7778920531272888, 0.2868463695049286, 1.1020417213439941, 1.1100760698318481, 0.6457338929176331, 1.0922859907150269, 0.4964047074317932, 1.0119900703430176]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.239768147468567, 1.5935419797897339, 1.5286879539489746, 1.3932311534881592, 1.429090976715088, 0.8579288125038147, 1.7435722351074219, 1.2281690835952759, 1.480046272277832, 1.4452998638153076, 1.2683734893798828, 1.1712979078292847, 1.1596431732177734, 1.3383830785751343, 1.2127339839935303, 1.2019355297088623]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0560418367385864, 1.5078978538513184, 1.6438323259353638, 1.789424180984497, 1.2597410678863525, 1.5819672346115112, 1.2758375406265259, 1.5138218402862549, 1.3529109954833984, 0.6545907258987427, 1.6948808431625366, 1.945669174194336, 1.76009202003479, 1.6777167320251465, 1.4265552759170532, 1.5339734554290771]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0853872299194336, 3.8540894985198975, 3.0454132556915283, 2.6662075519561768, 3.4866816997528076, 2.1082754135131836, 3.6550395488739014, 3.150601863861084, 3.819546699523926, 3.38694429397583, 3.1831347942352295, 3.9362614154815674, 2.1458146572113037, 3.9579145908355713, 3.1583011150360107, 2.8041183948516846]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3931620121002197, 2.3634579181671143, 2.8474698066711426, 2.4282584190368652, 2.799339771270752, 2.077136993408203, 1.5791457891464233, 2.082636594772339, 2.6297311782836914, 1.997734785079956, 1.3493319749832153, 1.944697618484497, 2.4357962608337402, 2.6736788749694824, 2.636540651321411, 1.9976186752319336]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.574047327041626, 3.4112255573272705, 3.0551986694335938, 3.1177024841308594, 3.0388925075531006, 2.84846568107605, 2.7798664569854736, 2.0440385341644287, 3.276902914047241, 2.945594549179077, 2.776360273361206, 1.3778226375579834, 2.9161911010742188, 2.7659084796905518, 3.0326616764068604, 2.071575880050659]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9260107278823853, 1.8384343385696411, 2.6390504837036133, 2.594968318939209, 2.628479480743408, 1.9929049015045166, 1.7644575834274292, 3.0831353664398193, 0.5634890198707581, 2.1668291091918945, 0.9747235178947449, 2.448528528213501, 2.7487435340881348, 2.922380208969116, 2.9564497470855713, 1.4253989458084106]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.6813864707946777, 3.945040225982666, 5.545619487762451, 4.936252117156982, 5.507615089416504, 5.663824558258057, 5.123446464538574, 5.1095662117004395, 6.1469597816467285, 5.023845195770264, 5.758865833282471, 6.062985897064209, 4.815207004547119, 5.385451793670654, 4.2965593338012695, 4.945232391357422]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.205536365509033, 4.551089763641357, 5.530584812164307, 5.16746711730957, 3.6983463764190674, 4.455385684967041, 4.867742538452148, 5.278542995452881, 5.360955715179443, 4.905674934387207, 4.267487525939941, 3.3433592319488525, 4.402894973754883, 3.384761333465576, 4.9109416007995605, 4.4888105392456055]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.878732204437256, 5.840171813964844, 6.041014671325684, 5.623077869415283, 6.07781982421875, 4.816460132598877, 5.689577579498291, 5.6339240074157715, 5.298555850982666, 5.361969947814941, 6.219016075134277, 5.907339096069336, 4.958756923675537, 5.861603736877441, 6.0437211990356445, 5.866878032684326]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.003145694732666, 2.9683239459991455, 2.8089845180511475, 2.2355854511260986, -0.4548044204711914, 3.0434658527374268, 2.719810962677002, 1.8553649187088013, 2.7136142253875732, 3.468173027038574, 2.6887459754943848, 3.2487001419067383, 2.858330011367798, 2.946167469024658, 2.320859909057617, 1.7371267080307007]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.556493759155273, 6.827167510986328, 6.5399909019470215, 6.309153079986572, 6.467467308044434, 6.484539985656738, 6.749855041503906, 6.647974014282227, 6.6446332931518555, 6.335238456726074, 6.9323272705078125, 6.262038230895996, 5.285490036010742, 7.211276054382324, 6.491887092590332, 6.4452996253967285]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  92%|████████████████████████████████████████████▍   | 185/200 [06:04<00:25,  1.70s/it]Layer: gate_22 - Captured router_logits: [5.0132341384887695, 5.356816291809082, 4.966195106506348, 4.051419258117676, 4.9030914306640625, 5.253559112548828, 4.632904529571533, 4.784246921539307, 5.0757246017456055, 4.850433826446533, 5.27733039855957, 4.878587245941162, 5.149536609649658, 5.4351911544799805, 4.6041646003723145, 5.475384712219238]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.157598495483398, 4.6720452308654785, 3.340956211090088, 4.201478004455566, 4.464809894561768, 4.0835137367248535, 4.245586395263672, 4.2339396476745605, 4.3673481941223145, 4.4785895347595215, 4.060624599456787, 4.434662818908691, 3.6771628856658936, 2.0270841121673584, 3.3792030811309814, 4.021975517272949]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.2888007164001465, 6.237321376800537, 5.91297721862793, 6.0842180252075195, 5.428532600402832, 5.850390911102295, 6.1757917404174805, 5.700545310974121, 5.344097137451172, 5.7523064613342285, 6.433588027954102, 5.52182674407959, 5.771697044372559, 6.211822509765625, 6.226511001586914, 5.7838568687438965]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.837678909301758, 3.673292398452759, 3.782423257827759, 3.9509570598602295, 2.6885645389556885, 3.8651320934295654, 3.9424731731414795, 3.6608622074127197, 3.9940645694732666, 3.615847587585449, 3.700059652328491, 3.625546455383301, 3.6656265258789062, 3.600100517272949, 3.711317539215088, 3.8976075649261475]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4546384811401367, 3.532503128051758, 3.40584659576416, 3.5576624870300293, 3.3846077919006348, 3.6293022632598877, 3.576674461364746, 3.3676350116729736, 3.606052875518799, 3.3011012077331543, 3.605517864227295, 3.4488961696624756, 3.4368503093719482, 2.7276902198791504, 3.6547763347625732, 3.573587417602539]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1864688396453857, 3.2125680446624756, 2.9843571186065674, 2.8990278244018555, 2.819646120071411, 3.0878090858459473, 3.1863622665405273, 3.276329755783081, 3.0327117443084717, 3.006807804107666, 2.28969144821167, 2.9997787475585938, 3.1608142852783203, 3.0670597553253174, 3.0796263217926025, 2.8882815837860107]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.3680267333984375, 4.485716342926025, 4.557772636413574, 4.3640217781066895, 4.4851226806640625, 4.459465980529785, 4.212064743041992, 4.703638076782227, 4.487179279327393, 4.4486470222473145, 4.462335586547852, 4.047894477844238, 4.189990520477295, 4.048240661621094, 4.566380500793457, 4.367014408111572]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.570667266845703, 8.903350830078125, 8.772046089172363, 8.456303596496582, 8.542499542236328, 8.25634765625, 8.62646198272705, 8.526793479919434, 8.61298656463623, 8.477742195129395, 8.586731910705566, 8.223721504211426, 8.330306053161621, 8.987147331237793, 8.889373779296875, 8.495734214782715]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.888587951660156, 5.4317755699157715, 5.384049892425537, 4.790304660797119, 5.135066509246826, 5.083863735198975, 4.6701884269714355, 5.135167121887207, 5.239092826843262, 4.927455425262451, 4.8322038650512695, 4.7303924560546875, 4.863613128662109, 4.976210594177246, 5.06650447845459, 4.832841396331787]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.229567289352417, 2.997150182723999, 3.104475259780884, 2.7590174674987793, 3.0250747203826904, 3.308811664581299, 2.751102924346924, 3.1719205379486084, 3.08357310295105, 3.1779556274414062, 3.1737208366394043, 3.260415554046631, 3.0314314365386963, 2.9772701263427734, 3.1215591430664062, 3.171969413757324]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11771656572818756, 0.13317306339740753, 0.1272869110107422, -0.24146035313606262, -0.24874365329742432, -0.08925159275531769, 0.14022816717624664, -0.16926684975624084, 0.10006880015134811, 0.11214469373226166, 0.11230570822954178, 0.07274575531482697, 0.10495872050523758, 0.1245948001742363, -1.1052426099777222, 0.13465817272663116]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08564542979001999, 0.04846059903502464, 0.03975122421979904, 0.062499284744262695, 0.08283977210521698, 0.03798184171319008, 0.05153956264257431, 0.0764649286866188, 0.015993282198905945, 0.061862822622060776, -0.17897924780845642, 0.060972705483436584, 0.003092158352956176, -0.0013604213017970324, 0.0217848252505064, 0.02648955211043358]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06936977058649063, 0.04338866099715233, 0.09046579897403717, 0.05340517312288284, 0.0933956429362297, 0.09932173788547516, 0.0528932623565197, -0.1442680060863495, 0.06435587257146835, 0.08487856388092041, 0.0029223694000393152, 0.07691530883312225, -0.22013095021247864, -0.006441101431846619, -0.06401732563972473, 0.1257392019033432]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21366211771965027, 0.14619581401348114, 0.12563428282737732, 0.14356647431850433, 0.04153812304139137, 0.05220480263233185, -0.09583257883787155, 0.175678089261055, 0.15137983858585358, -0.4470922648906708, 0.03377903252840042, 0.1608506739139557, 0.09156373143196106, -0.29100462794303894, -0.07697542011737823, -0.044569648802280426]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08036551624536514, -0.03155827149748802, -0.06387803703546524, 0.14871138334274292, -0.06254816055297852, -0.14983579516410828, 0.08883890509605408, 0.047860972583293915, 0.0678924098610878, 0.20260269939899445, -0.27971550822257996, 0.028221162036061287, 0.008280693553388119, -0.17667394876480103, 0.08202457427978516, 0.01870274916291237]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.2096463441848755, 0.23675763607025146, 0.13881494104862213, 0.05085610970854759, -0.3003842532634735, -0.0640425831079483, 0.09480362385511398, 0.009061583317816257, 0.1600593775510788, -0.06255126744508743, -0.4949265420436859, 0.04687908664345741, -0.34370550513267517, 0.2532680034637451, 0.15245702862739563, 0.14975225925445557]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3134666085243225, 0.3305458128452301, 0.1893800050020218, 0.26545172929763794, 0.2602934241294861, 0.24317783117294312, 0.1260073333978653, 0.151310533285141, -0.38046905398368835, 0.39210420846939087, -0.07505703717470169, 0.3200525641441345, 0.08962979912757874, -0.06949623674154282, 0.2744397521018982, 0.16592086851596832]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.0489254929125309, 0.2026837021112442, 0.12028218805789948, 0.16127905249595642, -0.35658973455429077, 0.20691552758216858, 0.058391887694597244, 0.215069979429245, -0.026737933978438377, -0.16703368723392487, 0.06312605738639832, -0.06612955033779144, 0.17654959857463837, 0.10931982100009918, -0.021183231845498085, 0.14762458205223083]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20819149911403656, 0.27904418110847473, -0.08328962326049805, 0.3201887011528015, 0.4581049382686615, 0.39851295948028564, 0.11382400244474411, -8.079506369540468e-05, 0.6083153486251831, -0.20181208848953247, 0.25663915276527405, 0.051132190972566605, 0.6664823889732361, -0.19747163355350494, -0.021824607625603676, 0.18979723751544952]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.865406334400177, 0.7511823177337646, 0.6148359775543213, 0.5645882487297058, 0.8468188643455505, 0.44699132442474365, 1.1107378005981445, 1.0573415756225586, 0.09589731693267822, 0.03452989459037781, 0.4392266273498535, 0.787661612033844, 0.7523245215415955, 0.8275866508483887, 1.1110273599624634, 0.7926995754241943]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1611274480819702, 1.1694589853286743, 0.6620568633079529, 1.0693367719650269, 0.9534677267074585, 0.3660542368888855, 0.9904533624649048, 1.4177273511886597, 0.7763618230819702, 0.28585687279701233, 1.102442979812622, 1.1095664501190186, 0.6463407278060913, 1.094699740409851, 0.49728909134864807, 1.0113160610198975]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2419124841690063, 1.5936706066131592, 1.5324124097824097, 1.3957867622375488, 1.431555151939392, 0.8595869541168213, 1.7433491945266724, 1.2304344177246094, 1.4814170598983765, 1.4469515085220337, 1.2702245712280273, 1.1745309829711914, 1.1629853248596191, 1.3428490161895752, 1.2154704332351685, 1.2048044204711914]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0564709901809692, 1.5112292766571045, 1.6437335014343262, 1.7905241250991821, 1.2602415084838867, 1.5837774276733398, 1.2774074077606201, 1.5140305757522583, 1.3537050485610962, 0.6563424468040466, 1.6980584859848022, 1.9482219219207764, 1.760512113571167, 1.6787323951721191, 1.427977442741394, 1.5356693267822266]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.087923526763916, 3.8550679683685303, 3.048126220703125, 2.666457176208496, 3.4885671138763428, 2.109307289123535, 3.656428813934326, 3.1500751972198486, 3.821506977081299, 3.389247179031372, 3.1854355335235596, 3.938699245452881, 2.1469719409942627, 3.9593136310577393, 3.160630226135254, 2.806015968322754]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3940281867980957, 2.3645617961883545, 2.8481757640838623, 2.4292373657226562, 2.8004372119903564, 2.078296422958374, 1.578141689300537, 2.082211971282959, 2.631429672241211, 1.9969571828842163, 1.349565029144287, 1.9452534914016724, 2.4359405040740967, 2.675015449523926, 2.637256622314453, 1.9976164102554321]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5748050212860107, 3.41217303276062, 3.0556037425994873, 3.1185057163238525, 3.039928913116455, 2.8488643169403076, 2.78123140335083, 2.044492483139038, 3.277486562728882, 2.9476687908172607, 2.7772228717803955, 1.3775252103805542, 2.9185943603515625, 2.7679851055145264, 3.0332021713256836, 2.071725368499756]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.925004243850708, 1.8388751745224, 2.64018177986145, 2.59413743019104, 2.628612995147705, 1.9931080341339111, 1.7646716833114624, 3.0816924571990967, 0.5616399049758911, 2.1657016277313232, 0.9748362302780151, 2.4480154514312744, 2.7492928504943848, 2.9226763248443604, 2.958179473876953, 1.4252125024795532]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.685081720352173, 3.9483249187469482, 5.550407886505127, 4.939358711242676, 5.513988494873047, 5.6699042320251465, 5.126936912536621, 5.114217758178711, 6.150174140930176, 5.025745868682861, 5.762215614318848, 6.0675578117370605, 4.820952892303467, 5.389652252197266, 4.301318645477295, 4.948215007781982]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.208019733428955, 4.552693843841553, 5.532612323760986, 5.169166564941406, 3.6985504627227783, 4.455455303192139, 4.870399475097656, 5.280662536621094, 5.3614044189453125, 4.908580303192139, 4.270423889160156, 3.3443400859832764, 4.404952049255371, 3.3846967220306396, 4.912071228027344, 4.490403175354004]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.88192081451416, 5.8427839279174805, 6.042396068572998, 5.6258931159973145, 6.0800042152404785, 4.81879186630249, 5.69265604019165, 5.636641979217529, 5.299262523651123, 5.365060806274414, 6.221165657043457, 5.909969329833984, 4.96169376373291, 5.863990783691406, 6.046902656555176, 5.868818759918213]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.0040018558502197, 2.9704983234405518, 2.8109748363494873, 2.2384040355682373, -0.45555466413497925, 3.0456926822662354, 2.721569538116455, 1.8556898832321167, 2.716872215270996, 3.4695627689361572, 2.690575122833252, 3.2491681575775146, 2.8593735694885254, 2.946146011352539, 2.3212246894836426, 1.7376521825790405]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.559310436248779, 6.829841136932373, 6.542496204376221, 6.3115129470825195, 6.471965312957764, 6.487813949584961, 6.752974033355713, 6.650219440460205, 6.647490978240967, 6.338229179382324, 6.935239315032959, 6.266430377960205, 5.287553310394287, 7.213708877563477, 6.495004653930664, 6.447991371154785]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.014965534210205, 5.358389377593994, 4.9669013023376465, 4.052731513977051, 4.905592441558838, 5.2551589012146, 4.635000228881836, 4.784965991973877, 5.077449798583984, 4.850882053375244, 5.278224945068359, 4.879810333251953, 5.150929927825928, 5.435965538024902, 4.605523109436035, 5.476123332977295]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.1569061279296875, 4.670934200286865, 3.3395822048187256, 4.20051908493042, 4.46432638168335, 4.0830078125, 4.245086669921875, 4.233426094055176, 4.367012023925781, 4.477716445922852, 4.060014724731445, 4.433310508728027, 3.6771247386932373, 2.0264666080474854, 3.3781089782714844, 4.0211501121521]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.289579391479492, 6.23814058303833, 5.913235664367676, 6.085315704345703, 5.428928375244141, 5.8514580726623535, 6.175764083862305, 5.700253486633301, 5.344794750213623, 5.75276517868042, 6.434260845184326, 5.521944522857666, 5.773019790649414, 6.2127580642700195, 6.2267045974731445, 5.784453868865967]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.837947130203247, 3.672658681869507, 3.782567024230957, 3.950808525085449, 2.687861442565918, 3.8651819229125977, 3.942716360092163, 3.6614181995391846, 3.9940555095672607, 3.615697145462036, 3.7005300521850586, 3.6253204345703125, 3.664862871170044, 3.6004738807678223, 3.71097731590271, 3.897561550140381]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  94%|█████████████████████████████████████████████▎  | 189/200 [06:11<00:18,  1.69s/it]Layer: gate_26 - Captured router_logits: [3.454166889190674, 3.5327417850494385, 3.4057629108428955, 3.5574347972869873, 3.384657382965088, 3.628845453262329, 3.5758745670318604, 3.367762565612793, 3.605883836746216, 3.300208568572998, 3.604173183441162, 3.4484052658081055, 3.4364538192749023, 2.728644847869873, 3.6543588638305664, 3.573540449142456]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.185140609741211, 3.2109107971191406, 2.9827377796173096, 2.8980159759521484, 2.8186376094818115, 3.0863568782806396, 3.185098648071289, 3.2749972343444824, 3.031327486038208, 3.005667209625244, 2.288543939590454, 2.9983274936676025, 3.15934419631958, 3.063955068588257, 3.078050136566162, 2.887216091156006]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.366781234741211, 4.48433256149292, 4.556332111358643, 4.362704277038574, 4.48286247253418, 4.4583306312561035, 4.210605621337891, 4.702043533325195, 4.485161304473877, 4.447463035583496, 4.4605631828308105, 4.046405792236328, 4.188601493835449, 4.046873569488525, 4.564889907836914, 4.365453720092773]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.570650100708008, 8.901758193969727, 8.77175235748291, 8.455952644348145, 8.541606903076172, 8.255818367004395, 8.626361846923828, 8.52488899230957, 8.612687110900879, 8.476872444152832, 8.585247039794922, 8.22382640838623, 8.32948112487793, 8.985689163208008, 8.888795852661133, 8.496150016784668]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.888675689697266, 5.431612014770508, 5.3835296630859375, 4.790277004241943, 5.13370418548584, 5.0822319984436035, 4.669820785522461, 5.134991645812988, 5.236454010009766, 4.926799297332764, 4.831858158111572, 4.729606628417969, 4.862863540649414, 4.974490165710449, 5.066159725189209, 4.832884311676025]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2306740283966064, 2.998457193374634, 3.105560064315796, 2.758007287979126, 3.026535749435425, 3.310237407684326, 2.7508699893951416, 3.1725049018859863, 3.084120750427246, 3.179112434387207, 3.174262762069702, 3.2619752883911133, 3.0324485301971436, 2.9778170585632324, 3.1214091777801514, 3.172762870788574]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11800436675548553, 0.13373494148254395, 0.12654700875282288, -0.24163074791431427, -0.2432364970445633, -0.0936577245593071, 0.14130765199661255, -0.16747161746025085, 0.10123768448829651, 0.11261346191167831, 0.11198146641254425, 0.07261670380830765, 0.10552666336297989, 0.12425616383552551, -1.10236394405365, 0.1353965550661087]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08742192387580872, 0.04983485862612724, 0.04092788323760033, 0.06217305734753609, 0.08370798826217651, 0.0377761647105217, 0.05256356671452522, 0.07661902904510498, 0.016063343733549118, 0.061053257435560226, -0.1812063455581665, 0.0603669248521328, 0.0045636799186468124, -0.0035751122049987316, 0.020947206765413284, 0.026175493374466896]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0698709711432457, 0.044159624725580215, 0.09219666570425034, 0.056118208914995193, 0.09307525306940079, 0.10218658298254013, 0.05568184703588486, -0.14396730065345764, 0.06445132941007614, 0.08206598460674286, 0.00209143222309649, 0.07760333269834518, -0.21763907372951508, -0.007167575880885124, -0.06078450009226799, 0.12490272521972656]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21400810778141022, 0.14784389734268188, 0.1254461705684662, 0.1472236067056656, 0.0421798974275589, 0.055209387093782425, -0.09248841553926468, 0.1800774335861206, 0.1520148515701294, -0.44782212376594543, 0.02754352055490017, 0.16228708624839783, 0.09055768698453903, -0.2839590609073639, -0.0838940367102623, -0.04617895931005478]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08000876009464264, -0.03021133877336979, -0.06278697401285172, 0.14397308230400085, -0.06582804024219513, -0.15095645189285278, 0.0920901671051979, 0.04820560663938522, 0.07094644755125046, 0.20317086577415466, -0.2755456566810608, 0.028300270438194275, 0.007652647327631712, -0.17898009717464447, 0.08001082390546799, 0.02012304961681366]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.20958150923252106, 0.2367778718471527, 0.13814744353294373, 0.04962535202503204, -0.2998514473438263, -0.06180313602089882, 0.09496348351240158, 0.009912462905049324, 0.15922331809997559, -0.06403540819883347, -0.4922315180301666, 0.046527594327926636, -0.3434567153453827, 0.25214269757270813, 0.15414801239967346, 0.148117795586586]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.31230175495147705, 0.3329591155052185, 0.1871519535779953, 0.2662332355976105, 0.25899866223335266, 0.2421659231185913, 0.12756770849227905, 0.15386894345283508, -0.3812630772590637, 0.3902735710144043, -0.0735328271985054, 0.3200879693031311, 0.0881032720208168, -0.06633923947811127, 0.27447566390037537, 0.1662147492170334]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05031305551528931, 0.2027796059846878, 0.12102721631526947, 0.16090363264083862, -0.35774970054626465, 0.20633426308631897, 0.057327382266521454, 0.21626396477222443, -0.028365839272737503, -0.16774727404117584, 0.06405313313007355, -0.06704805046319962, 0.17783500254154205, 0.10719460994005203, -0.019598407670855522, 0.15009447932243347]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2074381709098816, 0.2785331606864929, -0.08041689544916153, 0.3195916712284088, 0.4564875066280365, 0.39758190512657166, 0.11314937472343445, -0.0001802327751647681, 0.6072015166282654, -0.20318575203418732, 0.2540573179721832, 0.05219145119190216, 0.6643596291542053, -0.19637732207775116, -0.019283516332507133, 0.18805845081806183]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8645950555801392, 0.7524754405021667, 0.6128087043762207, 0.5617289543151855, 0.8462347388267517, 0.44916507601737976, 1.1096149682998657, 1.055066466331482, 0.09563169628381729, 0.034161023795604706, 0.44211336970329285, 0.7855077385902405, 0.7542700171470642, 0.8301947712898254, 1.109572172164917, 0.7903342843055725]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.15914785861969, 1.168023943901062, 0.6608558893203735, 1.0688894987106323, 0.9538841247558594, 0.36615586280822754, 0.988248884677887, 1.414762020111084, 0.7791942358016968, 0.2888931930065155, 1.0996747016906738, 1.1090738773345947, 0.6493009328842163, 1.0939100980758667, 0.49468356370925903, 1.0128211975097656]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2390615940093994, 1.5909911394119263, 1.529484510421753, 1.3946198225021362, 1.429805040359497, 0.8638901114463806, 1.741692066192627, 1.2318806648254395, 1.4782453775405884, 1.4426480531692505, 1.2699147462844849, 1.1744275093078613, 1.1653608083724976, 1.341458797454834, 1.2180544137954712, 1.2044124603271484]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0593860149383545, 1.510258436203003, 1.6417596340179443, 1.7889891862869263, 1.263214111328125, 1.5826677083969116, 1.274170994758606, 1.5113424062728882, 1.356062889099121, 0.6593883633613586, 1.6939622163772583, 1.945410132408142, 1.7599388360977173, 1.6763396263122559, 1.4266879558563232, 1.5339757204055786]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.089660406112671, 3.8514108657836914, 3.0461153984069824, 2.6717820167541504, 3.484168767929077, 2.116011619567871, 3.653465747833252, 3.146145820617676, 3.8181965351104736, 3.3865578174591064, 3.182742118835449, 3.937366247177124, 2.1513547897338867, 3.960085153579712, 3.1586618423461914, 2.806945323944092]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.39045786857605, 2.3613672256469727, 2.843947649002075, 2.42569899559021, 2.796621084213257, 2.07778263092041, 1.5801576375961304, 2.076892852783203, 2.628013849258423, 1.9952999353408813, 1.3521883487701416, 1.9453006982803345, 2.4327614307403564, 2.6697561740875244, 2.633854389190674, 1.9969143867492676]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.5777885913848877, 3.4126031398773193, 3.0562965869903564, 3.116973876953125, 3.0436062812805176, 2.850116729736328, 2.7798726558685303, 2.042602062225342, 3.276721954345703, 2.9466307163238525, 2.779754161834717, 1.3813984394073486, 2.917069911956787, 2.767543315887451, 3.033957004547119, 2.075019121170044]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9252017736434937, 1.840790033340454, 2.638084650039673, 2.5924296379089355, 2.6274454593658447, 1.9935643672943115, 1.7642070055007935, 3.0813992023468018, 0.566898763179779, 2.163872718811035, 0.9796507954597473, 2.450039863586426, 2.7485175132751465, 2.9207873344421387, 2.9548044204711914, 1.4267653226852417]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.689566135406494, 3.949045419692993, 5.546850681304932, 4.9407267570495605, 5.510791301727295, 5.6656036376953125, 5.121840953826904, 5.113055229187012, 6.14617395401001, 5.025768756866455, 5.759618759155273, 6.065091133117676, 4.815781593322754, 5.385886192321777, 4.302396297454834, 4.949671268463135]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.206759452819824, 4.551455020904541, 5.530747890472412, 5.166810512542725, 3.6999754905700684, 4.45767879486084, 4.867866039276123, 5.280542373657227, 5.360903739929199, 4.907097816467285, 4.271238803863525, 3.346280336380005, 4.406099319458008, 3.387590169906616, 4.9108076095581055, 4.489461898803711]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.881124973297119, 5.8419575691223145, 6.041668891906738, 5.624605178833008, 6.079221248626709, 4.820210933685303, 5.693316459655762, 5.636855125427246, 5.298742294311523, 5.36288595199585, 6.220206260681152, 5.908812046051025, 4.961966037750244, 5.863484859466553, 6.045504093170166, 5.867709636688232]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.0030288696289062, 2.969538450241089, 2.8095524311065674, 2.2364392280578613, -0.4528035521507263, 3.043609380722046, 2.7209858894348145, 1.856738805770874, 2.714703321456909, 3.468238592147827, 2.6894452571868896, 3.2477588653564453, 2.858200788497925, 2.945787191390991, 2.3224074840545654, 1.7400246858596802]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.5553460121154785, 6.825784683227539, 6.5385847091674805, 6.30784797668457, 6.467426776885986, 6.48373556137085, 6.749311923980713, 6.64726448059082, 6.64413595199585, 6.335091590881348, 6.931102275848389, 6.260483264923096, 5.2847490310668945, 7.210027694702148, 6.491214275360107, 6.444639205932617]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.012823581695557, 5.356200695037842, 4.965519428253174, 4.051181793212891, 4.902784824371338, 5.252727031707764, 4.632895469665527, 4.78337287902832, 5.074805736541748, 4.849410057067871, 5.275554180145264, 4.876789093017578, 5.148850917816162, 5.434182167053223, 4.604251861572266, 5.4741129875183105]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.156368732452393, 4.670387268066406, 3.339852809906006, 4.200033664703369, 4.463693141937256, 4.0818657875061035, 4.244769096374512, 4.232741832733154, 4.366312503814697, 4.476527214050293, 4.059200763702393, 4.4329352378845215, 3.6759908199310303, 2.0259761810302734, 3.3783159255981445, 4.02052640914917]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.286855220794678, 6.235559940338135, 5.911538124084473, 6.0824713706970215, 5.426994800567627, 5.848991870880127, 6.173825263977051, 5.698437213897705, 5.342217445373535, 5.750690460205078, 6.431180953979492, 5.520007133483887, 5.769848346710205, 6.210056304931641, 6.224584579467773, 5.781843185424805]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.836829662322998, 3.671999216079712, 3.7810306549072266, 3.9494991302490234, 2.687169075012207, 3.8640806674957275, 3.9416022300720215, 3.66074538230896, 3.9928152561187744, 3.6150519847869873, 3.6989543437957764, 3.624459981918335, 3.6642744541168213, 3.5989534854888916, 3.709890842437744, 3.8965930938720703]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4529225826263428, 3.5319700241088867, 3.404756546020508, 3.556440591812134, 3.3830690383911133, 3.627857208251953, 3.5755398273468018, 3.3659019470214844, 3.6047933101654053, 3.300065279006958, 3.6038317680358887, 3.447307825088501, 3.4358372688293457, 2.7272145748138428, 3.653465747833252, 3.5719900131225586]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1843554973602295, 3.210764169692993, 2.9823315143585205, 2.8970565795898438, 2.817869186401367, 3.086291551589966, 3.184988498687744, 3.274190902709961, 3.0308964252471924, 3.004884719848633, 2.2879738807678223, 2.997781753540039, 3.158520460128784, 3.0637590885162354, 3.077723979949951, 2.8864011764526367]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.367016792297363, 4.484324932098389, 4.556113243103027, 4.362443447113037, 4.482949733734131, 4.457985877990723, 4.210074424743652, 4.70178747177124, 4.485382556915283, 4.447207450866699, 4.460919380187988, 4.046566009521484, 4.188966274261475, 4.047114849090576, 4.564945220947266, 4.365665912628174]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.569965362548828, 8.901970863342285, 8.772522926330566, 8.45536994934082, 8.542007446289062, 8.25648307800293, 8.625895500183105, 8.52574634552002, 8.612062454223633, 8.476869583129883, 8.58525276184082, 8.22331428527832, 8.329476356506348, 8.986132621765137, 8.888821601867676, 8.49467945098877]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  96%|██████████████████████████████████████████████▎ | 193/200 [06:18<00:11,  1.70s/it]Layer: gate_30 - Captured router_logits: [4.887751579284668, 5.4304022789001465, 5.382342338562012, 4.788961887359619, 5.133440971374512, 5.08182430267334, 4.669151306152344, 5.134360313415527, 5.236264705657959, 4.9258599281311035, 4.830521583557129, 4.728220462799072, 4.861860275268555, 4.973416328430176, 5.0652618408203125, 4.831866264343262]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2296009063720703, 2.9976937770843506, 3.105048179626465, 2.7583179473876953, 3.025876522064209, 3.3097259998321533, 2.751253366470337, 3.1721620559692383, 3.0833752155303955, 3.178684711456299, 3.173964023590088, 3.2606780529022217, 3.032128095626831, 2.9775466918945312, 3.1213529109954834, 3.172152042388916]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.11704523116350174, 0.1321806013584137, 0.1262160986661911, -0.24278677999973297, -0.2449205368757248, -0.08984920382499695, 0.14011046290397644, -0.16806446015834808, 0.09775256365537643, 0.11213526129722595, 0.11147817224264145, 0.07041864097118378, 0.10429445654153824, 0.12364602088928223, -1.101145625114441, 0.13473644852638245]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08673688769340515, 0.04889857769012451, 0.03968930244445801, 0.06180042773485184, 0.08241327106952667, 0.03908772021532059, 0.05328594148159027, 0.076210156083107, 0.0165303573012352, 0.061427339911460876, -0.18142810463905334, 0.061156585812568665, 0.003497406607493758, -0.0015174051513895392, 0.020600657910108566, 0.026870815083384514]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06995090842247009, 0.043176792562007904, 0.09033950418233871, 0.05460546538233757, 0.09300938248634338, 0.10104311257600784, 0.05308157578110695, -0.14402663707733154, 0.06409689038991928, 0.08581235259771347, 0.001408143900334835, 0.07717032730579376, -0.21963226795196533, -0.006891496945172548, -0.06307914853096008, 0.12629148364067078]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.21464942395687103, 0.1470247209072113, 0.12670153379440308, 0.14295589923858643, 0.042718756943941116, 0.05145110934972763, -0.0981610119342804, 0.17752063274383545, 0.15036998689174652, -0.4439631402492523, 0.03250483423471451, 0.16150104999542236, 0.09044793248176575, -0.2879411578178406, -0.07550220936536789, -0.04682532325387001]
Layer: gate_3 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07915530353784561, -0.02965281344950199, -0.065248504281044, 0.1470704972743988, -0.06371160596609116, -0.14857929944992065, 0.08811167627573013, 0.04851377382874489, 0.06714494526386261, 0.2005317509174347, -0.27828460931777954, 0.028609924018383026, 0.012075399048626423, -0.17965549230575562, 0.08268491178750992, 0.016753114759922028]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.20962394773960114, 0.23680180311203003, 0.13939525187015533, 0.05144670233130455, -0.3010479807853699, -0.06421460956335068, 0.09613185375928879, 0.009872806258499622, 0.159173384308815, -0.06287513673305511, -0.493220716714859, 0.04586949199438095, -0.34215790033340454, 0.2528880834579468, 0.15245145559310913, 0.14879758656024933]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.3131854236125946, 0.3302135467529297, 0.19002842903137207, 0.2650732398033142, 0.26011425256729126, 0.242762491106987, 0.12350419163703918, 0.1507522463798523, -0.3804270327091217, 0.3920820355415344, -0.07548890262842178, 0.3211643695831299, 0.08828365057706833, -0.06797260046005249, 0.27395421266555786, 0.1659245491027832]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.04870283976197243, 0.20310047268867493, 0.12047023326158524, 0.1612706184387207, -0.35666316747665405, 0.2065931260585785, 0.05805657431483269, 0.2115400731563568, -0.025789247825741768, -0.16669145226478577, 0.06370849907398224, -0.06625726073980331, 0.17773960530757904, 0.10881401598453522, -0.01897387020289898, 0.14851579070091248]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20737390220165253, 0.27881231904029846, -0.08330968022346497, 0.3201490342617035, 0.4575968086719513, 0.3983079791069031, 0.11319947242736816, 0.0012822630815207958, 0.6074849367141724, -0.20142778754234314, 0.2556605935096741, 0.05115562304854393, 0.665772557258606, -0.1955195516347885, -0.021121732890605927, 0.18930858373641968]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8649353981018066, 0.7509820461273193, 0.6133464574813843, 0.5645676851272583, 0.8462786674499512, 0.4490339457988739, 1.1096798181533813, 1.0556553602218628, 0.09674805402755737, 0.035076674073934555, 0.4417303800582886, 0.7870468497276306, 0.7528051137924194, 0.8280872106552124, 1.1097214221954346, 0.7921061515808105]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1597647666931152, 1.1697640419006348, 0.66129070520401, 1.0700194835662842, 0.9533292651176453, 0.3651313781738281, 0.9896130561828613, 1.415920376777649, 0.7782023549079895, 0.28682661056518555, 1.1024730205535889, 1.1093965768814087, 0.649490475654602, 1.0921145677566528, 0.4973665475845337, 1.01129150390625]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.2400599718093872, 1.5913093090057373, 1.5288641452789307, 1.39266037940979, 1.4296202659606934, 0.861652672290802, 1.7428256273269653, 1.2289562225341797, 1.47996985912323, 1.4439268112182617, 1.2701717615127563, 1.1732487678527832, 1.162960171699524, 1.3400425910949707, 1.2140933275222778, 1.2027862071990967]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [1.0577332973480225, 1.5097274780273438, 1.643579125404358, 1.7885353565216064, 1.2601643800735474, 1.5826246738433838, 1.275822401046753, 1.5126910209655762, 1.3537955284118652, 0.6586913466453552, 1.6956180334091187, 1.9462283849716187, 1.7589974403381348, 1.677579641342163, 1.426396131515503, 1.5347896814346313]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [3.0872316360473633, 3.8532721996307373, 3.046893835067749, 2.667255163192749, 3.4869070053100586, 2.1124486923217773, 3.654433012008667, 3.1480302810668945, 3.8189620971679688, 3.3884875774383545, 3.1837077140808105, 3.9376707077026367, 2.1476643085479736, 3.956951856613159, 3.157850503921509, 2.806304931640625]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [2.3927414417266846, 2.3626646995544434, 2.845595598220825, 2.4272401332855225, 2.798645257949829, 2.077321767807007, 1.5796247720718384, 2.0815317630767822, 2.6283931732177734, 1.9971997737884521, 1.3497135639190674, 1.9452354907989502, 2.4357550144195557, 2.6739754676818848, 2.635575771331787, 1.9970738887786865]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [2.574294328689575, 3.410534381866455, 3.0542140007019043, 3.1155152320861816, 3.037619113922119, 2.8472723960876465, 2.7798209190368652, 2.042771577835083, 3.275763988494873, 2.945142984390259, 2.7752349376678467, 1.3775709867477417, 2.9160799980163574, 2.7663159370422363, 3.0312869548797607, 2.071333885192871]
Running loglikelihood requests:  98%|███████████████████████████████████████████████▎| 197/200 [06:25<00:05,  1.72s/it]Running loglikelihood requests: 100%|████████████████████████████████████████████████| 200/200 [06:25<00:00,  1.93s/it]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.9254066944122314, 1.8378366231918335, 2.6390151977539062, 2.592951774597168, 2.628312587738037, 1.9923137426376343, 1.7651299238204956, 3.080979347229004, 0.5644486546516418, 2.166729211807251, 0.9753726124763489, 2.448352336883545, 2.7494537830352783, 2.920846700668335, 2.9560725688934326, 1.4256565570831299]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [3.683265447616577, 3.9473514556884766, 5.545804977416992, 4.936214447021484, 5.5090179443359375, 5.664383411407471, 5.123262882232666, 5.110391616821289, 6.146317005157471, 5.023096084594727, 5.758585453033447, 6.062372207641602, 4.815268039703369, 5.385711669921875, 4.2984819412231445, 4.945167064666748]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [5.206377029418945, 4.551113128662109, 5.527840614318848, 5.167027950286865, 3.698093891143799, 4.456307411193848, 4.867518424987793, 5.278178691864014, 5.360482215881348, 4.905430793762207, 4.268436431884766, 3.3435912132263184, 4.402183532714844, 3.3843040466308594, 4.9103522300720215, 4.488109588623047]
Layer: gate_18 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [5.879384994506836, 5.840404033660889, 6.041037082672119, 5.623736381530762, 6.078165054321289, 4.817976474761963, 5.690803050994873, 5.6350579261779785, 5.298680782318115, 5.362430572509766, 6.2198567390441895, 5.9078898429870605, 4.960357666015625, 5.862017631530762, 6.044737339019775, 5.867190837860107]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [3.003323793411255, 2.9697418212890625, 2.8093883991241455, 2.236342668533325, -0.454283207654953, 3.0444722175598145, 2.7204902172088623, 1.8558322191238403, 2.713801860809326, 3.4676952362060547, 2.689934730529785, 3.247809886932373, 2.857987642288208, 2.9444549083709717, 2.3205671310424805, 1.7372092008590698]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [6.556358337402344, 6.826836585998535, 6.540403842926025, 6.308896064758301, 6.467957973480225, 6.484638214111328, 6.750102996826172, 6.647449970245361, 6.645235538482666, 6.336066246032715, 6.932231903076172, 6.262537956237793, 5.2861328125, 7.2112135887146, 6.492263317108154, 6.445445537567139]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [5.013644695281982, 5.358016490936279, 4.966971397399902, 4.052420616149902, 4.904238700866699, 5.2541303634643555, 4.634188175201416, 4.784224987030029, 5.076361656188965, 4.850411891937256, 5.277500629425049, 4.878863334655762, 5.14984655380249, 5.434950828552246, 4.605125904083252, 5.474820613861084]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [4.157793998718262, 4.6715240478515625, 3.3410890102386475, 4.200446605682373, 4.464470386505127, 4.083209037780762, 4.245506286621094, 4.233202934265137, 4.367169380187988, 4.477908611297607, 4.059622764587402, 4.43477201461792, 3.6771981716156006, 2.027942180633545, 3.3791399002075195, 4.021177291870117]
Layer: gate_23 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [6.287949562072754, 6.23654842376709, 5.912593841552734, 6.0837554931640625, 5.428885459899902, 5.850655555725098, 6.1744279861450195, 5.69904088973999, 5.3436479568481445, 5.75123929977417, 6.432121276855469, 5.520895481109619, 5.77147912979126, 6.211163520812988, 6.225578308105469, 5.783038139343262]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [3.8371126651763916, 3.6720170974731445, 3.781350612640381, 3.950119972229004, 2.6879031658172607, 3.8643743991851807, 3.9413154125213623, 3.660902261734009, 3.993163585662842, 3.615161657333374, 3.699960708618164, 3.624249219894409, 3.6642189025878906, 3.5994043350219727, 3.7101500034332275, 3.897005081176758]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [3.4533252716064453, 3.531703233718872, 3.4048995971679688, 3.5565061569213867, 3.3832266330718994, 3.6281957626342773, 3.575465440750122, 3.3667218685150146, 3.605128526687622, 3.299619436264038, 3.6039512157440186, 3.447497844696045, 3.4355766773223877, 2.7274556159973145, 3.6532959938049316, 3.572741985321045]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_27 - Captured router_logits: [3.1844582557678223, 3.210366725921631, 2.9817380905151367, 2.897024631500244, 2.817906379699707, 3.085496425628662, 3.1843855381011963, 3.273617744445801, 3.0303397178649902, 3.004769802093506, 2.288360357284546, 2.9973697662353516, 3.158804178237915, 3.064258575439453, 3.077336549758911, 2.8860161304473877]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [4.3661417961120605, 4.483294486999512, 4.555469989776611, 4.361498832702637, 4.481449127197266, 4.456912517547607, 4.209794998168945, 4.700602054595947, 4.484372138977051, 4.446163177490234, 4.459648609161377, 4.045655727386475, 4.187719821929932, 4.04580020904541, 4.5634307861328125, 4.364505767822266]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [8.566635131835938, 8.899149894714355, 8.76708984375, 8.452285766601562, 8.538249969482422, 8.25320053100586, 8.622289657592773, 8.521690368652344, 8.608489036560059, 8.473297119140625, 8.58178424835205, 8.220401763916016, 8.326150894165039, 8.982263565063477, 8.885107040405273, 8.491199493408203]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_30 - Captured router_logits: [4.886221885681152, 5.4292988777160645, 5.381574630737305, 4.788482666015625, 5.132101058959961, 5.079676628112793, 4.6684722900390625, 5.1336236000061035, 5.236061096191406, 4.925005912780762, 4.829946041107178, 4.728831768035889, 4.86159610748291, 4.971954345703125, 5.064377307891846, 4.831122398376465]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.228773593902588, 2.99648380279541, 3.104017972946167, 2.756404161453247, 3.024923801422119, 3.308082342147827, 2.749688148498535, 3.1710994243621826, 3.0821924209594727, 3.177197217941284, 3.1722571849823, 3.260458469390869, 3.0309362411499023, 2.976552963256836, 3.1210875511169434, 3.171506404876709]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
{'gate_0': [6.055912457406521, 6.812783241271973, 6.4791609942913055, -12.207513585686684, -12.218661472201347, -4.9767701998353004, 7.152299761772156, -8.719626143574715, 5.024781666696072, 5.781944468617439, 5.7558789029717445, 3.7207507491111755, 5.339862190186977, 6.314518719911575, -55.69952189922333, 6.875763118267059], 'gate_1': [4.43074344098568, 2.551650445908308, 2.0633377470076084, 3.1294533275067806, 4.216819852590561, 1.8998488932847977, 2.643252767622471, 3.9052248373627663, 0.9349480215460062, 3.088562447577715, -9.015337377786636, 3.0355658307671547, 0.09619520118576474, -0.17298969048169965, 0.9686653623357415, 1.3639909103512764], 'gate_2': [3.527227334678173, 2.1958063654601574, 4.56099983304739, 2.637462478131056, 4.694546580314636, 5.131143428385258, 2.8454532250761986, -7.156647503376007, 3.1806663163006306, 4.242653734982014, 0.05123011847172165, 3.85123198479414, -10.727026104927063, -0.42791217821650207, -2.9487452022731304, 6.220575697720051], 'gate_3': [10.70090451836586, 7.409030959010124, 6.292562112212181, 7.235318690538406, 2.2167591899633408, 2.718011762946844, -4.4976155534386635, 8.91120857000351, 7.621809363365173, -22.225987136363983, 1.436332356184721, 8.064257457852364, 4.525561109185219, -14.239234894514084, -3.9095079489052296, -2.252376526594162], 'gate_4': [3.8459300249814987, -1.5203588865697384, -3.3466733135282993, 7.11781670153141, -3.309370767325163, -7.484090179204941, 4.4852751940488815, 2.4563807249069214, 3.363164532929659, 10.138338193297386, -14.034395575523376, 1.3428790718317032, 0.5916628448758274, -9.002199500799179, 4.0807850882411, 1.0661327964626253], 'gate_5': [10.562470138072968, 12.03287398815155, 7.1912502348423, 2.7735791355371475, -15.385704964399338, -2.944135319441557, 4.688435845077038, 0.2932143707148498, 8.235615864396095, -2.935199223458767, -25.43262568116188, 2.590646792203188, -16.884093433618546, 12.985853493213654, 7.744637221097946, 7.739312902092934], 'gate_6': [15.714971274137497, 16.651814609766006, 9.780082747340202, 13.299052655696869, 13.441951274871826, 12.172270745038986, 6.077220790088177, 7.578227028250694, -19.611088812351227, 19.560334354639053, -4.095112606883049, 15.744018465280533, 4.644096806645393, -3.696794033050537, 14.178115159273148, 8.73325178027153], 'gate_7': [-2.581787768751383, 9.996256455779076, 5.873393155634403, 7.980625793337822, -17.915094763040543, 10.12183541059494, 2.9332968704402447, 10.360529318451881, -1.4004060225561261, -8.454342141747475, 3.0270187705755234, -3.310701597481966, 8.592899829149246, 5.428887724876404, -1.2759305369108915, 7.174463726580143], 'gate_8': [10.537058502435684, 14.074306011199951, -4.594963684678078, 16.25120085477829, 23.035868048667908, 19.965674459934235, 5.765268586575985, -0.5543970010276098, 31.005667209625244, -9.483859449625015, 12.798846155405045, 2.6085561625659466, 33.64196848869324, -10.410161659121513, -1.50349005125463, 9.585289090871811], 'gate_9': [43.14784771203995, 37.54398584365845, 30.93990558385849, 28.716515243053436, 41.98004150390625, 21.931225210428238, 55.213364362716675, 53.46770370006561, 4.4167305789887905, 1.2095925901085138, 21.39465555548668, 39.539639353752136, 37.086411237716675, 41.01793551445007, 55.892537236213684, 39.44560772180557], 'gate_10': [57.48891246318817, 58.436903953552246, 32.70385658740997, 52.65156590938568, 47.5121203660965, 18.856663644313812, 49.34638971090317, 71.05697536468506, 38.4229519367218, 14.258968740701675, 54.850974798202515, 55.31887590885162, 31.185773074626923, 54.79526960849762, 24.94605341553688, 50.06059032678604], 'gate_11': [61.96031737327576, 79.99797475337982, 76.45724487304688, 70.39529407024384, 71.05551433563232, 41.700665295124054, 87.45333075523376, 61.128477692604065, 74.13073670864105, 72.61388957500458, 62.81160044670105, 58.353403091430664, 57.19081211090088, 66.57878077030182, 60.25366938114166, 59.732189536094666], 'gate_12': [52.230321764945984, 75.23207640647888, 82.42919051647186, 89.60202312469482, 62.63908302783966, 79.24052906036377, 64.5013941526413, 76.26465547084808, 67.50185739994049, 31.497016429901123, 85.00596284866333, 97.56263065338135, 87.9367504119873, 83.74306809902191, 71.15160322189331, 76.55983984470367], 'gate_13': [153.87018966674805, 193.10230588912964, 151.76242661476135, 132.49531030654907, 174.20843482017517, 104.3372437953949, 183.01612401008606, 158.45567560195923, 191.26672434806824, 169.13412165641785, 159.50998258590698, 196.42922282218933, 106.77610349655151, 197.8783302307129, 157.8777768611908, 139.93706369400024], 'gate_14': [119.57334566116333, 118.38993191719055, 142.34495544433594, 121.46293616294861, 140.2360725402832, 103.44610261917114, 78.702561378479, 105.0453097820282, 131.835791349411, 99.31394016742706, 67.29881834983826, 97.05641949176788, 121.4610538482666, 133.73338317871094, 132.01407289505005, 99.86480724811554], 'gate_15': [128.5113914012909, 170.5913805961609, 152.7632110118866, 155.80627846717834, 151.9731912612915, 142.14952659606934, 139.41554069519043, 102.65413165092468, 163.90415382385254, 147.26429319381714, 138.27733159065247, 68.66811013221741, 146.04443740844727, 138.23355746269226, 151.7465376853943, 103.30431270599365], 'gate_16': [96.14560520648956, 92.04154455661774, 131.99324250221252, 129.6912064552307, 131.41868257522583, 99.506422996521, 88.75425255298615, 154.57416152954102, 28.42212074995041, 108.67795276641846, 49.15460902452469, 122.57039308547974, 137.50025987625122, 146.2756757736206, 147.86895775794983, 71.45879900455475], 'gate_17': [184.1523506641388, 197.65222311019897, 277.8699116706848, 247.21370935440063, 275.64443254470825, 283.7236371040344, 256.52833795547485, 255.72111701965332, 307.6901578903198, 252.08394765853882, 288.40141439437866, 303.6306080818176, 241.51603603363037, 269.95253562927246, 215.3209958076477, 247.82143640518188], 'gate_18': [260.4306049346924, 227.78683280944824, 277.3994822502136, 258.46639108657837, 185.0509011745453, 223.0586953163147, 243.84287548065186, 264.5923767089844, 268.1456365585327, 245.67950963974, 213.48167991638184, 167.44531440734863, 220.3355951309204, 169.24069666862488, 245.90179586410522, 224.73122119903564], 'gate_19': [293.845486164093, 291.972975730896, 301.66657733917236, 281.20947217941284, 303.58448362350464, 240.72538042068481, 284.249249458313, 281.78531312942505, 265.10525035858154, 267.84371852874756, 310.5917978286743, 294.9836540222168, 247.34645795822144, 292.8294553756714, 302.1107611656189, 293.1749849319458], 'gate_20': [149.73819732666016, 148.64672136306763, 140.57601761817932, 112.21882700920105, -21.893816471099854, 152.1055657863617, 136.0350947380066, 92.86974358558655, 135.86137557029724, 173.39189267158508, 134.35480046272278, 162.28393077850342, 142.989919424057, 147.43247437477112, 116.53727626800537, 87.04308474063873], 'gate_21': [327.28296756744385, 340.78512144088745, 326.5966658592224, 315.0624489784241, 322.8941812515259, 323.82222175598145, 337.06586837768555, 331.5636405944824, 331.85820484161377, 316.236412525177, 346.06455278396606, 312.71280813217163, 263.93164825439453, 359.86696004867554, 324.06429290771484, 321.7183566093445], 'gate_22': [250.4336495399475, 267.54067850112915, 247.9649486541748, 202.3502984046936, 245.05502653121948, 262.38431882858276, 231.51550388336182, 239.01924467086792, 253.54396963119507, 242.31688976287842, 263.42351961135864, 243.79260110855103, 257.14356994628906, 271.6125988960266, 229.8999056816101, 273.7832908630371], 'gate_23': [207.23819780349731, 233.27098989486694, 166.8297622203827, 209.7544527053833, 222.66253566741943, 203.81545209884644, 212.0530161857605, 211.53438806533813, 218.01537132263184, 223.42013835906982, 202.735209941864, 221.36823797225952, 183.65230131149292, 101.20265245437622, 168.83799242973328, 200.910542011261], 'gate_24': [314.1812391281128, 311.79549980163574, 295.47346782684326, 304.05728006362915, 271.36013650894165, 292.46420335769653, 308.7565178871155, 285.1096053123474, 267.27855253219604, 287.68071365356445, 321.8118133544922, 276.0594849586487, 288.3732113838196, 310.43928050994873, 311.27517652511597, 289.21794271469116], 'gate_25': [191.77245664596558, 183.53562116622925, 189.1771945953369, 197.34338402748108, 134.38260531425476, 193.03718280792236, 196.95499348640442, 182.80438876152039, 199.5322241783142, 180.65677499771118, 184.8354549407959, 181.05627727508545, 183.11020588874817, 179.9495165348053, 185.22740745544434, 194.73290944099426], 'gate_26': [172.55023407936096, 176.2956521511078, 170.25929760932922, 177.75028944015503, 169.2157907485962, 181.06510639190674, 178.63355112075806, 168.31547927856445, 180.02355432510376, 164.80174040794373, 180.02245330810547, 172.36084747314453, 171.7779984474182, 136.34020471572876, 182.45492148399353, 178.26554346084595], 'gate_27': [159.07020711898804, 160.3877522945404, 149.19168186187744, 144.8028838634491, 140.93345046043396, 154.24989438056946, 159.0834572315216, 163.72285985946655, 151.61325311660767, 150.26931476593018, 114.36329078674316, 149.88249897956848, 157.78302764892578, 153.11008167266846, 154.0870840549469, 144.28353238105774], 'gate_28': [218.6019377708435, 224.41568279266357, 228.267596244812, 218.62239027023315, 224.51874208450317, 223.14465427398682, 210.81024074554443, 235.36345720291138, 224.5858235359192, 222.85143423080444, 223.41951084136963, 202.6508526802063, 209.8154649734497, 202.75219106674194, 228.7460527420044, 218.62020444869995], 'gate_29': [428.97035789489746, 445.23089027404785, 439.19974517822266, 423.40747451782227, 427.57325172424316, 413.2000322341919, 431.6752691268921, 426.92903232574463, 431.26243114471436, 424.5168151855469, 429.8207550048828, 411.5127468109131, 416.56562423706055, 449.92629528045654, 444.79057025909424, 425.2199821472168], 'gate_30': [244.3241868019104, 271.33779764175415, 269.20585775375366, 239.33065938949585, 256.6930932998657, 254.2875747680664, 233.37143087387085, 256.5442991256714, 261.81041431427, 246.22318983078003, 241.69076681137085, 236.4057674407959, 243.13109493255615, 248.95120573043823, 253.22608041763306, 241.52500438690186], 'gate_31': [161.61113691329956, 149.9721908569336, 155.05613017082214, 138.00772666931152, 151.1510899066925, 165.48218297958374, 137.6766767501831, 158.86421728134155, 154.22904515266418, 159.10609650611877, 158.65000891685486, 162.95739269256592, 151.6155879497528, 148.9974446296692, 156.02256798744202, 158.837584733963]}
{'gate_0': [0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_1': [48, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_2': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50], 'gate_3': [50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_4': [0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0, 0], 'gate_5': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0], 'gate_6': [0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0, 0], 'gate_7': [0, 1, 0, 0, 0, 6, 0, 43, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_8': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0], 'gate_9': [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 48, 0], 'gate_10': [0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_11': [0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_12': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0], 'gate_13': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0], 'gate_14': [0, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_15': [0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_16': [0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_17': [0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0, 0, 0], 'gate_18': [0, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_19': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0], 'gate_20': [0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0, 0], 'gate_21': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0], 'gate_22': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50], 'gate_23': [0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_24': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0], 'gate_25': [0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0, 0, 0], 'gate_26': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 0], 'gate_27': [0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_28': [0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_29': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0], 'gate_30': [0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_31': [0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}
{'mmlu_formal_logic': {'alias': 'formal_logic', 'acc,none': 0.28, 'acc_stderr,none': 0.06414269805898185}}
