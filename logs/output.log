/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2025-01-07:16:32:37,338 INFO     [huggingface.py:496] Model type cannot be determined. Using default model type 'causal'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2025-01-07:16:32:37,516 INFO     [huggingface.py:352] Model parallel was set to True, setting max memory per GPU to {0: 24974655488} and device map to auto
Loading checkpoint shards:   0%|                                                                 | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   6%|███▎                                                     | 1/17 [00:01<00:19,  1.22s/it]Loading checkpoint shards:  12%|██████▋                                                  | 2/17 [00:02<00:17,  1.19s/it]Loading checkpoint shards:  18%|██████████                                               | 3/17 [00:03<00:16,  1.19s/it]Loading checkpoint shards:  24%|█████████████▍                                           | 4/17 [00:04<00:15,  1.20s/it]Loading checkpoint shards:  29%|████████████████▊                                        | 5/17 [00:05<00:10,  1.11it/s]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████| 17/17 [00:05<00:00,  3.30it/s]
2025-01-07:16:32:45,969 WARNING  [big_modeling.py:440] Some parameters are on the meta device because they were offloaded to the disk.
all_gate number:32

PhiMoEForCausalLM(
  (model): PhiMoEModel(
    (embed_tokens): Embedding(32064, 4096)
    (layers): ModuleList(
      (0-31): 32 x PhiMoEDecoderLayer(
        (self_attn): PhiMoESdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
        )
        (block_sparse_moe): PhiMoESparseMoeBlock(
          (gate): Linear(in_features=4096, out_features=16, bias=False)
          (experts): ModuleList(
            (0-15): 16 x PhiMoEBlockSparseTop2MLP(
              (w1): Linear(in_features=4096, out_features=6400, bias=False)
              (w2): Linear(in_features=6400, out_features=4096, bias=False)
              (w3): Linear(in_features=4096, out_features=6400, bias=False)
              (act_fn): SiLU()
            )
          )
        )
        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=4096, out_features=32064, bias=True)
)
model
PhiMoEModel(
  (embed_tokens): Embedding(32064, 4096)
  (layers): ModuleList(
    (0-31): 32 x PhiMoEDecoderLayer(
      (self_attn): PhiMoESdpaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
        (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
        (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
        (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
      )
      (block_sparse_moe): PhiMoESparseMoeBlock(
        (gate): Linear(in_features=4096, out_features=16, bias=False)
        (experts): ModuleList(
          (0-15): 16 x PhiMoEBlockSparseTop2MLP(
            (w1): Linear(in_features=4096, out_features=6400, bias=False)
            (w2): Linear(in_features=6400, out_features=4096, bias=False)
            (w3): Linear(in_features=4096, out_features=6400, bias=False)
            (act_fn): SiLU()
          )
        )
      )
      (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.embed_tokens
Embedding(32064, 4096)
model.layers
ModuleList(
  (0-31): 32 x PhiMoEDecoderLayer(
    (self_attn): PhiMoESdpaAttention(
      (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
      (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
      (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
      (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
      (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
    )
    (block_sparse_moe): PhiMoESparseMoeBlock(
      (gate): Linear(in_features=4096, out_features=16, bias=False)
      (experts): ModuleList(
        (0-15): 16 x PhiMoEBlockSparseTop2MLP(
          (w1): Linear(in_features=4096, out_features=6400, bias=False)
          (w2): Linear(in_features=6400, out_features=4096, bias=False)
          (w3): Linear(in_features=4096, out_features=6400, bias=False)
          (act_fn): SiLU()
        )
      )
    )
    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  )
)
model.layers.0
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.0.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.0.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.0.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.0.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.0.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.0.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.0.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.0.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.0.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.0.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.0.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.0.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.1
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.1.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.1.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.1.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.1.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.1.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.1.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.1.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.1.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.1.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.1.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.1.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.1.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.2
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.2.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.2.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.2.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.2.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.2.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.2.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.2.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.2.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.2.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.2.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.2.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.2.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.3
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.3.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.3.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.3.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.3.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.3.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.3.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.3.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.3.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.3.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.3.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.3.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.3.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.4
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.4.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.4.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.4.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.4.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.4.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.4.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.4.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.4.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.4.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.4.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.4.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.4.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.5
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.5.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.5.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.5.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.5.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.5.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.5.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.5.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.5.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.5.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.5.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.5.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.5.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.6
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.6.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.6.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.6.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.6.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.6.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.6.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.6.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.6.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.6.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.6.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.6.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.6.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.7
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.7.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.7.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.7.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.7.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.7.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.7.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.7.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.7.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.7.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.7.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.7.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.7.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.8
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.8.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.8.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.8.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.8.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.8.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.8.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.8.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.8.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.8.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.8.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.8.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.8.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.9
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.9.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.9.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.9.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.9.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.9.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.9.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.9.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.9.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.9.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.9.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.9.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.9.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.10
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.10.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.10.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.10.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.10.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.10.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.10.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.10.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.10.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.10.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.10.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.10.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.10.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.11
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.11.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.11.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.11.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.11.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.11.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.11.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.11.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.11.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.11.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.11.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.11.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.11.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.12
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.12.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.12.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.12.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.12.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.12.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.12.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.12.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.12.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.12.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.12.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.12.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.12.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.13
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.13.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.13.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.13.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.13.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.13.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.13.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.13.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.13.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.13.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.13.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.13.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.13.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.14
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.14.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.14.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.14.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.14.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.14.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.14.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.14.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.14.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.14.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.14.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.14.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.14.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.15
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.15.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.15.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.15.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.15.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.15.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.15.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.15.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.15.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.15.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.15.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.15.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.15.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.16
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.16.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.16.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.16.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.16.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.16.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.16.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.16.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.16.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.16.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.16.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.16.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.16.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.17
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.17.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.17.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.17.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.17.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.17.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.17.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.17.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.17.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.17.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.17.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.17.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.17.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.18
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.18.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.18.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.18.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.18.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.18.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.18.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.18.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.18.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.18.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.18.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.18.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.18.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.19
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.19.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.19.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.19.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.19.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.19.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.19.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.19.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.19.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.19.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.19.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.19.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.19.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.20
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.20.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.20.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.20.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.20.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.20.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.20.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.20.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.20.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.20.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.20.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.20.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.20.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.21
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.21.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.21.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.21.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.21.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.21.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.21.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.21.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.21.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.21.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.21.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.21.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.21.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.22
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.22.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.22.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.22.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.22.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.22.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.22.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.22.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.22.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.22.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.22.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.22.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.22.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.23
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.23.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.23.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.23.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.23.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.23.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.23.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.23.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.23.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.23.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.23.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.23.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.23.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.24
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.24.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.24.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.24.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.24.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.24.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.24.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.24.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.24.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.24.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.24.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.24.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.24.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.25
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.25.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.25.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.25.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.25.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.25.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.25.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.25.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.25.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.25.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.25.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.25.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.25.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.26
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.26.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.26.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.26.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.26.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.26.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.26.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.26.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.26.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.26.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.26.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.26.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.26.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.27
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.27.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.27.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.27.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.27.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.27.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.27.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.27.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.27.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.27.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.27.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.27.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.27.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.28
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.28.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.28.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.28.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.28.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.28.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.28.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.28.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.28.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.28.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.28.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.28.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.28.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.29
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.29.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.29.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.29.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.29.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.29.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.29.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.29.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.29.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.29.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.29.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.29.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.29.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.30
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.30.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.30.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.30.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.30.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.30.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.30.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.30.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.30.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.30.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.30.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.30.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.30.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.31
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.31.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.31.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.31.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.31.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.31.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.31.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.31.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.31.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.31.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.31.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
2025-01-07:16:33:00,421 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-01-07:16:33:00,422 INFO     [evaluator.py:217] Using pre-initialized model
2025-01-07:16:33:04,325 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_formal_logic from None to 0
2025-01-07:16:33:04,325 INFO     [task.py:415] Building contexts for mmlu_formal_logic on rank 0...
model.layers.31.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.31.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.31.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.norm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
lm_head
Linear(in_features=4096, out_features=32064, bias=True)
  0%|                                                                                           | 0/126 [00:00<?, ?it/s] 40%|████████████████████████████████▏                                                | 50/126 [00:00<00:00, 486.03it/s] 79%|███████████████████████████████████████████████████████████████▍                | 100/126 [00:00<00:00, 490.87it/s]100%|████████████████████████████████████████████████████████████████████████████████| 126/126 [00:00<00:00, 491.28it/s]
2025-01-07:16:33:04,589 INFO     [evaluator.py:496] Running loglikelihood requests
Running loglikelihood requests:   0%|                                                           | 0/504 [00:00<?, ?it/s]Layer: gate_0 - Captured router_logits: [0.139588862657547, 0.15120211243629456, 0.15430253744125366, -0.29362353682518005, -0.21828942000865936, -0.11318109929561615, 0.16248376667499542, -0.24965383112430573, 0.11676401644945145, 0.13503211736679077, 0.1340920776128769, 0.11811769753694534, 0.1337687224149704, 0.15643849968910217, -1.259987473487854, 0.1663304716348648]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.09706562012434006, 0.0630401000380516, 0.04460087791085243, 0.0618584118783474, 0.10121404379606247, 0.06495679169893265, 0.055736783891916275, 0.0850420743227005, 0.020825453102588654, 0.08047318458557129, -0.16627074778079987, 0.07846590131521225, -0.0020125817973166704, 0.02084009349346161, 0.020131856203079224, 0.023573527112603188]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07617246359586716, 0.09269634634256363, 0.10725792497396469, 0.03725855052471161, 0.11713459342718124, 0.10967816412448883, 0.09407451748847961, -0.06938566267490387, 0.09916180372238159, 0.08249599486589432, 0.031015584245324135, 0.09649510681629181, -0.17473219335079193, 0.03323714807629585, -0.0465395487844944, 0.10022146999835968]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13836564123630524, 0.16577278077602386, 0.14184235036373138, 0.14184100925922394, 0.17202237248420715, 0.13892881572246552, 0.11542626470327377, 0.15781955420970917, 0.19694408774375916, -0.5599781274795532, 0.029794545844197273, 0.08903765678405762, 0.16671569645404816, -0.2917845845222473, -0.04330551624298096, -0.0484481044113636]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0848076269030571, 0.12275344878435135, 0.09992126375436783, 0.10421239584684372, 0.0719103217124939, -0.08001665771007538, 0.06344064325094223, 0.09720181673765182, -0.1278841197490692, 0.0033239983022212982, -0.15403638780117035, 0.18240875005722046, -0.1005852147936821, -0.15948620438575745, 0.13185645639896393, -0.05284677445888519]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.1510705202817917, 0.17141170799732208, 0.13033469021320343, 0.09787309914827347, -0.0993519127368927, -0.009451799094676971, 0.008082134649157524, 0.027308087795972824, 0.1212163195014, -0.10205991566181183, -0.3122025728225708, 0.1073668822646141, -0.1366688311100006, 0.1530301719903946, 0.1375931203365326, 0.08448979258537292]
Layer: gate_5 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.27124735713005066, 0.15907956659793854, 0.16278591752052307, 0.29964420199394226, 0.18648077547550201, 0.1666458547115326, -0.06459249556064606, -0.08382969349622726, 0.25035467743873596, 0.2239978164434433, -0.48237964510917664, 0.2278711348772049, 0.21310457587242126, -0.11650429666042328, 0.13278217613697052, 0.16473224759101868]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.33365386724472046, 0.40336719155311584, 0.29531624913215637, 0.22503317892551422, -0.7108412981033325, 0.2552376091480255, 0.2440551370382309, -0.2553699016571045, 0.2910444140434265, 0.15353995561599731, -0.18721061944961548, 0.2864179015159607, 0.10634622722864151, 0.22263088822364807, -0.48832574486732483, -0.45377683639526367]
Layer: gate_7 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3138095736503601, 0.34722083806991577, -0.19842185080051422, 0.3164658844470978, 0.33818477392196655, 0.4222509264945984, 0.30903077125549316, -0.05960630998015404, 0.3241393268108368, 0.6848602294921875, 0.09447844326496124, -0.07161836326122284, 0.5240186452865601, -0.39604413509368896, -0.2515968084335327, 0.30435770750045776]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.6106692552566528, 0.3915351629257202, 0.4155735373497009, 0.7928561568260193, 0.5338396430015564, 0.39393723011016846, 0.8610947132110596, 0.6833754181861877, 0.847366988658905, 0.1070336326956749, 0.3070281147956848, 0.6705046892166138, 0.5962310433387756, 0.37664365768432617, 0.7161601185798645, 0.7502286434173584]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9834328293800354, 0.8406243324279785, 0.542832612991333, 0.8807923197746277, 0.9930196404457092, 0.4862726628780365, 0.6533443927764893, 0.7841212153434753, 0.4491405785083771, 0.6911120414733887, 0.7280806303024292, 0.8496160507202148, 0.10148481279611588, 0.4057137966156006, 1.0417747497558594, 0.6216127872467041]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [1.0236012935638428, 1.1480051279067993, 0.9816378951072693, 1.3754076957702637, 1.222159743309021, 0.7340558767318726, 1.2367992401123047, 1.122607946395874, 1.080814003944397, 1.411717414855957, 1.0550674200057983, 1.3215572834014893, 0.8157680630683899, 0.8109498023986816, 0.4262005388736725, 0.9709489941596985]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.4075649380683899, 1.1641982793807983, 1.3303050994873047, 1.1405837535858154, 0.6272496581077576, 0.8864195942878723, 1.043204665184021, 1.1983401775360107, 0.8319936990737915, 0.4047030508518219, 1.036534070968628, 0.9815691113471985, 1.0264188051223755, 0.9789746999740601, 0.74554443359375, 1.0147584676742554]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.3508472442626953, 1.1480644941329956, 1.5262451171875, 0.9241564869880676, 1.4272048473358154, 0.4418657422065735, 1.2932679653167725, 2.2759408950805664, 1.3167526721954346, 1.2796682119369507, 1.5963908433914185, 1.4458179473876953, 0.792354941368103, 1.3194563388824463, 1.7047204971313477, 1.092369794845581]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.1561126708984375, 0.9264019131660461, 1.092917799949646, 1.1115894317626953, 1.3156462907791138, 2.0746850967407227, 0.9939454793930054, 1.7178815603256226, 1.0084460973739624, 0.9311714768409729, 0.31312647461891174, 1.1039003133773804, 1.2778183221817017, 1.19125497341156, 1.1809349060058594, 1.0325998067855835]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.8673301935195923, 1.0859615802764893, 1.3348439931869507, 1.0661861896514893, 1.065584421157837, 1.1349650621414185, 1.703248381614685, 1.1701419353485107, 1.035885214805603, 1.1064796447753906, 0.9835557341575623, 0.1924421340227127, 1.3160041570663452, 1.0374257564544678, 0.9893609881401062, 1.6889044046401978]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.7621126770973206, 0.4218408167362213, 0.8730649352073669, 0.67591792345047, 0.6765488982200623, 0.6427382230758667, 0.4775640368461609, 0.9121259450912476, -0.20044848322868347, 1.8972357511520386, -0.38631319999694824, 0.755970299243927, 0.9720579385757446, 0.8921775221824646, 0.8478212952613831, 0.5299927592277527]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.543880045413971, 1.1481796503067017, 1.7260605096817017, 1.7788808345794678, 1.8224031925201416, 1.7129731178283691, 1.7971707582473755, 1.8058600425720215, 1.976311445236206, 1.5402556657791138, 2.280970811843872, 1.8388500213623047, 1.728667140007019, 1.5281552076339722, 1.1832561492919922, 1.5166231393814087]
Running loglikelihood requests:   0%|                                                 | 1/504 [00:14<1:59:00, 14.20s/it]Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.7495460510253906, 1.6252751350402832, 1.7219479084014893, 1.887117624282837, 1.2961864471435547, 1.5621217489242554, 2.0245344638824463, 2.131437063217163, 2.4044723510742188, 2.0377421379089355, 1.63409423828125, 1.5299304723739624, 1.6240346431732178, 1.2097837924957275, 1.799172043800354, 1.7098509073257446]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.0956344604492188, 1.9554494619369507, 1.9957361221313477, 1.8679577112197876, 2.1036531925201416, 1.7795977592468262, 1.9583241939544678, 2.0248823165893555, 2.4574575424194336, 2.0416207313537598, 2.1553902626037598, 2.0756354331970215, 2.059467077255249, 2.0459327697753906, 2.1240508556365967, 2.0433263778686523]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2351202964782715, 1.54409658908844, 1.3337265253067017, 1.1649196147918701, 0.20360736548900604, 1.549338698387146, 1.5106115341186523, 2.2565231323242188, 1.0763674974441528, 1.5452725887298584, 1.4100152254104614, 1.4086440801620483, 1.3362400531768799, 1.5319101810455322, 1.282006025314331, 0.7515344619750977]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.3276023864746094, 2.4184226989746094, 2.3092265129089355, 2.152742624282837, 2.217594623565674, 2.359182357788086, 2.255460500717163, 2.368370294570923, 2.5569019317626953, 2.382042169570923, 2.539475202560425, 2.6882426738739014, 1.9592044353485107, 2.6923415660858154, 2.50909161567688, 2.405287265777588]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.8570642471313477, 1.7633693218231201, 2.125866413116455, 1.4392756223678589, 1.8957141637802124, 1.9189316034317017, 1.80483877658844, 1.8622496128082275, 1.9312554597854614, 1.9563435316085815, 1.865138053894043, 2.186124563217163, 1.879305124282837, 1.9360694885253906, 1.7073888778686523, 2.1426331996917725]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.174006938934326, 1.908203125, 1.5348570346832275, 1.7490991353988647, 1.7616636753082275, 1.7125192880630493, 1.9604904651641846, 1.7313628196716309, 1.7399042844772339, 1.7440030574798584, 1.749903678894043, 1.7271316051483154, 1.646236777305603, 0.9591754078865051, 1.7222316265106201, 1.8211102485656738]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.5815911293029785, 2.6570751667022705, 2.360145330429077, 2.4752421379089355, 2.732229232788086, 2.5798580646514893, 2.707443952560425, 2.525197982788086, 2.4439921379089355, 2.5154874324798584, 2.7307162284851074, 2.4215173721313477, 2.5371367931365967, 2.6035430431365967, 2.5477828979492188, 2.6027729511260986]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7494086027145386, 1.6564700603485107, 1.459864616394043, 1.7236946821212769, 1.2911738157272339, 1.6290162801742554, 1.6123665571212769, 1.7750605344772339, 1.6337339878082275, 1.5917693376541138, 1.6562328338623047, 1.565195918083191, 1.7182273864746094, 1.431984543800354, 1.6315470933914185, 1.8954527378082275]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.8230221271514893, 1.7143056392669678, 1.696144700050354, 1.8251540660858154, 1.7427823543548584, 1.8216127157211304, 1.9735221862792969, 1.7126224040985107, 1.805247187614441, 1.843190312385559, 2.1594908237457275, 1.6400116682052612, 1.8323270082473755, 1.7402137517929077, 1.7671319246292114, 1.8323969841003418]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5687137842178345, 1.564821720123291, 1.644889235496521, 1.6247764825820923, 1.5982649326324463, 1.5788285732269287, 1.6887418031692505, 1.6249802112579346, 1.6043123006820679, 1.7221336364746094, 1.33822762966156, 1.5969995260238647, 1.621431589126587, 1.6313502788543701, 1.5460789203643799, 1.53415048122406]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.167762517929077, 2.2049405574798584, 2.372021198272705, 2.19696307182312, 2.191502571105957, 2.206453561782837, 2.4855785369873047, 2.2989587783813477, 2.197072982788086, 2.285059928894043, 2.0969409942626953, 2.0100131034851074, 2.2847161293029785, 2.0457746982574463, 2.3201611042022705, 2.177356243133545]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.351974964141846, 5.589142322540283, 5.370378494262695, 5.258967876434326, 5.240908145904541, 5.702602386474609, 5.531800270080566, 5.526284694671631, 5.659111022949219, 5.416868209838867, 5.339685440063477, 5.253232479095459, 5.3177266120910645, 5.440113544464111, 5.542198657989502, 5.376045227050781]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.538264751434326, 3.5715091228485107, 3.617173671722412, 3.439603090286255, 3.4557178020477295, 3.442960500717163, 3.2246885299682617, 3.525899648666382, 3.478102922439575, 3.5166015625, 3.5424184799194336, 3.069953203201294, 3.425980806350708, 3.5343310832977295, 3.5462560653686523, 3.3897979259490967]
Layer: gate_30 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.874298572540283, 2.7969300746917725, 2.8987951278686523, 2.76953125, 2.8803367614746094, 2.8255116939544678, 2.9242682456970215, 2.608604669570923, 2.813490390777588, 2.7652535438537598, 2.824328899383545, 2.0910420417785645, 2.858865976333618, 2.7330546379089355, 2.8704886436462402, 2.989766836166382]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_0 - Captured router_logits: [0.1282244622707367, 0.14645588397979736, 0.13568630814552307, -0.2556040585041046, -0.17444835603237152, -0.19295495748519897, 0.15248365700244904, -0.2628989517688751, 0.08017775416374207, 0.12244126200675964, 0.1316283941268921, 0.08015575259923935, 0.11662328988313675, 0.12193001061677933, -1.108657956123352, 0.14387023448944092]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08692550659179688, 0.05856922268867493, 0.035553861409425735, 0.05428445339202881, 0.09742946922779083, 0.04608389362692833, 0.06133514270186424, 0.051371295005083084, -0.002179061761125922, 0.07318559288978577, -0.14073972404003143, 0.04380900785326958, 0.01327631063759327, -0.015410061925649643, -0.01046686340123415, 0.005646074656397104]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05602766573429108, 0.06811750680208206, 0.10116057842969894, 0.04400493949651718, 0.08874054253101349, 0.12241582572460175, 0.08207429200410843, -0.04892231523990631, 0.061615951359272, 0.016605980694293976, 0.01409344095736742, 0.0974549651145935, -0.1222207099199295, 0.0537603534758091, -0.0025175318587571383, 0.0962500125169754]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11105195432901382, 0.17140792310237885, 0.13830751180648804, 0.1385192573070526, 0.12448616325855255, 0.15400190651416779, 0.0576339066028595, 0.19566020369529724, 0.1846495270729065, -0.4300878643989563, -0.05219229683279991, 0.12143649905920029, 0.07772932201623917, -0.13868707418441772, -0.16015462577342987, -0.0019358466379344463]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.09738820791244507, 0.11451631784439087, 0.077504463493824, -0.002899815095588565, 0.035582948476076126, -0.135896235704422, 0.12414366006851196, 0.10382245481014252, -0.016769548878073692, 0.02484501153230667, -0.018180623650550842, 0.1958698332309723, -0.2244396209716797, -0.09977318346500397, 0.12274837493896484, 0.010289388708770275]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.14556868374347687, 0.21250534057617188, 0.11628931015729904, 0.016329709440469742, -0.036123499274253845, 0.14607393741607666, -0.022245602682232857, 0.08818463981151581, 0.0710270032286644, -0.08008283376693726, -0.42355817556381226, 0.13442185521125793, -0.020628761500120163, 0.13994935154914856, 0.19443608820438385, 0.10789353400468826]
Layer: gate_5 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.15533952414989471, 0.23890775442123413, 0.021040495485067368, 0.2690783739089966, 0.22185572981834412, 0.11975501477718353, 0.29361292719841003, 0.1971679925918579, 0.2111843377351761, 0.22173061966896057, -0.14332221448421478, 0.20019230246543884, 0.2635859251022339, 0.04113050177693367, 0.04458988457918167, 0.12850694358348846]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.17142143845558167, 0.383595734834671, 0.4430471360683441, 0.27174293994903564, -0.48959383368492126, 0.249527707695961, 0.25726577639579773, -0.027171527966856956, 0.13516953587532043, -0.07345157861709595, 0.17500470578670502, 0.29772651195526123, 0.41809317469596863, 0.1694032996892929, -0.07117635756731033, -0.33376380801200867]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3423537611961365, 0.524181604385376, 0.05216867849230766, 0.6217525601387024, 0.30611392855644226, 0.5123883485794067, 0.38419559597969055, 0.07039575278759003, 0.22226445376873016, 0.16293369233608246, 0.2891361713409424, -0.38461101055145264, 0.572913646697998, 0.20688247680664062, 0.03462892398238182, 0.14941921830177307]
Layer: gate_8 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.742826521396637, 0.9534862637519836, 0.4794667065143585, 0.5003128051757812, 0.6602444648742676, 0.33607617020606995, 0.875311017036438, 0.7536836266517639, 0.6581088304519653, 0.07182043045759201, 0.6371960639953613, 0.8255579471588135, 0.5408538579940796, 0.620803713798523, 0.8414593935012817, 0.8008818030357361]
Layer: gate_9 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.8966225981712341, 1.0879013538360596, 0.7716889977455139, 1.0792595148086548, 1.2544735670089722, 0.1731308549642563, 0.879913330078125, 0.9306245446205139, 1.2767081260681152, 0.5903556942939758, 0.863823413848877, 0.9132546782493591, 1.0323580503463745, 0.5665438175201416, 0.741732656955719, 1.0993773937225342]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.205519676208496, 1.240649700164795, 1.250481128692627, 0.8912167549133301, 1.1453641653060913, 0.7631061673164368, 1.215867042541504, 1.2378928661346436, 1.3017560243606567, 1.2981890439987183, 1.362135887145996, 1.5459414720535278, 1.4102495908737183, 1.234162449836731, 1.0476269721984863, 1.44488525390625]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.9452295899391174, 1.394606590270996, 1.2056849002838135, 1.2407379150390625, 1.0801063776016235, 1.218855857849121, 0.7604084014892578, 1.0304700136184692, 1.0950480699539185, 0.4662587642669678, 1.3024650812149048, 1.1691104173660278, 1.2535669803619385, 1.1680477857589722, 0.930046558380127, 1.2503877878189087]
Layer: gate_12 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [2.123872756958008, 1.6959731578826904, 2.021292209625244, 1.6716270446777344, 1.7795984745025635, 0.8778414726257324, 1.8421989679336548, 1.6661018133163452, 1.7007553577423096, 1.887465476989746, 1.6066560745239258, 2.091165065765381, 1.3747649192810059, 1.983025074005127, 2.0145695209503174, 1.8910558223724365]
Layer: gate_13 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.2650092840194702, 1.218003273010254, 1.306015968322754, 1.5321691036224365, 1.624946117401123, 1.7465636730194092, 0.7877569794654846, 0.7511630058288574, 1.2577730417251587, 1.2554895877838135, 0.6492751836776733, 1.4797735214233398, 1.2489511966705322, 1.5109145641326904, 1.4248549938201904, 1.279853343963623]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [1.393721580505371, 1.343024730682373, 1.645005226135254, 1.3483670949935913, 2.043600559234619, 1.5411089658737183, 1.2322039604187012, 1.5045524835586548, 1.244865894317627, 1.5456184148788452, 1.5222526788711548, 0.334875226020813, 1.677314281463623, 1.464219093322754, 1.3536951541900635, 0.9968705773353577]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.124251365661621, 0.8397396206855774, 1.0431536436080933, 0.8051649928092957, 0.9039162993431091, 1.0341442823410034, -0.025705954059958458, 1.053086280822754, -0.1517277956008911, 1.1563677787780762, 0.18114741146564484, 1.0241889953613281, 1.3071001768112183, 0.92327880859375, 1.0236475467681885, 0.3391209840774536]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.295545220375061, 1.329292893409729, 1.8907973766326904, 2.148078441619873, 1.9254796504974365, 1.9434024095535278, 1.8078829050064087, 1.961784839630127, 1.83056640625, 1.7178524732589722, 1.878066062927246, 1.7151812314987183, 1.640831470489502, 1.6893813610076904, 1.6315240859985352, 1.7369052171707153]
Layer: gate_17 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.9510282278060913, 1.8291445970535278, 1.972160816192627, 2.0121209621429443, 1.62957763671875, 1.9175666570663452, 1.7297650575637817, 2.4892578125, 2.225740432739258, 2.012451171875, 2.036283493041992, 1.6057003736495972, 1.987290382385254, 1.5278666019439697, 1.8814338445663452, 1.8647173643112183]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.420869827270508, 2.1089441776275635, 2.0588810443878174, 2.237821578979492, 2.3109490871429443, 2.0294907093048096, 2.3558852672576904, 2.2741663455963135, 2.183187961578369, 2.1701228618621826, 2.242819309234619, 2.2096736431121826, 2.2212631702423096, 2.3239028453826904, 2.3478429317474365, 2.2240779399871826]
Layer: gate_19 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0109494924545288, 1.3528836965560913, 1.4553581476211548, 1.3772250413894653, 0.2524028718471527, 1.3259384632110596, 1.572007179260254, 1.6862043142318726, 1.066032886505127, 1.3486328125, 1.3281430006027222, 1.1760594844818115, 1.23291015625, 1.3069206476211548, 1.2932406663894653, 0.7381576299667358]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.559067726135254, 2.58203125, 2.4790613651275635, 2.4754278659820557, 2.6806640625, 2.547952175140381, 2.5808680057525635, 2.4993393421173096, 2.4424402713775635, 2.7793686389923096, 2.7593348026275635, 2.502340793609619, 2.289274215698242, 2.7928826808929443, 2.679716110229492, 2.6026251316070557]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:   1%|▌                                                  | 5/504 [00:27<42:00,  5.05s/it]Layer: gate_22 - Captured router_logits: [1.8608757257461548, 1.7112821340560913, 1.8476992845535278, 1.6019179821014404, 2.0509536266326904, 1.9096966981887817, 1.7948356866836548, 1.795884132385254, 1.9124252796173096, 1.9155560731887817, 1.69140625, 1.7641171216964722, 1.921645164489746, 1.9079877138137817, 1.8309972286224365, 1.9110107421875]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [1.8588292598724365, 1.810905933380127, 1.7269681692123413, 1.7731071710586548, 1.865119457244873, 1.8203125, 1.7565487623214722, 1.8884564638137817, 1.7936508655548096, 1.6606875658035278, 1.920769214630127, 1.7707412242889404, 1.773667335510254, 1.3024777173995972, 1.7418479919433594, 2.1022517681121826]
Layer: gate_23 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_24 - Captured router_logits: [2.589872360229492, 2.655244827270508, 2.4288833141326904, 2.588895797729492, 2.485825538635254, 2.7371037006378174, 2.640653610229492, 2.567253589630127, 2.6066319942474365, 2.6450769901275635, 2.5959041118621826, 2.635641574859619, 2.49462890625, 2.6958582401275635, 2.761488914489746, 2.8449275493621826]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_25 - Captured router_logits: [1.8455020189285278, 1.715087890625, 1.6952550411224365, 1.8623046875, 1.4766271114349365, 1.734360694885254, 1.8606818914413452, 1.7694594860076904, 1.8458251953125, 1.83349609375, 1.7733944654464722, 1.8098574876785278, 1.8100873231887817, 1.8857852220535278, 1.618178367614746, 2.1991612911224365]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.7639877796173096, 1.6768198013305664, 1.941542625427246, 1.800529956817627, 1.662102222442627, 1.740168809890747, 1.710799217224121, 1.7001378536224365, 1.8142915964126587, 1.712028980255127, 1.863783836364746, 1.8143633604049683, 1.9014676809310913, 1.551628589630127, 1.6942659616470337, 1.7446194887161255]
Layer: gate_26 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.4662116765975952, 1.4640215635299683, 1.5063750743865967, 1.476783275604248, 1.5048037767410278, 1.454986572265625, 1.5281784534454346, 1.5333753824234009, 1.5575889348983765, 1.6683385372161865, 1.3721708059310913, 1.4986069202423096, 1.4434814453125, 1.4131571054458618, 1.6389518976211548, 1.4647396802902222]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1119959354400635, 2.1481215953826904, 2.227822780609131, 2.1955997943878174, 2.156135082244873, 2.1540958881378174, 2.1322667598724365, 2.20599365234375, 2.249741554260254, 2.218146800994873, 2.1396484375, 2.1564223766326904, 2.0786850452423096, 2.076157569885254, 2.272489547729492, 2.187995433807373]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_29 - Captured router_logits: [5.172420501708984, 5.246610641479492, 5.039837837219238, 5.368638038635254, 5.178409576416016, 5.033834934234619, 5.296501636505127, 5.417423248291016, 5.242618560791016, 5.283662796020508, 5.152630805969238, 5.055319309234619, 5.092536449432373, 5.098374366760254, 5.227797508239746, 5.078670501708984]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.766334056854248, 3.773552417755127, 3.769990921020508, 3.714707374572754, 3.7044758796691895, 3.5780532360076904, 3.8778152465820312, 3.698392868041992, 3.7862350940704346, 3.779839038848877, 3.7758071422576904, 3.577949047088623, 3.831815719604492, 3.624074697494507, 3.7672693729400635, 3.7608749866485596]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.013298511505127, 2.9774672985076904, 2.8669002056121826, 2.5526769161224365, 2.866929054260254, 3.033590793609619, 2.7701056003570557, 2.923095703125, 2.869284152984619, 2.981675148010254, 2.738152027130127, 2.540367603302002, 3.009995460510254, 3.005227565765381, 2.916963577270508, 3.012077808380127]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12769919633865356, 0.14769478142261505, 0.137608602643013, -0.2513186037540436, -0.2114255577325821, -0.18400415778160095, 0.14883600175380707, -0.24400842189788818, 0.09702063351869583, 0.11799450218677521, 0.1270141899585724, 0.12570370733737946, 0.10701946169137955, 0.13227657973766327, -1.1312124729156494, 0.1508544683456421]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.08936595916748047, 0.07159833610057831, 0.03946221247315407, 0.06389153003692627, 0.10302731394767761, 0.05246482044458389, 0.06413403898477554, 0.06383775919675827, -0.006752584129571915, 0.07071784138679504, -0.16620633006095886, 0.04741951450705528, -0.005103079602122307, -0.01690775156021118, -0.000613603217061609, 0.019340867176651955]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08259433507919312, 0.07419154047966003, 0.11721184104681015, 0.04083750396966934, 0.12338940799236298, 0.10765288025140762, 0.06513828039169312, -0.061278846114873886, 0.08089714497327805, 0.06305909901857376, 0.01427359040826559, 0.09516701102256775, -0.15736503899097443, 0.015718460083007812, -0.04816563427448273, 0.09486442804336548]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11655164510011673, 0.18912407755851746, 0.14058220386505127, 0.14146125316619873, 0.14142507314682007, 0.11081650853157043, 0.12348312884569168, 0.16269129514694214, 0.16725553572177887, -0.4463473856449127, -0.05596349388360977, 0.12155472487211227, 0.15130038559436798, -0.27856963872909546, -0.09516481310129166, -0.04360925406217575]
Layer: gate_3 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.10562184453010559, 0.10439039021730423, 0.08074674010276794, 0.09024625271558762, 0.031786058098077774, -0.07028359919786453, 0.10159625858068466, 0.11252560466527939, -0.09797229617834091, 0.031041303649544716, -0.14714500308036804, 0.1526055783033371, -0.1078353226184845, -0.09830432385206223, 0.11439604312181473, -0.02241482399404049]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.12798498570919037, 0.15435488522052765, 0.1432194709777832, 0.126320019364357, -0.0722193717956543, 0.03698161616921425, 0.007483253721147776, 0.02597915753722191, 0.08588263392448425, -0.06400808691978455, -0.3007461130619049, 0.09260125458240509, -0.1653202474117279, 0.13856038451194763, 0.12559601664543152, 0.09605977684259415]
Layer: gate_5 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.18061144649982452, 0.21379579603672028, 0.12954480946063995, 0.2524854242801666, 0.1799723207950592, 0.12555472552776337, 0.08783493936061859, -0.013366361148655415, 0.35738787055015564, 0.21903549134731293, -0.3287440240383148, 0.1967199742794037, 0.22758275270462036, -0.05610223114490509, 0.1324281543493271, 0.11375536769628525]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.34066009521484375, 0.4558041989803314, 0.3260103464126587, 0.21808773279190063, -0.5153430104255676, 0.192474365234375, 0.2285095453262329, 0.0788116455078125, 0.2888810932636261, 0.06763841211795807, -0.12411307543516159, 0.2866910398006439, 0.2718246877193451, 0.20368929207324982, -0.41965532302856445, -0.34837672114372253]
Layer: gate_7 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3517363667488098, 0.397007554769516, -0.1098601296544075, 0.3948526382446289, 0.38176342844963074, 0.47821474075317383, 0.2936667203903198, -0.16492518782615662, 0.34094807505607605, 0.6538387537002563, 0.18486478924751282, -0.17171914875507355, 0.5493884682655334, -0.26905879378318787, -0.09493556618690491, 0.26069411635398865]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.71254962682724, 0.7267833352088928, 0.33562973141670227, 0.8426427841186523, 0.5343350768089294, 0.4368920624256134, 0.9981622099876404, 0.6491482853889465, 1.0545690059661865, 0.2849542796611786, 0.44381657242774963, 0.6781668066978455, 0.39575308561325073, 0.5175957083702087, 0.7663313150405884, 0.7110960483551025]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.8455612063407898, 0.8446977138519287, 0.5991030931472778, 0.8644704222679138, 1.1506104469299316, 0.37033209204673767, 0.6801442503929138, 0.7305660247802734, 0.6649895310401917, 0.7283554673194885, 0.7686015367507935, 0.9012883305549622, 0.2409675270318985, 0.464413583278656, 1.205061912536621, 0.7300131320953369]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [1.0487992763519287, 1.0558117628097534, 1.001592755317688, 1.4070909023284912, 1.2772482633590698, 0.7807571887969971, 1.140163779258728, 1.14023756980896, 1.1796514987945557, 1.4149922132492065, 0.9798597693443298, 1.4788795709609985, 0.9588378667831421, 0.9408248662948608, 0.6098915934562683, 1.062961220741272]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.4514223635196686, 1.2778500318527222, 1.4744184017181396, 1.1680047512054443, 0.8347882032394409, 0.9904001355171204, 1.0634233951568604, 1.209251046180725, 0.9133772850036621, 0.32504159212112427, 1.1677182912826538, 1.0643666982650757, 1.1301873922348022, 1.00205397605896, 0.7874588966369629, 1.1095768213272095]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.33305823802948, 1.1842892169952393, 1.7150599956512451, 1.058806300163269, 1.4820830821990967, 0.46946462988853455, 1.3247736692428589, 2.3226332664489746, 1.316045880317688, 1.3774791955947876, 1.604690432548523, 1.4848146438598633, 0.910311222076416, 1.475426197052002, 1.6515510082244873, 1.1533509492874146]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.2822706699371338, 1.0026521682739258, 1.161594033241272, 1.2690339088439941, 1.4203741550445557, 2.663343667984009, 1.0311418771743774, 1.86955988407135, 1.152829885482788, 1.0410507917404175, 0.4241179823875427, 1.2570756673812866, 1.2428723573684692, 1.3534687757492065, 1.2167130708694458, 1.1152938604354858]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.8516048192977905, 1.0394624471664429, 1.3367879390716553, 1.1690137386322021, 1.155261754989624, 1.1499149799346924, 1.888898491859436, 1.216032862663269, 1.0071278810501099, 1.192991852760315, 1.0326662063598633, 0.15609775483608246, 1.5068683624267578, 1.1011444330215454, 0.9582492709159851, 1.8444132804870605]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.7588698863983154, 0.5990695357322693, 0.9389477372169495, 0.6481143236160278, 0.6091493368148804, 0.6449735760688782, 0.4454767405986786, 0.7462770938873291, -0.33617764711380005, 2.048757791519165, -0.4169427454471588, 0.7563152313232422, 0.9906379580497742, 0.888003408908844, 0.8570840358734131, 0.39130404591560364]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.5761851668357849, 1.2188609838485718, 1.725834608078003, 1.7843255996704102, 1.6741524934768677, 1.7736681699752808, 1.746915340423584, 1.8326834440231323, 1.9986919164657593, 1.59555983543396, 2.388779878616333, 1.6887648105621338, 1.6713083982467651, 1.4140300750732422, 1.1249622106552124, 1.3746927976608276]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.7994840145111084, 1.7587782144546509, 1.780795931816101, 1.9361883401870728, 1.378684639930725, 1.576532244682312, 2.0239851474761963, 2.2557945251464844, 2.6465203762054443, 1.9968289136886597, 1.6976710557937622, 1.5128177404403687, 1.7718303203582764, 1.1460744142532349, 1.8142729997634888, 1.764760136604309]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.0430984497070312, 1.849155306816101, 1.9077850580215454, 1.7165086269378662, 1.9696004390716553, 1.7054060697555542, 1.9297739267349243, 1.873079776763916, 2.364304542541504, 1.8585461378097534, 1.981910228729248, 2.029895067214966, 1.922653317451477, 1.9344297647476196, 1.9868541955947876, 1.8839584589004517]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1173235177993774, 1.3426076173782349, 1.2637799978256226, 1.1380321979522705, 0.2603673040866852, 1.327027678489685, 1.327986240386963, 2.4664366245269775, 1.0250194072723389, 1.3770215511322021, 1.311627984046936, 1.33725905418396, 1.210310459136963, 1.3517570495605469, 1.2767324447631836, 0.6775655746459961]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.2406883239746094, 2.3358943462371826, 2.2197158336639404, 2.09004545211792, 2.234057903289795, 2.2499279975891113, 2.2232184410095215, 2.2510666847229004, 2.134657621383667, 2.371915340423584, 2.417060613632202, 2.6937270164489746, 1.9721301794052124, 2.7468576431274414, 2.4955315589904785, 2.3686001300811768]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.927640676498413, 1.6809560060501099, 2.213531970977783, 1.5320860147476196, 1.9755967855453491, 1.8735873699188232, 1.7697619199752808, 1.9746742248535156, 1.8529174327850342, 1.9530528783798218, 1.7959524393081665, 2.129958391189575, 1.8869494199752808, 1.9080085754394531, 1.6523653268814087, 2.013477325439453]
Layer: gate_22 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.2289624214172363, 1.8991438150405884, 1.6914459466934204, 1.8779188394546509, 1.914884090423584, 1.8818612098693848, 1.767722249031067, 1.8876844644546509, 1.767698884010315, 1.8076640367507935, 1.8500778675079346, 1.7927922010421753, 1.7903093099594116, 1.175247073173523, 1.9023149013519287, 2.11011004447937]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.631803512573242, 2.650484323501587, 2.466184377670288, 2.5517759323120117, 2.8894717693328857, 2.663745403289795, 2.8571553230285645, 2.692227840423584, 2.672480344772339, 2.7140798568725586, 2.807541608810425, 2.5537362098693848, 2.5372750759124756, 2.656480550765991, 2.6319189071655273, 2.8125576972961426]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.8296096324920654, 1.7178707122802734, 1.5894978046417236, 1.804175853729248, 1.4064931869506836, 1.698447585105896, 1.687507152557373, 1.8331987857818604, 1.6849991083145142, 1.6868658065795898, 1.6847937107086182, 1.6685020923614502, 1.8061721324920654, 1.6465924978256226, 1.6978205442428589, 2.2035140991210938]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Running loglikelihood requests:   2%|▉                                                  | 9/504 [00:41<34:31,  4.19s/it]Layer: gate_26 - Captured router_logits: [1.9512944221496582, 1.7391794919967651, 1.9069454669952393, 1.89267897605896, 1.7900084257125854, 1.8089401721954346, 1.900383472442627, 1.863071322441101, 1.8172729015350342, 1.865216851234436, 2.2399094104766846, 1.6847630739212036, 1.9259830713272095, 1.7155572175979614, 1.7631889581680298, 1.809188723564148]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5453110933303833, 1.5871554613113403, 1.6223946809768677, 1.6553815603256226, 1.633734107017517, 1.5824024677276611, 1.6924008131027222, 1.6769343614578247, 1.7204724550247192, 1.7921651601791382, 1.4601447582244873, 1.6394214630126953, 1.575710654258728, 1.657493233680725, 1.6659796237945557, 1.5703269243240356]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.190361261367798, 2.3327484130859375, 2.4204444885253906, 2.3222367763519287, 2.246187448501587, 2.2499170303344727, 2.5463380813598633, 2.3277647495269775, 2.4656906127929688, 2.335256338119507, 2.217830181121826, 2.144949197769165, 2.181680202484131, 2.1103262901306152, 2.4712178707122803, 2.248854160308838]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.6171875, 5.778309345245361, 5.540503978729248, 5.710231304168701, 5.596935749053955, 5.564575672149658, 5.630016326904297, 5.836398601531982, 5.751210689544678, 5.597526550292969, 5.579422473907471, 5.45416259765625, 5.546629905700684, 5.599054336547852, 5.667003154754639, 5.604330062866211]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.229128360748291, 4.204349994659424, 4.215953826904297, 4.2140655517578125, 4.1222758293151855, 4.089872360229492, 4.181647777557373, 4.182224273681641, 4.215088844299316, 4.384484767913818, 4.287145614624023, 3.973775625228882, 4.31265115737915, 4.152848243713379, 4.23397159576416, 4.225589752197266]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.04223370552063, 3.166512966156006, 2.936663866043091, 2.7019429206848145, 2.9316623210906982, 3.144834041595459, 2.8829853534698486, 2.8961455821990967, 2.972339153289795, 3.1975321769714355, 2.96073579788208, 2.8041343688964844, 3.195096254348755, 3.0552785396575928, 3.119320869445801, 3.165720224380493]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.10035715997219086, 0.10680526494979858, 0.10764982551336288, -0.21456578373908997, -0.1750195175409317, -0.08546271920204163, 0.1304255723953247, -0.20849387347698212, 0.07343693822622299, 0.09952399879693985, 0.09529189765453339, 0.08828895539045334, 0.09383564442396164, 0.11046844720840454, -0.9592918753623962, 0.12271019071340561]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08608554303646088, 0.05033370852470398, 0.027091892436146736, 0.06748711317777634, 0.07062530517578125, 0.05972210690379143, 0.040101148188114166, 0.06067309528589249, 0.0028575577307492495, 0.06626009941101074, -0.13047213852405548, 0.05352788046002388, 0.010338674299418926, 0.008797376416623592, 0.013480291701853275, 0.010499517433345318]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.060199424624443054, 0.09361617267131805, 0.1008082702755928, 0.026226932182908058, 0.09326007217168808, 0.12389781326055527, 0.0756424218416214, -0.04251401498913765, 0.08722762763500214, 0.06338436901569366, 0.005760775413364172, 0.10418567061424255, -0.11073967069387436, 0.029218804091215134, -0.021100517362356186, 0.09705524891614914]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.113129623234272, 0.15607845783233643, 0.15939059853553772, 0.13912419974803925, 0.13896243274211884, 0.12545722723007202, 0.05677763372659683, 0.16021737456321716, 0.16732043027877808, -0.4705857038497925, 0.07396063208580017, 0.12463583052158356, 0.13047803938388824, -0.23388671875, -0.051171980798244476, 0.0062769679352641106]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.11679662764072418, 0.09946223348379135, 0.09940315037965775, 0.04174632951617241, 0.1347304731607437, -0.07276246696710587, 0.06686189025640488, 0.0590498261153698, -0.11241167038679123, -0.005636375397443771, -0.1162019670009613, 0.15262772142887115, -0.11404791474342346, -0.14641296863555908, 0.14472879469394684, -0.10120455920696259]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.08872659504413605, 0.1441373974084854, 0.0909271240234375, 0.05917102098464966, -0.07828410714864731, 0.01516968198120594, 0.023633839562535286, 0.016596756875514984, 0.06992208957672119, -0.07077224552631378, -0.21243344247341156, 0.11724357306957245, -0.081373430788517, 0.17054641246795654, 0.11628063768148422, 0.0920952633023262]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.19288034737110138, 0.1388951539993286, 0.14711566269397736, 0.31534767150878906, 0.16524139046669006, 0.14605426788330078, -0.06574802845716476, -0.06465896219015121, 0.24159444868564606, 0.28505390882492065, -0.3227829933166504, 0.2860425114631653, 0.23606325685977936, -0.11867190897464752, 0.12535633146762848, 0.15097902715206146]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.30890050530433655, 0.34161660075187683, 0.2679392099380493, 0.1643434464931488, -0.6203394532203674, 0.2508840262889862, 0.30391886830329895, -0.28441929817199707, 0.2656894624233246, 0.15136541426181793, -0.12066976726055145, 0.2835761606693268, 0.19070543348789215, 0.2084510773420334, -0.35669994354248047, -0.34058257937431335]
Layer: gate_7 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.30710792541503906, 0.352027028799057, -0.26229798793792725, 0.36826619505882263, 0.3837759494781494, 0.5088841319084167, 0.2738615870475769, -0.1452154517173767, 0.23561429977416992, 0.5836463570594788, 0.20533792674541473, -0.09305030852556229, 0.5564132332801819, -0.2504761815071106, -0.32780736684799194, 0.3151848018169403]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.6327752470970154, 0.3726528286933899, 0.4054395258426666, 0.7624474167823792, 0.6011816263198853, 0.35850420594215393, 0.7655299305915833, 0.6640960574150085, 0.7796654105186462, 0.18867015838623047, 0.35756874084472656, 0.6475876569747925, 0.593318521976471, 0.26820337772369385, 0.7092937231063843, 0.8094985485076904]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_10 - Captured router_logits: [0.8857435584068298, 0.8212182521820068, 0.6913633942604065, 0.8233353495597839, 1.0492875576019287, 0.47682398557662964, 0.7395252585411072, 0.7302861213684082, 0.37344685196876526, 0.5058630108833313, 0.7493327856063843, 0.8313882946968079, 0.2789308428764343, 0.32427269220352173, 1.1977922916412354, 0.567147433757782]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [0.9665657877922058, 1.0606311559677124, 0.9880911707878113, 1.426790475845337, 1.2380539178848267, 0.6735723614692688, 0.9681088924407959, 1.0046945810317993, 1.0358271598815918, 1.2916157245635986, 1.0198183059692383, 1.2875603437423706, 0.7032435536384583, 0.7827810049057007, 0.4304661750793457, 0.9361246228218079]
Layer: gate_11 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.39548084139823914, 1.082510232925415, 1.3258215188980103, 1.1297560930252075, 0.6113653779029846, 0.9031842350959778, 1.083478331565857, 1.1890785694122314, 0.7993606925010681, 0.4089794456958771, 1.1389743089675903, 0.993417501449585, 0.9290482401847839, 0.989511251449585, 0.8359710574150085, 1.154781460762024]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.2029507160186768, 1.133662462234497, 1.7099870443344116, 0.916045069694519, 1.4485329389572144, 0.2669323682785034, 1.3380471467971802, 2.113579511642456, 1.2210161685943604, 1.3376073837280273, 1.5497263669967651, 1.4404892921447754, 0.8051338195800781, 1.2341350317001343, 1.5592236518859863, 1.1916743516921997]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.1200053691864014, 0.9639566540718079, 1.1441547870635986, 1.1354660987854004, 1.3460981845855713, 1.9479135274887085, 0.769950807094574, 1.5343856811523438, 0.9719592332839966, 0.9816391468048096, 0.06026126816868782, 1.1831315755844116, 1.2948147058486938, 1.3235180377960205, 1.2851711511611938, 0.9675682187080383]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.8667110800743103, 1.2002549171447754, 1.3101592063903809, 1.1091662645339966, 1.0214619636535645, 1.1525598764419556, 1.7097275257110596, 1.1624523401260376, 1.0811665058135986, 1.259281039237976, 0.8872233629226685, 0.27764007449150085, 1.490982174873352, 1.193463683128357, 1.0271648168563843, 1.6763771772384644]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.8370873928070068, 0.33236128091812134, 1.0858685970306396, 0.784141480922699, 0.6693958640098572, 0.6365640759468079, 0.42112067341804504, 0.939195454120636, -0.31542059779167175, 2.002307176589966, -0.5984683632850647, 0.8017539978027344, 1.0437812805175781, 1.044393539428711, 0.9383237361907959, 0.23836056888103485]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.3982890546321869, 1.2407602071762085, 1.6619155406951904, 1.5817880630493164, 1.7200546264648438, 1.7007842063903809, 1.6892145872116089, 1.7949963808059692, 1.890222430229187, 1.5330591201782227, 2.164196729660034, 1.657137155532837, 1.5917941331863403, 1.3816943168640137, 1.0627634525299072, 1.5053576231002808]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.7717974185943604, 1.5513776540756226, 1.74220609664917, 1.8488191366195679, 1.3415136337280273, 1.648105263710022, 1.7706494331359863, 2.0012075901031494, 2.340514659881592, 1.7493290901184082, 1.6448220014572144, 1.314241647720337, 1.5355280637741089, 1.1044765710830688, 1.759914755821228, 1.7007991075515747]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.9898914098739624, 1.8054031133651733, 1.809652328491211, 1.8326425552368164, 2.0772006511688232, 1.6672587394714355, 1.8144307136535645, 1.9143569469451904, 2.278663158416748, 1.8491770029067993, 2.0345747470855713, 1.9368664026260376, 2.145254373550415, 1.9984196424484253, 2.0313544273376465, 1.9473103284835815]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.142458438873291, 1.4107414484024048, 1.3329519033432007, 1.0995539426803589, 0.2019003927707672, 1.5351338386535645, 1.4751460552215576, 2.1087682247161865, 1.1694194078445435, 1.4759587049484253, 1.4013224840164185, 1.3540374040603638, 1.3870915174484253, 1.5466289520263672, 1.3011817932128906, 0.792504072189331]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.388627052307129, 2.4616830348968506, 2.3149750232696533, 2.1370317935943604, 2.273094654083252, 2.390028715133667, 2.2209715843200684, 2.366501569747925, 2.197519063949585, 2.484285593032837, 2.553077220916748, 2.589724540710449, 2.099609375, 2.6373748779296875, 2.5387046337127686, 2.436784267425537]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.7358063459396362, 1.6138030290603638, 2.062335968017578, 1.3754565715789795, 1.8619096279144287, 1.907457709312439, 1.737073540687561, 1.6576067209243774, 1.822012186050415, 1.8905056715011597, 1.7517145872116089, 2.1021440029144287, 1.7787302732467651, 1.8761032819747925, 1.636800765991211, 2.0324277877807617]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.281071186065674, 1.9036110639572144, 1.489210605621338, 1.6937768459320068, 1.785961389541626, 1.7385644912719727, 1.7153655290603638, 1.6785842180252075, 1.7956300973892212, 1.7416582107543945, 1.7152910232543945, 1.6771528720855713, 1.6472223997116089, 1.1571375131607056, 1.710720181465149, 1.85691499710083]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3311963081359863, 2.4540493488311768, 2.2374463081359863, 2.3323891162872314, 2.5177719593048096, 2.512523889541626, 2.564020872116089, 2.2578125, 2.2201812267303467, 2.3044190406799316, 2.583820343017578, 2.374150276184082, 2.370885133743286, 2.4666924476623535, 2.39163875579834, 2.464247465133667]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7974714040756226, 1.7110717296600342, 1.5314289331436157, 1.8225041627883911, 1.4097760915756226, 1.6308444738388062, 1.7157829999923706, 1.873136281967163, 1.7328393459320068, 1.6546398401260376, 1.6923753023147583, 1.7177510261535645, 1.7269054651260376, 1.4840693473815918, 1.6254621744155884, 1.8658158779144287]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8071773052215576, 1.740225076675415, 1.7217915058135986, 1.8550959825515747, 1.7112505435943604, 1.8552614450454712, 1.7635574340820312, 1.7026106119155884, 1.835968255996704, 1.897140383720398, 2.192897081375122, 1.7282025814056396, 1.8895217180252075, 1.740234375, 1.7491364479064941, 1.8187731504440308]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6020265817642212, 1.584863543510437, 1.6765263080596924, 1.6792113780975342, 1.5736000537872314, 1.5897165536880493, 1.7126669883728027, 1.6804534196853638, 1.5815200805664062, 1.7321162223815918, 1.4061791896820068, 1.5882726907730103, 1.6063520908355713, 1.670440435409546, 1.5616240501403809, 1.5678207874298096]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1107914447784424, 2.1282577514648438, 2.352233409881592, 2.107377052307129, 2.154252052307129, 2.1302332878112793, 2.3503994941711426, 2.214500904083252, 2.058936595916748, 2.160707950592041, 2.05765438079834, 2.0027060508728027, 2.067136764526367, 1.9536616802215576, 2.265371561050415, 2.201246500015259]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.386539936065674, 5.586056709289551, 5.295413970947266, 5.230379104614258, 5.213837146759033, 5.270992279052734, 5.462935447692871, 5.525390625, 5.517116069793701, 5.446356296539307, 5.3654279708862305, 5.185889720916748, 5.461921691894531, 5.411498069763184, 5.497077941894531, 5.303837776184082]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:   3%|█▎                                                | 13/504 [00:55<31:30,  3.85s/it]Layer: gate_30 - Captured router_logits: [3.828289031982422, 3.7596166133880615, 3.8321802616119385, 3.5454814434051514, 3.6556313037872314, 3.6981005668640137, 3.4944100379943848, 3.6691577434539795, 3.807915449142456, 3.768681526184082, 3.7573130130767822, 3.410707950592041, 3.7280235290527344, 3.720106840133667, 3.8136777877807617, 3.851256847381592]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [3.0391221046447754, 3.0255396366119385, 2.9938275814056396, 2.8306894302368164, 2.969942808151245, 3.0842528343200684, 2.9593868255615234, 2.8411855697631836, 2.96875, 2.962682008743286, 2.9309399127960205, 2.3127059936523438, 2.9813036918640137, 2.9740278720855713, 3.1303672790527344, 3.121854066848755]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.140153706073761, 0.14936892688274384, 0.13847307860851288, -0.22222517430782318, -0.18276527523994446, -0.19462226331233978, 0.15965300798416138, -0.25887244939804077, 0.10694912075996399, 0.12708599865436554, 0.1329241394996643, 0.08815068751573563, 0.11717646569013596, 0.13356395065784454, -1.080168604850769, 0.15258076786994934]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.10178247839212418, 0.07659804821014404, 0.03497281298041344, 0.06634104251861572, 0.10650482028722763, 0.055390436202287674, 0.06699247658252716, 0.05634288862347603, 0.010074983350932598, 0.08481884002685547, -0.14884324371814728, 0.05071064084768295, -0.004479986149817705, -0.0012086625210940838, -0.0021681878715753555, -0.0015216076280921698]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06474532186985016, 0.10937900096178055, 0.09978146106004715, 0.044769566506147385, 0.10158403217792511, 0.12268973886966705, 0.07434917241334915, -0.04304377734661102, 0.08075810968875885, 0.0431143119931221, -0.002330485498532653, 0.10097046941518784, -0.11690632998943329, 0.06706299632787704, -0.00427174661308527, 0.11244489997625351]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12496435642242432, 0.18198874592781067, 0.15738563239574432, 0.15650397539138794, 0.14281180500984192, 0.14770705997943878, 0.13468122482299805, 0.20913605391979218, 0.1757557988166809, -0.44624918699264526, -0.022379200905561447, 0.13164830207824707, 0.0778573527932167, -0.1980006843805313, -0.15225352346897125, -0.0007740153232589364]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.09875791519880295, 0.1264665573835373, 0.08797560632228851, -0.032358646392822266, 0.037537019699811935, -0.14006313681602478, 0.12530170381069183, 0.11120539158582687, 0.023378409445285797, -0.0178638007491827, 0.009792018681764603, 0.19395039975643158, -0.24195066094398499, -0.06281687319278717, 0.11507245153188705, -0.025010686367750168]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.13776780664920807, 0.18811152875423431, 0.1570225954055786, -0.018616974353790283, -0.02491794154047966, 0.16185151040554047, -0.022665811702609062, 0.09925733506679535, 0.0544775016605854, -0.1055091917514801, -0.41725659370422363, 0.11453299969434738, -0.04358894005417824, 0.10752353072166443, 0.20557093620300293, 0.09240108728408813]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.12003099918365479, 0.2705858647823334, 0.035300709307193756, 0.2804684340953827, 0.1868191808462143, 0.1033591479063034, 0.2741636037826538, 0.1627984195947647, 0.24934203922748566, 0.1873338669538498, -0.2326664924621582, 0.17913830280303955, 0.32582664489746094, 0.022060643881559372, 0.02173888497054577, 0.11407190561294556]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.17342032492160797, 0.5030739307403564, 0.3913193643093109, 0.28409719467163086, -0.5562514662742615, 0.19986067712306976, 0.24076539278030396, 0.11041966825723648, 0.12994329631328583, -0.036413658410310745, 0.1433008909225464, 0.2995833456516266, 0.3785426914691925, 0.14983458817005157, -0.12277515977621078, -0.38272222876548767]
Layer: gate_7 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3089528977870941, 0.4568837285041809, 0.09576310217380524, 0.5575050115585327, 0.3029092252254486, 0.5796162486076355, 0.43228450417518616, 0.06211638078093529, 0.2920645475387573, 0.14896248281002045, 0.22467730939388275, -0.35416179895401, 0.5704576373100281, 0.20189982652664185, 0.04803019016981125, 0.14030680060386658]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.728299617767334, 0.9499740600585938, 0.4717608094215393, 0.4888637959957123, 0.5925172567367554, 0.32890477776527405, 0.9594019651412964, 0.7320712208747864, 0.7716534733772278, 0.09588953107595444, 0.8226832151412964, 0.89145827293396, 0.5518025159835815, 0.6203219890594482, 0.8503257632255554, 0.8370234370231628]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9576159119606018, 1.1155190467834473, 0.718802809715271, 1.024523377418518, 1.1635496616363525, 0.14767980575561523, 0.8878621459007263, 0.9012729525566101, 1.1504884958267212, 0.6103100776672363, 0.8137140870094299, 0.9080014228820801, 0.9212990403175354, 0.5513922572135925, 0.752246618270874, 1.16670560836792]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_11 - Captured router_logits: [1.2869510650634766, 1.2609325647354126, 1.1959081888198853, 0.9809502959251404, 1.2343825101852417, 0.6855605244636536, 1.1534899473190308, 1.0933116674423218, 1.2751134634017944, 1.301723837852478, 1.3197393417358398, 1.6497043371200562, 1.4685251712799072, 1.1722626686096191, 1.0290601253509521, 1.4596970081329346]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.9852608442306519, 1.3346441984176636, 1.17802095413208, 1.247375726699829, 1.1247860193252563, 1.216614007949829, 0.7820605635643005, 0.9629660248756409, 1.066574215888977, 0.47913822531700134, 1.2839874029159546, 1.1548794507980347, 1.2513453960418701, 1.0986214876174927, 0.8870090842247009, 1.2563608884811401]
Layer: gate_12 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [2.089662790298462, 1.6170517206192017, 1.896173357963562, 1.7259217500686646, 1.7555652856826782, 0.8957538604736328, 1.7885119915008545, 1.5482276678085327, 1.6308518648147583, 1.811489462852478, 1.5411089658737183, 2.0768580436706543, 1.4040210247039795, 1.9761911630630493, 1.989080548286438, 1.7881312370300293]
Layer: gate_13 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.335855484008789, 1.2973651885986328, 1.329987645149231, 1.56564462184906, 1.4947589635849, 2.0178966522216797, 0.8086373209953308, 0.714092493057251, 1.4370532035827637, 1.21372389793396, 0.7210156321525574, 1.6291569471359253, 1.2076826095581055, 1.4752956628799438, 1.4584866762161255, 1.370021939277649]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [1.3586840629577637, 1.3933247327804565, 1.5923700332641602, 1.3233591318130493, 1.9362030029296875, 1.5000075101852417, 1.2682878971099854, 1.5462265014648438, 1.2506786584854126, 1.4777238368988037, 1.5013611316680908, 0.30946579575538635, 1.634803295135498, 1.3914846181869507, 1.2887609004974365, 1.1155390739440918]
Running loglikelihood requests:   3%|█▋                                                | 17/504 [01:09<29:48,  3.67s/it]Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.0757231712341309, 0.8866627812385559, 0.9738618731498718, 0.7515289187431335, 0.8549861311912537, 1.0334397554397583, -0.058093391358852386, 0.9818732738494873, -0.24759818613529205, 1.1457990407943726, 0.1660730093717575, 0.8805878162384033, 1.2012397050857544, 0.9624947309494019, 0.9178141355514526, 0.35445767641067505]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.3497976064682007, 1.3523441553115845, 1.9367308616638184, 2.3252651691436768, 2.100883722305298, 1.9420849084854126, 1.9505157470703125, 2.197906494140625, 1.939686894416809, 1.7559254169464111, 2.0390701293945312, 1.8313130140304565, 1.8427107334136963, 1.781427264213562, 1.6796733140945435, 1.828552007675171]
Layer: gate_17 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.9886281490325928, 1.8374985456466675, 1.9926135540008545, 2.054657220840454, 1.5343842506408691, 2.0060932636260986, 1.7587249279022217, 2.4228100776672363, 2.249079942703247, 2.0455477237701416, 2.085085391998291, 1.7268433570861816, 2.1052539348602295, 1.5974555015563965, 1.9408632516860962, 1.956231951713562]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.557523012161255, 2.202235221862793, 2.110008478164673, 2.2593207359313965, 2.345982074737549, 2.1186749935150146, 2.4113476276397705, 2.347414970397949, 2.171060562133789, 2.2841005325317383, 2.297267198562622, 2.2750513553619385, 2.307130813598633, 2.355152130126953, 2.3694045543670654, 2.235159158706665]
Layer: gate_19 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0609705448150635, 1.401672601699829, 1.4129313230514526, 1.3360072374343872, 0.22799152135849, 1.3216661214828491, 1.6550283432006836, 1.8695381879806519, 1.1746454238891602, 1.3496885299682617, 1.3289092779159546, 1.1994223594665527, 1.2106057405471802, 1.338923692703247, 1.3360018730163574, 0.9404640793800354]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.4875497817993164, 2.5025036334991455, 2.4149069786071777, 2.461178779602051, 2.6246984004974365, 2.5151875019073486, 2.4773919582366943, 2.5059423446655273, 2.435720205307007, 2.7615227699279785, 2.679928779602051, 2.4486002922058105, 2.2303781509399414, 2.774855136871338, 2.671663761138916, 2.554506540298462]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.9237602949142456, 1.7270753383636475, 1.9183608293533325, 1.5528098344802856, 1.9827009439468384, 1.9105333089828491, 1.7973878383636475, 1.953939437866211, 1.9711405038833618, 1.89815092086792, 1.701224684715271, 1.7255067825317383, 1.9804084300994873, 1.9213018417358398, 1.8482369184494019, 1.9717060327529907]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [1.8175374269485474, 1.7277690172195435, 1.6430833339691162, 1.6780058145523071, 1.7267887592315674, 1.7144666910171509, 1.663285732269287, 1.9328999519348145, 1.718187689781189, 1.5977392196655273, 1.8040691614151, 1.6662079095840454, 1.6868438720703125, 1.2603834867477417, 1.7321202754974365, 1.9620988368988037]
Layer: gate_23 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_24 - Captured router_logits: [2.5978825092315674, 2.724119186401367, 2.499969720840454, 2.6640625, 2.6115920543670654, 2.8184120655059814, 2.7284326553344727, 2.7081925868988037, 2.7256877422332764, 2.755791425704956, 2.6300976276397705, 2.6565515995025635, 2.638453245162964, 2.7507541179656982, 2.8272805213928223, 2.9424469470977783]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_25 - Captured router_logits: [1.9573781490325928, 1.7480393648147583, 1.7397140264511108, 1.9257586002349854, 1.5517314672470093, 1.8252744674682617, 1.9116568565368652, 1.8343915939331055, 1.9086253643035889, 1.8809725046157837, 1.8742364645004272, 1.9153143167495728, 1.8602497577667236, 1.9643007516860962, 1.6259502172470093, 2.2738749980926514]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.848378300666809, 1.7145293951034546, 2.0084571838378906, 1.8635904788970947, 1.7323917150497437, 1.7757809162139893, 1.77781081199646, 1.7746355533599854, 1.8456389904022217, 1.8051625490188599, 1.8863469362258911, 1.766822099685669, 1.9336955547332764, 1.6661815643310547, 1.6587601900100708, 1.8098371028900146]
Layer: gate_26 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5226061344146729, 1.5116432905197144, 1.5669755935668945, 1.5629147291183472, 1.6038775444030762, 1.5365337133407593, 1.605946660041809, 1.5893969535827637, 1.6595124006271362, 1.716542363166809, 1.454226016998291, 1.5956305265426636, 1.5124483108520508, 1.4891070127487183, 1.6495158672332764, 1.5525081157684326]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.2337565422058105, 2.2516062259674072, 2.3169643878936768, 2.2988393306732178, 2.181995153427124, 2.2658209800720215, 2.2192740440368652, 2.3197243213653564, 2.4481866359710693, 2.3240039348602295, 2.2068653106689453, 2.2491402626037598, 2.2484164237976074, 2.2000181674957275, 2.3360977172851562, 2.2831051349639893]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.445583820343018, 5.519999027252197, 5.2875542640686035, 5.681497573852539, 5.464315891265869, 5.334157943725586, 5.546754360198975, 5.626583576202393, 5.505157947540283, 5.54182243347168, 5.402373790740967, 5.3212175369262695, 5.41749382019043, 5.383536338806152, 5.492096900939941, 5.349722385406494]
Layer: gate_29 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.7763898372650146, 3.7608816623687744, 3.76674485206604, 3.667289972305298, 3.661961317062378, 3.5877633094787598, 3.895132541656494, 3.7182674407958984, 3.7181899547576904, 3.7677552700042725, 3.8215718269348145, 3.555762529373169, 3.771186590194702, 3.6303470134735107, 3.785499334335327, 3.7107865810394287]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.041475534439087, 3.0273587703704834, 2.9628076553344727, 2.619027614593506, 2.962777614593506, 3.0690455436706543, 2.8488779067993164, 2.9011974334716797, 2.91768217086792, 3.083350896835327, 2.7585666179656982, 2.588054656982422, 3.060659885406494, 2.989020347595215, 2.9375905990600586, 3.0636613368988037]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.13311420381069183, 0.1568097472190857, 0.15211018919944763, -0.3487105071544647, -0.24843764305114746, -0.18866318464279175, 0.15745772421360016, -0.22425739467144012, 0.07367797195911407, 0.13274696469306946, 0.13922721147537231, 0.11015161871910095, 0.13337205350399017, 0.14676810801029205, -1.277974009513855, 0.16163231432437897]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.09210135042667389, 0.06477474421262741, 0.05608033388853073, 0.06272733956575394, 0.10289980471134186, 0.0634034276008606, 0.05477811396121979, 0.07253550738096237, 0.002703617326915264, 0.07687918841838837, -0.1831924021244049, 0.053301382809877396, 0.011140101589262486, -0.01523292250931263, -0.002014357829466462, 0.016609758138656616]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07748091220855713, 0.08584442734718323, 0.107998788356781, 0.03961832821369171, 0.11440949141979218, 0.10495171695947647, 0.08160806447267532, -0.0698842778801918, 0.07902101427316666, 0.06396137923002243, 0.013766235671937466, 0.07455029338598251, -0.1640516221523285, 0.013833749108016491, -0.0008447958389297128, 0.09624236822128296]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11612559854984283, 0.18353286385536194, 0.1500985473394394, 0.1427026242017746, 0.11101900041103363, 0.15590214729309082, 0.09417691081762314, 0.19126655161380768, 0.18355204164981842, -0.4857175350189209, -0.027659503743052483, 0.1064486876130104, 0.17224819958209991, -0.18872471153736115, -0.09771795570850372, -0.06191098690032959]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.11267902702093124, 0.10571219027042389, 0.07907668501138687, 0.087373286485672, 0.059708788990974426, -0.10180232673883438, 0.0849936455488205, 0.08094046264886856, -0.03877422586083412, 0.03409525379538536, -0.13467873632907867, 0.1545267105102539, -0.08112092316150665, -0.1763424277305603, 0.14304354786872864, -0.0047082179225981236]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.1417677402496338, 0.14982876181602478, 0.10993149131536484, 0.13339190185070038, -0.006127771455794573, 0.02078651264309883, 0.05226968228816986, 0.09725575149059296, 0.12243153899908066, -0.07557641714811325, -0.3070807456970215, 0.11258849501609802, -0.10025328397750854, 0.14581213891506195, 0.13754381239414215, 0.06978210806846619]
Layer: gate_5 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.1543448120355606, 0.17295114696025848, 0.18976692855358124, 0.31871768832206726, 0.23697173595428467, 0.14048615097999573, -0.006267623510211706, 0.044980112463235855, 0.24560979008674622, 0.22184139490127563, -0.33254274725914, 0.1923612356185913, 0.24433515965938568, -0.06949472427368164, 0.17522366344928741, 0.1525028496980667]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.4070896804332733, 0.4087718427181244, 0.2984318733215332, 0.19023782014846802, -0.6435239315032959, 0.2393764853477478, 0.24945759773254395, -0.2075015902519226, 0.3377072811126709, 0.15439389646053314, 0.021786632016301155, 0.2887299060821533, 0.16427424550056458, 0.22118240594863892, -0.2517068684101105, -0.4055165946483612]
Layer: gate_7 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.359777569770813, 0.3941754996776581, -0.09111904352903366, 0.35656628012657166, 0.4091079533100128, 0.5466562509536743, 0.41469162702560425, -0.22677332162857056, 0.18443851172924042, 0.7897195219993591, 0.1847141534090042, -0.0950046256184578, 0.5710206031799316, -0.19857484102249146, -0.07498825341463089, 0.36991086602211]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8209705352783203, 0.5649135708808899, 0.4242500066757202, 0.8875255584716797, 0.705793023109436, 0.447687029838562, 0.9684560298919678, 0.6622455716133118, 1.1462169885635376, 0.34193986654281616, 0.45956215262413025, 0.704336941242218, 0.4230269491672516, 0.5356102585792542, 0.7675392031669617, 0.740475594997406]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.8683974742889404, 0.8473021984100342, 0.7071674466133118, 0.8415692448616028, 1.0788525342941284, 0.5737654566764832, 0.8041827082633972, 0.7248311638832092, 0.5633386969566345, 0.663661777973175, 0.7317546010017395, 0.9025946855545044, 0.18237535655498505, 0.2980845272541046, 1.2850252389907837, 0.7317383289337158]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [1.0139364004135132, 1.1275843381881714, 0.9754147529602051, 1.5258866548538208, 1.233057975769043, 0.7328931093215942, 1.1074413061141968, 1.1543861627578735, 1.1319682598114014, 1.4616650342941284, 0.9803160429000854, 1.3551380634307861, 0.9456787109375, 0.9248971939086914, 0.6692765355110168, 1.118833303451538]
Layer: gate_11 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.6161095499992371, 1.2711050510406494, 1.3862907886505127, 1.1955692768096924, 0.7763724327087402, 0.969479501247406, 1.153871774673462, 1.3220010995864868, 0.9051258563995361, 0.2974991500377655, 1.190310001373291, 0.9905611872673035, 1.22052800655365, 0.9744420647621155, 0.8455358147621155, 1.1178488731384277]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.5461591482162476, 1.3385831117630005, 1.7387948036193848, 1.1963430643081665, 1.5723044872283936, 0.5586403608322144, 1.4900398254394531, 2.5443227291107178, 1.491074800491333, 1.4900243282318115, 1.8345407247543335, 1.58869206905365, 1.024477243423462, 1.6759952306747437, 1.684224009513855, 1.3655239343643188]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.2908133268356323, 1.0639687776565552, 1.3145931959152222, 1.3192464113235474, 1.4197273254394531, 2.5249762535095215, 1.0850751399993896, 1.974394679069519, 1.159827470779419, 1.119364857673645, 0.4458220601081848, 1.3492076396942139, 1.294614553451538, 1.4306484460830688, 1.3121167421340942, 1.2648005485534668]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [1.0030959844589233, 1.1142189502716064, 1.377793550491333, 1.1486709117889404, 1.1796894073486328, 1.2311301231384277, 1.9364928007125854, 1.2771881818771362, 1.1504839658737183, 1.2473076581954956, 1.1104955673217773, 0.2603949308395386, 1.5819412469863892, 1.2486071586608887, 1.04216730594635, 1.9380778074264526]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8183690905570984, 0.5039162039756775, 0.9700056910514832, 0.6978769302368164, 0.6354173421859741, 0.8325612545013428, 0.6092272996902466, 0.7619872093200684, -0.21444252133369446, 2.0634336471557617, -0.3244216740131378, 0.8329756855964661, 0.9568970203399658, 0.9141831398010254, 0.8735137581825256, 0.4544789493083954]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.5553868412971497, 1.2155256271362305, 1.7055683135986328, 1.774949073791504, 1.7716789245605469, 1.7370206117630005, 1.8458976745605469, 1.748416543006897, 2.0037777423858643, 1.540592074394226, 2.4154632091522217, 1.7478951215744019, 1.6936984062194824, 1.500198483467102, 1.2782459259033203, 1.4691643714904785]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.7669011354446411, 1.6164716482162476, 1.7099881172180176, 1.870969295501709, 1.3674914836883545, 1.5909428596496582, 2.0116875171661377, 2.1653075218200684, 2.5879294872283936, 1.7865568399429321, 1.7439110279083252, 1.526898741722107, 1.6032121181488037, 1.0467298030853271, 1.9051917791366577, 1.7936691045761108]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.9420443773269653, 1.6885815858840942, 1.8968734741210938, 1.7596956491470337, 1.9720181226730347, 1.7231873273849487, 1.878571629524231, 1.8775084018707275, 2.3861896991729736, 1.8514925241470337, 1.968897819519043, 1.9991674423217773, 1.9398422241210938, 1.9035731554031372, 1.9283335208892822, 2.0011515617370605]
Running loglikelihood requests:   4%|██                                                | 21/504 [01:22<28:57,  3.60s/it]Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.3670411109924316, 1.4974788427352905, 1.4875712394714355, 1.3473498821258545, 0.40277475118637085, 1.569182276725769, 1.5189709663391113, 2.6753103733062744, 1.1271510124206543, 1.5861554145812988, 1.429368495941162, 1.4487576484680176, 1.5414824485778809, 1.5737674236297607, 1.4443018436431885, 0.976016640663147]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.35719633102417, 2.4398343563079834, 2.331657648086548, 2.170785665512085, 2.4327688217163086, 2.401705741882324, 2.307582139968872, 2.4373443126678467, 2.249953269958496, 2.4908647537231445, 2.554096221923828, 2.692168712615967, 2.191032648086548, 2.849975109100342, 2.6069626808166504, 2.4012699127197266]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.9477403163909912, 1.798509120941162, 2.309262990951538, 1.5398873090744019, 2.0877737998962402, 1.9982569217681885, 1.9948720932006836, 1.8486367464065552, 1.9578560590744019, 2.0464236736297607, 1.9430869817733765, 2.3183515071868896, 1.9713644981384277, 2.007096529006958, 1.8519048690795898, 2.1308982372283936]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.3705334663391113, 2.039436101913452, 1.741074800491333, 1.8579744100570679, 1.9417797327041626, 1.8596240282058716, 1.80353581905365, 1.794898509979248, 1.870587944984436, 1.8396492004394531, 1.946401834487915, 1.8360387086868286, 1.8764005899429321, 1.3015127182006836, 1.8493292331695557, 2.1614012718200684]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.5857818126678467, 2.6210782527923584, 2.3920257091522217, 2.4660420417785645, 2.719372510910034, 2.6289217472076416, 2.766963481903076, 2.5745766162872314, 2.4396164417266846, 2.530751943588257, 2.8128111362457275, 2.6064491271972656, 2.4975099563598633, 2.640625, 2.6706299781799316, 2.7512762546539307]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.900896430015564, 1.8427540063858032, 1.7265781164169312, 1.9405503273010254, 1.6097407341003418, 1.7090388536453247, 1.7863390445709229, 1.9721115827560425, 1.8013570308685303, 1.8002209663391113, 1.8007190227508545, 1.9424333572387695, 1.8720118999481201, 1.6432862281799316, 1.8399370908737183, 2.0736117362976074]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.8404040336608887, 1.7489690780639648, 1.8865786790847778, 1.8633590936660767, 1.7850006818771362, 1.8328590393066406, 1.7694315910339355, 1.6946355104446411, 1.8378838300704956, 1.8846760988235474, 2.2562096118927, 1.7686052322387695, 2.0450074672698975, 1.7555714845657349, 1.7988241910934448, 1.780108094215393]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.609697937965393, 1.6477906703948975, 1.7410942316055298, 1.6820900440216064, 1.63821280002594, 1.6390365362167358, 1.7659703493118286, 1.7197226285934448, 1.6515743732452393, 1.7982678413391113, 1.5536370277404785, 1.7110950946807861, 1.6427181959152222, 1.744516134262085, 1.6157984733581543, 1.7069456577301025]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.170318841934204, 2.2527389526367188, 2.3841118812561035, 2.33329176902771, 2.2062530517578125, 2.2033274173736572, 2.5270636081695557, 2.246591806411743, 2.1560475826263428, 2.290649890899658, 2.1867997646331787, 2.1409051418304443, 2.2625203132629395, 2.151129961013794, 2.396554470062256, 2.2284767627716064]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.439367294311523, 5.563869476318359, 5.308889389038086, 5.296571731567383, 5.410996437072754, 5.331984519958496, 5.341851234436035, 5.542128562927246, 5.5027079582214355, 5.527592658996582, 5.420240879058838, 5.183267116546631, 5.321604251861572, 5.331019878387451, 5.449234485626221, 5.243277072906494]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.8361165523529053, 3.7865257263183594, 3.770177125930786, 3.6492116451263428, 3.759244203567505, 3.6665446758270264, 3.6496124267578125, 3.781078815460205, 3.856492042541504, 3.7786431312561035, 3.7920660972595215, 3.668058156967163, 3.900664806365967, 3.7856154441833496, 3.8333852291107178, 3.894397020339966]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.939429759979248, 3.040992259979248, 2.8565425872802734, 2.669602870941162, 2.810601234436035, 3.0295848846435547, 2.817417860031128, 2.8197054862976074, 2.9464640617370605, 2.987907648086548, 2.952066659927368, 2.319098711013794, 2.912086009979248, 3.0526955127716064, 3.039653778076172, 3.021725654602051]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_0 - Captured router_logits: [0.1013130396604538, 0.1116839051246643, 0.10696709901094437, -0.21148182451725006, -0.1758694350719452, -0.10104139149188995, 0.13024933636188507, -0.20299789309501648, 0.06734208762645721, 0.09823741763830185, 0.10037925094366074, 0.08730476349592209, 0.0957440733909607, 0.10714141279459, -0.9341515302658081, 0.12161469459533691]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08585389703512192, 0.04541134089231491, 0.026947662234306335, 0.0658237561583519, 0.06707131862640381, 0.06017643213272095, 0.040412917733192444, 0.0553479827940464, -0.009275484830141068, 0.06922303885221481, -0.13792631030082703, 0.04895205423235893, 0.007622438482940197, 0.0034515478182584047, 0.0013436309527605772, 0.013117670081555843]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05845564976334572, 0.090453140437603, 0.09441949427127838, 0.02264871448278427, 0.09520447254180908, 0.11278828978538513, 0.06966730207204819, -0.03952821344137192, 0.07734975218772888, 0.05830589309334755, -0.0020452227909117937, 0.10205690562725067, -0.0921628326177597, 0.04164578765630722, -0.014180976897478104, 0.10248841345310211]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11072754859924316, 0.15278708934783936, 0.15108273923397064, 0.13510984182357788, 0.1310124695301056, 0.12318459153175354, 0.04384930431842804, 0.15880030393600464, 0.16948533058166504, -0.41193729639053345, 0.05619126185774803, 0.1174386590719223, 0.12710259854793549, -0.15786857903003693, -0.06180325523018837, 0.010988924652338028]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.11553945392370224, 0.07840529829263687, 0.09973756968975067, 0.04859126731753349, 0.12586301565170288, -0.05589703097939491, 0.06141667440533638, 0.05926194787025452, -0.08750710636377335, 0.0035672227386385202, -0.10932447761297226, 0.14261287450790405, -0.10235486924648285, -0.13477051258087158, 0.1472955197095871, -0.058974482119083405]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_5 - Captured router_logits: [0.05407246947288513, 0.1435355842113495, 0.08419400453567505, 0.07431165128946304, -0.06398362666368484, 0.014621381647884846, 0.029407532885670662, 0.014656932093203068, 0.07220567762851715, -0.042522285133600235, -0.22634926438331604, 0.12446267157793045, -0.08436648547649384, 0.17141175270080566, 0.11551804095506668, 0.09865465760231018]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.19945721328258514, 0.12236234545707703, 0.1733347475528717, 0.32700687646865845, 0.210239440202713, 0.1699373871088028, -0.06688333302736282, -0.05393558740615845, 0.2480892837047577, 0.3051828444004059, -0.29614847898483276, 0.2776322662830353, 0.2486967146396637, -0.1480233371257782, 0.1367606222629547, 0.15878526866436005]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.31414473056793213, 0.29672130942344666, 0.2865521013736725, 0.1464710384607315, -0.5832827091217041, 0.2555202543735504, 0.33158567547798157, -0.23748600482940674, 0.2833847105503082, 0.15502849221229553, -0.08944445848464966, 0.29966774582862854, 0.19029158353805542, 0.2283664345741272, -0.3775865435600281, -0.31774723529815674]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3029994070529938, 0.3956815004348755, -0.2274041622877121, 0.3626387119293213, 0.39726540446281433, 0.49531516432762146, 0.2968423068523407, -0.1763928234577179, 0.24723534286022186, 0.5824702978134155, 0.2430984526872635, -0.1141749769449234, 0.5713772773742676, -0.2640708386898041, -0.3997412919998169, 0.3087769150733948]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.6471555233001709, 0.38080263137817383, 0.3928404748439789, 0.7500991225242615, 0.6461941003799438, 0.3603783845901489, 0.7428147792816162, 0.669983446598053, 0.814949631690979, 0.25829052925109863, 0.28810471296310425, 0.6722278594970703, 0.5924127697944641, 0.24552904069423676, 0.6999491453170776, 0.8032267689704895]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9182272553443909, 0.8497714400291443, 0.7039466500282288, 0.7848854660987854, 1.0759671926498413, 0.4606282711029053, 0.7418038249015808, 0.7286469340324402, 0.253887414932251, 0.43553611636161804, 0.7448135614395142, 0.8132098317146301, 0.2901964485645294, 0.32409751415252686, 1.1974743604660034, 0.5400980710983276]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [0.9261505603790283, 1.0539047718048096, 0.9600794315338135, 1.3592426776885986, 1.2118443250656128, 0.6286341547966003, 0.946723997592926, 0.908493161201477, 1.0032867193222046, 1.2926323413848877, 0.9695706367492676, 1.14541757106781, 0.5235980153083801, 0.7610950469970703, 0.38179221749305725, 0.9196479916572571]
Layer: gate_11 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.4039927124977112, 1.0058244466781616, 1.3105428218841553, 1.1088989973068237, 0.6337221264839172, 0.8646619915962219, 1.099435806274414, 1.225684404373169, 0.7524809241294861, 0.3410521447658539, 1.1772009134292603, 0.9785894751548767, 0.9541035890579224, 0.9310784935951233, 0.8743599057197571, 1.1516133546829224]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.1108973026275635, 1.0848994255065918, 1.7340816259384155, 0.9896106719970703, 1.411912441253662, 0.06606850773096085, 1.2916475534439087, 2.056919574737549, 1.1932034492492676, 1.279321551322937, 1.527113914489746, 1.4305163621902466, 0.777614951133728, 1.252885103225708, 1.4086544513702393, 1.1247382164001465]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.114151120185852, 0.9719915390014648, 1.1570746898651123, 1.131919026374817, 1.3301438093185425, 1.9693797826766968, 0.7845069169998169, 1.6384406089782715, 0.9719340801239014, 1.0516531467437744, -0.09821505844593048, 1.111769199371338, 1.289514422416687, 1.3378413915634155, 1.270516037940979, 0.9462642073631287]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.7724600434303284, 1.2339524030685425, 1.2265952825546265, 1.129685878753662, 0.9552474021911621, 1.1533695459365845, 1.8055676221847534, 1.130662441253662, 1.0472853183746338, 1.2673155069351196, 0.8960222601890564, 0.3117358982563019, 1.5462820529937744, 1.1447938680648804, 0.9904887676239014, 1.706640362739563]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.7937401533126831, 0.3655594289302826, 1.055061936378479, 0.7704914212226868, 0.6923889517784119, 0.6420133113861084, 0.36622196435928345, 0.8283494114875793, -0.341579407453537, 2.0270626544952393, -0.7306936383247375, 0.845572829246521, 0.9622833728790283, 1.0259168148040771, 0.9291417598724365, 0.18037186563014984]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.32479947805404663, 1.2220897674560547, 1.5775505304336548, 1.4306702613830566, 1.59481680393219, 1.6231781244277954, 1.6020385026931763, 1.689354658126831, 1.8085609674453735, 1.5182408094406128, 2.1177866458892822, 1.5554916858673096, 1.5030510425567627, 1.267356514930725, 0.8830007314682007, 1.3836876153945923]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6931132078170776, 1.461134433746338, 1.7180360555648804, 1.7623589038848877, 1.330968976020813, 1.5710961818695068, 1.6873522996902466, 1.8900834321975708, 2.306492805480957, 1.6737788915634155, 1.4966087341308594, 1.2120732069015503, 1.489516258239746, 0.9863314628601074, 1.7139410972595215, 1.6321395635604858]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.833754539489746, 1.649147391319275, 1.6516215801239014, 1.6858997344970703, 1.935645341873169, 1.5215202569961548, 1.692358136177063, 1.748924970626831, 2.1578993797302246, 1.6971466541290283, 1.8668181896209717, 1.766585111618042, 2.0786666870117188, 1.8499869108200073, 1.870888590812683, 1.7699825763702393]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1028945446014404, 1.3197503089904785, 1.249593734741211, 1.0726102590560913, 0.2988678812980652, 1.492651104927063, 1.4363839626312256, 2.178234338760376, 1.1346558332443237, 1.4291787147521973, 1.3510454893112183, 1.275407075881958, 1.3844947814941406, 1.5561318397521973, 1.2856378555297852, 0.8046424984931946]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.2984671592712402, 2.3197543621063232, 2.2075235843658447, 2.072150707244873, 2.1771106719970703, 2.2526097297668457, 2.1357176303863525, 2.2976300716400146, 2.1452863216400146, 2.3621652126312256, 2.445706367492676, 2.471014976501465, 2.0518643856048584, 2.5950303077697754, 2.410583019256592, 2.3508732318878174]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.761833667755127, 1.5981814861297607, 2.062483549118042, 1.4405794143676758, 1.8805803060531616, 1.8965171575546265, 1.7438944578170776, 1.7013360261917114, 1.776260495185852, 1.8807281255722046, 1.704864740371704, 2.1178441047668457, 1.7907530069351196, 1.9048877954483032, 1.6546744108200073, 2.017528772354126]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.3065667152404785, 1.8999967575073242, 1.5402226448059082, 1.6983816623687744, 1.7856978178024292, 1.7560070753097534, 1.700942039489746, 1.6808364391326904, 1.7759076356887817, 1.712693691253662, 1.7330455780029297, 1.6569474935531616, 1.6577681303024292, 1.2021828889846802, 1.6669429540634155, 1.8810070753097534]
Running loglikelihood requests:   5%|██▍                                               | 25/504 [01:36<28:18,  3.55s/it]Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.2726168632507324, 2.394596815109253, 2.164522171020508, 2.279575824737549, 2.5179390907287598, 2.4797959327697754, 2.568441390991211, 2.2570247650146484, 2.1900603771209717, 2.3092830181121826, 2.5676865577697754, 2.3271565437316895, 2.2787225246429443, 2.406184434890747, 2.3764443397521973, 2.3990941047668457]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7130218744277954, 1.6654903888702393, 1.4838497638702393, 1.7652311325073242, 1.4318745136260986, 1.5648798942565918, 1.648355484008789, 1.808675765991211, 1.673943042755127, 1.6113773584365845, 1.6060103178024292, 1.6538537740707397, 1.6769301891326904, 1.445394515991211, 1.5765821933746338, 1.8491662740707397]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.7706472873687744, 1.6791059970855713, 1.6879103183746338, 1.8212316036224365, 1.6595489978790283, 1.7803955078125, 1.71762216091156, 1.6656135320663452, 1.7844761610031128, 1.8775029182434082, 2.162889003753662, 1.6938968896865845, 1.8558133840560913, 1.7266486883163452, 1.7066906690597534, 1.7347863912582397]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5919362306594849, 1.6051292419433594, 1.6895290613174438, 1.6923868656158447, 1.5718799829483032, 1.5983073711395264, 1.715591549873352, 1.693892240524292, 1.5906367301940918, 1.7251386642456055, 1.464958667755127, 1.5989036560058594, 1.5990442037582397, 1.6917345523834229, 1.580810546875, 1.5751173496246338]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1294479370117188, 2.1676077842712402, 2.412536144256592, 2.123030424118042, 2.150702476501465, 2.148535966873169, 2.396385908126831, 2.2216715812683105, 2.08940052986145, 2.163914680480957, 2.089416980743408, 2.0356321334838867, 2.069139003753662, 1.9904887676239014, 2.3059840202331543, 2.2213268280029297]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.361049175262451, 5.5362067222595215, 5.2549238204956055, 5.162741184234619, 5.177438735961914, 5.240053653717041, 5.365283489227295, 5.555262088775635, 5.433035850524902, 5.382467746734619, 5.3360276222229, 5.126871109008789, 5.424562454223633, 5.349100589752197, 5.423220634460449, 5.254349231719971]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.8966073989868164, 3.8016183376312256, 3.8221917152404785, 3.6210033893585205, 3.714498996734619, 3.747882843017578, 3.5931386947631836, 3.713284492492676, 3.8713111877441406, 3.8543198108673096, 3.8062632083892822, 3.5207114219665527, 3.8665709495544434, 3.7371773719787598, 3.8872439861297607, 3.9412543773651123]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [3.109834671020508, 3.1299238204956055, 3.0479910373687744, 2.8462774753570557, 3.0105862617492676, 3.231379747390747, 2.9950106143951416, 2.9581801891326904, 3.052603006362915, 3.080028772354126, 3.062270164489746, 2.4666624069213867, 3.0769596099853516, 3.0659384727478027, 3.2427456378936768, 3.1533284187316895]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.12212009727954865, 0.13829389214515686, 0.13363248109817505, -0.2479933649301529, -0.157303124666214, -0.237234964966774, 0.1485195755958557, -0.2621804475784302, 0.0722358450293541, 0.1251329928636551, 0.13480837643146515, 0.1089661717414856, 0.11922310292720795, 0.13088996708393097, -1.1112945079803467, 0.1498691886663437]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.10240551829338074, 0.06987660378217697, 0.04102469980716705, 0.05997791886329651, 0.09358182549476624, 0.04113633185625076, 0.04250151664018631, 0.055907875299453735, 0.007722172420471907, 0.07493573427200317, -0.14929093420505524, 0.05132012441754341, -0.0019347009947523475, -0.01958676055073738, -0.013494212180376053, 0.036781392991542816]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08046041429042816, 0.06697171926498413, 0.09551192820072174, 0.03159482777118683, 0.0914439857006073, 0.11400699615478516, 0.10966987907886505, -0.06658797711133957, 0.06384976953268051, 0.04041418433189392, -0.006657041143625975, 0.08663307130336761, -0.09168611466884613, 0.042851053178310394, 0.02545836940407753, 0.10155901312828064]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11961910873651505, 0.17737671732902527, 0.13235637545585632, 0.13244563341140747, 0.13872528076171875, 0.13901980221271515, 0.11190034449100494, 0.20446188747882843, 0.1758967489004135, -0.330497682094574, -0.06090270355343819, 0.09588951617479324, 0.08314453065395355, -0.07189691811800003, -0.1380046308040619, -0.0576690137386322]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07015117257833481, 0.1096515953540802, 0.08692461997270584, -0.028726495802402496, 0.008143091574311256, -0.06181493401527405, 0.07195597887039185, 0.06104632094502449, 0.0036567160859704018, 0.037121083587408066, -0.044450826942920685, 0.1784866601228714, -0.14227475225925446, -0.13788752257823944, 0.10733611136674881, 0.013483376242220402]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09618163853883743, 0.18042603135108948, 0.044189296662807465, 0.025556417182087898, 0.051200076937675476, 0.08355265855789185, -0.026533225551247597, 0.11199359595775604, 0.06852968782186508, -0.06456098705530167, -0.2306242287158966, 0.09640016406774521, -0.015825673937797546, 0.08638802915811539, 0.1632191240787506, 0.07038049399852753]
Layer: gate_5 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.1206456869840622, 0.15257355570793152, 0.12799908220767975, 0.29616624116897583, 0.21588371694087982, 0.05401960015296936, 0.036622144281864166, 0.10843119025230408, 0.31038153171539307, 0.14639025926589966, -0.2661103904247284, 0.21867302060127258, 0.22405493259429932, 0.01815970242023468, 0.06390292197465897, 0.09837298840284348]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.12665781378746033, 0.39090946316719055, 0.3941304385662079, 0.1961749941110611, -0.6200635433197021, 0.26554903388023376, 0.194015234708786, -0.06590534001588821, 0.10268474370241165, 0.16115649044513702, 0.15325118601322174, 0.25417131185531616, 0.3297674059867859, 0.1003173291683197, -0.06331976503133774, -0.3348776698112488]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.33412760496139526, 0.3710957169532776, 0.02770206891000271, 0.3804521858692169, 0.22866426408290863, 0.3728371858596802, 0.3759576082229614, -0.07021608203649521, 0.1189379170536995, 0.3583400249481201, 0.07958728075027466, 0.010803880169987679, 0.40367916226387024, -0.0008560048881918192, -0.03157280385494232, 0.2280639111995697]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.5783049464225769, 0.5933940410614014, 0.48376187682151794, 0.5812497138977051, 0.6575438380241394, 0.46071919798851013, 0.8594602346420288, 0.679569661617279, 0.8856453895568848, 0.4337846040725708, 0.7973289489746094, 0.7746371626853943, 0.6324552297592163, 0.7365588545799255, 0.7668667435646057, 0.7307760119438171]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.8867124319076538, 0.9333875179290771, 0.6692084074020386, 0.9295801520347595, 0.9844886660575867, 0.7653858661651611, 0.7355378270149231, 0.7727640271186829, 0.8746791481971741, 0.9612674117088318, 0.7027587890625, 0.8368740677833557, 0.7325599789619446, 0.5016818642616272, 0.7139362096786499, 0.8716930747032166]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.1922396421432495, 1.1621503829956055, 1.0710028409957886, 1.4683942794799805, 1.187552571296692, 0.9065461158752441, 1.0962018966674805, 1.2793552875518799, 1.1312634944915771, 1.2446794509887695, 1.1195783615112305, 1.382538914680481, 1.170290470123291, 1.154815673828125, 0.7917230725288391, 1.3682019710540771]
Layer: gate_11 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.9813398122787476, 1.3573756217956543, 1.1190143823623657, 1.2795337438583374, 1.194757342338562, 0.9637240767478943, 1.1831481456756592, 1.0595555305480957, 1.1308567523956299, 0.601897120475769, 1.176016926765442, 1.1985031366348267, 1.2956374883651733, 0.942753255367279, 0.9690025448799133, 1.1144009828567505]
Layer: gate_12 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.7856950759887695, 1.4745084047317505, 1.6961606740951538, 1.522033929824829, 1.5606310367584229, 0.8216789364814758, 1.626894235610962, 1.9573006629943848, 1.5008418560028076, 1.5534415245056152, 1.5968438386917114, 1.824959635734558, 1.247613787651062, 1.8562432527542114, 1.7809216976165771, 1.7260406017303467]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.187043309211731, 1.1728851795196533, 1.1557869911193848, 1.3448107242584229, 1.2405500411987305, 1.7486612796783447, 1.0910881757736206, 0.8266491293907166, 1.1444650888442993, 1.1945632696151733, 0.7395690679550171, 1.1793949604034424, 1.1417438983917236, 1.2869410514831543, 1.2834893465042114, 1.183022379875183]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [1.2161812782287598, 1.2256027460098267, 1.4479727745056152, 1.1486648321151733, 1.4158735275268555, 1.363095998764038, 1.203214406967163, 1.3345736265182495, 1.3336896896362305, 1.2354021072387695, 1.304413914680481, 0.3685454726219177, 1.4131885766983032, 1.127938151359558, 1.233162760734558, 1.3602889776229858]
Layer: gate_15 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9753797054290771, 0.7746242880821228, 0.9000991582870483, 0.7789356708526611, 0.8409507870674133, 0.9429816007614136, 0.5345547199249268, 0.9532723426818848, -0.03455668315291405, 1.2733982801437378, 0.21719175577163696, 0.9392626285552979, 1.135649561882019, 0.883515477180481, 0.9181590676307678, 0.7472192049026489]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.4120546579360962, 1.2989033460617065, 1.8119781017303467, 2.167954444885254, 1.9286625385284424, 1.9052566289901733, 1.7656586170196533, 1.964018702507019, 1.8335634469985962, 2.0728676319122314, 2.166652202606201, 1.8147919178009033, 1.7969739437103271, 1.7320157289505005, 1.6099916696548462, 1.774527668952942]
Layer: gate_17 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.9585970640182495, 1.9585970640182495, 1.8943039178848267, 2.0475821495056152, 1.6720012426376343, 1.9974744319915771, 2.113264322280884, 2.2919249534606934, 2.3601832389831543, 1.9729256629943848, 1.9147443771362305, 1.9856356382369995, 2.0298357009887695, 1.7934433221817017, 1.9077484607696533, 1.93359375]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.357051372528076, 2.082233190536499, 2.0110535621643066, 2.0206425189971924, 2.1736598014831543, 2.0238773822784424, 2.1555259227752686, 2.2325565814971924, 2.267742395401001, 2.110090494155884, 2.159752130508423, 2.208513021469116, 2.2509429454803467, 2.1675477027893066, 2.192281723022461, 2.1098127365112305]
Layer: gate_19 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2098939418792725, 1.3520171642303467, 1.3990941047668457, 1.1805577278137207, 0.4972098767757416, 1.391172170639038, 1.5784449577331543, 2.1940276622772217, 1.0956904888153076, 1.379036784172058, 1.3797775506973267, 1.1033220291137695, 1.2145448923110962, 1.3382947444915771, 1.4115242958068848, 0.98503178358078]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.3523032665252686, 2.399885416030884, 2.2865705490112305, 2.33984375, 2.325296401977539, 2.375774621963501, 2.373333215713501, 2.5208444595336914, 2.325498342514038, 2.5032663345336914, 2.567180871963501, 2.7777814865112305, 2.2467503547668457, 2.645979166030884, 2.605266809463501, 2.4509360790252686]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.9521484375, 1.7643284797668457, 2.0342469215393066, 1.6286368370056152, 2.0210297107696533, 1.936826467514038, 1.8526148796081543, 1.7815194129943848, 1.9906553030014038, 1.9930630922317505, 1.7429620027542114, 1.9488314390182495, 1.998299479484558, 1.9369443655014038, 1.8881415128707886, 2.061624526977539]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [1.9446306228637695, 1.8087116479873657, 1.7955448627471924, 1.8194706439971924, 1.7902663946151733, 1.7797009944915771, 1.7434840202331543, 1.8242019414901733, 1.7286839485168457, 1.6106209754943848, 1.860587239265442, 1.700990915298462, 1.7507997751235962, 1.3874995708465576, 1.8372381925582886, 1.793793797492981]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.4822535514831543, 2.641803503036499, 2.5011112689971924, 2.5976898670196533, 3.1110923290252686, 2.783708333969116, 2.6387054920196533, 2.529296875, 2.633216619491577, 2.657597064971924, 2.652815103530884, 2.5460500717163086, 2.5775861740112305, 2.6734914779663086, 2.781721353530884, 2.7284820079803467]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7764345407485962, 1.8085095882415771, 1.6080784797668457, 1.7421369552612305, 1.5429434776306152, 1.6466022729873657, 1.7086139917373657, 1.7405205965042114, 1.712217092514038, 1.662446141242981, 1.681943655014038, 1.622305989265442, 1.6302701234817505, 1.6628670692443848, 1.6014446020126343, 2.273252248764038]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.043355941772461, 1.7896827459335327, 1.7669804096221924, 1.8939924240112305, 1.7406280040740967, 1.7833822965621948, 1.772478461265564, 1.9166512489318848, 1.8508095741271973, 1.907706379890442, 2.020975112915039, 1.7780719995498657, 1.8242523670196533, 1.710634469985962, 1.754362940788269, 1.8248554468154907]
Layer: gate_26 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:   6%|██▉                                               | 29/504 [01:50<27:46,  3.51s/it]Layer: gate_27 - Captured router_logits: [1.4916555881500244, 1.5446642637252808, 1.5439010858535767, 1.6828062534332275, 1.5569521188735962, 1.4817394018173218, 1.6019213199615479, 1.576478123664856, 1.5662052631378174, 1.6933172941207886, 1.4722100496292114, 1.5395171642303467, 1.4918949604034424, 1.464505910873413, 1.5554051399230957, 1.4987592697143555]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1892173290252686, 2.315345525741577, 2.248699426651001, 2.197383403778076, 2.1561784744262695, 2.1406502723693848, 2.3675241470336914, 2.2124528884887695, 2.229609966278076, 2.203731060028076, 2.0883283615112305, 2.1606109142303467, 2.1354169845581055, 2.0532732009887695, 2.264850378036499, 2.1947989463806152]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.546201705932617, 5.654094696044922, 5.403253078460693, 5.568906784057617, 5.515625, 5.407192707061768, 5.614291667938232, 5.842891216278076, 5.678913116455078, 5.659903526306152, 5.532007694244385, 5.488988399505615, 5.635346412658691, 5.448915481567383, 5.595500946044922, 5.508199691772461]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.9018869400024414, 3.8215415477752686, 3.7912261486053467, 3.745218276977539, 3.7612640857696533, 3.747188091278076, 3.921994924545288, 3.833050012588501, 3.844038248062134, 3.8585309982299805, 3.8603515625, 3.785137414932251, 3.965770959854126, 3.733259439468384, 3.8747642040252686, 3.8191466331481934]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.0439789295196533, 3.0389952659606934, 2.891029119491577, 2.631162405014038, 2.9082367420196533, 3.040182113647461, 2.8557045459747314, 3.005354166030884, 2.9042632579803467, 3.080162286758423, 2.8332602977752686, 2.7272770404815674, 3.0615234375, 2.993281841278076, 3.0057246685028076, 3.085247278213501]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_0 - Captured router_logits: [0.10291451215744019, 0.11125751584768295, 0.10776127874851227, -0.20588406920433044, -0.15512646734714508, -0.15773551166057587, 0.12700718641281128, -0.25161418318748474, 0.07707065343856812, 0.09433643519878387, 0.10116570442914963, 0.08038610965013504, 0.10164044797420502, 0.10758563131093979, -0.939731776714325, 0.11609718948602676]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09234463423490524, 0.07127179205417633, 0.03500276058912277, 0.05930076912045479, 0.08300406485795975, 0.05292476341128349, 0.04903113469481468, 0.0516936220228672, -0.018066510558128357, 0.07525970041751862, -0.1298314929008484, 0.04657544940710068, 0.0033425851725041866, -0.0021671641152352095, 0.0009362481068819761, 0.010623498819768429]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05159367620944977, 0.0779179260134697, 0.10999991744756699, 0.03865997865796089, 0.10772545635700226, 0.11359246075153351, 0.07023268938064575, -0.028473645448684692, 0.06835710257291794, 0.02726672776043415, -0.009683053940534592, 0.10562945157289505, -0.10872604697942734, 0.05187508091330528, -0.01567975804209709, 0.09307084232568741]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12080494314432144, 0.16963313519954681, 0.16373535990715027, 0.1499941051006317, 0.1370302438735962, 0.1341434270143509, 0.03693251311779022, 0.18572230637073517, 0.17680469155311584, -0.4009074866771698, 0.0075265103951096535, 0.14598707854747772, 0.07813457399606705, -0.18876425921916962, -0.11253495514392853, 0.03489990159869194]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.10739075392484665, 0.10891834646463394, 0.09014202654361725, -0.005393149796873331, 0.0636809766292572, -0.056229159235954285, 0.06022747978568077, 0.12889397144317627, -0.016087930649518967, -0.04454557225108147, -0.03833715245127678, 0.1333787590265274, -0.2179620862007141, -0.06338827311992645, 0.13562199473381042, -0.07035980373620987]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_5 - Captured router_logits: [0.04364055395126343, 0.18111766874790192, 0.12402509897947311, 0.053635165095329285, -0.10737387835979462, 0.09645947813987732, -0.0156722329556942, 0.0607881024479866, 0.06908950954675674, -0.01575580984354019, -0.341762900352478, 0.11472445726394653, -0.08520140498876572, 0.12626869976520538, 0.222415953874588, 0.11368000507354736]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.18857461214065552, 0.09437422454357147, 0.018290607258677483, 0.30735307931900024, 0.15807855129241943, 0.13935871422290802, 0.1931983232498169, 0.11782927066087723, 0.33073729276657104, 0.2304469645023346, -0.166473388671875, 0.2139529138803482, 0.21869978308677673, -0.09357032924890518, -0.00986501481384039, 0.12078843265771866]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.1483258306980133, 0.3021888732910156, 0.43082982301712036, 0.21712133288383484, -0.469146728515625, 0.2841476500034332, 0.25402528047561646, -0.03759564459323883, 0.143827885389328, 0.01841486059129238, 0.014378217980265617, 0.320835679769516, 0.2804672122001648, 0.1776171624660492, -0.25899767875671387, -0.4976992607116699]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.682128369808197, 0.419625848531723, -0.019479647278785706, 0.5023842453956604, 0.32528048753738403, 0.5181130170822144, 0.35744336247444153, -0.01942554861307144, 0.21324171125888824, 0.278719961643219, 0.2309122234582901, -0.33637112379074097, 0.5070012807846069, -0.08981864154338837, -0.31429749727249146, 0.1707674264907837]
Layer: gate_8 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.6942976713180542, 0.8639851212501526, 0.4925287365913391, 0.4654119312763214, 0.5514021515846252, 0.23355227708816528, 0.7936268448829651, 0.7331809401512146, 0.5125953555107117, 0.21859560906887054, 0.4669429361820221, 0.8864856958389282, 0.5442198514938354, 0.6851135492324829, 0.8211115002632141, 0.8144962787628174]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.8300287127494812, 1.0173007249832153, 0.7966286540031433, 0.9869606494903564, 0.9932994246482849, 0.2927871644496918, 0.8737881779670715, 0.826904296875, 1.009118676185608, 0.7206526398658752, 0.7742076516151428, 0.7970257997512817, 0.6861383318901062, 0.5678194761276245, 0.5965319275856018, 0.8371071815490723]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.1361762285232544, 1.1037620306015015, 1.1349431276321411, 1.0709872245788574, 1.1261624097824097, 0.5915383100509644, 1.3022682666778564, 1.0838512182235718, 1.1244450807571411, 1.2106400728225708, 1.250238060951233, 1.5703812837600708, 1.0194058418273926, 1.0414174795150757, 0.8859918117523193, 1.274796962738037]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.8533056378364563, 1.2539639472961426, 1.1590243577957153, 1.2709383964538574, 0.9171554446220398, 1.3754216432571411, 0.7400130033493042, 0.9815340638160706, 0.9827240109443665, 0.40733838081359863, 1.2062522172927856, 1.1930930614471436, 1.3152332305908203, 1.1715598106384277, 0.865447461605072, 1.295880675315857]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.7564630508422852, 1.5124467611312866, 1.7266513109207153, 1.472960352897644, 1.6519087553024292, 0.6079062819480896, 1.6778186559677124, 1.816015601158142, 1.5487926006317139, 1.613343358039856, 1.346373438835144, 1.9581409692764282, 1.1609768867492676, 1.847753882408142, 1.859277367591858, 1.5639159679412842]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.215009093284607, 1.099707007408142, 1.2051669359207153, 1.3726118803024292, 1.510941982269287, 1.4007800817489624, 0.7343317270278931, 0.6761055588722229, 1.228229284286499, 1.0404984951019287, 0.5571555495262146, 1.3605124950408936, 1.119950771331787, 1.4433238506317139, 1.4165927171707153, 1.2791081666946411]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [1.3862072229385376, 1.3327414989471436, 1.5659091472625732, 1.3380149602890015, 1.6547141075134277, 1.4177645444869995, 1.0842787027359009, 1.3977450132369995, 1.1753418445587158, 1.4571200609207153, 1.459672451019287, 0.4349839687347412, 1.5287563800811768, 1.2966175079345703, 1.209410548210144, 1.3318322896957397]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.0253373384475708, 0.8263730406761169, 1.0570346117019653, 0.7780439853668213, 0.8399991393089294, 1.0301024913787842, -0.007235995028167963, 1.1098579168319702, -0.06550376117229462, 1.2504180669784546, 0.2571927011013031, 0.8572609424591064, 1.133726954460144, 0.8618519306182861, 1.0613747835159302, 0.5856043100357056]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.2726795673370361, 1.384160041809082, 1.9413707256317139, 2.0906472206115723, 2.121413469314575, 1.9484374523162842, 2.037348985671997, 2.128613233566284, 1.9254794120788574, 1.797039270401001, 1.9763338565826416, 1.8506925106048584, 1.8833121061325073, 1.759747862815857, 1.7378534078598022, 1.7484546899795532]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.9260653257369995, 1.7102805376052856, 1.8745827674865723, 1.9877841472625732, 1.5350741147994995, 1.9996448755264282, 1.852269172668457, 2.3938210010528564, 2.148810386657715, 1.9630858898162842, 1.936522364616394, 1.7144814729690552, 2.0096235275268555, 1.4825972318649292, 2.0438077449798584, 1.9102095365524292]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.3448331356048584, 2.1221413612365723, 2.0402698516845703, 2.145294666290283, 2.2551491260528564, 2.043701171875, 2.2550692558288574, 2.313629627227783, 2.1667258739471436, 2.144158363342285, 2.225621461868286, 2.2094104290008545, 2.222984790802002, 2.253693103790283, 2.3090553283691406, 2.1614346504211426]
Layer: gate_19 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0717129707336426, 1.4832385778427124, 1.497638463973999, 1.6285266876220703, 0.44573044776916504, 1.482990026473999, 1.528826355934143, 1.934106469154358, 1.1959927082061768, 1.4167791604995728, 1.4185458421707153, 1.2071821689605713, 1.283735752105713, 1.3907315731048584, 1.2790087461471558, 0.8555713891983032]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.4362571239471436, 2.48046875, 2.3935370445251465, 2.372354507446289, 2.5995383262634277, 2.4887783527374268, 2.4882988929748535, 2.3960227966308594, 2.3363990783691406, 2.6159446239471436, 2.649502754211426, 2.6634232997894287, 2.1645684242248535, 2.662961721420288, 2.5659446716308594, 2.5102806091308594]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.8758522272109985, 1.7034090757369995, 1.7402077913284302, 1.5506614446640015, 1.9291325807571411, 1.8641334772109985, 1.7185724973678589, 1.7032315731048584, 1.8914594650268555, 1.8322887420654297, 1.6225851774215698, 1.8910777568817139, 1.9285067319869995, 1.8310191631317139, 1.7916991710662842, 1.893572449684143]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [1.7279385328292847, 1.6568536758422852, 1.5281693935394287, 1.5860085487365723, 1.5764914751052856, 1.6746360063552856, 1.5955965518951416, 1.7901989221572876, 1.5963068008422852, 1.4832385778427124, 1.7582119703292847, 1.4729492664337158, 1.5526411533355713, 1.0910440683364868, 1.5374289751052856, 1.742755651473999]
Layer: gate_23 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.6015625, 2.785475969314575, 2.541903495788574, 2.6460938453674316, 2.7255327701568604, 2.790287733078003, 2.7470171451568604, 2.697460889816284, 2.7822177410125732, 2.7907581329345703, 2.6508166790008545, 2.6546430587768555, 2.669637680053711, 2.798295497894287, 2.8499999046325684, 2.8947088718414307]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_25 - Captured router_logits: [1.8103160858154297, 1.7114523649215698, 1.6901278495788574, 1.8217861652374268, 1.473100185394287, 1.679261326789856, 1.7862038612365723, 1.778338074684143, 1.7917081117630005, 1.8054510354995728, 1.8282670974731445, 1.7118430137634277, 1.775799036026001, 1.8641334772109985, 1.5602272748947144, 2.105095863342285]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.7326171398162842, 1.639692783355713, 1.8620383739471436, 1.7360973358154297, 1.8625400066375732, 1.7214510440826416, 1.676433801651001, 1.820374608039856, 1.7763155698776245, 1.7180219888687134, 1.8127930164337158, 1.7387651205062866, 1.7647372484207153, 1.5546164512634277, 1.658599853515625, 1.7558077573776245]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.551985263824463, 1.5212658643722534, 1.5877296924591064, 1.612835168838501, 1.569744348526001, 1.5263105630874634, 1.6058707237243652, 1.5811196565628052, 1.610581874847412, 1.633482813835144, 1.4124377965927124, 1.5411111116409302, 1.529984951019287, 1.4778375625610352, 1.6683282852172852, 1.548925757408142]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_28 - Captured router_logits: [2.2218217849731445, 2.259481430053711, 2.396519899368286, 2.311115026473999, 2.2072088718414307, 2.303622245788574, 2.2910687923431396, 2.363157033920288, 2.2973010540008545, 2.299201011657715, 2.246910572052002, 2.2198686599731445, 2.270965099334717, 2.3148348331451416, 2.345667600631714, 2.2731001377105713]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.498970031738281, 5.608593940734863, 5.336008548736572, 5.669069766998291, 5.559020042419434, 5.398685932159424, 5.640909194946289, 5.576793193817139, 5.563565254211426, 5.5227274894714355, 5.491406440734863, 5.375568389892578, 5.4363017082214355, 5.424929141998291, 5.5665483474731445, 5.383131980895996]
Layer: gate_29 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.9526190757751465, 3.9883878231048584, 3.914106845855713, 3.9151954650878906, 3.8728160858154297, 3.8089754581451416, 3.950906753540039, 3.888885021209717, 3.854092597961426, 4.002379417419434, 3.9638495445251465, 3.6941616535186768, 3.928468942642212, 3.849982261657715, 3.990518569946289, 3.9066195487976074]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:   7%|███▎                                              | 33/504 [02:04<27:13,  3.47s/it]Layer: gate_31 - Captured router_logits: [2.8803977966308594, 2.8925249576568604, 2.8804333209991455, 2.6334872245788574, 2.8728692531585693, 2.9265270233154297, 2.817258596420288, 2.7723188400268555, 2.8476738929748535, 2.866415023803711, 2.708380699157715, 2.493706226348877, 2.980273485183716, 2.8519885540008545, 2.8267579078674316, 2.8769707679748535]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09333585947751999, 0.10127000510692596, 0.09856968373060226, -0.19387705624103546, -0.1682741791009903, -0.07518484443426132, 0.12465900182723999, -0.18566787242889404, 0.07220422476530075, 0.09152604639530182, 0.08733542263507843, 0.09289790689945221, 0.08487377315759659, 0.10274339467287064, -0.874269962310791, 0.11426584422588348]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.087127685546875, 0.04901692643761635, 0.023011526092886925, 0.06960509717464447, 0.07440399378538132, 0.06635968387126923, 0.04236600920557976, 0.0622108019888401, -0.009962514974176884, 0.0713798776268959, -0.13658444583415985, 0.050117604434490204, 0.009238938800990582, 0.006413372233510017, 0.009932458400726318, 0.017370859161019325]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06002645567059517, 0.09836605191230774, 0.10052888095378876, 0.03275829926133156, 0.09711962938308716, 0.11482467502355576, 0.07247895002365112, -0.04082868620753288, 0.08163157105445862, 0.06836335361003876, -8.768275438342243e-05, 0.10157974809408188, -0.10799182951450348, 0.04248934984207153, -0.0261518694460392, 0.10419998317956924]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12198332697153091, 0.16129615902900696, 0.15989765524864197, 0.14751069247722626, 0.13728444278240204, 0.11564606428146362, 0.061322491616010666, 0.16899728775024414, 0.16732865571975708, -0.4216820299625397, 0.07063798606395721, 0.12231814116239548, 0.1244887188076973, -0.2098984271287918, -0.04666764289140701, 0.007768068928271532]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.11931359767913818, 0.09466785192489624, 0.10317650437355042, 0.04493079334497452, 0.12426893413066864, -0.04781508445739746, 0.06247340887784958, 0.06064071133732796, -0.10306213796138763, 0.003055664710700512, -0.10947027802467346, 0.14644640684127808, -0.08553089201450348, -0.14068518579006195, 0.13668572902679443, -0.07022035866975784]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07570401579141617, 0.1486113965511322, 0.09730181843042374, 0.07661909610033035, -0.0848347395658493, 0.012293129228055477, 0.048984989523887634, 0.01719672977924347, 0.07171992212533951, -0.0553792305290699, -0.1948276162147522, 0.11113698780536652, -0.09467355906963348, 0.16503368318080902, 0.1338161826133728, 0.09178394079208374]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.18695864081382751, 0.10040290653705597, 0.17622175812721252, 0.3256658911705017, 0.18677295744419098, 0.15373635292053223, -0.08215630799531937, -0.07295617461204529, 0.2807970941066742, 0.29031306505203247, -0.3272497057914734, 0.29247403144836426, 0.22259020805358887, -0.15604478120803833, 0.12584328651428223, 0.15021926164627075]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3028690814971924, 0.29968100786209106, 0.28722620010375977, 0.14142081141471863, -0.5995179414749146, 0.2596580684185028, 0.30907943844795227, -0.22120556235313416, 0.2751774787902832, 0.19411616027355194, -0.12590594589710236, 0.2829850912094116, 0.20016877353191376, 0.2106122076511383, -0.40080076456069946, -0.31094080209732056]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.31919124722480774, 0.35476139187812805, -0.22607922554016113, 0.35254010558128357, 0.40373945236206055, 0.486540287733078, 0.28209248185157776, -0.198883056640625, 0.2494789958000183, 0.6136917471885681, 0.21710780262947083, -0.022838352248072624, 0.5469009280204773, -0.2798585891723633, -0.39048391580581665, 0.3252176344394684]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.6392468214035034, 0.3930034637451172, 0.4090183973312378, 0.7435025572776794, 0.6103696823120117, 0.362484335899353, 0.7274358868598938, 0.6593447923660278, 0.8173663020133972, 0.28196415305137634, 0.2886563241481781, 0.662085771560669, 0.6547197103500366, 0.2548157274723053, 0.6808810830116272, 0.7829247713088989]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9013742804527283, 0.8535203337669373, 0.698728084564209, 0.7918601036071777, 1.0248433351516724, 0.532959520816803, 0.7346474528312683, 0.73486328125, 0.28328627347946167, 0.4758424758911133, 0.7545714378356934, 0.8442453742027283, 0.36665138602256775, 0.32142385840415955, 1.1848630905151367, 0.5312169194221497]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [0.9442698955535889, 1.0791770219802856, 0.9787043333053589, 1.4223272800445557, 1.2674884796142578, 0.5903700590133667, 0.9937537908554077, 0.9770310521125793, 1.0167100429534912, 1.2821416854858398, 0.9935626983642578, 1.1985723972320557, 0.5575558543205261, 0.7639845609664917, 0.41185256838798523, 0.9099321365356445]
Layer: gate_11 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.41771259903907776, 1.0497338771820068, 1.31882643699646, 1.1824426651000977, 0.6631428599357605, 0.9105725884437561, 1.1049050092697144, 1.2438198328018188, 0.7491281032562256, 0.34364208579063416, 1.1634091138839722, 1.0634623765945435, 0.9761331677436829, 0.97728431224823, 0.8959606885910034, 1.1913402080535889]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.1290477514266968, 1.1671572923660278, 1.7854464054107666, 0.9810513854026794, 1.4569557905197144, 0.028297314420342445, 1.3676121234893799, 2.0587446689605713, 1.2704559564590454, 1.3656589984893799, 1.5601506233215332, 1.4918667078018188, 0.8302192687988281, 1.2312241792678833, 1.5415109395980835, 1.1713374853134155]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.1101770401000977, 0.9816387295722961, 1.124742865562439, 1.1042925119400024, 1.3018286228179932, 1.6882253885269165, 0.707282304763794, 1.4997540712356567, 0.9214716553688049, 1.020043134689331, -0.22332793474197388, 1.1012487411499023, 1.2101083993911743, 1.3249547481536865, 1.231723666191101, 0.9063207507133484]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.7571120858192444, 1.1978175640106201, 1.2210993766784668, 1.0862866640090942, 0.9495230317115784, 1.1326237916946411, 1.723516583442688, 1.1293779611587524, 1.0387983322143555, 1.246112585067749, 0.8729681372642517, 0.24437753856182098, 1.5181164741516113, 1.1419081687927246, 1.0258718729019165, 1.5279771089553833]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.7959173321723938, 0.2735215425491333, 1.0672023296356201, 0.7901334166526794, 0.660442054271698, 0.58488529920578, 0.28510287404060364, 0.812176525592804, -0.48959216475486755, 1.96267831325531, -0.862339198589325, 0.72975754737854, 0.9424347281455994, 1.0494178533554077, 0.9172941446304321, 0.08448327332735062]
Running loglikelihood requests:   7%|███▋                                              | 37/504 [02:17<26:51,  3.45s/it]Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.285344660282135, 1.2175860404968262, 1.6361337900161743, 1.4483460187911987, 1.6607412099838257, 1.6938594579696655, 1.6409835815429688, 1.7311763763427734, 1.8004226684570312, 1.5084410905838013, 2.1160740852355957, 1.6291327476501465, 1.5218561887741089, 1.2674554586410522, 0.8499767780303955, 1.3946945667266846]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.7677950859069824, 1.5041327476501465, 1.7801271677017212, 1.8564311265945435, 1.3475996255874634, 1.5786439180374146, 1.662156581878662, 1.9732601642608643, 2.3021020889282227, 1.736092209815979, 1.52568781375885, 1.17945396900177, 1.4877103567123413, 0.9744833111763, 1.73505437374115, 1.6791402101516724]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.8869452476501465, 1.747131586074829, 1.7652381658554077, 1.78447687625885, 2.0432140827178955, 1.5589617490768433, 1.7591428756713867, 1.8157882690429688, 2.158203125, 1.7871942520141602, 1.9664666652679443, 1.87601900100708, 2.172356128692627, 1.95822012424469, 1.9648815393447876, 1.8732450008392334]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1134369373321533, 1.2880576848983765, 1.2138317823410034, 1.0126198530197144, 0.22119715809822083, 1.457036018371582, 1.3560254573822021, 1.9975591897964478, 1.074130892753601, 1.395786166191101, 1.35105299949646, 1.2318274974822998, 1.3534495830535889, 1.4943010807037354, 1.2020549774169922, 0.7487462759017944]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.3718297481536865, 2.4024758338928223, 2.2765324115753174, 2.1052801609039307, 2.2215616703033447, 2.3318235874176025, 2.1590616703033447, 2.3225393295288086, 2.2014455795288086, 2.410703420639038, 2.5011699199676514, 2.5064916610717773, 2.0344862937927246, 2.6936895847320557, 2.451785087585449, 2.408778667449951]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.7659080028533936, 1.646946668624878, 2.0909383296966553, 1.4542194604873657, 1.8446747064590454, 1.9281400442123413, 1.7203539609909058, 1.6961616277694702, 1.8319557905197144, 1.8810763359069824, 1.7484525442123413, 2.1014304161071777, 1.8057442903518677, 1.9187235832214355, 1.6483054161071777, 2.012643337249756]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.346099376678467, 1.9414817094802856, 1.4845695495605469, 1.7430744171142578, 1.8168779611587524, 1.7719089984893799, 1.71552312374115, 1.6733468770980835, 1.8225581645965576, 1.7306386232376099, 1.7296384572982788, 1.66873300075531, 1.6151753664016724, 1.133269190788269, 1.6397451162338257, 1.8495811223983765]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.2879679203033447, 2.4016454219818115, 2.216674327850342, 2.284193754196167, 2.465749502182007, 2.432027578353882, 2.5745396614074707, 2.229091167449951, 2.1412193775177, 2.2779664993286133, 2.5799365043640137, 2.287043333053589, 2.3392210006713867, 2.4234602451324463, 2.363186836242676, 2.36379075050354]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7813820838928223, 1.7138813734054565, 1.5557442903518677, 1.8473354578018188, 1.4441466331481934, 1.6369829177856445, 1.7315821647644043, 1.9048724174499512, 1.7538496255874634, 1.679479956626892, 1.6857638359069824, 1.7095222473144531, 1.7447350025177002, 1.498962163925171, 1.6563254594802856, 1.9094958305358887]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.792251706123352, 1.7487062215805054, 1.7357902526855469, 1.8418062925338745, 1.6818199157714844, 1.8411611318588257, 1.7622754573822021, 1.7022993564605713, 1.8362689018249512, 1.9198275804519653, 2.2046866416931152, 1.7155561447143555, 1.8656495809555054, 1.7695972919464111, 1.7318981885910034, 1.8056098222732544]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6099929809570312, 1.6163630485534668, 1.6977366209030151, 1.705275058746338, 1.5795072317123413, 1.6096367835998535, 1.749751091003418, 1.7182401418685913, 1.6037349700927734, 1.7270011901855469, 1.4516435861587524, 1.615746259689331, 1.6254953145980835, 1.729751706123352, 1.5896692276000977, 1.6028622388839722]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1032702922821045, 2.1311330795288086, 2.4089202880859375, 2.0870132446289062, 2.138171911239624, 2.129387378692627, 2.34867525100708, 2.2067339420318604, 2.0685386657714844, 2.12449049949646, 2.0656325817108154, 1.9833984375, 2.0245885848999023, 1.9604185819625854, 2.3011021614074707, 2.2160892486572266]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.4230828285217285, 5.601109504699707, 5.296837329864502, 5.187971591949463, 5.216636657714844, 5.274418830871582, 5.409684658050537, 5.558272838592529, 5.48422384262085, 5.416289329528809, 5.3259735107421875, 5.17764949798584, 5.488620758056641, 5.407684326171875, 5.473920822143555, 5.288609504699707]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.045195579528809, 3.9114394187927246, 3.944246292114258, 3.7160873413085938, 3.8249547481536865, 3.8836333751678467, 3.698218584060669, 3.8405797481536865, 3.9797139167785645, 3.9592297077178955, 3.914477586746216, 3.5645687580108643, 3.9308383464813232, 3.8528645038604736, 3.9957351684570312, 4.121301174163818]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [3.068387746810913, 3.1232638359069824, 2.989621162414551, 2.8069896697998047, 2.97078800201416, 3.1857261657714844, 2.9187800884246826, 2.8616299629211426, 2.985356330871582, 2.9929046630859375, 3.0051705837249756, 2.3092541694641113, 3.030193328857422, 3.03074049949646, 3.2127113342285156, 3.099505662918091]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09264788776636124, 0.1057610735297203, 0.1092054471373558, -0.2979320287704468, -0.27869245409965515, -0.1167915090918541, 0.12618722021579742, -0.13068704307079315, 0.06228187680244446, 0.09370318800210953, 0.10523823648691177, 0.07084383815526962, 0.08769507706165314, 0.10370322316884995, -1.2156721353530884, 0.1223166361451149]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08401385694742203, 0.05026591569185257, 0.039597149938344955, 0.04360990971326828, 0.07655638456344604, 0.01632717251777649, 0.04535721242427826, 0.08777407556772232, 0.028703225776553154, 0.07255368679761887, -0.2090258151292801, 0.05892331525683403, 0.0015668775886297226, -0.03246007487177849, 0.03514828905463219, 0.04055485129356384]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09821780771017075, 0.06489814072847366, 0.09568171948194504, 0.061693135648965836, 0.10896234959363937, 0.11836057156324387, 0.0688435435295105, -0.10429534316062927, 0.07479453086853027, 0.12575620412826538, 0.02131628803908825, 0.08932993561029434, -0.21016359329223633, 0.025209296494722366, -0.04293489828705788, 0.10124143213033676]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13629823923110962, 0.17016267776489258, 0.11598198115825653, 0.11990933865308762, 0.13839936256408691, 0.10515107214450836, 0.013060356490314007, 0.16390828788280487, 0.1964370608329773, -0.5105831027030945, 0.03214595466852188, 0.06745530664920807, 0.21630226075649261, -0.2625035345554352, 0.02466857247054577, -0.06596501916646957]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.05967145785689354, 0.09986136853694916, 0.07031320035457611, 0.21217940747737885, 0.06144142150878906, -0.06418342888355255, 0.028471261262893677, 0.022667894139885902, -0.11040522903203964, 0.030201736837625504, -0.22011351585388184, 0.1380673050880432, 0.03249206393957138, -0.22592343389987946, 0.1385696977376938, -0.04415234178304672]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.0764521136879921, 0.1082656979560852, 0.07208056002855301, 0.1911422610282898, -0.038609329611063004, -0.08306647837162018, 0.05080206319689751, -0.041620902717113495, 0.1953875720500946, -0.12390743941068649, -0.1456977277994156, 0.08830372244119644, -0.21934783458709717, 0.14258097112178802, 0.1089881956577301, 0.05486005172133446]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.09279654920101166, -0.022053690627217293, 0.20185540616512299, 0.3064015209674835, 0.22924263775348663, 0.14826631546020508, -0.11897826194763184, -0.0835929587483406, 0.39470168948173523, 0.17292563617229462, -0.5749630331993103, 0.26777175068855286, 0.21582001447677612, -0.16903479397296906, 0.32233157753944397, 0.21811284124851227]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.4731777012348175, 0.18823593854904175, 0.24077573418617249, 0.1481533646583557, -1.0257059335708618, 0.29507336020469666, 0.1934524029493332, -0.40907183289527893, 0.5193158984184265, 0.17731031775474548, -0.19404469430446625, 0.3062530755996704, 0.17587187886238098, 0.21969500184059143, -0.6016229391098022, -0.10746631771326065]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.25255006551742554, 0.3865038752555847, -0.2725083529949188, 0.2853727638721466, 0.49970489740371704, 0.28887465596199036, 0.27213191986083984, -0.25947749614715576, 0.1856716126203537, 0.8487476110458374, 0.008348854258656502, 0.2850918769836426, 0.412783145904541, -0.5191324353218079, -0.3643840253353119, 0.3743269145488739]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.40948575735092163, 0.17495283484458923, 0.42103177309036255, 1.0947253704071045, 0.3630948066711426, 0.41871538758277893, 0.6396644115447998, 0.5551437735557556, 0.795115053653717, 0.03532031923532486, 0.27439799904823303, 0.6582884788513184, 0.6216732859611511, 0.37233319878578186, 0.7682489156723022, 0.664484441280365]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9377536177635193, 0.7802283763885498, 0.542243242263794, 0.7712283730506897, 1.0937334299087524, 0.5891920924186707, 0.6423304080963135, 0.7008068561553955, 0.235774964094162, 0.6471835970878601, 0.7389923334121704, 0.9740357398986816, 0.14555226266384125, 0.2312793880701065, 1.011425256729126, 0.3960917890071869]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9526793956756592, 1.0801421403884888, 0.9244207143783569, 1.108273983001709, 1.257792353630066, 0.6733565926551819, 0.8707097768783569, 1.1601752042770386, 1.1144095659255981, 1.4712578058242798, 0.8644960522651672, 1.0046055316925049, 0.5038450956344604, 0.3569061756134033, -0.09226508438587189, 0.7390591502189636]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.14606602489948273, 0.8799937963485718, 1.3002479076385498, 1.1340924501419067, 0.23318688571453094, 0.7479787468910217, 0.999740481376648, 1.4020830392837524, 0.8596124649047852, 0.4212978184223175, 0.7600939273834229, 0.8737390041351318, 0.9201837778091431, 0.8586354851722717, 0.5410591959953308, 0.9101752042770386]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.685765266418457, 0.9602785706520081, 0.8813962340354919, 0.47752735018730164, 1.1924681663513184, -0.005631530191749334, 1.0980924367904663, 2.2869386672973633, 1.0015928745269775, 0.8683252334594727, 1.3834477663040161, 1.1021219491958618, -0.13469037413597107, 0.8336442112922668, 1.0583282709121704, 0.6358017325401306]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7679093480110168, 0.7618988156318665, 1.0235512256622314, 0.8823491334915161, 1.0169049501419067, 1.3971220254898071, 0.31038421392440796, 1.6310133934020996, 0.8775860071182251, 0.9467630982398987, -0.030570020899176598, 0.8304632902145386, 1.1106449365615845, 1.1222789287567139, 1.140861988067627, 0.6854680776596069]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4002920985221863, 1.0775750875473022, 0.9696222543716431, 0.9480431079864502, 0.6145481467247009, 0.8611859083175659, 1.4689037799835205, 1.0374317169189453, 0.8510884642601013, 1.0114153623580933, 0.6510166525840759, -0.4427490234375, 0.967781126499176, 0.9056432247161865, 0.9442505836486816, 1.5485479831695557]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.6401171684265137, 0.16243113577365875, 0.8402099609375, 0.7654827833175659, 0.6386425495147705, 0.4327583312988281, 0.2856753468513489, 0.848800539970398, -0.8927606344223022, 2.612579584121704, -0.7283660173416138, 0.7727868556976318, 0.837539792060852, 0.8602496385574341, 0.8435160517692566, 0.3678745925426483]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.15958648920059204, 0.5013396143913269, 1.3144056797027588, 1.0552204847335815, 1.331538200378418, 1.3558954000473022, 1.3513917922973633, 1.2595617771148682, 1.8925544023513794, 1.1571844816207886, 2.107668399810791, 1.3400760889053345, 1.4341484308242798, 1.223368525505066, 0.5589599609375, 1.1588964462280273]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3648172616958618, 1.2302032709121704, 1.3211658000946045, 1.71080482006073, 0.8585146069526672, 1.1136889457702637, 1.77916419506073, 1.6096973419189453, 2.1704859733581543, 1.3457978963851929, 1.0478882789611816, 0.7764790654182434, 1.079092025756836, 0.9235878586769104, 1.3544921875, 1.3830211162567139]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.2428796291351318, 1.175041675567627, 1.2124924659729004, 1.1689263582229614, 1.418461799621582, 0.8461001515388489, 0.982843816280365, 1.107417106628418, 1.967531681060791, 1.3751707077026367, 1.4437764883041382, 1.3174965381622314, 1.5860655307769775, 1.3379095792770386, 1.2616523504257202, 1.3059674501419067]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.9717650413513184, 1.0681650638580322, 1.075053095817566, 0.9432266354560852, -0.013520768843591213, 1.327778935432434, 1.1513671875, 1.8452433347702026, 0.829646110534668, 1.2584571838378906, 1.18470299243927, 1.2364490032196045, 1.1013728380203247, 1.3291584253311157, 1.0456146001815796, 0.4221348464488983]
Running loglikelihood requests:   8%|████                                              | 41/504 [02:30<26:10,  3.39s/it]Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.8806886672973633, 2.0460216999053955, 1.8856568336486816, 1.740936040878296, 1.7309049367904663, 1.9122041463851929, 1.6962132453918457, 1.9515321254730225, 1.795623540878296, 1.8055503368377686, 2.15507435798645, 2.47239089012146, 1.6243457794189453, 2.2361574172973633, 2.012249708175659, 1.8930901288986206]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.2852510213851929, 1.2909587621688843, 1.8066216707229614, 1.0331426858901978, 1.378318428993225, 1.4884709119796753, 1.2458661794662476, 1.328504204750061, 1.4024101495742798, 1.495951533317566, 1.5458130836486816, 2.090374708175659, 1.416849970817566, 1.4458434581756592, 1.1199702024459839, 1.5427980422973633]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.6092612743377686, 1.7978230714797974, 1.136576533317566, 1.3937727212905884, 1.6263747215270996, 1.4769986867904663, 1.4484602212905884, 1.4071980714797974, 1.558977723121643, 1.5330324172973633, 1.5355544090270996, 1.5429201126098633, 1.349505066871643, 0.8046560883522034, 1.4017760753631592, 1.507433295249939]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.357213258743286, 2.307873249053955, 2.1693150997161865, 2.119425058364868, 2.549984931945801, 2.184314250946045, 2.5282540321350098, 2.143127918243408, 2.129892349243164, 2.1204299926757812, 2.857137441635132, 2.1431660652160645, 2.2093825340270996, 2.3846328258514404, 2.2506446838378906, 2.2638425827026367]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6180787086486816, 1.641876459121704, 1.4433025121688843, 1.6486555337905884, 1.4058897495269775, 1.522849678993225, 1.5507148504257202, 1.8467271327972412, 1.5475196838378906, 1.483881950378418, 1.5266847610473633, 1.3820350170135498, 1.4923771619796753, 1.368647575378418, 1.7377123832702637, 1.8329793214797974]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.795196771621704, 1.7284919023513794, 1.657615303993225, 1.7776566743850708, 1.692247748374939, 1.7797638177871704, 1.678845763206482, 1.7045756578445435, 1.7809607982635498, 1.8046306371688843, 2.222964286804199, 1.6277892589569092, 1.6779998540878296, 1.7685167789459229, 1.8510125875473022, 1.7470952272415161]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.612650752067566, 1.6517363786697388, 1.6062133312225342, 1.7081298828125, 1.6244405508041382, 1.6112382411956787, 1.7505249977111816, 1.6966090202331543, 1.5976117849349976, 1.6969029903411865, 1.4199408292770386, 1.706685185432434, 1.6826409101486206, 1.9147261381149292, 1.5804076194763184, 1.6006144285202026]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9956670999526978, 2.078144073486328, 2.3873331546783447, 1.9710633754730225, 2.0972390174865723, 2.000047445297241, 2.506380796432495, 2.0616371631622314, 1.974855899810791, 1.99036705493927, 1.9228230714797974, 1.8254133462905884, 1.9416160583496094, 1.7959837913513184, 2.3084988594055176, 1.9943492412567139]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.217157363891602, 5.574142932891846, 5.221632480621338, 5.128129005432129, 5.093086242675781, 5.044485569000244, 5.198422431945801, 5.580627918243408, 5.361536502838135, 5.144208908081055, 5.241296291351318, 5.093901634216309, 5.227121829986572, 5.277040481567383, 5.4036712646484375, 5.209629058837891]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.63014817237854, 3.489020824432373, 3.4683139324188232, 3.305455446243286, 3.5865917205810547, 3.5822019577026367, 3.231940746307373, 3.55351185798645, 3.605102777481079, 3.5076892375946045, 3.3801956176757812, 3.09663462638855, 3.4045214653015137, 3.560106039047241, 3.5936646461486816, 3.630967140197754]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.870088815689087, 2.9518356323242188, 2.8717384338378906, 2.8524348735809326, 2.798354148864746, 2.9180636405944824, 2.9964730739593506, 2.750530958175659, 2.851107358932495, 2.7447853088378906, 3.214616298675537, 2.2503104209899902, 2.8726487159729004, 2.7996435165405273, 3.168499708175659, 2.889183759689331]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.13290786743164062, 0.1540692150592804, 0.14203056693077087, -0.24324579536914825, -0.18662525713443756, -0.1998893767595291, 0.161042258143425, -0.2656586766242981, 0.1029612123966217, 0.12906435132026672, 0.13209250569343567, 0.11076511442661285, 0.11352642625570297, 0.13009050488471985, -1.1270443201065063, 0.15807588398456573]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09789480268955231, 0.05879518389701843, 0.03597629815340042, 0.05936996638774872, 0.09919515997171402, 0.0492318794131279, 0.046684782952070236, 0.051773108541965485, -8.743588114157319e-05, 0.067779541015625, -0.16381710767745972, 0.04566206783056259, 0.009389925748109818, -0.021438807249069214, -0.029557010158896446, 0.003109960351139307]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0584091916680336, 0.07155181467533112, 0.0903710126876831, 0.03809685260057449, 0.09676644206047058, 0.10957774519920349, 0.08613035082817078, -0.06348200142383575, 0.06665205210447311, 0.014929176308214664, -0.0004414851136971265, 0.09825292974710464, -0.15453247725963593, 0.043379247188568115, -0.006993812974542379, 0.11142511665821075]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.10456915944814682, 0.16699166595935822, 0.13785341382026672, 0.1583085060119629, 0.11564983427524567, 0.12985393404960632, 0.1391538679599762, 0.19881559908390045, 0.14915019273757935, -0.396745890378952, -0.054830946028232574, 0.12211748957633972, 0.08680634200572968, -0.16274647414684296, -0.15519630908966064, -0.03309193253517151]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.11287099123001099, 0.12052460759878159, 0.10548597574234009, 0.0034943760838359594, 0.03610539063811302, -0.1348869353532791, 0.12966306507587433, 0.06838872283697128, -0.021378206089138985, 0.039025940001010895, -0.04425728693604469, 0.18390965461730957, -0.20292481780052185, -0.1284475028514862, 0.122784323990345, 0.030175426974892616]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.15089699625968933, 0.1861594170331955, 0.09685000777244568, 0.02930835634469986, -0.00020289656822569668, 0.10862255841493607, 0.011172160506248474, 0.11507653445005417, 0.0676216185092926, -0.07818596065044403, -0.30508512258529663, 0.11686007678508759, -0.08470606803894043, 0.11158756166696548, 0.18588079512119293, 0.09602248668670654]
Layer: gate_5 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08494575321674347, 0.28393879532814026, 0.04320148378610611, 0.27958372235298157, 0.18832729756832123, 0.10353519022464752, 0.24099761247634888, 0.16107524931430817, 0.2266775518655777, 0.2018846571445465, -0.2150239795446396, 0.2348751425743103, 0.2508714199066162, 0.09668222069740295, 0.07189080119132996, 0.12808862328529358]
Layer: gate_6 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.2080399990081787, 0.4588172137737274, 0.40461835265159607, 0.15849672257900238, -0.48891153931617737, 0.26009082794189453, 0.2096027284860611, 0.12188678979873657, 0.16784048080444336, -0.05981127917766571, 0.13731761276721954, 0.266492635011673, 0.4488878846168518, 0.1602257490158081, -0.1522899717092514, -0.2370155304670334]
Layer: gate_7 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.29847580194473267, 0.4444033205509186, 0.08922720700502396, 0.49538642168045044, 0.3480629622936249, 0.5585587024688721, 0.3714064061641693, 0.0042786551639437675, 0.1878422647714615, 0.27729418873786926, 0.23345570266246796, -0.19142559170722961, 0.5397369265556335, 0.034259457141160965, 0.07871782779693604, 0.1718187928199768]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.7497727870941162, 0.9230899810791016, 0.4083919823169708, 0.5695604085922241, 0.5657623410224915, 0.48304763436317444, 0.9754191637039185, 0.7175607085227966, 0.7168083786964417, 0.311859130859375, 0.7679075002670288, 0.8857905268669128, 0.6302416920661926, 0.611323893070221, 0.8478496074676514, 0.8218015432357788]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.8930156230926514, 1.0770952701568604, 0.6662316918373108, 0.9997872710227966, 1.0840375423431396, 0.3077752888202667, 0.7909975051879883, 0.8411442041397095, 1.002008080482483, 0.7428534626960754, 0.7794612646102905, 0.8792060017585754, 0.9522534608840942, 0.5511725544929504, 0.7154891490936279, 1.0687291622161865]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.1352490186691284, 1.1837382316589355, 1.1323193311691284, 1.1150109767913818, 1.2822023630142212, 0.9567444920539856, 1.1155775785446167, 1.1188021898269653, 1.204256296157837, 1.2309908866882324, 1.1939395666122437, 1.519769310951233, 1.3281941413879395, 1.4133566617965698, 0.9356780052185059, 1.3997766971588135]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.8423622250556946, 1.4436590671539307, 1.136849284172058, 1.1854405403137207, 1.3391597270965576, 0.9676429033279419, 0.9295769333839417, 1.0053846836090088, 1.1058814525604248, 0.687125027179718, 1.2599347829818726, 1.137296438217163, 1.297330617904663, 1.0808225870132446, 0.8697932958602905, 1.216709852218628]
Layer: gate_12 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [2.0058207511901855, 1.4880105257034302, 1.9361584186553955, 1.6101914644241333, 1.5459661483764648, 0.9057303071022034, 1.6342918872833252, 1.7578511238098145, 1.51953125, 1.7095451354980469, 1.5321588516235352, 2.004060983657837, 1.3486863374710083, 1.8447458744049072, 1.840733289718628, 1.5693408250808716]
Layer: gate_13 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.1319363117218018, 1.0791982412338257, 1.1801612377166748, 1.3294496536254883, 1.4158029556274414, 1.7021687030792236, 0.9019014239311218, 0.7958126068115234, 1.0821834802627563, 1.1493632793426514, 0.5604621171951294, 1.2422213554382324, 1.1888705492019653, 1.317141056060791, 1.2828260660171509, 1.1029045581817627]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [1.0953912734985352, 1.2779239416122437, 1.5392365455627441, 1.2183536291122437, 1.755661129951477, 1.4628809690475464, 1.3045721054077148, 1.3606899976730347, 1.1595567464828491, 1.3716448545455933, 1.3869749307632446, 0.37171727418899536, 1.5651975870132446, 1.2448078393936157, 1.117477536201477, 1.262791395187378]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.0260480642318726, 0.7778448462486267, 0.9767788648605347, 0.7169141173362732, 0.8886380195617676, 0.9340143203735352, 0.0735410749912262, 0.9846505522727966, -0.12274925410747528, 1.2886300086975098, -0.05105485022068024, 0.9263577461242676, 1.152701497077942, 0.8587803840637207, 0.9087397456169128, 0.42808789014816284]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.1519007682800293, 1.2739070653915405, 1.6822208166122437, 2.1366608142852783, 1.8215887546539307, 1.7586053609848022, 1.765296220779419, 1.8590075969696045, 1.6597791910171509, 1.6418360471725464, 1.932730793952942, 1.648108720779419, 1.6298160552978516, 1.530718207359314, 1.3979878425598145, 1.7331523895263672]
Layer: gate_17 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.8799312114715576, 1.827680230140686, 1.9370601177215576, 1.9395304918289185, 1.6665377616882324, 1.785272240638733, 1.7190110683441162, 2.415532112121582, 2.2966713905334473, 1.889561414718628, 1.8965327739715576, 1.7285144329071045, 1.9503209590911865, 1.5314228534698486, 1.873288631439209, 1.8387415409088135]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.2836480140686035, 1.8558168411254883, 1.8160291910171509, 1.9025177955627441, 2.0593671798706055, 1.8437886238098145, 2.074779510498047, 1.939095377922058, 2.0286755561828613, 1.9609568119049072, 2.013362407684326, 2.0731358528137207, 2.083897352218628, 2.0788018703460693, 2.0572593212127686, 1.954343318939209]
Layer: gate_19 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1142549514770508, 1.3196356296539307, 1.4653562307357788, 1.281893014907837, 0.45678800344467163, 1.388270616531372, 1.5529857873916626, 2.0315029621124268, 1.1048825979232788, 1.3834699392318726, 1.4052927494049072, 1.2114208936691284, 1.2626179456710815, 1.4214979410171509, 1.4586870670318604, 0.8441754579544067]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.3827545642852783, 2.402846574783325, 2.3043007850646973, 2.2506768703460693, 2.353360891342163, 2.3866219520568848, 2.352374792098999, 2.3054416179656982, 2.207534074783325, 2.6867072582244873, 2.5854732990264893, 2.545637369155884, 2.2108020782470703, 2.661045789718628, 2.5590386390686035, 2.416576385498047]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.7466932535171509, 1.5815091133117676, 1.7977064847946167, 1.4417253732681274, 1.9387376308441162, 1.7821009159088135, 1.6522663831710815, 1.6875193119049072, 1.7885210514068604, 1.8813234567642212, 1.5631381273269653, 1.7219988107681274, 1.8256304264068604, 1.8373491764068604, 1.7091197967529297, 1.8529354333877563]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [1.9181814193725586, 1.7716680765151978, 1.7156003713607788, 1.7730507850646973, 1.7851369380950928, 1.7353031635284424, 1.6916673183441162, 1.8187848329544067, 1.7167068719863892, 1.6411083936691284, 1.789217233657837, 1.6670695543289185, 1.6682298183441162, 1.3052151203155518, 1.8291499614715576, 1.9674543142318726]
Layer: gate_23 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_24 - Captured router_logits: [2.530747175216675, 2.6168007850646973, 2.4033493995666504, 2.6129331588745117, 2.6702892780303955, 2.6277847290039062, 2.6577582359313965, 2.4859414100646973, 2.6851794719696045, 2.6659576892852783, 2.5909652709960938, 2.5713181495666504, 2.5491955280303955, 2.661045789718628, 2.726059675216675, 2.781733512878418]
Running loglikelihood requests:   9%|████▍                                             | 45/504 [02:44<25:52,  3.38s/it]Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_25 - Captured router_logits: [1.8463412523269653, 1.682820200920105, 1.6381884813308716, 1.831944227218628, 1.4660016298294067, 1.7129099369049072, 1.8209216594696045, 1.760500431060791, 1.7878345251083374, 1.7234103679656982, 1.6873549222946167, 1.713180661201477, 1.7133934497833252, 1.7771503925323486, 1.5738706588745117, 2.3861000537872314]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9299050569534302, 1.6856441497802734, 1.9086527824401855, 1.857136607170105, 1.7003923654556274, 1.7714674472808838, 1.7588680982589722, 1.7189197540283203, 1.846280813217163, 1.789585828781128, 1.9339278936386108, 1.7617417573928833, 1.9914042949676514, 1.6888246536254883, 1.7139209508895874, 1.744875431060791]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5057481527328491, 1.5009397268295288, 1.5597370862960815, 1.565185546875, 1.5923564434051514, 1.5051414966583252, 1.6224238872528076, 1.6133440732955933, 1.6187200546264648, 1.8231019973754883, 1.4731396436691284, 1.5763120651245117, 1.5054218769073486, 1.5393985509872437, 1.710734486579895, 1.5232658386230469]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.121816396713257, 2.189211368560791, 2.342696189880371, 2.2662148475646973, 2.141925573348999, 2.1950416564941406, 2.210080623626709, 2.2466158866882324, 2.294100046157837, 2.2577931880950928, 2.1646716594696045, 2.1885056495666504, 2.126237630844116, 2.084699869155884, 2.3445403575897217, 2.2321159839630127]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_29 - Captured router_logits: [5.466545581817627, 5.5608367919921875, 5.290532112121582, 5.535620212554932, 5.413791656494141, 5.316560745239258, 5.46383810043335, 5.790473937988281, 5.5595221519470215, 5.5558671951293945, 5.471167087554932, 5.282719612121582, 5.397770404815674, 5.40706205368042, 5.518100261688232, 5.328724384307861]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.939201831817627, 3.861502170562744, 3.8788676261901855, 3.758866310119629, 3.8447301387786865, 3.7366762161254883, 3.8726963996887207, 3.8110785484313965, 3.864320755004883, 3.9585299491882324, 3.819563150405884, 3.735079526901245, 3.981508731842041, 3.751847982406616, 3.8730082511901855, 3.9486265182495117]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.0479578971862793, 3.1252708435058594, 2.9497601985931396, 2.660388231277466, 2.9279470443725586, 3.1294476985931396, 2.8251469135284424, 3.022064447402954, 2.956141710281372, 3.1757233142852783, 2.8377552032470703, 2.696462392807007, 3.0614943504333496, 3.0381343364715576, 3.047687292098999, 3.1618192195892334]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09884888678789139, 0.11260464787483215, 0.11133629083633423, -0.2893145680427551, -0.25242823362350464, -0.12717312574386597, 0.13344569504261017, -0.1505921334028244, 0.061812665313482285, 0.09872312098741531, 0.11000130325555801, 0.08299097418785095, 0.09534059464931488, 0.10742893069982529, -1.2005149126052856, 0.12388403713703156]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07963132858276367, 0.052145376801490784, 0.035575080662965775, 0.045821912586688995, 0.07939819246530533, 0.018110400065779686, 0.04940544813871384, 0.08274757117033005, 0.021449027583003044, 0.07584676146507263, -0.19826234877109528, 0.0412282720208168, 0.004323029424995184, -0.02733280509710312, 0.015738219022750854, 0.023686092346906662]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07853418588638306, 0.06297185271978378, 0.09308536350727081, 0.05912727117538452, 0.10638968646526337, 0.12127869576215744, 0.0799390971660614, -0.09904978424310684, 0.07052984088659286, 0.10789524018764496, 0.005674256943166256, 0.09283436089754105, -0.18432003259658813, 0.02743629924952984, -0.032787151634693146, 0.100373774766922]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1421215832233429, 0.17254239320755005, 0.1178116500377655, 0.15543313324451447, 0.14431606233119965, 0.10300559550523758, 0.04392901808023453, 0.16974733769893646, 0.17964141070842743, -0.4680558443069458, 0.022609155625104904, 0.06395044922828674, 0.19641375541687012, -0.2322680950164795, -0.006201202515512705, -0.06690151244401932]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.05895238742232323, 0.08953796327114105, 0.08216574043035507, 0.18823786079883575, 0.06529641896486282, -0.04469199478626251, 0.017783429473638535, 0.05547277256846428, -0.11147281527519226, -0.008238979615271091, -0.1648685336112976, 0.11647628247737885, 0.0013706840109080076, -0.1706746369600296, 0.12297280132770538, -0.0416548065841198]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06030929088592529, 0.1370425522327423, 0.0652177482843399, 0.19183291494846344, -0.06109868362545967, -0.05149894952774048, 0.062121711671352386, -0.05031714215874672, 0.17213056981563568, -0.1066347062587738, -0.17042917013168335, 0.08958159387111664, -0.1817910224199295, 0.1388983428478241, 0.09301610291004181, 0.07040297985076904]
Layer: gate_5 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.10149015486240387, -0.02405383251607418, 0.2270461767911911, 0.2868581712245941, 0.21976029872894287, 0.14891459047794342, -0.08882700651884079, -0.10347211360931396, 0.44343236088752747, 0.17441581189632416, -0.5284920930862427, 0.23700979351997375, 0.2034795582294464, -0.22247561812400818, 0.2603689730167389, 0.1913760006427765]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.44222214818000793, 0.18103410303592682, 0.26508933305740356, 0.12558022141456604, -0.9524440765380859, 0.29692837595939636, 0.18402279913425446, -0.3306202292442322, 0.48626041412353516, 0.2400912046432495, -0.22613771259784698, 0.27233877778053284, 0.16399088501930237, 0.1972450315952301, -0.5942943096160889, -0.1973012089729309]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.25610917806625366, 0.3518138527870178, -0.29897964000701904, 0.2599933445453644, 0.5345857739448547, 0.2571433484554291, 0.2576085329055786, -0.2935745120048523, 0.19683830440044403, 0.9059463739395142, -0.003372106235474348, 0.2012433409690857, 0.3771575391292572, -0.49858546257019043, -0.45328450202941895, 0.35387739539146423]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3809071481227875, 0.18857623636722565, 0.4167393147945404, 1.0834052562713623, 0.3739608824253082, 0.3476541042327881, 0.5976801514625549, 0.5143269896507263, 0.7790073156356812, 0.1243148148059845, 0.19854460656642914, 0.6470732688903809, 0.6069651246070862, 0.4227944314479828, 0.6938537955284119, 0.6274291276931763]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9388120770454407, 0.7642141580581665, 0.5402678847312927, 0.751982569694519, 1.0574393272399902, 0.6690937876701355, 0.6345632076263428, 0.675511360168457, 0.21315746009349823, 0.69022536277771, 0.7156755328178406, 0.9573968648910522, 0.15854418277740479, 0.25495976209640503, 0.9837795495986938, 0.36548328399658203]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9412884712219238, 1.0485385656356812, 0.8965432643890381, 1.1670703887939453, 1.2422034740447998, 0.5758457183837891, 0.8633695840835571, 1.1210225820541382, 1.1068109273910522, 1.4434574842453003, 0.8164369463920593, 0.9103402495384216, 0.43936294317245483, 0.38801199197769165, -0.10010133683681488, 0.7281646132469177]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.15252681076526642, 0.8540378212928772, 1.3154199123382568, 1.125510334968567, 0.2991955578327179, 0.7064675092697144, 1.0951768159866333, 1.3476831912994385, 0.75336092710495, 0.2910959720611572, 0.7754231095314026, 0.8898496627807617, 0.9836984276771545, 0.8365558385848999, 0.5661142468452454, 0.9017843008041382]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6813532114028931, 0.9595192670822144, 0.8826293349266052, 0.4454290568828583, 1.1825042963027954, -0.1757962852716446, 1.0640311241149902, 2.290515184402466, 0.9741578698158264, 0.8916119933128357, 1.3394217491149902, 1.1234688758850098, -0.13056723773479462, 0.9264388084411621, 1.078723669052124, 0.6532980799674988]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7546003460884094, 0.79767245054245, 0.9945430159568787, 0.8969702124595642, 0.998414933681488, 1.2518246173858643, 0.36624404788017273, 1.5964447259902954, 0.8535499572753906, 0.953455924987793, -0.09842455387115479, 0.7724621891975403, 1.0621877908706665, 1.0904080867767334, 1.0783802270889282, 0.6440196633338928]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.39519068598747253, 1.0946136713027954, 0.9320332407951355, 0.9386041760444641, 0.6286982893943787, 0.8709563612937927, 1.4620673656463623, 1.0057121515274048, 0.8518863916397095, 0.9832266569137573, 0.6669521331787109, -0.38847097754478455, 0.9458964467048645, 0.8976327180862427, 1.0207433700561523, 1.5338281393051147]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.6408430933952332, 0.2690126299858093, 0.8295726776123047, 0.8078742027282715, 0.6206373572349548, 0.4412265121936798, 0.36721864342689514, 0.8119828701019287, -0.8264690637588501, 2.5778207778930664, -0.797126829624176, 0.7444209456443787, 0.8478181958198547, 0.8842650651931763, 0.829182505607605, 0.43469667434692383]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.1311565786600113, 0.5700429081916809, 1.3143550157546997, 1.0290472507476807, 1.369003176689148, 1.3453706502914429, 1.372178316116333, 1.295549988746643, 1.8520734310150146, 1.2769355773925781, 2.0647573471069336, 1.3787354230880737, 1.3657495975494385, 1.1843616962432861, 0.590491533279419, 1.1242133378982544]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3339745998382568, 1.2717101573944092, 1.3361586332321167, 1.6201907396316528, 0.8591020107269287, 1.1485921144485474, 1.7569488286972046, 1.5830715894699097, 2.085819721221924, 1.3169951438903809, 1.009787678718567, 0.7477520704269409, 1.0668165683746338, 0.8509021401405334, 1.349442481994629, 1.3607097864151]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.1683441400527954, 1.1199748516082764, 1.160018801689148, 1.1213784217834473, 1.3432985544204712, 0.8134023547172546, 0.9202850461006165, 1.0699763298034668, 1.8738811016082764, 1.307298183441162, 1.3582954406738281, 1.2534105777740479, 1.574110746383667, 1.2558691501617432, 1.2002688646316528, 1.2357097864151]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.9408302307128906, 1.069365382194519, 1.0232633352279663, 0.9095367193222046, 0.031154153868556023, 1.2653943300247192, 1.1065876483917236, 1.8844736814498901, 0.8267607688903809, 1.2403324842453003, 1.167428970336914, 1.2195106744766235, 1.0844457149505615, 1.2718573808670044, 1.0311224460601807, 0.40492716431617737]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.7944998741149902, 1.9736181497573853, 1.8342297077178955, 1.6742894649505615, 1.66015625, 1.8483432531356812, 1.6114164590835571, 1.9219927787780762, 1.7381831407546997, 1.7126845121383667, 2.065915584564209, 2.408644676208496, 1.5150214433670044, 2.172032117843628, 1.944664716720581, 1.8554884195327759]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.2823295593261719, 1.2602072954177856, 1.8209799528121948, 1.0322829484939575, 1.3632615804672241, 1.4840805530548096, 1.2000432014465332, 1.290505290031433, 1.4456560611724854, 1.418596863746643, 1.5416536331176758, 2.0933966636657715, 1.4141606092453003, 1.4610944986343384, 1.093401551246643, 1.5361573696136475]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.517803907394409, 1.764977216720581, 1.150524377822876, 1.464333415031433, 1.5848774909973145, 1.4381870031356812, 1.401067852973938, 1.389839768409729, 1.5160764455795288, 1.4480409622192383, 1.5100306272506714, 1.5216904878616333, 1.302675485610962, 0.7471174001693726, 1.393147349357605, 1.4685341119766235]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.30087947845459, 2.260305404663086, 2.153599977493286, 2.0290122032165527, 2.5721969604492188, 2.17691969871521, 2.483079433441162, 2.1127512454986572, 2.077908992767334, 2.0939855575561523, 2.7456421852111816, 2.077693223953247, 2.190091133117676, 2.330087900161743, 2.2185537815093994, 2.2118797302246094]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6227229833602905, 1.6838881969451904, 1.4670618772506714, 1.6403894424438477, 1.3844926357269287, 1.5184516906738281, 1.569115161895752, 1.8550761938095093, 1.5534312725067139, 1.4882420301437378, 1.55973219871521, 1.388642430305481, 1.494896411895752, 1.3931572437286377, 1.7198688983917236, 1.852347731590271]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8063265085220337, 1.7067441940307617, 1.6208189725875854, 1.745553970336914, 1.6878385543823242, 1.7426941394805908, 1.654197335243225, 1.7067760229110718, 1.7291933298110962, 1.7891654968261719, 2.189227342605591, 1.614498257637024, 1.6469457149505615, 1.7486652135849, 1.8216975927352905, 1.7499046325683594]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.601015329360962, 1.6533411741256714, 1.5867888927459717, 1.6952756643295288, 1.5922188758850098, 1.5752365589141846, 1.7228983640670776, 1.6770951747894287, 1.5743045806884766, 1.6749076843261719, 1.3858256340026855, 1.682568073272705, 1.6462047100067139, 1.851019024848938, 1.5642690658569336, 1.5896719694137573]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  10%|████▊                                             | 49/504 [02:57<25:20,  3.34s/it]Layer: gate_28 - Captured router_logits: [1.9576987028121948, 2.0390820503234863, 2.343867778778076, 1.9215608835220337, 2.079145669937134, 1.956187129020691, 2.460113048553467, 2.0310440063476562, 1.9209327697753906, 1.9238379001617432, 1.8733707666397095, 1.7433652877807617, 1.8878425359725952, 1.7518255710601807, 2.2494895458221436, 1.9632341861724854]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.303823947906494, 5.644747257232666, 5.310458660125732, 5.249322891235352, 5.1808552742004395, 5.133813381195068, 5.339156627655029, 5.645394802093506, 5.427567481994629, 5.234767436981201, 5.281093120574951, 5.208425045013428, 5.27561616897583, 5.330009460449219, 5.470909118652344, 5.276264190673828]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.7483105659484863, 3.585387945175171, 3.569124937057495, 3.3880069255828857, 3.6306116580963135, 3.6673007011413574, 3.357686758041382, 3.5906732082366943, 3.6608188152313232, 3.642914295196533, 3.504995584487915, 3.216017723083496, 3.522019386291504, 3.625213384628296, 3.681591510772705, 3.6982715129852295]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.947255849838257, 3.0231235027313232, 2.9530465602874756, 2.8890154361724854, 2.895120143890381, 2.9758753776550293, 3.0306217670440674, 2.8077595233917236, 2.9009499549865723, 2.8569016456604004, 3.234375, 2.3057055473327637, 2.9738929271698, 2.8829989433288574, 3.1801979541778564, 2.9800760746002197]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06722865998744965, 0.08197099715471268, 0.09249749779701233, -0.2741701900959015, -0.25845634937286377, -0.06915036588907242, 0.08992323279380798, -0.05670259892940521, 0.05677865818142891, 0.0662662610411644, 0.0652853325009346, 0.03128845989704132, 0.07090109586715698, 0.10204176604747772, -1.0311404466629028, 0.10105355829000473]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07289194315671921, 0.04557707533240318, 0.01958831399679184, 0.051569677889347076, 0.0668444111943245, 0.023460984230041504, 0.052958134561777115, 0.10414194315671921, 0.05480927601456642, 0.06998195499181747, -0.19044174253940582, 0.06643566489219666, 0.008240137249231339, -0.019466105848550797, 0.04454365372657776, 0.0432116761803627]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09364838898181915, 0.04378823563456535, 0.07698896527290344, 0.057994674891233444, 0.13926884531974792, 0.09332463890314102, 0.07797589153051376, -0.08359085768461227, 0.08974260836839676, 0.14402002096176147, 0.024769006296992302, 0.07441919296979904, -0.21817047894001007, 0.014555652625858784, -0.040820859372615814, 0.10067760944366455]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12438135594129562, 0.14748965203762054, 0.11997288465499878, 0.11684257537126541, 0.13844150304794312, 0.09207629412412643, -0.055063530802726746, 0.1423010379076004, 0.16035743057727814, -0.5052530765533447, 0.039256833493709564, 0.08051753789186478, 0.2825073301792145, -0.33051133155822754, 0.11399023234844208, -0.07206287980079651]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.05280480161309242, 0.09265171736478806, 0.048931632190942764, 0.2943199872970581, 0.06690517067909241, -0.033890318125486374, 0.032099250704050064, 0.0356861911714077, -0.10959190875291824, -0.030944354832172394, -0.2620677351951599, 0.10429522395133972, 0.11857330799102783, -0.23782365024089813, 0.14872342348098755, -0.14414764940738678]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.03559531271457672, 0.1520427167415619, 0.05956827849149704, 0.268271267414093, -0.18395069241523743, -0.14380285143852234, 0.06501615792512894, -0.027485696598887444, 0.22411154210567474, -0.07629328221082687, -0.1940414011478424, 0.11663106083869934, -0.2467832863330841, 0.19542106986045837, 0.1183772161602974, 0.08772364258766174]
Layer: gate_5 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.12626484036445618, -0.01720072515308857, 0.18263962864875793, 0.25719401240348816, 0.1851571947336197, 0.15199553966522217, -0.19397372007369995, -0.09327318519353867, 0.3325708508491516, 0.1667075902223587, -0.5323693156242371, 0.23578155040740967, 0.28876203298568726, -0.31670093536376953, 0.31993839144706726, 0.22118037939071655]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5895623564720154, 0.24771618843078613, 0.2470349371433258, 0.17627085745334625, -0.760977566242218, 0.24049495160579681, 0.17701391875743866, -0.4691396951675415, 0.6939359307289124, 0.19628670811653137, -0.187666118144989, 0.26560699939727783, 0.09250832349061966, 0.29899510741233826, -0.6080459952354431, -0.43358221650123596]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2692627012729645, 0.2648307681083679, -0.22681672871112823, 0.3285425305366516, 0.3841979205608368, 0.4110373556613922, 0.2811669111251831, -0.4130403995513916, 0.20082193613052368, 0.9517625570297241, 0.13979172706604004, 0.21016907691955566, 0.5227163434028625, -0.41907191276550293, -0.5548194050788879, 0.4931783080101013]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.5093493461608887, 0.3183582127094269, 0.49147796630859375, 1.237682819366455, 0.49258363246917725, 0.21223536133766174, 0.6726800203323364, 0.6338704228401184, 1.1669176816940308, 0.22517214715480804, 0.24504664540290833, 0.6894330978393555, 0.45092490315437317, 0.0011872120667248964, 0.8283804059028625, 0.6457081437110901]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0054537057876587, 0.8862429857254028, 0.5888671875, 0.643629789352417, 0.8822178244590759, 0.7459222078323364, 0.7533478736877441, 0.7278195023536682, 0.0814288780093193, 0.44292134046554565, 0.7609400153160095, 1.0409905910491943, 0.20258913934230804, 0.19399605691432953, 1.290684461593628, 0.5513483881950378]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [0.9593750238418579, 1.4255809783935547, 0.914533257484436, 1.3625307083129883, 1.34632408618927, 0.27948811650276184, 0.9874098300933838, 0.901939332485199, 1.0834885835647583, 1.7635116577148438, 0.9188627004623413, 1.0817457437515259, 0.36523938179016113, 0.6044325828552246, 0.23004166781902313, 0.8577899932861328]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.25891998410224915, 0.8527694344520569, 1.413912296295166, 1.2654247283935547, 0.5732121467590332, 0.6525853872299194, 1.3118482828140259, 1.660534381866455, 0.4767429530620575, 0.08494967222213745, 0.955033004283905, 0.9105969667434692, 0.9235376715660095, 0.885506808757782, 0.8268329501152039, 1.0273537635803223]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.123702883720398, 1.2598657608032227, 1.3833708763122559, 0.7473654747009277, 1.3365684747695923, -0.07611772418022156, 1.339453101158142, 2.2823917865753174, 1.4866787195205688, 1.1517928838729858, 1.9435396194458008, 1.3765023946762085, 0.5819007158279419, 1.0674829483032227, 1.1954026222229004, 0.9657924771308899]
Running loglikelihood requests:  11%|█████▎                                            | 53/504 [03:10<25:07,  3.34s/it]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8782276511192322, 0.8359775543212891, 1.1749398708343506, 0.9498998522758484, 1.1549679040908813, 1.8449431657791138, 0.5685723423957825, 2.02477765083313, 0.8731557726860046, 0.8468524813652039, -0.40152180194854736, 1.0227439403533936, 0.9863892197608948, 1.1276742219924927, 1.146334171295166, 0.6706098318099976]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.5355703234672546, 1.1297175884246826, 1.0001201629638672, 1.0005608797073364, 0.873957097530365, 0.969020426273346, 2.0294408798217773, 1.0585737228393555, 1.005969524383545, 0.9921674728393555, 0.7884640693664551, -0.2107582688331604, 1.1973345279693604, 0.9403345584869385, 1.0128905773162842, 1.5883737802505493]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6009373068809509, 0.0985562801361084, 0.6967241168022156, 0.7359249591827393, 0.6886656284332275, 0.49171018600463867, 0.7575916647911072, 0.7062525153160095, -0.6822950839996338, 2.0668418407440186, -0.7487764954566956, 0.6289650797843933, 0.7558343410491943, 0.7811473608016968, 0.7391025424003601, 0.48489704728126526]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.20088626444339752, 0.8775217533111572, 1.448352336883545, 1.3159254789352417, 1.5253205299377441, 1.3952323198318481, 1.5011242628097534, 1.4146108627319336, 1.9831737279891968, 1.300538420677185, 2.574779748916626, 1.7595338821411133, 1.5129889249801636, 1.3846279382705688, 0.5699300169944763, 1.2010316848754883]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6208332777023315, 1.3828125, 1.5351287126541138, 1.993709921836853, 1.1024163961410522, 1.3656400442123413, 2.3487980365753174, 1.8524038791656494, 2.729687452316284, 1.5741386413574219, 1.3271734714508057, 1.0425618886947632, 1.3214681148529053, 0.8307445049285889, 1.5993388891220093, 1.6832531690597534]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.6534054279327393, 1.5342347621917725, 2.010136127471924, 1.505278468132019, 1.8457932472229004, 1.3242037296295166, 1.5716246366500854, 1.521444320678711, 2.491766929626465, 1.614372968673706, 1.9084434509277344, 1.6587039232254028, 1.6616486310958862, 1.7351963520050049, 1.762660264968872, 1.726953148841858]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.4467923641204834, 1.096714735031128, 0.9944210648536682, 1.0448267459869385, 0.1325007528066635, 1.4489834308624268, 1.2136017084121704, 1.9879926443099976, 0.796962320804596, 1.365384578704834, 1.2097856998443604, 1.1864558458328247, 1.1198316812515259, 1.2149138450622559, 0.9834021925926208, 0.44804468750953674]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.192988872528076, 2.3441104888916016, 2.317307710647583, 2.09663462638855, 2.081390142440796, 2.2345752716064453, 2.114262819290161, 2.3975160121917725, 2.1279046535491943, 2.3177082538604736, 2.468669891357422, 2.7899839878082275, 1.855428695678711, 2.5424678325653076, 2.294170618057251, 2.2648236751556396]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.489142656326294, 1.5130008459091187, 2.271634578704834, 1.149098515510559, 1.4819711446762085, 1.6521234512329102, 1.3374449014663696, 1.4860175848007202, 1.6052684783935547, 1.626672625541687, 1.6895031929016113, 2.1595752239227295, 1.5570513010025024, 1.6536458730697632, 1.3147436380386353, 1.6833432912826538]
Layer: gate_22 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.485056161880493, 1.914022445678711, 1.2261818647384644, 1.5836538076400757, 1.789102554321289, 1.536358118057251, 1.5131410360336304, 1.4469150304794312, 1.5284254550933838, 1.568549633026123, 1.5475960969924927, 1.611984133720398, 1.2732973098754883, 0.7415170669555664, 1.3529647588729858, 1.5307492017745972]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.551722764968872, 2.4991185665130615, 2.489823818206787, 2.184415102005005, 2.6097755432128906, 2.321514368057251, 2.459815740585327, 2.2852563858032227, 2.179647445678711, 2.2598557472229004, 3.060176372528076, 2.223076820373535, 2.4113380908966064, 2.5284454822540283, 2.3391025066375732, 2.307652235031128]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7957732677459717, 1.7471554279327393, 1.6710937023162842, 1.9159655570983887, 1.3964343070983887, 1.7558293342590332, 1.8025741577148438, 2.1096153259277344, 1.796364188194275, 1.7162059545516968, 1.7247695922851562, 1.5968549251556396, 1.7565304040908813, 1.5872997045516968, 2.3691906929016113, 2.0648036003112793]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_26 - Captured router_logits: [1.7830429077148438, 2.0701935291290283, 1.821213960647583, 1.8112479448318481, 1.820748209953308, 1.971719741821289, 1.8409918546676636, 1.812314748764038, 1.879447102546692, 1.900275468826294, 2.5883262157440186, 1.7739232778549194, 1.8549078702926636, 1.8295572996139526, 1.9425380229949951, 1.890312671661377]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.628395438194275, 1.7131773233413696, 1.6558581590652466, 1.6645758152008057, 1.5510667562484741, 1.6040873527526855, 1.8711676597595215, 1.712740421295166, 1.5794283151626587, 1.6731621026992798, 1.3002203702926636, 1.6402143239974976, 1.7194410562515259, 2.0943446159362793, 1.6500275135040283, 1.6332281827926636]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.104677438735962, 2.2182490825653076, 2.3410305976867676, 2.0384414196014404, 2.1838040351867676, 2.116095781326294, 2.7198917865753174, 2.2000300884246826, 2.0969350337982178, 2.0610175132751465, 2.030168294906616, 1.863862156867981, 1.9453099966049194, 1.9350961446762085, 2.3443310260772705, 2.007892608642578]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.953044891357422, 5.5675482749938965, 4.975440502166748, 4.809294700622559, 4.947015285491943, 4.869991779327393, 4.9402642250061035, 5.048197269439697, 5.114142417907715, 4.95725154876709, 4.891867160797119, 4.882612228393555, 4.920192241668701, 5.1072516441345215, 5.166866779327393, 4.927323818206787]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.517648220062256, 3.412339687347412, 3.327604055404663, 3.2053885459899902, 3.4539663791656494, 3.440504789352417, 3.0578925609588623, 3.584134578704834, 3.614142656326294, 3.388741970062256, 3.2677483558654785, 2.816481351852417, 3.126161813735962, 3.4232370853424072, 3.4847354888916016, 3.43235182762146]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.5224359035491943, 2.610496759414673, 2.714503288269043, 2.586057662963867, 2.605929374694824, 2.5521233081817627, 2.5767228603363037, 2.2152042388916016, 2.5813300609588623, 2.277083396911621, 3.1413261890411377, 1.610731601715088, 2.5736777782440186, 2.452604055404663, 3.0527243614196777, 2.5206329822540283]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09845104068517685, 0.10249148309230804, 0.10204681754112244, -0.21151496469974518, -0.1585298627614975, -0.09622132033109665, 0.12828150391578674, -0.17938870191574097, 0.07475030422210693, 0.09515482932329178, 0.098026804625988, 0.08002593368291855, 0.09433171153068542, 0.10562055557966232, -0.914981484413147, 0.12106307595968246]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08717643469572067, 0.045574501156806946, 0.029511984437704086, 0.06331582367420197, 0.06723673641681671, 0.058581072837114334, 0.03919346258044243, 0.05643580108880997, -0.003989342134445906, 0.07035338878631592, -0.14460496604442596, 0.04671694338321686, 0.017835548147559166, -0.004228249657899141, 0.007261765189468861, 0.012101295404136181]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05653005838394165, 0.08918740600347519, 0.10022398829460144, 0.034735262393951416, 0.09008032083511353, 0.11897864937782288, 0.07199746370315552, -0.04751273989677429, 0.07847403734922409, 0.06070532277226448, 0.005086959805339575, 0.10244680196046829, -0.10208208113908768, 0.019320013001561165, -0.01577189564704895, 0.10047130286693573]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11594582349061966, 0.1561843454837799, 0.15417629480361938, 0.14523358643054962, 0.12411737442016602, 0.12487742304801941, 0.04385344684123993, 0.16703902184963226, 0.1696692854166031, -0.44403567910194397, 0.04525668919086456, 0.12812843918800354, 0.12836100161075592, -0.19153036177158356, -0.06573517620563507, -0.0020150209311395884]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.1028551310300827, 0.0995996743440628, 0.09394311904907227, 0.045604608952999115, 0.1180000901222229, -0.06211434304714203, 0.06104990467429161, 0.06023043021559715, -0.08552895486354828, 0.005905386060476303, -0.11896315217018127, 0.14552456140518188, -0.09251967072486877, -0.15765047073364258, 0.1478121280670166, -0.07471282035112381]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_5 - Captured router_logits: [0.0560811348259449, 0.13910701870918274, 0.08812013268470764, 0.06717506051063538, -0.07155691087245941, 0.007574384566396475, 0.04598991200327873, 0.028654225170612335, 0.06484194844961166, -0.05310199409723282, -0.14896146953105927, 0.10921603441238403, -0.07068067044019699, 0.1608586609363556, 0.12262968719005585, 0.08622352033853531]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.17455221712589264, 0.10072705894708633, 0.15949100255966187, 0.33417999744415283, 0.17418412864208221, 0.140821173787117, -0.09450267255306244, -0.05413145571947098, 0.28326353430747986, 0.2738567590713501, -0.32178109884262085, 0.30059272050857544, 0.22199299931526184, -0.1361008882522583, 0.1285325288772583, 0.1638539582490921]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.31789347529411316, 0.2906181812286377, 0.27973976731300354, 0.14201723039150238, -0.603650689125061, 0.2786399722099304, 0.2867041826248169, -0.26828598976135254, 0.2846730649471283, 0.1717732548713684, -0.07951723039150238, 0.26955410838127136, 0.22350870072841644, 0.19566713273525238, -0.32616186141967773, -0.22196170687675476]
Layer: gate_7 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.29976028203964233, 0.3366257846355438, -0.23871131241321564, 0.3674601912498474, 0.42104342579841614, 0.4717322587966919, 0.2728964686393738, -0.18591433763504028, 0.23475846648216248, 0.5702838897705078, 0.17693105340003967, 0.033156488090753555, 0.5303760766983032, -0.24309676885604858, -0.35031142830848694, 0.31419381499290466]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.611943781375885, 0.3394107222557068, 0.41293397545814514, 0.7432228922843933, 0.6334118247032166, 0.3696819543838501, 0.7360289096832275, 0.6456305384635925, 0.7252222299575806, 0.23303911089897156, 0.3085397481918335, 0.6517027020454407, 0.7230749130249023, 0.22291822731494904, 0.705939531326294, 0.7903445363044739]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_10 - Captured router_logits: [0.964664101600647, 0.8661758899688721, 0.7246444225311279, 0.8208483457565308, 1.0127860307693481, 0.5781922340393066, 0.7530636787414551, 0.7384515404701233, 0.315530002117157, 0.5356057286262512, 0.753022313117981, 0.8655204772949219, 0.41630983352661133, 0.3478829264640808, 1.106107234954834, 0.5401411056518555]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [0.9203876256942749, 1.0812963247299194, 0.9508163332939148, 1.4090347290039062, 1.30042564868927, 0.6033841371536255, 0.9346554279327393, 1.0009164810180664, 0.992658257484436, 1.248327374458313, 0.9899695515632629, 1.1642627716064453, 0.549752414226532, 0.721881628036499, 0.37201568484306335, 0.870800793170929]
Layer: gate_11 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.40543338656425476, 1.029767632484436, 1.2869491577148438, 1.1810897588729858, 0.6389961242675781, 0.8702499270439148, 1.0896309614181519, 1.2185547351837158, 0.7735070586204529, 0.46679961681365967, 1.091662883758545, 0.9886518716812134, 0.9051018953323364, 0.9227213263511658, 0.8743839859962463, 1.174148678779602]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.1138972043991089, 1.0828626155853271, 1.660965085029602, 0.9583710432052612, 1.352654218673706, 0.13969507813453674, 1.3081430196762085, 2.0616185665130615, 1.1920772790908813, 1.2842648029327393, 1.514573335647583, 1.4334135055541992, 0.7653647661209106, 1.1651614904403687, 1.3908002376556396, 1.1671446561813354]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.0485758781433105, 0.9654797911643982, 1.107887625694275, 1.0740290880203247, 1.3029496669769287, 1.6341296434402466, 0.7467676401138306, 1.5175862312316895, 0.9270482659339905, 1.0654296875, -0.06955503672361374, 1.1109538078308105, 1.2713686227798462, 1.3096153736114502, 1.2787059545516968, 0.9500588178634644]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.8205698132514954, 1.283483624458313, 1.250540852546692, 1.0911158323287964, 0.9786921739578247, 1.1775341033935547, 1.666894555091858, 1.138131022453308, 1.1021133661270142, 1.2490484714508057, 0.9027450084686279, 0.3151673972606659, 1.472358226776123, 1.181640625, 1.0699118375778198, 1.7000072002410889]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8727414011955261, 0.33299434185028076, 1.0774776935577393, 0.7818406224250793, 0.6829301714897156, 0.6430094242095947, 0.4510568380355835, 0.9038405418395996, -0.33145689964294434, 2.059612989425659, -0.613064706325531, 0.8368740081787109, 1.0143529176712036, 1.114971399307251, 0.9533002972602844, 0.29902616143226624]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.4493268132209778, 1.2295407056808472, 1.6339142322540283, 1.56242036819458, 1.637580156326294, 1.6542267799377441, 1.6134815216064453, 1.7546474933624268, 1.817968726158142, 1.58598792552948, 2.18923282623291, 1.6329426765441895, 1.493645429611206, 1.2630459070205688, 0.9928761124610901, 1.5438501834869385]
Running loglikelihood requests:  11%|█████▋                                            | 57/504 [03:24<25:10,  3.38s/it]Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.709735631942749, 1.483433485031128, 1.7514322996139526, 1.8327724933624268, 1.3625613451004028, 1.6016682386398315, 1.728084921836853, 1.9504607915878296, 2.3761417865753174, 1.7084134817123413, 1.6018028259277344, 1.3117276430130005, 1.5127729177474976, 1.1761630773544312, 1.7152143716812134, 1.6699318885803223]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.877614140510559, 1.712700366973877, 1.7121193408966064, 1.725620985031128, 1.9739183187484741, 1.5762419700622559, 1.747536063194275, 1.8104066848754883, 2.1738781929016113, 1.7230769395828247, 1.9285056591033936, 1.862620234489441, 2.233253240585327, 1.9202324151992798, 1.9025039672851562, 1.8357772827148438]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1614129543304443, 1.3275991678237915, 1.2732322216033936, 1.0533716678619385, 0.31587257981300354, 1.4882712364196777, 1.4333432912826538, 2.0626978874206543, 1.086465835571289, 1.430548906326294, 1.4044771194458008, 1.2633262872695923, 1.3509565591812134, 1.5748096704483032, 1.2650835514068604, 0.8469998836517334]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.351101875305176, 2.4126203060150146, 2.3020031452178955, 2.1762821674346924, 2.2470953464508057, 2.339162588119507, 2.2138822078704834, 2.3633813858032227, 2.2094149589538574, 2.4833734035491943, 2.536698818206787, 2.5909855365753174, 2.1142427921295166, 2.6804888248443604, 2.5128204822540283, 2.42311692237854]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.7141025066375732, 1.5713140964508057, 2.05961537361145, 1.4047225713729858, 1.822495937347412, 1.8435497283935547, 1.7200521230697632, 1.6421674489974976, 1.7540264129638672, 1.9072315692901611, 1.6918669939041138, 2.1209936141967773, 1.757151484489441, 1.8517427444458008, 1.6452925205230713, 1.9886418581008911]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.397125482559204, 1.9078325033187866, 1.524924874305725, 1.6522235870361328, 1.7713141441345215, 1.7204928398132324, 1.6911457777023315, 1.688722014427185, 1.7737981081008911, 1.6789262294769287, 1.717307686805725, 1.6506309509277344, 1.6616986989974976, 1.196597695350647, 1.7018028497695923, 1.8520232439041138]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3050079345703125, 2.447636127471924, 2.233773946762085, 2.293429374694824, 2.5680289268493652, 2.493910312652588, 2.59743595123291, 2.288541555404663, 2.2492387294769287, 2.3256008625030518, 2.615865468978882, 2.3813300132751465, 2.332892656326294, 2.463942289352417, 2.406209945678711, 2.4632210731506348]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7867387533187866, 1.7366987466812134, 1.550821304321289, 1.836838960647583, 1.4582231044769287, 1.6347355842590332, 1.7017027139663696, 1.8871394395828247, 1.7225761413574219, 1.6692308187484741, 1.6656550168991089, 1.724098563194275, 1.7439703941345215, 1.526181936264038, 1.6473157405853271, 1.8901642560958862]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.8048477172851562, 1.7085074186325073, 1.7108774185180664, 1.8216546773910522, 1.6793869733810425, 1.803613305091858, 1.7478491067886353, 1.6973958015441895, 1.798433780670166, 1.9165915250778198, 2.196624517440796, 1.7097456455230713, 1.8702123165130615, 1.7844150066375732, 1.718727469444275, 1.7653107643127441]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6175105571746826, 1.6244597434997559, 1.7124098539352417, 1.7237855195999146, 1.6151742935180664, 1.626960277557373, 1.739941120147705, 1.7272635698318481, 1.628476858139038, 1.7483924627304077, 1.478325366973877, 1.6176632642745972, 1.638276219367981, 1.753720998764038, 1.6007411479949951, 1.6188551187515259]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.134655475616455, 2.167187452316284, 2.4547276496887207, 2.1338140964508057, 2.1713340282440186, 2.162780523300171, 2.389302968978882, 2.2518930435180664, 2.094210624694824, 2.1819310188293457, 2.1041667461395264, 2.04194712638855, 2.082491874694824, 2.0075721740722656, 2.3504207134246826, 2.231590509414673]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.394070625305176, 5.6060895919799805, 5.297075271606445, 5.211518287658691, 5.222736358642578, 5.287299633026123, 5.40993595123291, 5.589903831481934, 5.512980937957764, 5.4311699867248535, 5.351161956787109, 5.1559295654296875, 5.439563274383545, 5.425320625305176, 5.49383020401001, 5.284735679626465]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.9510817527770996, 3.8786659240722656, 3.9212138652801514, 3.6757185459136963, 3.773257255554199, 3.816866874694824, 3.6485939025878906, 3.776742696762085, 3.922435998916626, 3.894611358642578, 3.8792667388916016, 3.56125807762146, 3.9072964191436768, 3.790114164352417, 3.935817241668701, 3.99364972114563]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [3.0599958896636963, 3.0787060260772705, 3.0181491374969482, 2.7988781929016113, 2.9703927040100098, 3.164963960647583, 2.9306089878082275, 2.8697516918182373, 2.993509531021118, 3.0064704418182373, 3.0135016441345215, 2.372330665588379, 3.014503240585327, 3.03493595123291, 3.2201120853424072, 3.1091346740722656]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.06863889843225479, 0.08743072301149368, 0.09118280559778214, -0.2850661277770996, -0.25125375390052795, -0.08594904094934464, 0.09123433381319046, -0.052675869315862656, 0.06365536153316498, 0.07313434779644012, 0.07106644660234451, 0.038286592811346054, 0.0692780613899231, 0.11016383022069931, -1.0609681606292725, 0.10777666419744492]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07651365548372269, 0.04390239343047142, 0.024152211844921112, 0.050981491804122925, 0.06748630106449127, 0.021872783079743385, 0.05788365378975868, 0.09839112311601639, 0.06843139976263046, 0.07019137591123581, -0.1928107738494873, 0.058685287833213806, 0.013616690412163734, -0.018291177228093147, 0.04003706946969032, 0.03803854063153267]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08257614076137543, 0.035465970635414124, 0.064311183989048, 0.07094340771436691, 0.13938666880130768, 0.08277632296085358, 0.09992097318172455, -0.10296826809644699, 0.09301461279392242, 0.1138090193271637, 0.033570364117622375, 0.06915715336799622, -0.24285663664340973, 0.019812963902950287, -0.039860017597675323, 0.10677219182252884]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12385401129722595, 0.14490564167499542, 0.11520601063966751, 0.12459283322095871, 0.12479899078607559, 0.09065228700637817, -0.026654653251171112, 0.15114617347717285, 0.18146827816963196, -0.5394508242607117, 0.029300116002559662, 0.08367949724197388, 0.30141615867614746, -0.3266589045524597, 0.08901029080152512, -0.08232227712869644]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.048685282468795776, 0.07436715066432953, 0.056801099330186844, 0.27767786383628845, 0.0710066556930542, -0.06000198423862457, 0.037431154400110245, 0.015062124468386173, -0.12569740414619446, -0.008610463701188564, -0.2559795379638672, 0.12468580901622772, 0.11487685889005661, -0.23630687594413757, 0.1398940086364746, -0.11988312751054764]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.0551416277885437, 0.15748639404773712, 0.05786982551217079, 0.2956377863883972, -0.19627530872821808, -0.13797716796398163, 0.07622895389795303, -0.033888980746269226, 0.20803998410701752, -0.10067582875490189, -0.1689767837524414, 0.11302027106285095, -0.24793745577335358, 0.1979893445968628, 0.11958032101392746, 0.08330556005239487]
Layer: gate_5 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.13954506814479828, 0.017467163503170013, 0.17947301268577576, 0.25779178738594055, 0.19019179046154022, 0.14143411815166473, -0.17410658299922943, -0.09151478856801987, 0.334439754486084, 0.1835923194885254, -0.5653676986694336, 0.2492813616991043, 0.28313353657722473, -0.3141193389892578, 0.34494373202323914, 0.22878094017505646]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_7 - Captured router_logits: [0.5501971244812012, 0.21009229123592377, 0.2586073875427246, 0.1517440378665924, -0.8470485806465149, 0.2547946572303772, 0.20314908027648926, -0.44164344668388367, 0.6878135800361633, 0.15450148284435272, -0.21465088427066803, 0.26828083395957947, 0.06793149560689926, 0.3127952218055725, -0.6377190351486206, -0.33189305663108826]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.26418063044548035, 0.28525832295417786, -0.25349071621894836, 0.3334146738052368, 0.4172181487083435, 0.39237621426582336, 0.26591745018959045, -0.38341808319091797, 0.22218851745128632, 0.9822213649749756, 0.12665632367134094, 0.22673974931240082, 0.5505080223083496, -0.4534812569618225, -0.5207706093788147, 0.4828503727912903]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.48631325364112854, 0.3499610424041748, 0.4399189054965973, 1.3073515892028809, 0.4258628487586975, 0.2582634687423706, 0.7018904089927673, 0.6693500876426697, 1.2259894609451294, 0.18005038797855377, 0.2631581425666809, 0.7071425914764404, 0.5217190980911255, -0.004882496315985918, 0.8127732276916504, 0.668505072593689]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0771788358688354, 0.931493878364563, 0.5340101718902588, 0.6801631450653076, 1.015347957611084, 0.7805631756782532, 0.7481809854507446, 0.7568916082382202, 0.13812176883220673, 0.4757152795791626, 0.7712819576263428, 1.0672056674957275, 0.22117404639720917, 0.19805561006069183, 1.3780614137649536, 0.5660577416419983]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [1.0223547220230103, 1.4403133392333984, 0.9633055329322815, 1.4410423040390015, 1.4186062812805176, 0.47425833344459534, 1.0506396293640137, 0.980013370513916, 1.1295791864395142, 1.872591495513916, 0.9702856540679932, 1.102487325668335, 0.46382543444633484, 0.6343320608139038, 0.2292180061340332, 0.8717363476753235]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.21191225945949554, 0.9142851233482361, 1.4724639654159546, 1.2706241607666016, 0.5961471199989319, 0.6105273962020874, 1.357438325881958, 1.6982725858688354, 0.5859931707382202, 0.1797453761100769, 1.0577563047409058, 0.9336038827896118, 0.9495527148246765, 0.8897863030433655, 0.818359375, 1.0572680234909058]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.1920760869979858, 1.241944670677185, 1.4828468561172485, 0.8970743417739868, 1.3583756685256958, 0.03696121275424957, 1.311164140701294, 2.3636252880096436, 1.4476944208145142, 1.170964241027832, 1.9850833415985107, 1.435415267944336, 0.6066746115684509, 1.1467044353485107, 1.1852223873138428, 0.9538865089416504]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.9506374001502991, 0.8721967935562134, 1.1590936183929443, 1.0150178670883179, 1.1694715023040771, 1.9843952655792236, 0.609513521194458, 2.0598978996276855, 0.8676884174346924, 0.9801740050315857, -0.24274155497550964, 1.003955602645874, 1.0620534420013428, 1.2134623527526855, 1.1932379007339478, 0.6844944357872009]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.58673095703125, 1.2508095502853394, 1.1184120178222656, 1.055942416191101, 1.0261255502700806, 1.1306670904159546, 2.1259424686431885, 1.1764694452285767, 1.1454217433929443, 1.092758297920227, 0.9250197410583496, -0.10809294879436493, 1.3616468906402588, 1.0119414329528809, 1.104396104812622, 1.822085976600647]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6494697332382202, 0.27143147587776184, 0.7645080089569092, 0.8421004414558411, 0.7166805267333984, 0.5513052940368652, 0.7585866451263428, 0.7454308867454529, -0.6743091344833374, 2.263252019882202, -0.6404238939285278, 0.8472138047218323, 0.8375920653343201, 0.8423168063163757, 0.824284553527832, 0.6044700741767883]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.4965987801551819, 1.0517208576202393, 1.6101036071777344, 1.5428529977798462, 1.691031813621521, 1.5527647733688354, 1.6666582822799683, 1.595981478691101, 2.1353020668029785, 1.4620026350021362, 2.794243812561035, 1.9066458940505981, 1.6345800161361694, 1.4519283771514893, 0.7225746512413025, 1.4055365324020386]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6378116607666016, 1.4384815692901611, 1.5507711172103882, 2.026048421859741, 1.138336181640625, 1.3722574710845947, 2.3562581539154053, 1.9425194263458252, 2.8176915645599365, 1.6459075212478638, 1.423974871635437, 1.2210649251937866, 1.4139182567596436, 0.9868537187576294, 1.634310245513916, 1.6933492422103882]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.7648862600326538, 1.5963002443313599, 2.026503801345825, 1.5350347757339478, 1.9151149988174438, 1.4053999185562134, 1.7612937688827515, 1.5883207321166992, 2.596806287765503, 1.6163171529769897, 1.9580634832382202, 1.7270886898040771, 1.8225287199020386, 1.8137750625610352, 1.8109819889068604, 1.766910195350647]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.4592399597167969, 1.1867713928222656, 1.0653740167617798, 1.110090970993042, 0.22515647113323212, 1.5403831005096436, 1.335188627243042, 2.148350477218628, 0.8189305067062378, 1.4263883829116821, 1.2791147232055664, 1.2626903057098389, 1.2141555547714233, 1.3137750625610352, 1.1189230680465698, 0.6224747896194458]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.2761495113372803, 2.4235751628875732, 2.4262871742248535, 2.1818935871124268, 2.2063026428222656, 2.266637086868286, 2.2288496494293213, 2.441831350326538, 2.1968305110931396, 2.415034055709839, 2.536228895187378, 2.8907463550567627, 1.943490982055664, 2.6544690132141113, 2.3859293460845947, 2.336463689804077]
Running loglikelihood requests:  12%|██████                                            | 61/504 [03:37<24:48,  3.36s/it]Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.5893175601959229, 1.5479881763458252, 2.3338730335235596, 1.2615264654159546, 1.574593186378479, 1.7377347946166992, 1.4449633359909058, 1.5830028057098389, 1.6475470066070557, 1.7559504508972168, 1.7476117610931396, 2.2552218437194824, 1.6296956539154053, 1.7326951026916504, 1.4306893348693848, 1.7764936685562134]
Layer: gate_22 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5116984844207764, 1.9455958604812622, 1.3539254665374756, 1.6231176853179932, 1.9053999185562134, 1.5976359844207764, 1.5925153493881226, 1.5097757577896118, 1.6098202466964722, 1.6759027242660522, 1.61702561378479, 1.650522232055664, 1.3754655122756958, 0.8394177556037903, 1.457618236541748, 1.6464135646820068]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.5861804485321045, 2.529064178466797, 2.5109293460845947, 2.1967291831970215, 2.6757609844207764, 2.3681185245513916, 2.549546718597412, 2.3544366359710693, 2.240042209625244, 2.3343992233276367, 3.103060245513916, 2.2776472568511963, 2.481905698776245, 2.5961382389068604, 2.3772263526916504, 2.401594877243042]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.8033314943313599, 1.7427542209625244, 1.6533759832382202, 1.934261679649353, 1.409488320350647, 1.809423565864563, 1.8083913326263428, 2.1012792587280273, 1.8193409442901611, 1.6738989353179932, 1.7065050601959229, 1.627408504486084, 1.7574279308319092, 1.6021089553833008, 2.3554282188415527, 2.08642315864563]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_26 - Captured router_logits: [1.818814754486084, 2.063490390777588, 1.8647183179855347, 1.8462191820144653, 1.855974793434143, 2.00398850440979, 1.8941998481750488, 1.8785823583602905, 1.899658203125, 1.9450898170471191, 2.610933542251587, 1.7793374061584473, 1.8851197957992554, 1.8716503381729126, 2.0366134643554688, 1.9195411205291748]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6763479709625244, 1.7653011083602905, 1.6933289766311646, 1.7114636898040771, 1.5917463302612305, 1.6334601640701294, 1.8968385457992554, 1.7606865167617798, 1.6329339742660522, 1.7282980680465698, 1.340835452079773, 1.6554404497146606, 1.7751072645187378, 2.12157940864563, 1.6863058805465698, 1.6701951026916504]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.150906801223755, 2.273073196411133, 2.4041450023651123, 2.1136252880096436, 2.2692277431488037, 2.1813673973083496, 2.8167502880096436, 2.273498296737671, 2.162767171859741, 2.151230573654175, 2.093061923980713, 1.9443814754486084, 1.984132170677185, 2.0132367610931396, 2.4027891159057617, 2.0837314128875732]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.014329433441162, 5.7256317138671875, 5.0717291831970215, 4.938228607177734, 5.015139102935791, 4.945029258728027, 4.9907708168029785, 5.161431312561035, 5.201344013214111, 5.0472798347473145, 4.976603031158447, 4.96777868270874, 5.0004048347473145, 5.213406562805176, 5.282302379608154, 5.017244338989258]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.6538212299346924, 3.53922438621521, 3.4630019664764404, 3.3761537075042725, 3.540195941925049, 3.5413293838500977, 3.260807991027832, 3.7233645915985107, 3.775299549102783, 3.5371599197387695, 3.374635696411133, 2.954838752746582, 3.2694501876831055, 3.565819263458252, 3.6033031940460205, 3.6398558616638184]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.5480287075042725, 2.6403820514678955, 2.681832790374756, 2.5394673347473145, 2.5688552856445312, 2.627145290374756, 2.5125486850738525, 2.263742685317993, 2.5905520915985107, 2.304141044616699, 3.1622815132141113, 1.7396619319915771, 2.5850470066070557, 2.4678595066070557, 3.0382933616638184, 2.516596555709839]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.12877975404262543, 0.1426902711391449, 0.14390280842781067, -0.26461586356163025, -0.19901415705680847, -0.20134985446929932, 0.1504376232624054, -0.2221541851758957, 0.12165623158216476, 0.1289457529783249, 0.12952569127082825, 0.07984571903944016, 0.11176847666501999, 0.12795962393283844, -1.0972338914871216, 0.1516190469264984]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.10139383375644684, 0.06977544724941254, 0.05046601966023445, 0.05994456261396408, 0.09175901859998703, 0.04463222622871399, 0.07227472960948944, 0.06237107887864113, 0.008260388858616352, 0.07872156798839569, -0.1722462922334671, 0.044270362704992294, 0.028171826153993607, -0.02843102067708969, 0.027696752920746803, 0.0032328327652066946]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08226685971021652, 0.07347219437360764, 0.13377651572227478, 0.060852788388729095, 0.10323778539896011, 0.1214291974902153, 0.06478914618492126, -0.07148037105798721, 0.08511467278003693, 0.03651606664061546, 0.0563887320458889, 0.09048232436180115, -0.1478043496608734, 0.02203594706952572, -0.022058958187699318, 0.10144604742527008]
Layer: gate_2 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13201560080051422, 0.18758006393909454, 0.15503224730491638, 0.16425269842147827, 0.152319997549057, 0.14929383993148804, 0.03195206820964813, 0.19791962206363678, 0.18719466030597687, -0.4572025537490845, -0.0516870953142643, 0.15038971602916718, 0.10608205199241638, -0.2264292687177658, -0.11995413899421692, -0.062310412526130676]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.1376931369304657, 0.1508866548538208, 0.11318431049585342, 0.03570954501628876, 0.04018102586269379, -0.13427799940109253, 0.13744089007377625, 0.09761084616184235, -0.03481481224298477, 0.05110061913728714, -0.1424240618944168, 0.17409466207027435, -0.22881193459033966, -0.18791240453720093, 0.12177596241235733, -0.04399324208498001]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.1443551778793335, 0.1860453337430954, 0.11648846417665482, -0.019944796338677406, -0.014946886338293552, 0.10588401556015015, 0.0977829173207283, 0.06354252249002457, 0.06214691326022148, -0.10755658149719238, -0.2724645435810089, 0.11124581843614578, -0.16049891710281372, 0.14254170656204224, 0.2265843152999878, 0.05658988654613495]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.09559303522109985, 0.1263687014579773, -0.019647987559437752, 0.3443167209625244, 0.17472848296165466, 0.3374781310558319, 0.07852041721343994, 0.07074663788080215, 0.25052618980407715, 0.21005171537399292, -0.4519384205341339, 0.28507623076438904, 0.22568799555301666, -0.07125821709632874, 0.006537980865687132, 0.18067677319049835]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.17884105443954468, 0.28831928968429565, 0.4373326599597931, 0.18600603938102722, -0.7413691282272339, 0.29907816648483276, 0.24302221834659576, -0.09590378403663635, 0.21592138707637787, 0.03644602745771408, 0.026085473597049713, 0.3382394313812256, 0.5620224475860596, 0.1491679549217224, -0.37235382199287415, -0.38187530636787415]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.40117621421813965, 0.45011311769485474, 0.010148243047297001, 0.38939183950424194, 0.5202085375785828, 0.4387981593608856, 0.4784308671951294, 0.07355605810880661, 0.24663569033145905, 0.15839311480522156, 0.22292926907539368, -0.24139469861984253, 0.5265561938285828, -0.1993899643421173, -0.08849015086889267, 0.13638350367546082]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8760671019554138, 0.5118992328643799, 0.563865065574646, 0.5310289263725281, 0.5760337114334106, 0.3537760078907013, 0.801123857498169, 0.7397854924201965, 0.5899088978767395, 0.09355778992176056, 0.6155796051025391, 0.9541540741920471, 0.6840896010398865, 0.4485391676425934, 0.9667338728904724, 0.8138861060142517]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.9454687237739563, 1.0098522901535034, 0.7149776220321655, 1.0706695318222046, 1.0946398973464966, 0.14941783249378204, 0.8286566138267517, 0.8633180260658264, 0.6768543720245361, 0.7782231569290161, 1.0254825353622437, 0.8631709814071655, 0.8861234784126282, 0.3979564309120178, 0.6359128355979919, 0.8458461761474609]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.3499643802642822, 1.1900207996368408, 1.1751407384872437, 0.9268345832824707, 1.2264548540115356, 0.6029991507530212, 1.3759450912475586, 1.0425959825515747, 1.401934266090393, 1.1469752788543701, 1.161824107170105, 1.3945995569229126, 1.432152509689331, 1.1013749837875366, 0.4254583418369293, 1.175245761871338]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.6241337060928345, 1.2931840419769287, 1.2611569166183472, 1.2099334001541138, 0.7204996943473816, 0.9065545201301575, 0.7895783185958862, 0.9210516810417175, 0.9090482592582703, 0.5739877223968506, 1.121293306350708, 1.2330623865127563, 1.343461275100708, 1.0785765647888184, 0.864037275314331, 1.2027050256729126]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.888503909111023, 1.6389553546905518, 1.8556630611419678, 1.2512903213500977, 1.8137390613555908, 0.7841199636459351, 1.7835968732833862, 1.5417978763580322, 1.564894199371338, 2.2290406227111816, 1.5461084842681885, 2.047736167907715, 0.9707838296890259, 1.7939767837524414, 2.005796432495117, 1.4556083679199219]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.3843430280685425, 1.120873212814331, 1.2360879182815552, 1.3983850479125977, 1.3806545734405518, 1.7977843284606934, 0.7002851963043213, 0.7111979126930237, 1.3847105503082275, 1.3723118305206299, 0.5913102626800537, 1.231369137763977, 1.3942201137542725, 1.4018397331237793, 1.4796496629714966, 1.1197916269302368]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.9182537198066711, 1.2793599367141724, 1.5120967626571655, 1.352927565574646, 1.3597136735916138, 1.5017746686935425, 1.0699095726013184, 1.3436659574508667, 1.0285828113555908, 1.3368037939071655, 1.2753643989562988, 0.04153934493660927, 1.3192518949508667, 1.6463059186935425, 1.2027889490127563, 0.9651364684104919]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_16 - Captured router_logits: [1.4665343761444092, 0.6995362043380737, 1.2111449241638184, 0.994361162185669, 1.0274592638015747, 1.1617995500564575, -0.023021185770630836, 1.276755690574646, -0.4349201023578644, 1.644608736038208, -0.14619101583957672, 0.8976922631263733, 1.1916228532791138, 1.0833884477615356, 1.1016963720321655, 0.4728003144264221]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.6868325471878052, 0.9814825654029846, 1.9856350421905518, 1.955610990524292, 1.7927272319793701, 1.7966439723968506, 1.9502688646316528, 1.8735928535461426, 1.8471941947937012, 1.500367522239685, 1.8530348539352417, 1.7407541275024414, 1.5714442729949951, 1.5915553569793701, 1.4142354726791382, 1.7022613286972046]
Layer: gate_17 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.9254242181777954, 1.8069976568222046, 1.9011518955230713, 1.9612630605697632, 1.5412505865097046, 1.7018518447875977, 1.6624454259872437, 2.135387897491455, 2.2015604972839355, 1.9225122928619385, 1.711669921875, 1.515088438987732, 1.9093161821365356, 1.4932376146316528, 2.2092363834381104, 1.8857002258300781]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_19 - Captured router_logits: [2.323551893234253, 2.0231854915618896, 1.9214234352111816, 2.099252462387085, 2.1213667392730713, 1.785623550415039, 2.0673303604125977, 1.9301915168762207, 1.9444199800491333, 2.026944637298584, 2.107001781463623, 2.1373698711395264, 2.2354986667633057, 2.2561533451080322, 2.105783700942993, 2.0272176265716553]
Layer: gate_19 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0947291851043701, 1.277732253074646, 1.5600533485412598, 1.2090545892715454, 0.0684148296713829, 1.30078125, 1.5129315853118896, 1.6437922716140747, 1.0945494174957275, 1.3698179721832275, 1.502318024635315, 1.4297137260437012, 1.4433056116104126, 1.4727717638015747, 1.2010406255722046, 0.6753162741661072]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.6631593704223633, 2.8883988857269287, 2.616032361984253, 2.6576361656188965, 2.5536375045776367, 2.649277448654175, 2.773080587387085, 2.6053426265716553, 2.6053848266601562, 2.750345230102539, 2.8493363857269287, 2.7787928581237793, 2.354337215423584, 2.887758255004883, 2.820380687713623, 2.689831256866455]
Layer: gate_21 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.8160518407821655, 1.6985677480697632, 1.8528382778167725, 1.3718445301055908, 1.8896090984344482, 1.8524445295333862, 1.6501491069793701, 1.9231350421905518, 1.839087724685669, 1.895387053489685, 1.7743825912475586, 1.7826255559921265, 1.963646650314331, 1.8350974321365356, 1.7046475410461426, 1.9976269006729126]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.040438175201416, 1.9180107116699219, 1.5700329542160034, 1.7880123853683472, 1.9469926357269287, 1.7643649578094482, 1.8457976579666138, 1.9221479892730713, 1.851095199584961, 1.893334150314331, 1.9217699766159058, 1.7647114992141724, 1.705687165260315, 1.1424330472946167, 1.7894247770309448, 1.8348664045333862]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.790700674057007, 2.641759157180786, 2.4810988903045654, 2.5844674110412598, 2.5500881671905518, 2.5448169708251953, 2.7107274532318115, 2.543010711669922, 2.5319325923919678, 2.6684937477111816, 2.6457912921905518, 2.8142430782318115, 2.509702682495117, 2.7196741104125977, 2.7349209785461426, 2.5974252223968506]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Running loglikelihood requests:  13%|██████▍                                           | 65/504 [03:51<24:40,  3.37s/it]Layer: gate_25 - Captured router_logits: [1.8934391736984253, 1.7175949811935425, 1.7375881671905518, 1.9030787944793701, 1.4654003381729126, 1.9856350421905518, 1.8659169673919678, 1.8829175233840942, 1.849409818649292, 1.8418599367141724, 1.7958459854125977, 1.7179099321365356, 1.7551243305206299, 1.8155032396316528, 1.5777469873428345, 2.051600217819214]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.7999306917190552, 1.718748688697815, 1.7568883895874023, 1.811890959739685, 1.7799952030181885, 1.7554603815078735, 1.7398498058319092, 1.8400064706802368, 1.819410800933838, 1.822244644165039, 1.9491137266159058, 1.7797011137008667, 1.7185084819793701, 1.679220199584961, 1.8368405103683472, 1.7332993745803833]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.4942915439605713, 1.5735282897949219, 1.5562028884887695, 1.5842455625534058, 1.5793640613555908, 1.7021681070327759, 1.6250840425491333, 1.6174591779708862, 1.5877082347869873, 1.6589329242706299, 1.3678700923919678, 1.590843915939331, 1.5053623914718628, 1.5161755084991455, 1.660597324371338, 1.5871602296829224]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.3703901767730713, 2.381741523742676, 2.541842460632324, 2.4352529048919678, 2.455939292907715, 2.381972551345825, 2.3659799098968506, 2.4241445064544678, 2.5511276721954346, 2.4186198711395264, 2.4397051334381104, 2.3084888458251953, 2.4348747730255127, 2.262873888015747, 2.556420087814331, 2.4759535789489746]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_29 - Captured router_logits: [5.640288829803467, 5.630922317504883, 5.4579973220825195, 5.42971134185791, 5.465977668762207, 5.416162490844727, 5.578419208526611, 5.572958469390869, 5.70241117477417, 5.562646865844727, 5.657132148742676, 5.411416530609131, 5.518570423126221, 5.565188407897949, 5.592637062072754, 5.545300006866455]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.9688949584960938, 3.962628126144409, 3.946950674057007, 3.8883161544799805, 4.010185718536377, 3.904690742492676, 3.9983041286468506, 3.953744649887085, 3.8903965950012207, 4.045104503631592, 3.9642136096954346, 3.7073776721954346, 3.913800001144409, 3.8604564666748047, 3.9876091480255127, 3.97587513923645]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.8427419662475586, 2.877016067504883, 2.790952682495117, 2.5939600467681885, 2.7460098266601562, 2.914839506149292, 2.806703567504883, 2.686386823654175, 3.1096270084381104, 2.871072769165039, 2.728557586669922, 2.374542236328125, 2.8715977668762207, 2.839003801345825, 2.840599775314331, 2.9057459831237793]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09173139929771423, 0.10466615110635757, 0.10746174305677414, -0.26360154151916504, -0.21705442667007446, -0.12236945331096649, 0.1230730339884758, -0.15533682703971863, 0.05756548047065735, 0.09312451630830765, 0.10691212862730026, 0.08291896432638168, 0.09299162775278091, 0.1027834564447403, -1.1301262378692627, 0.11924073100090027]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08122387528419495, 0.05209910124540329, 0.033481452614068985, 0.045732006430625916, 0.07207966595888138, 0.016415417194366455, 0.0415838249027729, 0.07568436861038208, 0.020632252097129822, 0.07273789495229721, -0.1786523312330246, 0.049710337072610855, -0.005341959651559591, -0.02355441451072693, 0.006139534991234541, 0.02542208507657051]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07921705394983292, 0.05775976553559303, 0.09423542767763138, 0.05323856323957443, 0.1097034439444542, 0.1100444570183754, 0.07865449041128159, -0.0880865603685379, 0.059310659766197205, 0.11539484560489655, 0.014655668288469315, 0.0938195064663887, -0.1739937961101532, 0.03318832814693451, -0.03260635584592819, 0.09195323288440704]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1412660926580429, 0.16032543778419495, 0.10437766462564468, 0.15006113052368164, 0.13480795919895172, 0.11017101258039474, 0.038500189781188965, 0.16784274578094482, 0.18042512238025665, -0.4308546185493469, 0.019348936155438423, 0.07254357635974884, 0.21227356791496277, -0.23539310693740845, -0.005366566590964794, -0.048420581966638565]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.05931299924850464, 0.08826404809951782, 0.08846504986286163, 0.16821381449699402, 0.056921981275081635, -0.019415803253650665, 0.021785086020827293, 0.06471332162618637, -0.10246013104915619, -0.02270033024251461, -0.15824002027511597, 0.1247263252735138, -0.0025161071680486202, -0.15611515939235687, 0.12238852679729462, -0.04668720066547394]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.05869234353303909, 0.13516955077648163, 0.05511843413114548, 0.16156241297721863, -0.07374840974807739, -0.04761580750346184, 0.07754243910312653, -0.041039541363716125, 0.17396019399166107, -0.08708526194095612, -0.17613472044467926, 0.08493570238351822, -0.16254785656929016, 0.1278805136680603, 0.09151677042245865, 0.07652974128723145]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.09394727647304535, -0.02439025230705738, 0.2216753214597702, 0.2944040894508362, 0.20693130791187286, 0.12001875787973404, -0.08932461589574814, -0.09019763767719269, 0.4501766562461853, 0.1616421341896057, -0.5069271326065063, 0.26438644528388977, 0.19093817472457886, -0.20507560670375824, 0.25098493695259094, 0.16700710356235504]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.42552855610847473, 0.18344442546367645, 0.25356417894363403, 0.14699779450893402, -0.9035443067550659, 0.3247261345386505, 0.17534109950065613, -0.2942323684692383, 0.44014212489128113, 0.24112215638160706, -0.19223575294017792, 0.26757410168647766, 0.1849280595779419, 0.1855996996164322, -0.5327457189559937, -0.2386479675769806]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2635599970817566, 0.3166530728340149, -0.29600977897644043, 0.2837204039096832, 0.508497953414917, 0.25072336196899414, 0.2547164857387543, -0.2622970640659332, 0.18912380933761597, 0.8793542981147766, 0.0007041679346002638, 0.2098606675863266, 0.37885963916778564, -0.4449431002140045, -0.40351632237434387, 0.34322163462638855]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3628949224948883, 0.21682806313037872, 0.44312864542007446, 0.9950326681137085, 0.3813895881175995, 0.30655544996261597, 0.6060127019882202, 0.5113807320594788, 0.6815168857574463, 0.1544298380613327, 0.24737012386322021, 0.6660048961639404, 0.6653817892074585, 0.48530930280685425, 0.6943654417991638, 0.636911928653717]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9265706539154053, 0.7803270816802979, 0.5609976053237915, 0.7557252049446106, 0.993455171585083, 0.6924788951873779, 0.6249195337295532, 0.6654093265533447, 0.25928807258605957, 0.7914674282073975, 0.6823810935020447, 0.9258456230163574, 0.2574516534805298, 0.25995516777038574, 0.8818449974060059, 0.35763517022132874]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9499163031578064, 1.0494505167007446, 0.8855704665184021, 1.2138168811798096, 1.2498000860214233, 0.5550068616867065, 0.8528878092765808, 1.1755075454711914, 1.0218063592910767, 1.4211130142211914, 0.8365421295166016, 0.9318284392356873, 0.5102354884147644, 0.4474555253982544, -0.03664087876677513, 0.7505882382392883]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.18242770433425903, 0.8580349087715149, 1.2509443759918213, 1.100897192955017, 0.34514039754867554, 0.6953795552253723, 1.0941293239593506, 1.2592263221740723, 0.7234775424003601, 0.3564355969429016, 0.7651018500328064, 0.8566277623176575, 0.9954760074615479, 0.8151077628135681, 0.5658549070358276, 0.8881460428237915]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7091225385665894, 0.9374302625656128, 0.8670666217803955, 0.4055202603340149, 1.1368260383605957, -0.09861168265342712, 1.0460700988769531, 2.352764368057251, 0.9311731457710266, 0.8885015249252319, 1.240110993385315, 1.1338776350021362, -0.1026911661028862, 0.9393970966339111, 1.0259432792663574, 0.7118121385574341]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.736612856388092, 0.7849067449569702, 0.9684602618217468, 0.8997896909713745, 0.9843428134918213, 1.093587040901184, 0.44199514389038086, 1.48785400390625, 0.836745023727417, 0.9505561590194702, -0.01550191268324852, 0.7919385433197021, 1.0490158796310425, 1.0720349550247192, 1.1102067232131958, 0.65579754114151]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4401717185974121, 1.1496824026107788, 0.9643608331680298, 0.9110469818115234, 0.6659250855445862, 0.8735136985778809, 1.4012887477874756, 1.0333962440490723, 0.892578125, 1.0079948902130127, 0.6847822666168213, -0.34183841943740845, 0.9690384268760681, 0.9030037522315979, 1.0499709844589233, 1.5199098587036133]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.6882457733154297, 0.3002973198890686, 0.8320460319519043, 0.8317951560020447, 0.6525060534477234, 0.46550825238227844, 0.3822784423828125, 0.8934586048126221, -0.7921075224876404, 2.5939056873321533, -0.7392106056213379, 0.6651902794837952, 0.8738517165184021, 0.8943568468093872, 0.8297293782234192, 0.4431821405887604]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.16504652798175812, 0.5937879085540771, 1.3358570337295532, 1.0859328508377075, 1.3996824026107788, 1.344537377357483, 1.3563674688339233, 1.3024312257766724, 1.8129433393478394, 1.3467977046966553, 2.011605978012085, 1.3332586288452148, 1.3711473941802979, 1.1851813793182373, 0.6175684928894043, 1.1976593732833862]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3754292726516724, 1.2757340669631958, 1.3747209310531616, 1.641826868057251, 0.8970464468002319, 1.228880524635315, 1.7898244857788086, 1.636031985282898, 2.078951358795166, 1.3202695846557617, 1.087234616279602, 0.8134143352508545, 1.1217986345291138, 0.9922233819961548, 1.3633027076721191, 1.3894875049591064]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.2014187574386597, 1.1384035348892212, 1.1646312475204468, 1.1520432233810425, 1.3793569803237915, 0.8423206806182861, 0.9364885687828064, 1.1052541732788086, 1.8250236511230469, 1.3220617771148682, 1.3605232238769531, 1.298313021659851, 1.6812328100204468, 1.2755300998687744, 1.2390002012252808, 1.2616221904754639]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.9290352463722229, 1.091110110282898, 1.0059157609939575, 0.8974958062171936, -0.004846342373639345, 1.256524682044983, 1.108012080192566, 1.8553487062454224, 0.8187229037284851, 1.2256503105163574, 1.1650712490081787, 1.1702492237091064, 1.0792732238769531, 1.3103430271148682, 0.9754992723464966, 0.38549309968948364]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.8439860343933105, 2.0202395915985107, 1.8770389556884766, 1.7262834310531616, 1.716260313987732, 1.9038246870040894, 1.6777129173278809, 1.9775497913360596, 1.7629421949386597, 1.7800265550613403, 2.134143114089966, 2.5057520866394043, 1.5499308109283447, 2.1809751987457275, 1.9962224960327148, 1.9070656299591064]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3381911516189575, 1.2978193759918213, 1.8558014631271362, 1.0512346029281616, 1.4393994808197021, 1.5245535373687744, 1.2737594842910767, 1.3228451013565063, 1.4952781200408936, 1.464403748512268, 1.5621994733810425, 2.1805031299591064, 1.451171875, 1.5040457248687744, 1.1329761743545532, 1.5932992696762085]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.47778582572937, 1.7994719743728638, 1.1477187871932983, 1.4725275039672852, 1.5778030157089233, 1.464156985282898, 1.417281985282898, 1.4071729183197021, 1.5222033262252808, 1.4632984399795532, 1.5227292776107788, 1.5643243789672852, 1.3527859449386597, 0.7836270332336426, 1.4045974016189575, 1.4947844743728638]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.352635622024536, 2.3554043769836426, 2.228215217590332, 2.1065847873687744, 2.694840431213379, 2.265152931213379, 2.5484204292297363, 2.1656508445739746, 2.166165828704834, 2.166638135910034, 2.7618045806884766, 2.131159782409668, 2.241264581680298, 2.414362907409668, 2.2764852046966553, 2.310267925262451]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6263736486434937, 1.7149295806884766, 1.4784727096557617, 1.6364182233810425, 1.3503471612930298, 1.5118045806884766, 1.5728236436843872, 1.8924493789672852, 1.556125521659851, 1.4840744733810425, 1.5867102146148682, 1.3971282243728638, 1.5051511526107788, 1.4048978090286255, 1.7287087440490723, 1.8813315629959106]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8530434370040894, 1.721218228340149, 1.6474931240081787, 1.7417796850204468, 1.7059688568115234, 1.7580640316009521, 1.6704442501068115, 1.7384958267211914, 1.7417421340942383, 1.8283073902130127, 2.192554473876953, 1.6217215061187744, 1.6515281200408936, 1.7713556289672852, 1.8211965560913086, 1.7827812433242798]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6138821840286255, 1.6702250242233276, 1.6057960987091064, 1.7251620292663574, 1.6077438592910767, 1.5845595598220825, 1.7343535423278809, 1.6815627813339233, 1.589036226272583, 1.6763768196105957, 1.4009486436843872, 1.6907570362091064, 1.647093415260315, 1.8606520891189575, 1.5775588750839233, 1.6064372062683105]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9955357313156128, 2.07348895072937, 2.4088685512542725, 1.9542839527130127, 2.104996681213379, 1.993389368057251, 2.5020389556884766, 2.07348895072937, 1.945977807044983, 1.963255524635315, 1.903030514717102, 1.7788032293319702, 1.941481351852417, 1.781614899635315, 2.2586281299591064, 1.996759057044983]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  14%|██████▊                                           | 69/504 [04:04<24:12,  3.34s/it]Layer: gate_29 - Captured router_logits: [5.298162937164307, 5.627962112426758, 5.312328338623047, 5.284641265869141, 5.192071437835693, 5.143715858459473, 5.389594554901123, 5.623948097229004, 5.461366653442383, 5.25390625, 5.299557685852051, 5.235877513885498, 5.292120933532715, 5.345510005950928, 5.466346263885498, 5.289083957672119]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.830822706222534, 3.669116973876953, 3.652477979660034, 3.4570372104644775, 3.6707723140716553, 3.708137273788452, 3.429821729660034, 3.6588470935821533, 3.722313404083252, 3.711575984954834, 3.580582618713379, 3.2792699337005615, 3.6034512519836426, 3.694761276245117, 3.7500858306884766, 3.74267840385437]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.9459564685821533, 3.022686243057251, 2.9468579292297363, 2.884787082672119, 2.9049623012542725, 2.9498627185821533, 3.037731885910034, 2.7814645767211914, 2.8867616653442383, 2.8581838607788086, 3.1689131259918213, 2.3626790046691895, 2.9599502086639404, 2.900862693786621, 3.1875429153442383, 2.9594995975494385]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.08656641840934753, 0.10252230614423752, 0.1053924560546875, -0.2759140133857727, -0.25167301297187805, -0.11182980984449387, 0.12294429540634155, -0.13979846239089966, 0.06084711104631424, 0.0903206318616867, 0.09867886453866959, 0.07983200252056122, 0.08687644451856613, 0.1011894941329956, -1.1390550136566162, 0.1149996742606163]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07951214164495468, 0.04838975518941879, 0.03304421901702881, 0.035950444638729095, 0.0722208172082901, 0.014073939993977547, 0.04319886490702629, 0.07234663516283035, 0.022412249818444252, 0.07004258036613464, -0.19474951922893524, 0.06101386621594429, 0.0018371405312791467, -0.03933486342430115, 0.021496789529919624, 0.03881469741463661]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09329783916473389, 0.05336761474609375, 0.08580299466848373, 0.060409512370824814, 0.09968831390142441, 0.10496962070465088, 0.0696479082107544, -0.09580182284116745, 0.07227036356925964, 0.11586181819438934, 0.019606545567512512, 0.08511991798877716, -0.19354777038097382, 0.033185042440891266, -0.03034139610826969, 0.09780603647232056]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13483062386512756, 0.16632327437400818, 0.11311666667461395, 0.11780623346567154, 0.13672775030136108, 0.11382889002561569, 0.03186282142996788, 0.1657271832227707, 0.19295576214790344, -0.451324462890625, 0.012540431693196297, 0.08674069494009018, 0.2142379879951477, -0.24268215894699097, 0.018552439287304878, -0.048113785684108734]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06669691950082779, 0.09851665049791336, 0.08809364587068558, 0.19223830103874207, 0.05131758004426956, -0.04931768402457237, 0.03968572989106178, 0.03815310448408127, -0.09659960120916367, 0.011179091408848763, -0.1905796229839325, 0.14626680314540863, 0.010163147002458572, -0.2119949460029602, 0.12845253944396973, -0.031475793570280075]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.08482129126787186, 0.12571540474891663, 0.06412400305271149, 0.17474277317523956, -0.03938443213701248, -0.04941073805093765, 0.09241582453250885, -0.01483057253062725, 0.17612269520759583, -0.09953508526086807, -0.1658533364534378, 0.09737467020750046, -0.18663042783737183, 0.14809174835681915, 0.11521121859550476, 0.060884177684783936]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08681757003068924, 0.013658380135893822, 0.19564589858055115, 0.32062509655952454, 0.22003509104251862, 0.15701566636562347, -0.08976313471794128, -0.08128074556589127, 0.44206351041793823, 0.1632392257452011, -0.5206630229949951, 0.31418657302856445, 0.20948323607444763, -0.1399552822113037, 0.2534520924091339, 0.21261420845985413]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.4238379895687103, 0.2146923989057541, 0.28338754177093506, 0.1440180540084839, -0.947265625, 0.31854549050331116, 0.18582020699977875, -0.29479944705963135, 0.4798882007598877, 0.18747220933437347, -0.13815748691558838, 0.2951692044734955, 0.2531374990940094, 0.1815120279788971, -0.5024603009223938, -0.07316704094409943]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21029402315616608, 0.3677244782447815, -0.19270916283130646, 0.32314974069595337, 0.5340664386749268, 0.29045185446739197, 0.28082460165023804, -0.21891158819198608, 0.18805943429470062, 0.7826566100120544, 0.012343918904662132, 0.3387671411037445, 0.40225061774253845, -0.41274935007095337, -0.24326135218143463, 0.3502137362957001]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.43967556953430176, 0.20288297533988953, 0.422088086605072, 0.989355206489563, 0.40829625725746155, 0.47571611404418945, 0.6688662767410278, 0.5545802712440491, 0.7035850286483765, 0.09238182753324509, 0.3801093101501465, 0.6729418635368347, 0.7175596356391907, 0.5187254548072815, 0.75550377368927, 0.6695436835289001]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9658277034759521, 0.7973491549491882, 0.6047815084457397, 0.8148539066314697, 1.1319767236709595, 0.6671590805053711, 0.6575398445129395, 0.7037798166275024, 0.3527278006076813, 0.806675910949707, 0.7552158832550049, 0.966827929019928, 0.2910889983177185, 0.29243549704551697, 0.9475732445716858, 0.4491199553012848]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0086365938186646, 1.106360673904419, 0.9760544896125793, 1.2035808563232422, 1.3588330745697021, 0.8003212213516235, 0.935693621635437, 1.2661584615707397, 1.139708399772644, 1.4669944047927856, 0.9250502586364746, 1.0878454446792603, 0.6765535473823547, 0.4626784026622772, -0.012471546418964863, 0.844356119632721]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.24957099556922913, 0.9496343731880188, 1.3160054683685303, 1.169571876525879, 0.3448422849178314, 0.8205079436302185, 1.0207180976867676, 1.327839970588684, 0.9505936503410339, 0.5384203791618347, 0.7416678071022034, 0.8870235681533813, 1.0453113317489624, 0.8704163432121277, 0.5760533213615417, 0.9346098303794861]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7552405595779419, 0.9420441389083862, 0.8801357746124268, 0.5205686092376709, 1.19846510887146, 0.12528719007968903, 1.0936596393585205, 2.347904682159424, 0.9883264303207397, 0.9106586575508118, 1.3548760414123535, 1.162668228149414, -0.10973980277776718, 0.932531476020813, 1.1784851551055908, 0.7011260390281677]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8026638031005859, 0.7877373695373535, 1.0077617168426514, 0.8960115909576416, 0.9911601543426514, 1.1685357093811035, 0.478515088558197, 1.56340491771698, 0.9231451153755188, 1.0367763042449951, 0.16344615817070007, 0.8374870419502258, 1.1540979146957397, 1.132755994796753, 1.157932162284851, 0.7189531922340393]
Running loglikelihood requests:  14%|███████▏                                          | 73/504 [04:17<23:44,  3.30s/it]Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.46501222252845764, 1.0777411460876465, 1.020987629890442, 0.9621003270149231, 0.65223228931427, 0.9047710299491882, 1.3752243518829346, 1.0445493459701538, 0.8862107396125793, 0.9985097646713257, 0.717169463634491, -0.3660329580307007, 0.9164651036262512, 0.9109578132629395, 0.9952808618545532, 1.6174613237380981]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7162472009658813, 0.23660507798194885, 0.8510953783988953, 0.7662205100059509, 0.6673583984375, 0.4731692373752594, 0.32359886169433594, 0.8643985986709595, -0.7907677888870239, 2.692732810974121, -0.617032527923584, 0.8112179040908813, 0.8750112652778625, 0.9134218096733093, 0.8544244766235352, 0.5048208832740784]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.045788537710905075, 0.5632098317146301, 1.3344472646713257, 1.1733276844024658, 1.3237783908843994, 1.3634957075119019, 1.3893605470657349, 1.2938634157180786, 1.817794919013977, 1.3053948879241943, 2.059000253677368, 1.3433717489242554, 1.3903414011001587, 1.1647117137908936, 0.6822628974914551, 1.2600605487823486]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3332843780517578, 1.2488371133804321, 1.3289434909820557, 1.6758489608764648, 0.9159591794013977, 1.1813075542449951, 1.7626445293426514, 1.6238709688186646, 2.129451036453247, 1.3349891901016235, 1.1054970026016235, 0.8925548195838928, 1.1330126523971558, 1.0837535858154297, 1.4033485651016235, 1.3895411491394043]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3119807243347168, 1.240708589553833, 1.209131121635437, 1.2242707014083862, 1.4460350275039673, 0.9260931015014648, 1.0211033821105957, 1.1929924488067627, 1.9660743474960327, 1.3944183588027954, 1.4617277383804321, 1.3743678331375122, 1.751749873161316, 1.3787933588027954, 1.2954411506652832, 1.3494174480438232]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.9555452466011047, 1.1077888011932373, 1.12624192237854, 0.958673894405365, 0.040537573397159576, 1.3049246072769165, 1.1927045583724976, 1.9059761762619019, 0.8782541155815125, 1.2734375, 1.258851170539856, 1.2437877655029297, 1.1348446607589722, 1.4181041717529297, 1.1436065435409546, 0.45460590720176697]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.9647083282470703, 2.1376445293426514, 1.9506638050079346, 1.8532785177230835, 1.8212834596633911, 1.9688628911972046, 1.824015498161316, 2.03000807762146, 1.8564397096633911, 1.9161850214004517, 2.2381231784820557, 2.5856666564941406, 1.7672507762908936, 2.3184382915496826, 2.1434247493743896, 1.9972904920578003]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3564058542251587, 1.2991554737091064, 1.8117097616195679, 1.079883337020874, 1.4435174465179443, 1.5162571668624878, 1.3063921928405762, 1.377009630203247, 1.4314261674880981, 1.5818054676055908, 1.5436009168624878, 2.147556781768799, 1.476359248161316, 1.4839459657669067, 1.183837890625, 1.6062816381454468]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.616058588027954, 1.8339956998825073, 1.230859637260437, 1.4521089792251587, 1.6139134168624878, 1.5150378942489624, 1.4646631479263306, 1.4623600244522095, 1.5825166702270508, 1.5221730470657349, 1.5790055990219116, 1.5490200519561768, 1.4161906242370605, 0.9093734622001648, 1.5310806035995483, 1.543262243270874]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3855671882629395, 2.345466136932373, 2.192580461502075, 2.148844003677368, 2.6472182273864746, 2.27610182762146, 2.6021947860717773, 2.2056989669799805, 2.2291817665100098, 2.2090859413146973, 2.8135838508605957, 2.2171242237091064, 2.246793746948242, 2.42973256111145, 2.3257315158843994, 2.359149217605591]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6315028667449951, 1.6911578178405762, 1.4463963508605957, 1.6467440128326416, 1.4229249954223633, 1.5221730470657349, 1.5481394529342651, 1.8656069040298462, 1.5300307273864746, 1.484939455986023, 1.5249052047729492, 1.4111272096633911, 1.4883716106414795, 1.3915281295776367, 1.702854037284851, 1.8598040342330933]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.885115623474121, 1.7443748712539673, 1.7109826803207397, 1.8167901039123535, 1.7401553392410278, 1.7928065061569214, 1.7045594453811646, 1.766189455986023, 1.7968273162841797, 1.867661714553833, 2.22993803024292, 1.6434587240219116, 1.703418493270874, 1.8419549465179443, 1.8914886713027954, 1.775696873664856]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6113563776016235, 1.666650652885437, 1.6186001300811768, 1.7262548208236694, 1.6530437469482422, 1.617689847946167, 1.765809178352356, 1.7108535766601562, 1.6258975267410278, 1.732150912284851, 1.4658372402191162, 1.7244682312011719, 1.6629278659820557, 1.8877052068710327, 1.5945290327072144, 1.6209723949432373]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.039785146713257, 2.12766432762146, 2.456794261932373, 2.0201408863067627, 2.128070831298828, 2.035404682159424, 2.514496088027954, 2.104520320892334, 2.00639009475708, 2.0307984352111816, 1.9547733068466187, 1.8606846332550049, 1.9616600275039673, 1.8337472677230835, 2.3598265647888184, 2.0562002658843994]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.408778667449951, 5.757225513458252, 5.408688545227051, 5.337743759155273, 5.265692710876465, 5.208408832550049, 5.350027084350586, 5.765444278717041, 5.55753231048584, 5.339076042175293, 5.402592182159424, 5.289784908294678, 5.437579154968262, 5.431177616119385, 5.561370849609375, 5.378657817840576]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.8430726528167725, 3.66841459274292, 3.6670796871185303, 3.5132484436035156, 3.754974603652954, 3.732616662979126, 3.4296875, 3.7477645874023438, 3.773327350616455, 3.726184368133545, 3.568765878677368, 3.3238348960876465, 3.6525468826293945, 3.7087042331695557, 3.767036199569702, 3.8206992149353027]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.02339243888855, 3.11594557762146, 2.994264841079712, 2.873645305633545, 2.9093658924102783, 3.0851924419403076, 3.030121088027954, 2.898256778717041, 2.97123384475708, 2.949873447418213, 3.314758062362671, 2.4114205837249756, 3.009280204772949, 2.9723401069641113, 3.282243490219116, 3.031475782394409]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.1015215739607811, 0.11877255886793137, 0.11455976963043213, -0.26120826601982117, -0.22315087914466858, -0.1245318278670311, 0.13229449093341827, -0.18184630572795868, 0.0697195902466774, 0.10329825431108475, 0.11431880295276642, 0.09073473513126373, 0.10030964761972427, 0.11380767822265625, -1.176806092262268, 0.12828443944454193]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08617839962244034, 0.05710705369710922, 0.03940117731690407, 0.051267243921756744, 0.07733815908432007, 0.02382386475801468, 0.046049993485212326, 0.07534525543451309, 0.019247066229581833, 0.07357863336801529, -0.1833864152431488, 0.04642239585518837, -0.00786617025732994, -0.018558083102107048, 0.005185904446989298, 0.022630592808127403]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07988809794187546, 0.06263887137174606, 0.0975460410118103, 0.05919951573014259, 0.10700742155313492, 0.11795714497566223, 0.07956913858652115, -0.09441997855901718, 0.07377762347459793, 0.10120934247970581, 0.003654744476079941, 0.08947837352752686, -0.16280193626880646, 0.03437187895178795, -0.017228208482265472, 0.10236424952745438]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13830433785915375, 0.15960551798343658, 0.1181565672159195, 0.16807211935520172, 0.14089983701705933, 0.1095358356833458, 0.055894266813993454, 0.17243154346942902, 0.18289804458618164, -0.42518889904022217, 0.0016423925990238786, 0.07993786036968231, 0.18357311189174652, -0.2070239782333374, -0.02901092730462551, -0.06181599944829941]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06020249426364899, 0.0873098224401474, 0.08674872666597366, 0.1502741575241089, 0.055599894374608994, -0.03468084707856178, 0.021504363045096397, 0.06251448392868042, -0.09831210970878601, -0.003960493486374617, -0.14276370406150818, 0.11858413368463516, -0.02105027064681053, -0.1452770084142685, 0.12249332666397095, -0.013725832104682922]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.055283915251493454, 0.14018051326274872, 0.06306003034114838, 0.16485314071178436, -0.06192947179079056, -0.03350159898400307, 0.08494647592306137, -0.02030005492269993, 0.1425010859966278, -0.08728168159723282, -0.17440496385097504, 0.08912261575460434, -0.17069107294082642, 0.1270570307970047, 0.08999382704496384, 0.0721670612692833]
Layer: gate_5 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.09060858935117722, -0.01277001854032278, 0.2404603511095047, 0.3031247556209564, 0.20056717097759247, 0.14145030081272125, -0.08893713355064392, -0.08450202643871307, 0.4531877934932709, 0.16086642444133759, -0.504634439945221, 0.26718634366989136, 0.19169466197490692, -0.19846674799919128, 0.20463639497756958, 0.17404334247112274]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.4071694016456604, 0.18772093951702118, 0.26885518431663513, 0.14048026502132416, -0.8851443529129028, 0.31991299986839294, 0.1769590824842453, -0.20445877313613892, 0.44821596145629883, 0.27137282490730286, -0.18759191036224365, 0.2451905757188797, 0.19915039837360382, 0.1702146977186203, -0.5363497734069824, -0.23135358095169067]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2404959797859192, 0.3274860680103302, -0.25009915232658386, 0.27504152059555054, 0.5247485041618347, 0.26245787739753723, 0.26172855496406555, -0.27503886818885803, 0.18832141160964966, 0.8507376313209534, -0.00988130085170269, 0.2065364420413971, 0.33565402030944824, -0.4486468434333801, -0.41689833998680115, 0.3335322439670563]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.37472444772720337, 0.2141892910003662, 0.4453142583370209, 0.9634339213371277, 0.40235787630081177, 0.3091351389884949, 0.6203616857528687, 0.49963873624801636, 0.702271580696106, 0.16781280934810638, 0.2722661793231964, 0.6656317710876465, 0.6629670262336731, 0.5002279281616211, 0.6621996760368347, 0.6365268230438232]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9222143888473511, 0.7822604179382324, 0.575903058052063, 0.7662854790687561, 1.0512751340866089, 0.7062317728996277, 0.6340106129646301, 0.663148045539856, 0.25820639729499817, 0.7944738268852234, 0.7017250657081604, 0.922735869884491, 0.22847963869571686, 0.25535547733306885, 0.8931726813316345, 0.3836687505245209]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9759754538536072, 1.0381720066070557, 0.9071362614631653, 1.2427294254302979, 1.285169005393982, 0.6434795260429382, 0.8694115877151489, 1.1812821626663208, 1.0776621103286743, 1.4152535200119019, 0.8424072265625, 0.9900735020637512, 0.5703684091567993, 0.48897507786750793, -0.004677127581089735, 0.7811907529830933]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.1901206374168396, 0.9059777855873108, 1.2755769491195679, 1.114816665649414, 0.3982905447483063, 0.7178298830986023, 1.1234173774719238, 1.2811625003814697, 0.786749005317688, 0.3965083658695221, 0.7195776700973511, 0.8741984367370605, 1.053634762763977, 0.8068156242370605, 0.5891945958137512, 0.9236023426055908]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7596117854118347, 0.92444908618927, 0.8894325494766235, 0.47779062390327454, 1.1647511720657349, -0.04431293532252312, 1.068415880203247, 2.3295700550079346, 0.9665915369987488, 0.9160664081573486, 1.2662742137908936, 1.183097004890442, -0.0733928382396698, 1.0372159481048584, 1.0546197891235352, 0.7314372062683105]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.738871157169342, 0.7934654951095581, 0.9454423189163208, 0.8880735039710999, 0.9575167298316956, 1.0808656215667725, 0.4844328463077545, 1.5118249654769897, 0.8028388023376465, 0.981200098991394, 0.010311766527593136, 0.7848725914955139, 1.070124864578247, 1.0545690059661865, 1.0842609405517578, 0.662959635257721]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4217669665813446, 1.123396873474121, 0.9712789058685303, 0.9115110039710999, 0.6600602865219116, 0.8769079446792603, 1.4030768871307373, 1.0535359382629395, 0.8853526711463928, 0.9899972677230835, 0.6996703147888184, -0.3185950517654419, 0.907768726348877, 0.907892644405365, 1.0684609413146973, 1.5630186796188354]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.6969392895698547, 0.3064626455307007, 0.825093686580658, 0.7909930348396301, 0.6237796545028687, 0.46039876341819763, 0.3666141927242279, 0.842187762260437, -0.7596697211265564, 2.592383861541748, -0.7487881183624268, 0.724392294883728, 0.8441112637519836, 0.8646670579910278, 0.8195194005966187, 0.4610331058502197]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.11952588707208633, 0.6055149435997009, 1.3307554721832275, 1.0774139165878296, 1.3488303422927856, 1.3242342472076416, 1.3613648414611816, 1.3117436170578003, 1.7558907270431519, 1.3952001333236694, 1.9857861995697021, 1.3447338342666626, 1.3513197898864746, 1.1592262983322144, 0.6531763672828674, 1.1810470819473267]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3375858068466187, 1.2894350290298462, 1.3689204454421997, 1.5936822891235352, 0.9490748047828674, 1.1907119750976562, 1.800837755203247, 1.6125812530517578, 1.999875783920288, 1.3190028667449951, 1.0985665321350098, 0.8334816098213196, 1.136195182800293, 0.9909787774085999, 1.3922507762908936, 1.390128254890442]
Running loglikelihood requests:  15%|███████▋                                          | 77/504 [04:30<23:22,  3.28s/it]Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.1974800825119019, 1.1180342435836792, 1.1175713539123535, 1.1472972631454468, 1.3613168001174927, 0.8685422539710999, 0.9302717447280884, 1.095291018486023, 1.8497787714004517, 1.331827998161316, 1.3059632778167725, 1.2760906219482422, 1.663379430770874, 1.2651057243347168, 1.2155210971832275, 1.2528337240219116]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.869622528553009, 1.0309169292449951, 0.9851399064064026, 0.8827080726623535, 0.01758500561118126, 1.2008274793624878, 1.0995190143585205, 1.8612546920776367, 0.8131519556045532, 1.20632004737854, 1.1916207075119019, 1.152140498161316, 1.0732535123825073, 1.3021022081375122, 0.985987663269043, 0.3664025068283081]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.8118000030517578, 1.9909230470657349, 1.8358471393585205, 1.728459119796753, 1.7003477811813354, 1.8667811155319214, 1.6607884168624878, 1.9504832029342651, 1.7329976558685303, 1.7540643215179443, 2.0965046882629395, 2.4678919315338135, 1.5675662755966187, 2.1355221271514893, 1.9966356754302979, 1.8988438844680786]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3548591136932373, 1.2682216167449951, 1.8373825550079346, 1.0873953104019165, 1.4265941381454468, 1.5154443979263306, 1.2799742221832275, 1.316699743270874, 1.4913069009780884, 1.472385287284851, 1.5513231754302979, 2.1801843643188477, 1.46875, 1.5063222646713257, 1.1359115839004517, 1.575607419013977]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5167765617370605, 1.7509031295776367, 1.1624141931533813, 1.453147530555725, 1.5320628881454468, 1.4524250030517578, 1.3778902292251587, 1.3953214883804321, 1.484081506729126, 1.4067580699920654, 1.5080609321594238, 1.5096752643585205, 1.3280572891235352, 0.8044994473457336, 1.408948302268982, 1.4672597646713257]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3141708374023438, 2.3068325519561768, 2.198496103286743, 2.082437753677368, 2.7029895782470703, 2.258647918701172, 2.5555455684661865, 2.1532695293426514, 2.154240369796753, 2.1653268337249756, 2.6962156295776367, 2.138547658920288, 2.1917901039123535, 2.3724710941314697, 2.2843434810638428, 2.2852466106414795]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.5989658832550049, 1.7019509077072144, 1.4464188814163208, 1.6044527292251587, 1.3791983127593994, 1.474959373474121, 1.544797658920288, 1.8409501314163208, 1.5251535177230835, 1.479407548904419, 1.5446622371673584, 1.3724033832550049, 1.4671016931533813, 1.375496745109558, 1.6399928331375122, 1.8621071577072144]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.8352601528167725, 1.6773245334625244, 1.6142522096633911, 1.7377393245697021, 1.6790326833724976, 1.7266485691070557, 1.6290732622146606, 1.71546471118927, 1.7198126316070557, 1.8085598945617676, 2.14457631111145, 1.6009584665298462, 1.6443731784820557, 1.7592463493347168, 1.8147269487380981, 1.7417393922805786]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5892778635025024, 1.6499842405319214, 1.5866402387619019, 1.7148408889770508, 1.5983335971832275, 1.5618548393249512, 1.7129372358322144, 1.6619230508804321, 1.5661565065383911, 1.6786319017410278, 1.4172687530517578, 1.680593490600586, 1.6186213493347168, 1.824650526046753, 1.5574506521224976, 1.5812240839004517]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9881458282470703, 2.061506509780884, 2.3933796882629395, 1.9497380256652832, 2.066586971282959, 1.9870619773864746, 2.472249746322632, 2.0557260513305664, 1.9339098930358887, 1.9490606784820557, 1.8881864547729492, 1.7720376253128052, 1.9100546836853027, 1.7721278667449951, 2.2681539058685303, 2.0025062561035156]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.356304168701172, 5.674132823944092, 5.362355709075928, 5.3866286277771, 5.2542901039123535, 5.184067726135254, 5.447209358215332, 5.73295259475708, 5.532107830047607, 5.339030742645264, 5.364658832550049, 5.289965629577637, 5.354927062988281, 5.401372909545898, 5.524476051330566, 5.331060409545898]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.942537307739258, 3.76957631111145, 3.7474429607391357, 3.5619750022888184, 3.7789723873138428, 3.782047986984253, 3.554935932159424, 3.738715887069702, 3.811674118041992, 3.8434536457061768, 3.697141408920288, 3.4117424488067627, 3.745393753051758, 3.76584792137146, 3.840430736541748, 3.862220048904419]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.0287210941314697, 3.1207549571990967, 3.013728380203247, 2.897714853286743, 2.9760656356811523, 3.038836717605591, 3.039649486541748, 2.895885944366455, 2.9591312408447266, 2.984781503677368, 3.2464098930358887, 2.434722661972046, 3.0539424419403076, 3.0082414150238037, 3.2425036430358887, 3.061732292175293]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.1194593533873558, 0.14954490959644318, 0.13117392361164093, -0.251800000667572, -0.19511467218399048, -0.21818400919437408, 0.14679008722305298, -0.22867880761623383, 0.09303368628025055, 0.12735344469547272, 0.12041158974170685, 0.08969604223966599, 0.10735949128866196, 0.11944633722305298, -1.0877398252487183, 0.14917398989200592]
Layer: gate_0 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.10701671242713928, 0.07470779120922089, 0.05636541545391083, 0.06969167292118073, 0.08605854958295822, 0.055547669529914856, 0.05862642079591751, 0.055395595729351044, 0.007049941923469305, 0.07067323476076126, -0.1532430350780487, 0.039912741631269455, 0.012086150236427784, -0.030109675601124763, -0.014707071706652641, -0.004201238509267569]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09023751318454742, 0.0965878888964653, 0.1093403548002243, 0.04997090622782707, 0.10463041067123413, 0.12815138697624207, 0.10832095146179199, -0.04995978996157646, 0.08548256009817123, 0.030812252312898636, -0.006124507635831833, 0.08443962782621384, -0.1244465559720993, 0.03537833318114281, 0.019487718120217323, 0.10561083257198334]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.128977432847023, 0.22632549703121185, 0.18863044679164886, 0.18139755725860596, 0.12204921990633011, 0.15187911689281464, 0.1501341015100479, 0.23805542290210724, 0.1695777028799057, -0.47736626863479614, -0.0790940374135971, 0.16807912290096283, 0.1135302409529686, -0.1502922922372818, -0.20417560636997223, -0.04490930959582329]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.12047412246465683, 0.12960359454154968, 0.09365314990282059, 0.00046317718806676567, 0.04057096689939499, -0.1291859745979309, 0.13840000331401825, 0.1355917602777481, 0.058019839227199554, 0.007024428341537714, -0.053425148129463196, 0.1703554093837738, -0.189056396484375, -0.14908231794834137, 0.12879358232021332, 0.011623315513134003]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.12555424869060516, 0.1636875867843628, 0.13178279995918274, 0.09198644012212753, -0.006336526479572058, 0.09165038913488388, -0.0006049661315046251, 0.1647411584854126, 0.08597084134817123, -0.06888966262340546, -0.3644549250602722, 0.11269167810678482, -0.0594642199575901, 0.1106666550040245, 0.1737155169248581, 0.0619855672121048]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.13351108133792877, 0.27572256326675415, 0.05551237240433693, 0.31582748889923096, 0.1730339527130127, 0.12735308706760406, 0.11282465606927872, 0.1060582771897316, 0.2951326370239258, 0.19727836549282074, -0.21175609529018402, 0.265755295753479, 0.23801986873149872, 0.058988504111766815, 0.08715030550956726, 0.12246201187372208]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.26483216881752014, 0.5326420068740845, 0.42111602425575256, 0.22841796278953552, -0.532002866268158, 0.26022616028785706, 0.23125790059566498, 0.07009133696556091, 0.19599959254264832, -0.003113600891083479, 0.18088306486606598, 0.26949140429496765, 0.4068230092525482, 0.1119176521897316, -0.1153043881058693, -0.2902534008026123]
Layer: gate_7 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3775218427181244, 0.43109527230262756, 0.09634363651275635, 0.5097268223762512, 0.3675591051578522, 0.5990291833877563, 0.3531094789505005, -0.09442748874425888, 0.18763203918933868, 0.4343014061450958, 0.1847054809331894, -0.0687270238995552, 0.48149701952934265, 0.0017549402546137571, 0.009241261519491673, 0.21492475271224976]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.8633974194526672, 0.8764554858207703, 0.45989057421684265, 0.6914457678794861, 0.6560152173042297, 0.440216064453125, 1.06005859375, 0.6889418363571167, 0.9064366817474365, 0.23619815707206726, 0.7974138855934143, 0.8488510847091675, 0.6249552965164185, 0.645190417766571, 0.8594209551811218, 0.8415900468826294]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9810812473297119, 1.0061495304107666, 0.893928050994873, 1.0080193281173706, 1.1182674169540405, 0.4165814518928528, 0.930461585521698, 0.816681981086731, 1.1251795291900635, 0.7898320555686951, 0.7962660789489746, 0.9360753893852234, 0.6604549288749695, 0.4843175411224365, 0.8837150931358337, 0.9754236340522766]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.113608717918396, 1.2592716217041016, 1.0991153717041016, 1.273405909538269, 1.2943589687347412, 0.830890417098999, 1.2182961702346802, 1.4390740394592285, 1.2061408758163452, 1.3427619934082031, 1.1451976299285889, 1.8237433433532715, 1.245695948600769, 1.131813883781433, 0.9462668299674988, 1.3586339950561523]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.93963623046875, 1.4337775707244873, 1.3145450353622437, 1.3742647171020508, 1.1755449771881104, 1.323325514793396, 0.9546085000038147, 1.2156968116760254, 1.2070872783660889, 0.664306640625, 1.1267807483673096, 1.109843134880066, 1.2237132787704468, 1.0887982845306396, 0.8824448585510254, 1.3018670082092285]
Layer: gate_12 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.7517462968826294, 1.4080767631530762, 1.7565429210662842, 1.5863087177276611, 1.5631433725357056, 0.9903808832168579, 1.532766580581665, 1.970542311668396, 1.4350643157958984, 1.5981847047805786, 1.650338888168335, 1.8290901184082031, 1.3892822265625, 1.6228630542755127, 1.7429227828979492, 1.7053940296173096]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.3207203149795532, 1.1740005016326904, 1.2641774415969849, 1.3639131784439087, 1.4225069284439087, 2.175043821334839, 1.2192713022232056, 1.1634255647659302, 1.1880055665969849, 1.2009388208389282, 0.7618135213851929, 1.5847326517105103, 1.285744309425354, 1.3715993165969849, 1.4178423881530762, 1.4701602458953857]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [1.3610179424285889, 1.291291356086731, 1.5953469276428223, 1.247610330581665, 1.591172218322754, 1.3700482845306396, 1.6196614503860474, 1.490234375, 1.167532205581665, 1.3456801176071167, 1.4171242713928223, 0.4561324119567871, 1.546599268913269, 1.3287569284439087, 1.338591456413269, 1.6193130016326904]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9943267703056335, 0.6654329299926758, 0.951528012752533, 0.7516458034515381, 0.8546257615089417, 0.9765625, 0.20329123735427856, 0.9743544459342957, 0.0927935466170311, 1.6421953439712524, 0.28559210896492004, 0.9359346032142639, 1.1536535024642944, 0.8541762232780457, 0.8923081159591675, 0.6576631665229797]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.1026068925857544, 1.320132851600647, 1.7551815509796143, 2.284209728240967, 1.850356101989746, 1.8602482080459595, 1.8763211965560913, 2.1074678897857666, 1.9301700592041016, 1.6965835094451904, 2.236046552658081, 1.8368394374847412, 2.109823703765869, 1.614206075668335, 1.6219191551208496, 1.7536025047302246]
Layer: gate_17 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.998046875, 1.911833643913269, 1.9441744089126587, 2.1013097763061523, 1.714568018913269, 2.0311810970306396, 1.8719209432601929, 2.4705421924591064, 2.5553998947143555, 2.060983419418335, 2.190659523010254, 1.9952133893966675, 2.0750460624694824, 1.689555048942566, 2.034306049346924, 2.0629825592041016]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.3438878059387207, 2.0127756595611572, 2.0128331184387207, 2.0244715213775635, 2.1618566513061523, 2.044588804244995, 2.1734375953674316, 2.2518153190612793, 2.2728171348571777, 2.212890625, 2.1566176414489746, 2.179136037826538, 2.3107824325561523, 2.2060892581939697, 2.1968750953674316, 2.106870412826538]
Layer: gate_19 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.293076515197754, 1.4759421348571777, 1.5296070575714111, 1.7904555797576904, 0.567828893661499, 1.5120633840560913, 1.652458667755127, 2.5064854621887207, 1.257292628288269, 1.5200368165969849, 1.478745460510254, 1.3569737672805786, 1.4011833667755127, 1.6221966743469238, 1.5632532835006714, 0.948418140411377]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.5196690559387207, 2.5828585624694824, 2.4556524753570557, 2.4254136085510254, 2.535569906234741, 2.4538373947143555, 2.4809741973876953, 2.5230698585510254, 2.4006433486938477, 2.8599724769592285, 2.6837775707244873, 2.522242546081543, 2.3326056003570557, 2.8220129013061523, 2.7425551414489746, 2.5774357318878174]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [2.010294198989868, 1.7975413799285889, 2.120427370071411, 1.6098575592041016, 2.0921874046325684, 1.9711856842041016, 1.9384421110153198, 1.8938418626785278, 1.9620634317398071, 2.074310779571533, 1.832582712173462, 2.0332260131835938, 2.061305046081543, 1.9829044342041016, 1.9436120986938477, 2.112086296081543]
Layer: gate_22 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  16%|████████                                          | 81/504 [04:43<23:09,  3.29s/it]Layer: gate_23 - Captured router_logits: [2.1698758602142334, 1.9169232845306396, 1.8167738914489746, 1.8619025945663452, 1.8766084909439087, 1.892853856086731, 1.7559282779693604, 1.914774775505066, 1.8046537637710571, 1.7354434728622437, 1.9445542097091675, 1.709814429283142, 1.8202550411224365, 1.411543846130371, 1.9395105838775635, 1.951884150505066]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.716452121734619, 2.7483456134796143, 2.5749080181121826, 2.715165376663208, 2.848069906234741, 2.8532168865203857, 2.8115808963775635, 2.7202205657958984, 2.6839613914489746, 2.8196232318878174, 2.706295967102051, 2.7497241497039795, 2.555238962173462, 2.7569851875305176, 2.829411745071411, 2.971874952316284]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_25 - Captured router_logits: [1.9663143157958984, 1.8448529243469238, 1.7774356603622437, 1.9593520164489746, 1.6629825830459595, 1.8134076595306396, 1.8791015148162842, 1.9475643634796143, 1.891865849494934, 1.914338231086731, 1.7849206924438477, 1.9279870986938477, 1.8793658018112183, 1.8812040090560913, 1.6636488437652588, 2.2074217796325684]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9513155221939087, 1.8148638010025024, 2.187023162841797, 2.0001609325408936, 1.877211570739746, 1.877228856086731, 1.8704417943954468, 1.8322569131851196, 1.9373406171798706, 1.9630858898162842, 2.12642240524292, 1.8646354675292969, 2.045214891433716, 1.834145188331604, 1.8457720279693604, 1.8327335119247437]
Layer: gate_26 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5184340476989746, 1.562307596206665, 1.6496758460998535, 1.644468069076538, 1.615820288658142, 1.5600873231887817, 1.6822035312652588, 1.6653119325637817, 1.6387393474578857, 1.8173081874847412, 1.522886037826538, 1.7310030460357666, 1.5326229333877563, 1.5767406225204468, 1.6499253511428833, 1.6066769361495972]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.370349884033203, 2.4440832138061523, 2.6107020378112793, 2.513798236846924, 2.4107248783111572, 2.391664743423462, 2.45582914352417, 2.460357189178467, 2.478860378265381, 2.5005743503570557, 2.3607594966888428, 2.429595470428467, 2.378354787826538, 2.376539468765259, 2.5247902870178223, 2.511446714401245]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.829733371734619, 5.904733657836914, 5.566636085510254, 5.890096664428711, 5.849954128265381, 5.660386085510254, 5.779044151306152, 5.961994647979736, 5.918106555938721, 5.90496301651001, 5.738832950592041, 5.605330944061279, 5.773805141448975, 5.725046157836914, 5.79990816116333, 5.617371559143066]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.1468634605407715, 4.096645355224609, 4.10068941116333, 4.151838302612305, 4.132077217102051, 3.987729787826538, 4.054124355316162, 4.082651615142822, 4.126821041107178, 4.223184585571289, 4.164567947387695, 3.957288980484009, 4.261638164520264, 4.018830299377441, 4.107996463775635, 4.136342525482178]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1852481365203857, 3.2099263668060303, 2.946277618408203, 2.706709623336792, 2.9235293865203857, 3.185018301010132, 2.8760111331939697, 3.0380513668060303, 3.0245864391326904, 3.1981618404388428, 2.8953585624694824, 2.5701918601989746, 3.060753583908081, 3.147886037826538, 3.124907970428467, 3.154090166091919]
Layer: gate_31 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.086512491106987, 0.0978802889585495, 0.10416750609874725, -0.26295205950737, -0.23695218563079834, -0.10115864127874374, 0.1130744144320488, -0.07477697730064392, 0.0777454525232315, 0.08400370180606842, 0.09521137177944183, 0.061523403972387314, 0.08384247869253159, 0.10591447353363037, -1.0809566974639893, 0.11244964599609375]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08090045303106308, 0.05229351297020912, 0.04338644817471504, 0.049042709171772, 0.07593440264463425, 0.029226452112197876, 0.05247091129422188, 0.09802648425102234, 0.028285684064030647, 0.07690200954675674, -0.19821012020111084, 0.048356473445892334, 0.01157443132251501, -0.01525496318936348, 0.030997510999441147, 0.026118341833353043]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08659954369068146, 0.06347827613353729, 0.10519936680793762, 0.0757782980799675, 0.1138109415769577, 0.10302121937274933, 0.07415653020143509, -0.09464962780475616, 0.07218814641237259, 0.10732867568731308, 0.030211791396141052, 0.08363305777311325, -0.20552237331867218, 0.010609906166791916, -0.057003822177648544, 0.1010221391916275]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12809118628501892, 0.13746286928653717, 0.12320010364055634, 0.13543391227722168, 0.144630566239357, 0.1118355467915535, -0.0142822265625, 0.17106372117996216, 0.18156112730503082, -0.5563162565231323, 0.04009610041975975, 0.0625249445438385, 0.272305965423584, -0.30222344398498535, 0.0060161203145980835, -0.06709577143192291]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.063051737844944, 0.11266981065273285, 0.06654755026102066, 0.2090480625629425, 0.04405074194073677, -0.06846252828836441, 0.02620020881295204, 0.04797191917896271, -0.10802382230758667, 0.021780597046017647, -0.19923803210258484, 0.14703601598739624, 0.01415010541677475, -0.19286876916885376, 0.12954337894916534, -0.07897327840328217]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.08254428952932358, 0.13584086298942566, 0.07396789640188217, 0.1557304710149765, -0.11228001862764359, -0.058861371129751205, 0.05348360911011696, -0.0416620671749115, 0.22653056681156158, -0.09693753719329834, -0.12587514519691467, 0.0786355510354042, -0.20927438139915466, 0.149395152926445, 0.12790268659591675, 0.08081753551959991]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11017201840877533, -0.011504853144288063, 0.1036529541015625, 0.2954404950141907, 0.18931972980499268, 0.15733511745929718, -0.12441276758909225, -0.09184128046035767, 0.34770870208740234, 0.1964244693517685, -0.5351138710975647, 0.23196643590927124, 0.2261553555727005, -0.2528723180294037, 0.32139816880226135, 0.21897947788238525]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.562856912612915, 0.2400790899991989, 0.2113490253686905, 0.16126509010791779, -0.8950557112693787, 0.2799035310745239, 0.197865292429924, -0.4031839966773987, 0.5571779012680054, 0.162493497133255, -0.2379017472267151, 0.3139296770095825, 0.14333978295326233, 0.23692870140075684, -0.6698939204216003, -0.3117707669734955]
Layer: gate_7 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3439890146255493, 0.31668996810913086, -0.339691698551178, 0.2973669469356537, 0.37465900182724, 0.287945955991745, 0.2356114685535431, -0.28272417187690735, 0.18265482783317566, 0.8251466751098633, 0.09183607250452042, 0.07390955090522766, 0.4631281793117523, -0.49904829263687134, -0.50392746925354, 0.3550648093223572]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.35017284750938416, 0.2647339701652527, 0.43204084038734436, 1.0904139280319214, 0.2914193570613861, 0.14521944522857666, 0.5108598470687866, 0.495121568441391, 0.680169403553009, 0.012904367409646511, 0.18279892206192017, 0.6162840127944946, 0.525255024433136, 0.20938457548618317, 0.762230396270752, 0.611670196056366]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0099395513534546, 0.787781834602356, 0.5773776173591614, 0.725448489189148, 0.9242973923683167, 0.5084806084632874, 0.6307628750801086, 0.7294512391090393, 0.24631860852241516, 0.4749419689178467, 0.838367223739624, 0.9179102778434753, 0.198505699634552, 0.2735500633716583, 0.9883261919021606, 0.414960116147995]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9053640961647034, 1.050907015800476, 0.831602931022644, 1.0084658861160278, 1.0963997840881348, 0.270169734954834, 0.78229159116745, 0.8229761123657227, 0.9064722061157227, 1.4004491567611694, 0.833583414554596, 0.9099026322364807, 0.26598024368286133, 0.4359259605407715, -0.04995965212583542, 0.7063101530075073]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.15213213860988617, 0.778817355632782, 1.1786407232284546, 1.102217435836792, 0.16280177235603333, 0.666986346244812, 0.972147524356842, 1.2193669080734253, 0.471852570772171, 0.14659292995929718, 0.88860684633255, 0.8473989367485046, 0.711342453956604, 0.8467381596565247, 0.5920921564102173, 0.8541354537010193]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6745550632476807, 0.9504905939102173, 1.0662484169006348, 0.416027694940567, 1.0079236030578613, -0.2459559589624405, 1.0427114963531494, 1.9176998138427734, 0.96550452709198, 0.8957124948501587, 1.2346994876861572, 1.0726983547210693, 0.04242857173085213, 0.6992706656455994, 1.088014841079712, 0.585466742515564]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7350858449935913, 0.7147794365882874, 0.9752204418182373, 0.84538733959198, 1.024080753326416, 1.25444495677948, 0.13998906314373016, 1.37870192527771, 0.7401028275489807, 0.716147780418396, -0.44561585783958435, 0.6667187809944153, 0.9505505561828613, 1.060570240020752, 1.10093092918396, 0.4481698274612427]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.399453729391098, 1.0372496843338013, 0.9571248292922974, 0.9675102829933167, 0.7417898774147034, 0.8772104382514954, 1.437000036239624, 0.953475832939148, 0.830230176448822, 0.996082067489624, 0.60105299949646, -0.36510080099105835, 1.0940474271774292, 0.827657163143158, 0.7599000930786133, 1.4181145429611206]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.647800862789154, 0.156466543674469, 0.8346495628356934, 0.8211633563041687, 0.7194064259529114, 0.3926926255226135, 0.1757790595293045, 0.828181266784668, -1.064242959022522, 2.222350597381592, -0.8535905480384827, 0.5245869159698486, 0.8140262365341187, 0.7750338912010193, 0.798994779586792, 0.18320296704769135]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.006447067018598318, 0.639261782169342, 1.4783401489257812, 1.203863263130188, 1.4985615015029907, 1.415734887123108, 1.2811330556869507, 1.3306628465652466, 1.940727949142456, 1.1785486936569214, 2.1337714195251465, 1.4668787717819214, 1.363510012626648, 1.2818493843078613, 0.4193706512451172, 1.1356005668640137]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3807541131973267, 1.1859562397003174, 1.421708345413208, 1.6970667839050293, 0.82789546251297, 1.1314090490341187, 1.7729228734970093, 1.6511741876602173, 2.376169443130493, 1.3706727027893066, 1.0858293771743774, 0.692362368106842, 1.1197410821914673, 0.7249807119369507, 1.3047693967819214, 1.427067756652832]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3830697536468506, 1.227849006652832, 1.3999228477478027, 1.2920682430267334, 1.4732176065444946, 0.859752357006073, 1.1955785751342773, 1.1725329160690308, 1.8758537769317627, 1.340451955795288, 1.499695897102356, 1.3461943864822388, 1.587371587753296, 1.4766794443130493, 1.3604276180267334, 1.3151665925979614]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0613049268722534, 1.0354251861572266, 1.0004327297210693, 1.03909170627594, -0.287386417388916, 1.2686599493026733, 1.11624014377594, 1.7016031742095947, 0.7503640055656433, 1.216153621673584, 1.1050652265548706, 1.1076382398605347, 1.0626519918441772, 1.16923189163208, 0.8050602674484253, 0.29959380626678467]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.968048334121704, 2.1303799152374268, 2.070172071456909, 1.8934786319732666, 1.853223204612732, 2.0408401489257812, 1.8490830659866333, 2.0778911113739014, 1.858696699142456, 1.980117917060852, 2.233766794204712, 2.5433197021484375, 1.5594241619110107, 2.1859095096588135, 2.0249111652374268, 1.9480258226394653]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.340826153755188, 1.3676319122314453, 1.8614567518234253, 1.0215998888015747, 1.406296730041504, 1.5561376810073853, 1.2925009727478027, 1.3644741773605347, 1.4526103734970093, 1.504526138305664, 1.5221978425979614, 2.009215831756592, 1.459908366203308, 1.4640485048294067, 1.221495509147644, 1.6174447536468506]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.2914717197418213, 1.836545705795288, 1.0480785369873047, 1.4054781198501587, 1.5985684394836426, 1.4663641452789307, 1.444657564163208, 1.405922532081604, 1.498970866203308, 1.478223204612732, 1.5674588680267334, 1.4923746585845947, 1.286746859550476, 0.6303831338882446, 1.2583855390548706, 1.4862228631973267]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3194704055786133, 2.36845064163208, 2.27142596244812, 2.09122371673584, 2.508373975753784, 2.196107864379883, 2.4079341888427734, 2.14970064163208, 2.096323013305664, 2.100275993347168, 2.7372286319732666, 2.0752713680267334, 2.297015428543091, 2.423746347427368, 2.253836154937744, 2.3220434188842773]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6718047857284546, 1.6600393056869507, 1.6875702142715454, 1.7847352027893066, 1.2539881467819214, 1.6397595405578613, 1.682003140449524, 1.9845855236053467, 1.703405737876892, 1.57522451877594, 1.7031952142715454, 1.5290980339050293, 1.6140297651290894, 1.5873643159866333, 1.9732409715652466, 1.959908366203308]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.7348662614822388, 1.7622954845428467, 1.6621912717819214, 1.7218610048294067, 1.7209603786468506, 1.785280466079712, 1.69877290725708, 1.6926050186157227, 1.7882686853408813, 1.8042430877685547, 2.2946529388427734, 1.6726468801498413, 1.6202282905578613, 1.7137911319732666, 1.7776873111724854, 1.7652134895324707]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Running loglikelihood requests:  17%|████████▍                                         | 85/504 [04:57<23:17,  3.33s/it]Layer: gate_27 - Captured router_logits: [1.5919387340545654, 1.6404013633728027, 1.583746075630188, 1.6597601175308228, 1.54716157913208, 1.5497227907180786, 1.7563740015029907, 1.653799057006836, 1.5455411672592163, 1.6431686878204346, 1.266303300857544, 1.574734091758728, 1.6340218782424927, 1.873486876487732, 1.6063488721847534, 1.6063809394836426]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9718960523605347, 2.0165605545043945, 2.2968573570251465, 1.9467393159866333, 2.0754292011260986, 2.0152740478515625, 2.5454013347625732, 2.0786688327789307, 1.9596978425979614, 1.962878942489624, 1.9702937602996826, 1.7912144660949707, 1.9276057481765747, 1.8342533111572266, 2.192739486694336, 1.938798189163208]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.785694122314453, 5.123713493347168, 4.689043998718262, 4.665138244628906, 4.646636486053467, 4.584580898284912, 4.788875579833984, 4.909314155578613, 4.940821647644043, 4.720527648925781, 4.712984085083008, 4.608907222747803, 4.692996978759766, 4.728995323181152, 4.895162582397461, 4.736994743347168]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.4957427978515625, 3.4130566120147705, 3.355959892272949, 3.267624855041504, 3.4480960369110107, 3.482456922531128, 3.1521799564361572, 3.41860032081604, 3.609515428543091, 3.433055877685547, 3.3409664630889893, 2.974036931991577, 3.2731802463531494, 3.470574378967285, 3.4905970096588135, 3.418459892272949]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.645303249359131, 2.672576665878296, 2.6519460678100586, 2.625514507293701, 2.580043077468872, 2.6611387729644775, 2.7420003414154053, 2.4091973304748535, 2.56830096244812, 2.476515769958496, 3.1089539527893066, 1.9581196308135986, 2.6234328746795654, 2.5398108959198, 3.0225486755371094, 2.5922763347625732]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.07424593716859818, 0.09111420810222626, 0.09480497986078262, -0.2788047194480896, -0.2669661045074463, -0.09866328537464142, 0.09625126421451569, -0.04345814138650894, 0.06625768542289734, 0.0724010095000267, 0.06962483376264572, 0.03502867743372917, 0.0696534588932991, 0.11167852580547333, -1.057247281074524, 0.11262162029743195]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.07555879652500153, 0.0428512804210186, 0.028344448655843735, 0.04618992656469345, 0.06779821962118149, 0.021599868312478065, 0.05243682861328125, 0.10158661007881165, 0.08269942551851273, 0.07040534913539886, -0.19234101474285126, 0.05714162066578865, 0.018149636685848236, -0.025295743718743324, 0.037655387073755264, 0.03646184876561165]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08252489566802979, 0.03921749070286751, 0.06498714536428452, 0.07372380793094635, 0.14624005556106567, 0.08857718110084534, 0.09668828547000885, -0.10246803611516953, 0.09033376723527908, 0.10933414846658707, 0.030390918254852295, 0.07389429956674576, -0.22855150699615479, 0.029299834743142128, -0.016137417405843735, 0.10559086501598358]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12461742013692856, 0.14422644674777985, 0.12003053724765778, 0.13781356811523438, 0.12922650575637817, 0.10835465043783188, -0.0035400390625, 0.16119958460330963, 0.17946814000606537, -0.5562118887901306, 0.01635483279824257, 0.08514831215143204, 0.3020191192626953, -0.3146846890449524, 0.06995188444852829, -0.0797400251030922]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.05318048596382141, 0.08286882191896439, 0.06328411400318146, 0.27830344438552856, 0.07008791714906693, -0.0591980442404747, 0.045125626027584076, 0.024340612813830376, -0.1148647889494896, -0.004154459573328495, -0.26825398206710815, 0.11812059581279755, 0.10611183941364288, -0.23460669815540314, 0.1465134173631668, -0.10945887863636017]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.05732153728604317, 0.15817080438137054, 0.05873857066035271, 0.28160253167152405, -0.17181220650672913, -0.11366724967956543, 0.1030472069978714, -0.026419397443532944, 0.21339620649814606, -0.10025487095117569, -0.16533388197422028, 0.12061099708080292, -0.26485225558280945, 0.19367416203022003, 0.12296586483716965, 0.08826693892478943]
Layer: gate_5 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.12437479197978973, 0.004383341409265995, 0.2007625699043274, 0.26327329874038696, 0.19188722968101501, 0.15130355954170227, -0.17480486631393433, -0.07439908385276794, 0.36840227246284485, 0.16081880033016205, -0.5852631330490112, 0.2763683497905731, 0.26967182755470276, -0.3099547028541565, 0.3356090188026428, 0.2242058962583542]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.536924421787262, 0.2170613557100296, 0.2556627690792084, 0.15605062246322632, -0.8629971742630005, 0.24431300163269043, 0.1988060176372528, -0.3925209641456604, 0.6875662207603455, 0.18339630961418152, -0.19663122296333313, 0.24556033313274384, 0.10271437466144562, 0.2841208577156067, -0.6275101900100708, -0.3169434666633606]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2558571696281433, 0.2704501152038574, -0.22666186094284058, 0.29620158672332764, 0.45260193943977356, 0.3922378122806549, 0.26267290115356445, -0.3820549249649048, 0.2032422572374344, 0.967363178730011, 0.10974676162004471, 0.2603722810745239, 0.5029770135879517, -0.48471197485923767, -0.5249008536338806, 0.48171016573905945]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.46465620398521423, 0.3042965531349182, 0.4737463891506195, 1.243533968925476, 0.4277969002723694, 0.26456788182258606, 0.6675736904144287, 0.6365589499473572, 1.1242187023162842, 0.17264218628406525, 0.27323442697525024, 0.720566987991333, 0.5367398262023926, 0.003287020605057478, 0.7769057750701904, 0.6456261873245239]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0444239377975464, 0.8972685933113098, 0.5165660381317139, 0.6544389128684998, 0.9451112747192383, 0.750706136226654, 0.7341042160987854, 0.7520359754562378, 0.1320134997367859, 0.5235532522201538, 0.7824100255966187, 1.0462002754211426, 0.17944946885108948, 0.16557098925113678, 1.2596924304962158, 0.527358889579773]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [1.0182942152023315, 1.4254024028778076, 0.937168538570404, 1.387406826019287, 1.4551076889038086, 0.46374326944351196, 1.0538115501403809, 0.9595940113067627, 1.1279947757720947, 1.8346235752105713, 0.917149007320404, 1.1289002895355225, 0.4446209669113159, 0.6204867362976074, 0.1848706752061844, 0.8747632503509521]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.22557854652404785, 0.8870383501052856, 1.4601325988769531, 1.2735795974731445, 0.5680205225944519, 0.6182572841644287, 1.3255149126052856, 1.6413943767547607, 0.5508278608322144, 0.19284982979297638, 0.9782966375350952, 0.9250828623771667, 0.9424242377281189, 0.8756392002105713, 0.7811967134475708, 1.034706473350525]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.1381702423095703, 1.2040246725082397, 1.3849490880966187, 0.7970238924026489, 1.3202769756317139, 0.020994244143366814, 1.2853456735610962, 2.3358192443847656, 1.4355586767196655, 1.124550223350525, 1.9303089380264282, 1.394649624824524, 0.5151472687721252, 1.0502108335494995, 1.1777461767196655, 0.9006297588348389]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.9380223155021667, 0.8824455738067627, 1.161115050315857, 0.9553266763687134, 1.1592921018600464, 1.8330022096633911, 0.5883138179779053, 2.0340824127197266, 0.8495383262634277, 0.9493992924690247, -0.26820364594459534, 1.0028908252716064, 1.058100700378418, 1.1916548013687134, 1.143673062324524, 0.6807395219802856]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.5654481649398804, 1.2349313497543335, 1.1028053760528564, 1.0318892002105713, 0.9982392191886902, 1.0766808986663818, 2.0438549518585205, 1.1617779731750488, 1.1235085725784302, 1.0751538276672363, 0.8802090883255005, -0.17999933660030365, 1.2536487579345703, 0.981581449508667, 1.086209774017334, 1.8055546283721924]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6445416212081909, 0.1894037425518036, 0.7517282366752625, 0.8273378610610962, 0.6908558011054993, 0.5361809134483337, 0.7482090592384338, 0.7159438729286194, -0.7102057933807373, 2.3151752948760986, -0.6399093866348267, 0.780916154384613, 0.8115648627281189, 0.8334161639213562, 0.813065230846405, 0.5982170104980469]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.40073832869529724, 1.0143465995788574, 1.5843276977539062, 1.527269721031189, 1.6102864742279053, 1.5497987270355225, 1.6740870475769043, 1.5898674726486206, 2.1443514823913574, 1.4704663753509521, 2.7599077224731445, 1.884311318397522, 1.6609078645706177, 1.4122692346572876, 0.7194589376449585, 1.409712314605713]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6585227251052856, 1.4329545497894287, 1.5868371725082397, 2.1165246963500977, 1.1938742399215698, 1.4150331020355225, 2.393655300140381, 1.9832385778427124, 2.803835153579712, 1.7156250476837158, 1.4681463241577148, 1.2657936811447144, 1.4601562023162842, 1.0517653226852417, 1.7070549726486206, 1.7294270992279053]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.7452178001403809, 1.593205451965332, 2.0230824947357178, 1.543241024017334, 1.9024147987365723, 1.3929657936096191, 1.6962239742279053, 1.5803208351135254, 2.546661853790283, 1.6685606241226196, 1.9738517999649048, 1.7108073234558105, 1.8974668979644775, 1.8138494491577148, 1.815980076789856, 1.7691524028778076]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.4499911069869995, 1.159842610359192, 1.0663233995437622, 1.1092077493667603, 0.17263877391815186, 1.5427497625350952, 1.3033853769302368, 2.081389904022217, 0.8172237277030945, 1.4165009260177612, 1.27734375, 1.2252278327941895, 1.2046046257019043, 1.2581439018249512, 1.0677926540374756, 0.5430821180343628]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.314725399017334, 2.4644887447357178, 2.457575798034668, 2.2161457538604736, 2.227888345718384, 2.3352746963500977, 2.2627129554748535, 2.4900567531585693, 2.2289061546325684, 2.4519412517547607, 2.5883049964904785, 2.9120264053344727, 1.996780276298523, 2.7155776023864746, 2.4184186458587646, 2.3705966472625732]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.6319838762283325, 1.5836647748947144, 2.3634469509124756, 1.2904119491577148, 1.608806848526001, 1.774431824684143, 1.498496651649475, 1.6299716234207153, 1.7052556276321411, 1.8025331497192383, 1.7837594747543335, 2.2872631549835205, 1.6736268997192383, 1.7744791507720947, 1.4631391763687134, 1.8188446760177612]
Layer: gate_22 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.555255651473999, 1.9788589477539062, 1.3472774028778076, 1.6707385778427124, 1.9410984516143799, 1.619839072227478, 1.6164772510528564, 1.5529118776321411, 1.6378077268600464, 1.686245322227478, 1.6357481479644775, 1.6743371486663818, 1.384765625, 0.8454116582870483, 1.4654592275619507, 1.6689393520355225]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.6451704502105713, 2.5915720462799072, 2.5971591472625732, 2.262878894805908, 2.7409090995788574, 2.4258995056152344, 2.5846590995788574, 2.3986268043518066, 2.289630651473999, 2.3812973499298096, 3.1466856002807617, 2.3382575511932373, 2.5322442054748535, 2.6544506549835205, 2.447774648666382, 2.4414772987365723]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.8064393997192383, 1.7611268758773804, 1.6703598499298096, 1.9487216472625732, 1.4314156770706177, 1.8138257265090942, 1.8302793502807617, 2.1019885540008545, 1.8377131223678589, 1.706581473350525, 1.742069125175476, 1.6281013488769531, 1.7958096265792847, 1.6093276739120483, 2.3754498958587646, 2.1170928478240967]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_26 - Captured router_logits: [1.8311553001403809, 2.1071689128875732, 1.8794981241226196, 1.8646306991577148, 1.856723427772522, 2.0340139865875244, 1.9114139080047607, 1.8672112226486206, 1.9301609992980957, 1.9449573755264282, 2.635298252105713, 1.8087358474731445, 1.8944602012634277, 1.8732243776321411, 2.0177674293518066, 1.9221235513687134]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6616122722625732, 1.764630675315857, 1.6916074752807617, 1.7229403257369995, 1.5888967514038086, 1.6369199752807617, 1.9061789512634277, 1.7601207494735718, 1.6264441013336182, 1.7257871627807617, 1.346993327140808, 1.6829427480697632, 1.7685606479644775, 2.1430397033691406, 1.6763849258422852, 1.6627840995788574]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1641335487365723, 2.283143997192383, 2.4197442531585693, 2.1105587482452393, 2.2431344985961914, 2.1807291507720947, 2.7768466472625732, 2.268110752105713, 2.1798768043518066, 2.125331401824951, 2.0980112552642822, 1.9470643997192383, 1.9901041984558105, 2.005871295928955, 2.426941394805908, 2.098461151123047]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.048579692840576, 5.70767068862915, 5.075094699859619, 4.951657295227051, 5.028693199157715, 4.937215805053711, 5.0132575035095215, 5.191003799438477, 5.225473403930664, 5.063162803649902, 4.981913089752197, 4.96875, 5.002556800842285, 5.203977108001709, 5.279071807861328, 5.0512309074401855]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.6491951942443848, 3.4993371963500977, 3.405918598175049, 3.3203835487365723, 3.521448850631714, 3.5384469032287598, 3.2010180950164795, 3.6608901023864746, 3.7467803955078125, 3.502225399017334, 3.3299715518951416, 2.944851875305176, 3.253929853439331, 3.531013250350952, 3.560511350631714, 3.6130919456481934]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  18%|████████▊                                         | 89/504 [05:10<23:00,  3.33s/it]Layer: gate_31 - Captured router_logits: [2.690624952316284, 2.8254261016845703, 2.8572916984558105, 2.7350378036499023, 2.7208333015441895, 2.7722537517547607, 2.659327745437622, 2.438020944595337, 2.7573390007019043, 2.4608664512634277, 3.3489582538604736, 1.84945547580719, 2.736079454421997, 2.626373052597046, 3.25331449508667, 2.693655252456665]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09139325469732285, 0.10302910208702087, 0.11009498685598373, -0.2678224444389343, -0.23500476777553558, -0.1015719324350357, 0.11853905767202377, -0.10854501277208328, 0.07633213698863983, 0.0889272540807724, 0.10100356489419937, 0.06084584444761276, 0.08603677153587341, 0.11264495551586151, -1.0940784215927124, 0.11828689277172089]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08462607860565186, 0.05093961954116821, 0.04694962874054909, 0.05371032655239105, 0.07283122092485428, 0.028419483453035355, 0.05430757999420166, 0.09248527884483337, 0.026057017967104912, 0.07880059629678726, -0.19887307286262512, 0.04984210804104805, 0.009955990128219128, -0.015161393210291862, 0.03463199362158775, 0.020999053493142128]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08856695890426636, 0.0705420970916748, 0.10599243640899658, 0.07243319600820541, 0.11217258125543594, 0.10982832312583923, 0.06524094194173813, -0.09274417161941528, 0.07890883833169937, 0.11124562472105026, 0.03898594155907631, 0.08874622732400894, -0.22371789813041687, 0.0137616703286767, -0.06731016933917999, 0.10049526393413544]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1308467984199524, 0.1362062394618988, 0.13561807572841644, 0.14572106301784515, 0.1421678066253662, 0.09426847845315933, -0.024438371881842613, 0.1822921335697174, 0.17749041318893433, -0.5555645823478699, 0.046443477272987366, 0.06910441815853119, 0.2545810639858246, -0.3317652940750122, 0.0065704346634447575, -0.06612326949834824]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07031688094139099, 0.11264394223690033, 0.07834671437740326, 0.19416582584381104, 0.0516674630343914, -0.06405281275510788, 0.021589012816548347, 0.05331890285015106, -0.10568759590387344, 0.019578367471694946, -0.20997461676597595, 0.1379343718290329, -0.0032531737815588713, -0.20505616068840027, 0.1382170170545578, -0.0934760794043541]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07958387583494186, 0.13953851163387299, 0.08343200385570526, 0.1481032818555832, -0.09966171532869339, -0.08149155229330063, 0.051795635372400284, -0.03959618881344795, 0.20061974227428436, -0.0855526551604271, -0.14233046770095825, 0.07630675286054611, -0.22172851860523224, 0.145854651927948, 0.1291254162788391, 0.07997552305459976]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.1262519657611847, -0.027862548828125, 0.11682434380054474, 0.3040026128292084, 0.20462752878665924, 0.15558934211730957, -0.13179154694080353, -0.07655778527259827, 0.32011839747428894, 0.2012423425912857, -0.5547991991043091, 0.2403891384601593, 0.2328224927186966, -0.2767452299594879, 0.3049491047859192, 0.2129153162240982]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5179495215415955, 0.22597171366214752, 0.22736428678035736, 0.1680983155965805, -0.931121289730072, 0.30218079686164856, 0.19511468708515167, -0.4190092980861664, 0.5214919447898865, 0.16849097609519958, -0.2924641966819763, 0.33736228942871094, 0.10745757073163986, 0.23764541745185852, -0.7116824984550476, -0.3496044874191284]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3687344491481781, 0.33250880241394043, -0.3751017153263092, 0.3051528334617615, 0.40919890999794006, 0.3151591122150421, 0.245483860373497, -0.22684159874916077, 0.19096310436725616, 0.7454575300216675, 0.11256536096334457, 0.05110825225710869, 0.47926875948905945, -0.5632642507553101, -0.5789236426353455, 0.33855411410331726]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.34939977526664734, 0.19439567625522614, 0.4204430878162384, 1.008192777633667, 0.2845967710018158, 0.1048717126250267, 0.5322580933570862, 0.5156852602958679, 0.671970784664154, -0.04682358354330063, 0.08431359380483627, 0.6501035690307617, 0.5694008469581604, 0.02560369297862053, 0.794890820980072, 0.6540187001228333]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0427453517913818, 0.7987630367279053, 0.5166823863983154, 0.7103692889213562, 0.963427722454071, 0.386583536863327, 0.6243326663970947, 0.7109907865524292, 0.10508071631193161, 0.34096550941467285, 0.836991012096405, 0.9065311551094055, 0.17956949770450592, 0.18688446283340454, 0.9342353343963623, 0.36918169260025024]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.8817604780197144, 1.0174567699432373, 0.8714074492454529, 0.9374234080314636, 1.1645854711532593, 0.3072134256362915, 0.7989265322685242, 0.8203864693641663, 0.9359330534934998, 1.328261137008667, 0.8739516735076904, 0.9263257384300232, 0.21879272162914276, 0.4404844343662262, -0.11672992259263992, 0.7120043039321899]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.0933556780219078, 0.8029977679252625, 1.3019294738769531, 1.0753077268600464, 0.1520714908838272, 0.5967079997062683, 0.9229699373245239, 1.1537375450134277, 0.42440760135650635, 0.1306278109550476, 0.9088090658187866, 0.8863281011581421, 0.6765003800392151, 0.9047526121139526, 0.6522253751754761, 0.9115766882896423]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6986916065216064, 0.957247257232666, 1.13101327419281, 0.447433203458786, 1.0420868396759033, -0.22894157469272614, 1.0792850255966187, 1.852935552597046, 0.9759025573730469, 0.9488162994384766, 1.2231193780899048, 1.150076985359192, 0.02901056408882141, 0.7264459729194641, 0.9784120321273804, 0.589128315448761]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8100290298461914, 0.777539074420929, 1.0048295259475708, 0.8905066251754761, 1.0621448755264282, 1.3095651865005493, 0.02377929724752903, 1.3039684295654297, 0.7831809520721436, 0.7640978097915649, -0.47861236333847046, 0.7135066986083984, 0.9761319160461426, 1.108984351158142, 1.1751893758773804, 0.4326360523700714]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.3875303268432617, 1.1005563735961914, 0.9798768758773804, 0.9812736511230469, 0.736964762210846, 0.9103101491928101, 1.379407525062561, 0.9467092752456665, 0.8488399386405945, 1.0284919738769531, 0.6320201754570007, -0.4323219954967499, 1.0497891902923584, 0.863908588886261, 0.813127338886261, 1.4003276824951172]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.6775715947151184, 0.10984940826892853, 0.893770694732666, 0.8712860941886902, 0.734649658203125, 0.4002581834793091, 0.10396303236484528, 0.9201005697250366, -1.1804732084274292, 2.270324230194092, -0.9364546537399292, 0.4864635169506073, 0.8487215638160706, 0.8008567094802856, 0.8152536153793335, 0.08810887485742569]
Running loglikelihood requests:  18%|█████████▏                                        | 93/504 [05:24<23:00,  3.36s/it]Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.07730010151863098, 0.6361818313598633, 1.5244081020355225, 1.213999629020691, 1.5435606241226196, 1.4378551244735718, 1.3106770515441895, 1.3743845224380493, 1.9204071760177612, 1.1750088930130005, 2.1444365978240967, 1.4979403018951416, 1.3913553953170776, 1.3052023649215698, 0.4056389033794403, 1.154842495918274]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.453551173210144, 1.2530540227890015, 1.4868830442428589, 1.771756649017334, 0.8789063692092896, 1.167184591293335, 1.7531249523162842, 1.7174242734909058, 2.3523201942443848, 1.4594223499298096, 1.1601029634475708, 0.7384822964668274, 1.1733412742614746, 0.8368327021598816, 1.357421875, 1.5081676244735718]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.4665719270706177, 1.3014914989471436, 1.4680752754211426, 1.386174201965332, 1.5564393997192383, 0.9430926442146301, 1.2933298349380493, 1.2553740739822388, 1.9012192487716675, 1.4546756744384766, 1.621212124824524, 1.4554214477539062, 1.6518317461013794, 1.5688210725784302, 1.4542850255966187, 1.4157078266143799]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0508049726486206, 1.0723958015441895, 1.0416193008422852, 1.0733132362365723, -0.33792540431022644, 1.3226089477539062, 1.175485372543335, 1.6323922872543335, 0.7823419570922852, 1.243477702140808, 1.1790986061096191, 1.1608072519302368, 1.1064867973327637, 1.2176610231399536, 0.7891179919242859, 0.30568477511405945]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.9826468229293823, 2.186079502105713, 2.099100351333618, 1.90414297580719, 1.9004971981048584, 2.073674201965332, 1.8537405729293823, 2.0871212482452393, 1.9226325750350952, 1.9846117496490479, 2.2428977489471436, 2.529924154281616, 1.5189275741577148, 2.2417614459991455, 2.049076795578003, 1.9802082777023315]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.337310552597046, 1.3647490739822388, 1.8462594747543335, 0.9986387491226196, 1.3988163471221924, 1.5678976774215698, 1.2837002277374268, 1.397750973701477, 1.4718276262283325, 1.5087121725082397, 1.5271307229995728, 1.9350378513336182, 1.4840672016143799, 1.470288872718811, 1.1982659101486206, 1.6299952268600464]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.2857718467712402, 1.8282907009124756, 1.039376139640808, 1.4558712244033813, 1.6480350494384766, 1.4937973022460938, 1.4829308986663818, 1.4511127471923828, 1.537500023841858, 1.4991713762283325, 1.5992424488067627, 1.541583776473999, 1.3202178478240967, 0.6012584567070007, 1.2652758359909058, 1.553125023841858]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3748579025268555, 2.390056848526001, 2.287689447402954, 2.149573802947998, 2.5023674964904785, 2.247253894805908, 2.454545497894287, 2.1979167461395264, 2.1276988983154297, 2.138115644454956, 2.7367424964904785, 2.0825283527374268, 2.3714487552642822, 2.457812547683716, 2.2984848022460938, 2.356628894805908]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6894649267196655, 1.6470880508422852, 1.6887784004211426, 1.8046401739120483, 1.2406960725784302, 1.636150598526001, 1.6913352012634277, 1.9887547492980957, 1.7241122722625732, 1.6021779775619507, 1.7063802480697632, 1.5278408527374268, 1.6343276500701904, 1.5897964239120483, 1.923295497894287, 1.9615767002105713]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.7669507265090942, 1.7747902870178223, 1.6939512491226196, 1.744531273841858, 1.7714725732803345, 1.8445686101913452, 1.7419008016586304, 1.7258522510528564, 1.8183430433273315, 1.835641622543335, 2.277059555053711, 1.7169655561447144, 1.658901572227478, 1.788103699684143, 1.8100401163101196, 1.8288241624832153]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6153165102005005, 1.6375635862350464, 1.6119377613067627, 1.6735262870788574, 1.5655362606048584, 1.5723236799240112, 1.7744228839874268, 1.672667384147644, 1.569762110710144, 1.6404355764389038, 1.2363636493682861, 1.6158099174499512, 1.6547452211380005, 1.8764811754226685, 1.6201527118682861, 1.6329219341278076]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9997632503509521, 2.013399600982666, 2.334031820297241, 1.975568175315857, 2.0927910804748535, 2.0422348976135254, 2.5279829502105713, 2.1060843467712402, 2.005042552947998, 1.9899147748947144, 2.0012309551239014, 1.7997395992279053, 1.997496485710144, 1.8551136255264282, 2.2462594509124756, 1.9554095268249512]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.766761302947998, 5.056297302246094, 4.663778305053711, 4.597312927246094, 4.657315254211426, 4.589109897613525, 4.7749528884887695, 4.826751708984375, 4.922679901123047, 4.713281154632568, 4.654284954071045, 4.618655204772949, 4.647644519805908, 4.749431610107422, 4.8695549964904785, 4.674171447753906]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.511671304702759, 3.4205965995788574, 3.3673768043518066, 3.2596678733825684, 3.471235752105713, 3.5232717990875244, 3.1554510593414307, 3.4376420974731445, 3.5698981285095215, 3.466015577316284, 3.353551149368286, 2.9114251136779785, 3.254554271697998, 3.5029356479644775, 3.5082623958587646, 3.4242186546325684]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.68719220161438, 2.7030067443847656, 2.684138298034668, 2.7043559551239014, 2.627462148666382, 2.672443151473999, 2.830350399017334, 2.396804094314575, 2.620312452316284, 2.4599905014038086, 3.0263731479644775, 1.9516875743865967, 2.6651041507720947, 2.5551846027374268, 2.9851326942443848, 2.6109611988067627]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.08909290283918381, 0.10183892399072647, 0.10593828558921814, -0.2572775185108185, -0.23246383666992188, -0.10052322596311569, 0.11567053943872452, -0.09088371694087982, 0.08078998327255249, 0.08619187772274017, 0.09667336195707321, 0.06394337117671967, 0.08476196974515915, 0.10937462747097015, -1.085231900215149, 0.11344258487224579]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08183356374502182, 0.05249474570155144, 0.0431329682469368, 0.05029691010713577, 0.07071685791015625, 0.029313616454601288, 0.054937735199928284, 0.09214150160551071, 0.02016582153737545, 0.07286006957292557, -0.19122016429901123, 0.04941045865416527, 0.007142508868128061, -0.01630553975701332, 0.035157460719347, 0.024066809564828873]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08459410071372986, 0.06379341334104538, 0.10532183945178986, 0.07183632999658585, 0.11327303946018219, 0.10524652153253555, 0.06844008713960648, -0.09049015492200851, 0.07687871158123016, 0.10025171935558319, 0.03247298300266266, 0.08353598415851593, -0.2075493037700653, 0.008544317446649075, -0.06395041942596436, 0.09843898564577103]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12404334545135498, 0.1353623867034912, 0.13140739500522614, 0.13980484008789062, 0.1427067071199417, 0.10392463207244873, -0.02349732629954815, 0.17707090079784393, 0.17389492690563202, -0.5367156267166138, 0.03652944043278694, 0.0657171830534935, 0.2583639621734619, -0.31783461570739746, -0.0011529456824064255, -0.06545359641313553]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07202675193548203, 0.1095438152551651, 0.0740617886185646, 0.19800782203674316, 0.04464033246040344, -0.05852489918470383, 0.02222037874162197, 0.05396779999136925, -0.10899292677640915, 0.021597769111394882, -0.193385049700737, 0.13838279247283936, -0.004455938469618559, -0.18236753344535828, 0.12954214215278625, -0.0854284018278122]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07135428488254547, 0.14058052003383636, 0.07266877591609955, 0.14297224581241608, -0.10713288933038712, -0.053557395935058594, 0.04340669512748718, -0.03595389425754547, 0.1970355361700058, -0.07687508314847946, -0.13779951632022858, 0.07383109629154205, -0.20472903549671173, 0.14542098343372345, 0.131190225481987, 0.0806838721036911]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11467240750789642, -0.020678451284766197, 0.1019342914223671, 0.3000643253326416, 0.18287114799022675, 0.153233602643013, -0.1054651066660881, -0.08287402242422104, 0.33320727944374084, 0.20017540454864502, -0.505279541015625, 0.22779133915901184, 0.2302711308002472, -0.24228481948375702, 0.2949329614639282, 0.21142438054084778]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5336672067642212, 0.24332325160503387, 0.22776272892951965, 0.15341037511825562, -0.8565592169761658, 0.28541284799575806, 0.18996433913707733, -0.3464728593826294, 0.5410091280937195, 0.18412524461746216, -0.24774374067783356, 0.3277374804019928, 0.1537853330373764, 0.233983114361763, -0.6417798399925232, -0.3523447811603546]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3654673397541046, 0.32373082637786865, -0.32628053426742554, 0.3107195496559143, 0.38536128401756287, 0.30515697598457336, 0.24100038409233093, -0.294709175825119, 0.17857983708381653, 0.8063648343086243, 0.10711520910263062, 0.05882560834288597, 0.46865472197532654, -0.4717995226383209, -0.5157533884048462, 0.3531001806259155]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3343869745731354, 0.2690199017524719, 0.43627557158470154, 1.05364990234375, 0.28640878200531006, 0.11280673742294312, 0.53489089012146, 0.49835455417633057, 0.6661264300346375, 0.03434009104967117, 0.15324662625789642, 0.6202571392059326, 0.5299801826477051, 0.20649458467960358, 0.7660782933235168, 0.5836271047592163]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0066912174224854, 0.7823396921157837, 0.546528160572052, 0.6784518957138062, 0.9026057720184326, 0.48854249715805054, 0.6170356273651123, 0.7023746967315674, 0.21502777934074402, 0.44722962379455566, 0.8265857100486755, 0.8971155881881714, 0.2457141876220703, 0.24516533315181732, 0.9962430000305176, 0.4044627547264099]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.8968029618263245, 1.0459654331207275, 0.8576169013977051, 0.9961525797843933, 1.090824007987976, 0.2463538944721222, 0.7770914435386658, 0.796668291091919, 0.9115675091743469, 1.3887255191802979, 0.8332117795944214, 0.9094342589378357, 0.25774550437927246, 0.4693795144557953, -0.043243035674095154, 0.7191727757453918]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.13092562556266785, 0.7957227826118469, 1.2241151332855225, 1.087390422821045, 0.17660820484161377, 0.6192850470542908, 0.9491428136825562, 1.1943418979644775, 0.40238431096076965, 0.056761205196380615, 0.8784328699111938, 0.8505383133888245, 0.7059188485145569, 0.8468762040138245, 0.6172173023223877, 0.8623761534690857]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6830965280532837, 0.9408078193664551, 1.116385817527771, 0.4382277727127075, 1.0315953493118286, -0.294240802526474, 1.055675983428955, 1.8911728858947754, 0.9706852436065674, 0.9373094439506531, 1.210642695426941, 1.0700504779815674, 0.05377532169222832, 0.7176105976104736, 1.0860878229141235, 0.5743579268455505]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7581474781036377, 0.7400736212730408, 0.993854820728302, 0.8688250184059143, 1.0495188236236572, 1.2706544399261475, 0.10408355295658112, 1.3710105419158936, 0.7308364510536194, 0.7388164401054382, -0.4890523850917816, 0.6604778170585632, 0.938954770565033, 1.0847346782684326, 1.1233208179473877, 0.43917325139045715]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.39185574650764465, 1.0235029458999634, 0.9516363143920898, 0.9881026148796082, 0.753398597240448, 0.8981993198394775, 1.421387791633606, 0.9659155607223511, 0.8285418152809143, 1.0001786947250366, 0.6298258900642395, -0.3628796935081482, 1.082021951675415, 0.84033203125, 0.7878239154815674, 1.3358582258224487]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6534032821655273, 0.20114247500896454, 0.8526402711868286, 0.8291849493980408, 0.7524697184562683, 0.37064602971076965, 0.16963130235671997, 0.8333204388618469, -1.0722789764404297, 2.2162997722625732, -0.8715823888778687, 0.49753281474113464, 0.7963926792144775, 0.7716362476348877, 0.7993476390838623, 0.11209962517023087]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.024693187326192856, 0.7070428133010864, 1.512707233428955, 1.2232457399368286, 1.5300352573394775, 1.4543874263763428, 1.3128691911697388, 1.3704149723052979, 1.9579601287841797, 1.190944790840149, 2.1545350551605225, 1.4679282903671265, 1.4262233972549438, 1.3054676055908203, 0.442169189453125, 1.1409138441085815]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4040110111236572, 1.2136409282684326, 1.4241199493408203, 1.7086985111236572, 0.843342125415802, 1.17763090133667, 1.7812023162841797, 1.688881516456604, 2.35349178314209, 1.4173494577407837, 1.1223828792572021, 0.6954224705696106, 1.148854374885559, 0.8040187358856201, 1.3254215717315674, 1.4548637866973877]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.4459556341171265, 1.2635527849197388, 1.436618685722351, 1.3271245956420898, 1.5018101930618286, 0.917339026927948, 1.2571991682052612, 1.227488398551941, 1.8611911535263062, 1.4088462591171265, 1.5505669116973877, 1.4088104963302612, 1.6290119886398315, 1.5005478858947754, 1.3956745862960815, 1.371653437614441]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  19%|█████████▌                                        | 97/504 [05:37<22:58,  3.39s/it]Layer: gate_20 - Captured router_logits: [1.0875535011291504, 1.0669541358947754, 1.0381513833999634, 1.0780386924743652, -0.23939290642738342, 1.2602062225341797, 1.1539753675460815, 1.6877344846725464, 0.780585527420044, 1.2303615808486938, 1.125803828239441, 1.1372487545013428, 1.0737066268920898, 1.1957411766052246, 0.7907059788703918, 0.3205101191997528]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.003572702407837, 2.1687309741973877, 2.1113758087158203, 1.9489328861236572, 1.921946406364441, 2.0689311027526855, 1.8882907629013062, 2.117521047592163, 1.8935308456420898, 2.006026029586792, 2.265767812728882, 2.5811262130737305, 1.5915229320526123, 2.2271342277526855, 2.07907772064209, 1.9915682077407837]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4122523069381714, 1.4128239154815674, 1.9115853309631348, 1.0900015830993652, 1.4570789337158203, 1.5985851287841797, 1.3494306802749634, 1.4321885108947754, 1.5183165073394775, 1.5432069301605225, 1.5669541358947754, 2.0072884559631348, 1.5075504779815674, 1.5207698345184326, 1.2606558799743652, 1.6199980974197388]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.2528343200683594, 1.832436203956604, 1.0478506088256836, 1.4358564615249634, 1.6012290716171265, 1.4684641361236572, 1.4375476837158203, 1.4284727573394775, 1.5049304962158203, 1.4757050275802612, 1.5648342370986938, 1.4967488050460815, 1.3137861490249634, 0.6360384225845337, 1.2626953125, 1.5012624263763428]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3911490440368652, 2.4237327575683594, 2.3373665809631348, 2.149533271789551, 2.5611186027526855, 2.2645769119262695, 2.478515625, 2.218083143234253, 2.179830312728882, 2.1843559741973877, 2.7662441730499268, 2.1158535480499268, 2.3573741912841797, 2.478325128555298, 2.3078792095184326, 2.401867389678955]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7438310384750366, 1.6998142004013062, 1.728515625, 1.8276724815368652, 1.2852991819381714, 1.6796875, 1.7205126285552979, 2.0386815071105957, 1.7569788694381714, 1.6318597793579102, 1.7459269762039185, 1.5720988512039185, 1.6577744483947754, 1.6517244577407837, 1.9929258823394775, 2.0275580883026123]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.7786537408828735, 1.7917990684509277, 1.6902748346328735, 1.733803391456604, 1.754233717918396, 1.8194280862808228, 1.7134655714035034, 1.7300876379013062, 1.812221646308899, 1.830161452293396, 2.3073372840881348, 1.6926209926605225, 1.639243483543396, 1.7527153491973877, 1.7983167171478271, 1.811801791191101]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6165637969970703, 1.6682590246200562, 1.6130602359771729, 1.683398723602295, 1.5865508317947388, 1.5760200023651123, 1.7884342670440674, 1.6832892894744873, 1.570719599723816, 1.6469249725341797, 1.2770103216171265, 1.6143218278884888, 1.6498874425888062, 1.8890351057052612, 1.6295762062072754, 1.633473515510559]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.009753704071045, 2.0466368198394775, 2.3425471782684326, 1.9856849908828735, 2.12229061126709, 2.052436590194702, 2.55078125, 2.11525821685791, 2.0093607902526855, 2.0002143383026123, 2.0002143383026123, 1.819407343864441, 1.9662073850631714, 1.8723561763763428, 2.2627906799316406, 1.9638195037841797]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.812690734863281, 5.132193088531494, 4.746855735778809, 4.701540946960449, 4.7162370681762695, 4.629049301147461, 4.820407867431641, 4.9162774085998535, 4.972513198852539, 4.779011249542236, 4.708889007568359, 4.6685404777526855, 4.723811626434326, 4.8192644119262695, 4.937595367431641, 4.735160827636719]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.583674669265747, 3.489424467086792, 3.431974172592163, 3.327535390853882, 3.537776231765747, 3.561356782913208, 3.2341368198394775, 3.5033583641052246, 3.6561546325683594, 3.541849374771118, 3.424494981765747, 3.019900321960449, 3.350919485092163, 3.544779062271118, 3.5745761394500732, 3.496593952178955]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.712557077407837, 2.7357089519500732, 2.690548896789551, 2.6810214519500732, 2.632478952407837, 2.7096035480499268, 2.797208547592163, 2.445431709289551, 2.6550590991973877, 2.5149104595184326, 3.0728847980499268, 2.0161476135253906, 2.6913585662841797, 2.5842702388763428, 3.0469703674316406, 2.6411967277526855]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.058511700481176376, 0.07486771792173386, 0.08701857924461365, -0.28186091780662537, -0.26920846104621887, -0.08399832248687744, 0.0868658497929573, -0.06363308429718018, 0.05806531012058258, 0.059166304767131805, 0.0676012635231018, 0.040924787521362305, 0.06880674511194229, 0.09293463826179504, -1.0517997741699219, 0.10000029951334]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.0738789364695549, 0.02610652521252632, 0.025310048833489418, 0.04529424011707306, 0.06092076376080513, 0.012746951542794704, 0.03875505551695824, 0.09354379773139954, 0.053493548184633255, 0.06265521049499512, -0.19857965409755707, 0.057357367128133774, 0.004685103427618742, -0.02221742831170559, 0.03443433716893196, 0.037405237555503845]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08999147266149521, 0.04087483137845993, 0.07864291220903397, 0.052776314318180084, 0.1422281265258789, 0.07939541339874268, 0.07714062184095383, -0.09702081233263016, 0.07497841864824295, 0.12836943566799164, 0.03780294954776764, 0.08496887236833572, -0.2193891853094101, 0.014745455235242844, -0.027959156781435013, 0.09266699105501175]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13066396117210388, 0.13827720284461975, 0.12335783243179321, 0.13800479471683502, 0.11834876239299774, 0.11422471702098846, 0.0004397901357151568, 0.15258899331092834, 0.1830921769142151, -0.5744284391403198, 0.056552160531282425, 0.06285116076469421, 0.3083473742008209, -0.3343142569065094, 0.08488454669713974, -0.045798927545547485]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.05496005341410637, 0.10089261084794998, 0.055077582597732544, 0.26483049988746643, 0.08799835294485092, -0.05990476533770561, 0.03109670989215374, 0.008254138752818108, -0.1071752980351448, 0.010258774273097515, -0.26381024718284607, 0.11256614327430725, 0.10214963555335999, -0.22297032177448273, 0.15359628200531006, -0.11732622981071472]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.05912869796156883, 0.13914382457733154, 0.07805170118808746, 0.2287769913673401, -0.1560470461845398, -0.1037556454539299, 0.07469017803668976, -0.04061908274888992, 0.2413487285375595, -0.10520532727241516, -0.15824824571609497, 0.12041950970888138, -0.25423258543014526, 0.19268986582756042, 0.12183422595262527, 0.09697140753269196]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.12483440339565277, 0.009076733142137527, 0.1725109964609146, 0.269535094499588, 0.19138753414154053, 0.1533903330564499, -0.15059202909469604, -0.04881089925765991, 0.3827151358127594, 0.18454989790916443, -0.5906456112861633, 0.2638825476169586, 0.2752258777618408, -0.29776933789253235, 0.3681621849536896, 0.22148582339286804]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5962381958961487, 0.20825885236263275, 0.2264965921640396, 0.1595241278409958, -0.9328209161758423, 0.28398188948631287, 0.2071334719657898, -0.4079870581626892, 0.6563383936882019, 0.1645069718360901, -0.1930733025074005, 0.2784040570259094, 0.06788139045238495, 0.29100215435028076, -0.6810834407806396, -0.34717610478401184]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3118683099746704, 0.29561591148376465, -0.2535330057144165, 0.3161194324493408, 0.4266008138656616, 0.39078038930892944, 0.26799920201301575, -0.3100305199623108, 0.1753995418548584, 0.9815119504928589, 0.09366291761398315, 0.2295449823141098, 0.543831467628479, -0.5006939768791199, -0.46115541458129883, 0.487469345331192]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.42371729016304016, 0.23269616067409515, 0.4848153591156006, 1.2236298322677612, 0.36552783846855164, 0.24928928911685944, 0.685075044631958, 0.6446421146392822, 1.0359591245651245, 0.12956471741199493, 0.19961117208003998, 0.7131901979446411, 0.5671263933181763, 0.09111734479665756, 0.8649647831916809, 0.6808527708053589]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0715256929397583, 0.9350705742835999, 0.5084385871887207, 0.688194990158081, 0.904481828212738, 0.7313271760940552, 0.7170799374580383, 0.7936158180236816, 0.11904654651880264, 0.5489925146102905, 0.8052866458892822, 1.0759053230285645, 0.21345622837543488, 0.16398920118808746, 1.2040832042694092, 0.515180230140686]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [1.0706120729446411, 1.4036378860473633, 0.9274108409881592, 1.2483224868774414, 1.3702579736709595, 0.2815512418746948, 1.0014678239822388, 0.9507255554199219, 1.0734938383102417, 1.7766786813735962, 0.9247986674308777, 1.0846145153045654, 0.28838858008384705, 0.4920184314250946, 0.005086887162178755, 0.8224858045578003]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.1845141500234604, 0.8815273642539978, 1.360225796699524, 1.2908837795257568, 0.40184274315834045, 0.6220388412475586, 1.2840553522109985, 1.582594394683838, 0.46514472365379333, 0.10545901209115982, 0.9915134906768799, 0.9077238440513611, 0.8997945189476013, 0.9600987434387207, 0.7138911485671997, 0.9866876006126404]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8832947611808777, 1.2119140625, 1.1726717948913574, 0.5568435788154602, 1.2857553958892822, -0.23135319352149963, 1.2486145496368408, 2.3261120319366455, 1.4189332723617554, 1.0347728729248047, 1.74765145778656, 1.3186529874801636, 0.18685294687747955, 0.9384436011314392, 1.1805981397628784, 0.7611622214317322]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8952816724777222, 0.8754613399505615, 1.2326854467391968, 1.013336420059204, 1.2214280366897583, 1.5498214960098267, 0.37019506096839905, 1.9615830183029175, 0.851334810256958, 0.86332768201828, -0.3530520498752594, 0.9721919298171997, 1.02768075466156, 1.2187260389328003, 1.2788594961166382, 0.6503496170043945]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.48639702796936035, 1.2853479385375977, 1.0505056381225586, 1.0823367834091187, 0.935075044631958, 0.9661977291107178, 1.854129672050476, 1.1500312089920044, 1.097033143043518, 1.100867509841919, 0.7978942394256592, -0.40681540966033936, 1.2277559041976929, 1.0116229057312012, 1.0297162532806396, 1.6805273294448853]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.5960437059402466, 0.10294534265995026, 0.7609129548072815, 0.7945594191551208, 0.7404680252075195, 0.4822571277618408, 0.7078161239624023, 0.8547902703285217, -0.9628415703773499, 2.1590778827667236, -0.7996550798416138, 0.6019938588142395, 0.8291555047035217, 0.8095459938049316, 0.8337986469268799, 0.45439738035202026]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.15693973004817963, 0.759361207485199, 1.5245099067687988, 1.4088703393936157, 1.614880919456482, 1.513611912727356, 1.6182929277420044, 1.4736511707305908, 2.1173830032348633, 1.3836348056793213, 2.580198049545288, 1.8466497659683228, 1.5638630390167236, 1.4137330055236816, 0.5906649231910706, 1.329790711402893]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6604198217391968, 1.4054831266403198, 1.5910240411758423, 2.024252414703369, 1.0157058238983154, 1.2825140953063965, 2.358464241027832, 1.9727760553359985, 2.7035324573516846, 1.6096625328063965, 1.3715670108795166, 1.0352824926376343, 1.3599696159362793, 0.9232915639877319, 1.6548360586166382, 1.6919574737548828]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.6912624835968018, 1.4854294061660767, 1.913211703300476, 1.509070634841919, 1.8191622495651245, 1.2206919193267822, 1.4953268766403198, 1.4720691442489624, 2.4017746448516846, 1.5612537860870361, 1.8744966983795166, 1.6698260307312012, 1.8032256364822388, 1.7152271270751953, 1.7294622659683228, 1.6747747659683228]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.4233368635177612, 1.153194546699524, 1.0842360258102417, 1.0419561862945557, -0.02647886611521244, 1.5107061862945557, 1.2722392082214355, 1.977380633354187, 0.8443319201469421, 1.4158837795257568, 1.2656129598617554, 1.2529476881027222, 1.199853777885437, 1.336632490158081, 1.020963191986084, 0.41207006573677063]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.2094037532806396, 2.4013612270355225, 2.3239550590515137, 2.098111629486084, 2.1155338287353516, 2.2866179943084717, 2.099501609802246, 2.4079275131225586, 2.1348016262054443, 2.282064914703369, 2.4960696697235107, 2.8030099868774414, 1.8172210454940796, 2.552482843399048, 2.3216545581817627, 2.2429542541503906]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4735908508300781, 1.5225028991699219, 2.1942100524902344, 1.1018590927124023, 1.5480972528457642, 1.6779141426086426, 1.357008457183838, 1.4904381036758423, 1.5844995975494385, 1.6721506118774414, 1.6905195713043213, 2.186253786087036, 1.5733799934387207, 1.6700537204742432, 1.2825920581817627, 1.7440087795257568]
Layer: gate_22 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.518716335296631, 1.9563602209091187, 1.2102545499801636, 1.5715347528457642, 1.8130033016204834, 1.5500143766403198, 1.5416746139526367, 1.4868193864822388, 1.5948164463043213, 1.6118673086166382, 1.5739072561264038, 1.6519722938537598, 1.3186589479446411, 0.735680341720581, 1.418424129486084, 1.6151025295257568]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  20%|█████████▊                                       | 101/504 [05:51<22:41,  3.38s/it]Layer: gate_24 - Captured router_logits: [2.5405962467193604, 2.5238208770751953, 2.4671683311462402, 2.1983320713043213, 2.628786325454712, 2.2837424278259277, 2.528853416442871, 2.2048983573913574, 2.177866220474243, 2.2388803958892822, 3.023197889328003, 2.2291507720947266, 2.4285850524902344, 2.5377204418182373, 2.373753786087036, 2.343989610671997]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7887749671936035, 1.759537935256958, 1.6592216491699219, 1.898605227470398, 1.381194829940796, 1.7669191360473633, 1.8315998315811157, 2.116947889328003, 1.7935439348220825, 1.6630560159683228, 1.740006685256958, 1.6012989282608032, 1.7326735258102417, 1.5807850360870361, 2.282639980316162, 2.083085775375366]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_26 - Captured router_logits: [1.8288439512252808, 2.062049150466919, 1.8628498315811157, 1.834619402885437, 1.8534437417984009, 1.9922069311141968, 1.8322018384933472, 1.8273820877075195, 1.9080294370651245, 1.9063278436660767, 2.5456767082214355, 1.747016429901123, 1.8503162860870361, 1.8752875328063965, 1.960697889328003, 1.908215880393982]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6645597219467163, 1.7277143001556396, 1.6528230905532837, 1.708549976348877, 1.5905267000198364, 1.6172631978988647, 1.8614068031311035, 1.743820071220398, 1.6197726726531982, 1.7097631692886353, 1.3121885061264038, 1.6506931781768799, 1.744703769683838, 2.090613603591919, 1.6656562089920044, 1.6241612434387207]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0970091819763184, 2.1779141426086426, 2.404740333557129, 2.034125804901123, 2.186134099960327, 2.099837064743042, 2.687835454940796, 2.2037241458892822, 2.083852529525757, 2.0343174934387207, 2.032376289367676, 1.875862717628479, 1.9781920909881592, 1.8757908344268799, 2.3717408180236816, 2.0273916721343994]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.949817657470703, 5.468414306640625, 4.9340009689331055, 4.787384986877441, 4.849621295928955, 4.806604862213135, 4.950201511383057, 5.075992107391357, 5.116755962371826, 4.917153835296631, 4.921491622924805, 4.841640949249268, 4.890026092529297, 5.069258213043213, 5.13353157043457, 4.919670104980469]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.4892399311065674, 3.373274564743042, 3.3236916065216064, 3.148838996887207, 3.3958733081817627, 3.374281167984009, 3.0381758213043213, 3.508867025375366, 3.5825345516204834, 3.335026741027832, 3.218965768814087, 2.8256492614746094, 3.092719554901123, 3.421395778656006, 3.423145055770874, 3.4252419471740723]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.5282304286956787, 2.6539013385772705, 2.705521583557129, 2.684144973754883, 2.6118674278259277, 2.5888612270355225, 2.6928679943084717, 2.265505075454712, 2.596721649169922, 2.3376870155334473, 3.094229221343994, 1.6946752071380615, 2.5364742279052734, 2.4539637565612793, 3.081000804901123, 2.5454370975494385]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06423931568861008, 0.07851732522249222, 0.09123482555150986, -0.28675270080566406, -0.26761481165885925, -0.08338057994842529, 0.09293574094772339, -0.07180730253458023, 0.06176196038722992, 0.06597994267940521, 0.07182297855615616, 0.03999267891049385, 0.0721883475780487, 0.10194551199674606, -1.0579766035079956, 0.10068301111459732]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07904979586601257, 0.04142051935195923, 0.028681984171271324, 0.04905403405427933, 0.06756202131509781, 0.020647073164582253, 0.05134297162294388, 0.10003124177455902, 0.059953585267066956, 0.06816845387220383, -0.1852683275938034, 0.05956520885229111, 0.0016370902303606272, -0.014677644707262516, 0.039406899362802505, 0.042006369680166245]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08969762176275253, 0.03727656602859497, 0.08441995084285736, 0.057076554745435715, 0.14683158695697784, 0.09238909184932709, 0.07678981125354767, -0.09307730197906494, 0.09469959884881973, 0.13727648556232452, 0.03451598808169365, 0.07796978950500488, -0.20908154547214508, 0.011165314354002476, -0.018887946382164955, 0.09084399789571762]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13065792620182037, 0.13633905351161957, 0.12320021539926529, 0.14274896681308746, 0.1334543079137802, 0.11397023499011993, 0.004390283487737179, 0.1552066057920456, 0.1907840073108673, -0.5651084184646606, 0.05157627537846565, 0.0621504969894886, 0.2841968834400177, -0.3072153925895691, 0.09799877554178238, -0.047998346388339996]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.052656207233667374, 0.09791279584169388, 0.0634656548500061, 0.26418787240982056, 0.07907114177942276, -0.04139929637312889, 0.036303866654634476, 0.03043867088854313, -0.11610309779644012, -0.0007743601454421878, -0.2571624517440796, 0.11313208192586899, 0.0990922600030899, -0.21132898330688477, 0.14387090504169464, -0.10786858946084976]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.04254627972841263, 0.14883451163768768, 0.09515684843063354, 0.23897922039031982, -0.1590617597103119, -0.10760746151208878, 0.07621952146291733, -0.02303323522210121, 0.23113368451595306, -0.09384520351886749, -0.1937709003686905, 0.12522204220294952, -0.25068148970603943, 0.19995005428791046, 0.11506980657577515, 0.10304133594036102]
Layer: gate_5 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.16668806970119476, 0.024881936609745026, 0.1825595200061798, 0.2728668451309204, 0.1960984617471695, 0.1709059327840805, -0.15358991920948029, -0.058943673968315125, 0.361237496137619, 0.19162267446517944, -0.5832463502883911, 0.26272058486938477, 0.274818480014801, -0.32661494612693787, 0.3267368674278259, 0.20927223563194275]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5596620440483093, 0.20651088654994965, 0.22799129784107208, 0.1594897359609604, -0.873749315738678, 0.29153892397880554, 0.20381660759449005, -0.3977620005607605, 0.6352196335792542, 0.16836847364902496, -0.20762325823307037, 0.26862677931785583, 0.07473590970039368, 0.2963477671146393, -0.6867720484733582, -0.4058499038219452]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2916983962059021, 0.28985339403152466, -0.2619917094707489, 0.295974463224411, 0.41977807879447937, 0.37597355246543884, 0.2675296366214752, -0.316032737493515, 0.23610766232013702, 0.9601195454597473, 0.10568368434906006, 0.1995023936033249, 0.5278709530830383, -0.518280565738678, -0.49614468216896057, 0.4870631694793701]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4363740384578705, 0.26283085346221924, 0.4841653108596802, 1.186632752418518, 0.4044578969478607, 0.21880598366260529, 0.6777437329292297, 0.6300954818725586, 1.074449062347412, 0.17753815650939941, 0.20335285365581512, 0.7094995975494385, 0.557572603225708, 0.1055496335029602, 0.8379086256027222, 0.6652427911758423]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0288434028625488, 0.9116600155830383, 0.49631091952323914, 0.6916818618774414, 0.903778612613678, 0.7434303164482117, 0.6681933999061584, 0.777229905128479, 0.12989161908626556, 0.5852949619293213, 0.7969768643379211, 1.045333743095398, 0.22736456990242004, 0.23897922039031982, 1.2136253118515015, 0.5419667363166809]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [1.0739483833312988, 1.3817161321640015, 0.9425685405731201, 1.2800847291946411, 1.322981595993042, 0.22691559791564941, 1.069503664970398, 0.9273015260696411, 1.0862669944763184, 1.7345397472381592, 0.9336791038513184, 1.0864670276641846, 0.30622684955596924, 0.5128061771392822, 0.04572431743144989, 0.8173438906669617]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.1287323236465454, 0.8311805129051208, 1.3142014741897583, 1.2418760061264038, 0.42455753684043884, 0.5801799893379211, 1.2559694051742554, 1.5213196277618408, 0.41424936056137085, 0.01962411403656006, 0.9543471932411194, 0.8442652225494385, 0.8696798086166382, 0.9144698977470398, 0.713699460029602, 0.9412145614624023]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8348143696784973, 1.1381019353866577, 1.1541471481323242, 0.4993392825126648, 1.2523905038833618, -0.275184690952301, 1.199051022529602, 2.313650369644165, 1.3757429122924805, 0.9746153950691223, 1.6806939840316772, 1.2722752094268799, 0.23184877634048462, 0.9405614733695984, 1.1955304145812988, 0.7339690923690796]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.9063548445701599, 0.8316837549209595, 1.2009083032608032, 0.9696486592292786, 1.168496012687683, 1.521070957183838, 0.4397601783275604, 1.9963963031768799, 0.8437230587005615, 0.8329651355743408, -0.3663105368614197, 0.941520094871521, 0.9968224167823792, 1.176476240158081, 1.2233272790908813, 0.5990492105484009]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4805637300014496, 1.2325836420059204, 1.0379960536956787, 1.0866204500198364, 0.8662319183349609, 0.9260448813438416, 1.858659029006958, 1.122591495513916, 1.0439033508300781, 1.0567364692687988, 0.7941010594367981, -0.32163017988204956, 1.1678804159164429, 0.9542273879051208, 1.0227904319763184, 1.6670341491699219]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.5645424127578735, 0.11008775979280472, 0.7470598220825195, 0.7699401378631592, 0.7258128523826599, 0.47133293747901917, 0.6698192954063416, 0.792435884475708, -0.8705006241798401, 2.171126127243042, -0.8053922057151794, 0.5604742169380188, 0.8110740780830383, 0.7871962189674377, 0.8103342056274414, 0.45700541138648987]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.07838936150074005, 0.77522212266922, 1.4540895223617554, 1.3402960300445557, 1.5334068536758423, 1.4780362844467163, 1.5478709936141968, 1.3981618881225586, 2.047520637512207, 1.3525944948196411, 2.4756877422332764, 1.7684693336486816, 1.5153359174728394, 1.3921467065811157, 0.539567232131958, 1.2494244575500488]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6171875, 1.369607925415039, 1.5538487434387207, 1.9783118963241577, 0.9947408437728882, 1.2470043897628784, 2.3127875328063965, 1.9180885553359985, 2.5803537368774414, 1.6458253860473633, 1.244589924812317, 0.9544127583503723, 1.3145774602890015, 0.8631649613380432, 1.5814800262451172, 1.59530770778656]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.6531585454940796, 1.4707151651382446, 1.8987969160079956, 1.4913606643676758, 1.7898054122924805, 1.1637060642242432, 1.4211201667785645, 1.4107613563537598, 2.3710217475891113, 1.5966497659683228, 1.838417887687683, 1.6212135553359985, 1.7195287942886353, 1.6769195795059204, 1.7068156003952026, 1.6552435159683228]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.4069404602050781, 1.1833840608596802, 1.075716495513916, 0.9524060487747192, -0.04878880828619003, 1.5071414709091187, 1.245398759841919, 1.9241276979446411, 0.835250735282898, 1.3877372741699219, 1.225699782371521, 1.2302261590957642, 1.1431652307510376, 1.2845091819763184, 0.9889013171195984, 0.4014761447906494]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.1358559131622314, 2.305742025375366, 2.2669191360473633, 1.9922354221343994, 1.957893967628479, 2.1844325065612793, 1.9855493307113647, 2.325392961502075, 2.1634395122528076, 2.160611629486084, 2.4148292541503906, 2.716689109802246, 1.7037241458892822, 2.4784317016601562, 2.212543249130249, 2.1767637729644775]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4305741786956787, 1.508842945098877, 2.171970844268799, 1.065841555595398, 1.5012701749801636, 1.651481032371521, 1.2816814184188843, 1.4810439348220825, 1.55859375, 1.602664828300476, 1.6694306135177612, 2.178537130355835, 1.5364503860473633, 1.6554591655731201, 1.2260712385177612, 1.6797114610671997]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.500119924545288, 1.934216856956482, 1.1702154874801636, 1.5520274639129639, 1.7845571041107178, 1.517014980316162, 1.5897958278656006, 1.4599789381027222, 1.5628833770751953, 1.573883295059204, 1.547402262687683, 1.6445192098617554, 1.2576687335968018, 0.6706026196479797, 1.3700153827667236, 1.5483368635177612]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.516104221343994, 2.507908344268799, 2.450920343399048, 2.1758053302764893, 2.5783166885375977, 2.2442004680633545, 2.4830808639526367, 2.1792562007904053, 2.1425421237945557, 2.201303720474243, 2.9862921237945557, 2.1666507720947266, 2.406968832015991, 2.512269973754883, 2.337662935256958, 2.266871213912964]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7192533016204834, 1.6806939840316772, 1.5755128860473633, 1.8373754024505615, 1.29922354221344, 1.7164733409881592, 1.740701675415039, 2.038583278656006, 1.7335362434387207, 1.6183618307113647, 1.6922210454940796, 1.5048408508300781, 1.6779381036758423, 1.4982506036758423, 2.226658344268799, 2.0201303958892822]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_26 - Captured router_logits: [1.793496012687683, 2.041637420654297, 1.7499520778656006, 1.7777031660079956, 1.7899372577667236, 1.9678243398666382, 1.8750312328338623, 1.7853718996047974, 1.875641107559204, 1.8666542768478394, 2.502755880355835, 1.7018908262252808, 1.7872891426086426, 1.828292727470398, 1.935795545578003, 1.8948771953582764]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6549932956695557, 1.709404468536377, 1.6280345916748047, 1.6670647859573364, 1.5595762729644775, 1.6168853044509888, 1.8430317640304565, 1.7128965854644775, 1.6111068725585938, 1.6820061206817627, 1.274827480316162, 1.6300865411758423, 1.7488527297973633, 2.074549674987793, 1.6180202960968018, 1.594233751296997]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  21%|██████████▏                                      | 105/504 [06:04<22:18,  3.35s/it]Layer: gate_28 - Captured router_logits: [2.116516590118408, 2.1868770122528076, 2.4021880626678467, 2.023341655731201, 2.195528268814087, 2.1247124671936035, 2.6810054779052734, 2.2340633869171143, 2.1063315868377686, 2.033454656600952, 2.032160758972168, 1.8608129024505615, 1.9598710536956787, 1.8732266426086426, 2.3820695877075195, 2.0346291065216064]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.901360988616943, 5.458780765533447, 4.898293495178223, 4.723950386047363, 4.773329734802246, 4.971314430236816, 4.910995006561279, 5.037240982055664, 5.082294940948486, 4.83751916885376, 4.860261917114258, 4.838956832885742, 4.840886116027832, 5.036857604980469, 5.109230995178223, 4.888180732727051]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.407424211502075, 3.3256804943084717, 3.2445120811462402, 3.114546775817871, 3.332390785217285, 3.3167178630828857, 2.952105760574341, 3.462207555770874, 3.4988975524902344, 3.2781825065612793, 3.1712520122528076, 2.691936492919922, 2.996370553970337, 3.3388612270355225, 3.3603336811065674, 3.3083016872406006]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.5612058639526367, 2.6630799770355225, 2.7850842475891113, 2.6919095516204834, 2.671060085296631, 2.6303679943084717, 2.705665349960327, 2.274348258972168, 2.633052110671997, 2.333852529525757, 3.1362154483795166, 1.686204433441162, 2.6150786876678467, 2.5083396434783936, 3.1238975524902344, 2.5877349376678467]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.08820134401321411, 0.09857016801834106, 0.10522551089525223, -0.26760920882225037, -0.24298758804798126, -0.10395973920822144, 0.11536426097154617, -0.08588550239801407, 0.07936077564954758, 0.0838816836476326, 0.09646397829055786, 0.05898911505937576, 0.0848112553358078, 0.10774363577365875, -1.0816036462783813, 0.11298460513353348]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08338113129138947, 0.05104086175560951, 0.042380575090646744, 0.049282535910606384, 0.07147273421287537, 0.027279041707515717, 0.052524589002132416, 0.0956011489033699, 0.026540815830230713, 0.07721425592899323, -0.20187027752399445, 0.04630950093269348, 0.013392726890742779, -0.017570149153470993, 0.035152457654476166, 0.022955095395445824]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08345232903957367, 0.06563146412372589, 0.10509809851646423, 0.07606790959835052, 0.11181183159351349, 0.1037929579615593, 0.06812912225723267, -0.0930197611451149, 0.0744328498840332, 0.10473310947418213, 0.035127557814121246, 0.08103857934474945, -0.21002595126628876, 0.00775665370747447, -0.05716037005186081, 0.10126832127571106]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12487413734197617, 0.14016635715961456, 0.12769289314746857, 0.13899193704128265, 0.1412719339132309, 0.1059919074177742, -0.014100448228418827, 0.17246520519256592, 0.1823374181985855, -0.5432773232460022, 0.04163360595703125, 0.06392039358615875, 0.2651737630367279, -0.30217260122299194, 0.003672582097351551, -0.06893029808998108]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06606413424015045, 0.10895486176013947, 0.0711839571595192, 0.20099735260009766, 0.04490922391414642, -0.06936889886856079, 0.02784496732056141, 0.051327627152204514, -0.10155247896909714, 0.029216505587100983, -0.20545116066932678, 0.13892103731632233, 0.007477375213056803, -0.19650216400623322, 0.12892146408557892, -0.0779828354716301]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.0796164870262146, 0.13300134241580963, 0.07725207507610321, 0.14648474752902985, -0.10198263823986053, -0.06500111520290375, 0.05919765681028366, -0.03649115562438965, 0.21077239513397217, -0.09445370733737946, -0.11316733062267303, 0.07361309230327606, -0.216303288936615, 0.14386579394340515, 0.13112910091876984, 0.07577931880950928]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.10854233056306839, -0.013777454383671284, 0.10561702400445938, 0.29464587569236755, 0.19057294726371765, 0.15119601786136627, -0.1312812715768814, -0.09045163542032242, 0.34549349546432495, 0.19564364850521088, -0.5372306704521179, 0.24125662446022034, 0.22474585473537445, -0.2691399157047272, 0.31804490089416504, 0.22350600361824036]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5458388924598694, 0.23213906586170197, 0.22371810674667358, 0.15088629722595215, -0.8895483613014221, 0.2971155345439911, 0.19031624495983124, -0.40185925364494324, 0.5603188276290894, 0.16718874871730804, -0.24686142802238464, 0.31812262535095215, 0.15038228034973145, 0.23234690725803375, -0.6861854791641235, -0.29509928822517395]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.35455891489982605, 0.3249868154525757, -0.3283221423625946, 0.29480379819869995, 0.38494759798049927, 0.29486045241355896, 0.23627837002277374, -0.27501043677330017, 0.17971304059028625, 0.8030888438224792, 0.10160303860902786, 0.08669740706682205, 0.4588751792907715, -0.5158699154853821, -0.5109741687774658, 0.3537791073322296]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3462793529033661, 0.25156038999557495, 0.43394291400909424, 1.08423912525177, 0.27693161368370056, 0.12226682901382446, 0.5139349699020386, 0.4961460530757904, 0.6709591150283813, 0.015346929430961609, 0.1647460162639618, 0.6316357851028442, 0.5604233741760254, 0.15504996478557587, 0.7768842577934265, 0.6168144941329956]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0168519020080566, 0.7982397675514221, 0.5510891079902649, 0.7007381916046143, 0.9055888652801514, 0.5029340386390686, 0.6200853586196899, 0.7090510725975037, 0.17583422362804413, 0.45858442783355713, 0.8282402753829956, 0.9137986302375793, 0.20962846279144287, 0.2328675091266632, 0.9404996037483215, 0.39323890209198]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.8811596035957336, 1.0645668506622314, 0.8449771404266357, 1.0240767002105713, 1.1283164024353027, 0.2883293330669403, 0.7726444005966187, 0.8076353669166565, 0.8906735181808472, 1.3671996593475342, 0.8341845273971558, 0.9016158580780029, 0.24776387214660645, 0.4513053297996521, -0.05348992347717285, 0.7084388732910156]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.12302564829587936, 0.7631259560585022, 1.1737067699432373, 1.0841299295425415, 0.16585981845855713, 0.6282769441604614, 0.9554731249809265, 1.1939477920532227, 0.43823280930519104, 0.1355207860469818, 0.8781920075416565, 0.8255167603492737, 0.6748221516609192, 0.8606244921684265, 0.5935589075088501, 0.8628809452056885]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6909483075141907, 0.9484817981719971, 1.0848714113235474, 0.42810171842575073, 0.9747670888900757, -0.2714928984642029, 1.0461622476577759, 1.8862577676773071, 0.9597046375274658, 0.9201887845993042, 1.2090222835540771, 1.104740858078003, 0.044068899005651474, 0.6633611917495728, 1.0440176725387573, 0.5388039350509644]
Running loglikelihood requests:  22%|██████████▌                                      | 109/504 [06:18<22:19,  3.39s/it]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7378627061843872, 0.7306962013244629, 0.9828373789787292, 0.8430736660957336, 1.034100890159607, 1.2191143035888672, 0.09123428910970688, 1.3087753057479858, 0.7472765445709229, 0.7351665496826172, -0.4798322319984436, 0.6472810506820679, 0.9279982447624207, 1.0841906070709229, 1.1573660373687744, 0.41710469126701355]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.36328351497650146, 1.0522066354751587, 0.9190120100975037, 0.9241071343421936, 0.7212532162666321, 0.8712393045425415, 1.3490357398986816, 0.9351708292961121, 0.8272637128829956, 0.9773146510124207, 0.5803040862083435, -0.40412551164627075, 1.0404975414276123, 0.8263174295425415, 0.744268000125885, 1.3623543977737427]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.6677473783493042, 0.13505545258522034, 0.8404351472854614, 0.8207461833953857, 0.741684079170227, 0.39350104331970215, 0.1914926916360855, 0.8625169992446899, -1.0801236629486084, 2.2200958728790283, -0.8646044731140137, 0.5029637813568115, 0.8056458830833435, 0.7807814478874207, 0.7959893941879272, 0.17436246573925018]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.014585128054022789, 0.6426615118980408, 1.4629513025283813, 1.223443627357483, 1.4829920530319214, 1.4043939113616943, 1.2708171606063843, 1.327833890914917, 1.9032657146453857, 1.1676806211471558, 2.1561286449432373, 1.4675489664077759, 1.3740204572677612, 1.2921255826950073, 0.4037665128707886, 1.1335066556930542]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4040906429290771, 1.1909937858581543, 1.4264302253723145, 1.7025426626205444, 0.8448418378829956, 1.147507905960083, 1.7917556762695312, 1.6547456979751587, 2.380289316177368, 1.381575107574463, 1.1043800115585327, 0.7228833436965942, 1.1271840333938599, 0.7803435921669006, 1.3192145824432373, 1.4323806762695312]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.4040663242340088, 1.2276421785354614, 1.4100348949432373, 1.3000291585922241, 1.4761743545532227, 0.8823727369308472, 1.207534670829773, 1.1750928163528442, 1.8724039793014526, 1.3387277126312256, 1.5213388204574585, 1.3680731058120728, 1.6000343561172485, 1.4732385873794556, 1.3700504302978516, 1.3214770555496216]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0812962055206299, 1.031795859336853, 0.9993994832038879, 1.0547921657562256, -0.259221613407135, 1.2788115739822388, 1.1249271631240845, 1.6730986833572388, 0.7453086376190186, 1.2190532684326172, 1.102969765663147, 1.0862468481063843, 1.0503990650177002, 1.1933108568191528, 0.8144561648368835, 0.325787752866745]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.9473748207092285, 2.1201961040496826, 2.0587635040283203, 1.8785423040390015, 1.8448175191879272, 2.0104329586029053, 1.8231512308120728, 2.0621118545532227, 1.848093032836914, 1.9713703393936157, 2.2161295413970947, 2.5165956020355225, 1.5446867942810059, 2.1865293979644775, 2.008686065673828, 1.947981357574463]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.283239483833313, 1.3155571222305298, 1.8196816444396973, 0.9786362051963806, 1.3696379661560059, 1.510772466659546, 1.2345448732376099, 1.3129609823226929, 1.399674892425537, 1.464128017425537, 1.4751310348510742, 1.948418140411377, 1.4201037883758545, 1.4271399974822998, 1.1666343212127686, 1.5781493186950684]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.2484471797943115, 1.7855443954467773, 1.0178146362304688, 1.3711180686950684, 1.5686869621276855, 1.435073733329773, 1.4116605520248413, 1.3664354085922241, 1.4694900512695312, 1.4482967853546143, 1.5370486974716187, 1.4603673219680786, 1.2403557300567627, 0.6034871339797974, 1.2134668827056885, 1.4665663242340088]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.31046199798584, 2.357045888900757, 2.248059034347534, 2.0770089626312256, 2.4948079586029053, 2.179105281829834, 2.3915956020355225, 2.142274856567383, 2.083632469177246, 2.0902318954467773, 2.6987576484680176, 2.056483030319214, 2.260918140411377, 2.406977891921997, 2.2497572898864746, 2.298670530319214]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.665469765663147, 1.6556676626205444, 1.680488109588623, 1.774165391921997, 1.2627559900283813, 1.6240051984786987, 1.670467734336853, 1.9871408939361572, 1.6959675550460815, 1.585646390914917, 1.6883856058120728, 1.525426983833313, 1.6127474308013916, 1.5727629661560059, 1.9502135515213013, 1.9410666227340698]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.7391425371170044, 1.765329360961914, 1.67331862449646, 1.7271448373794556, 1.7245365381240845, 1.7900981903076172, 1.7046171426773071, 1.699509859085083, 1.7823023796081543, 1.8152356147766113, 2.2750933170318604, 1.6829659938812256, 1.6244661808013916, 1.7389121055603027, 1.7726353406906128, 1.766478180885315]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5801751613616943, 1.6395922899246216, 1.5870747566223145, 1.6569156646728516, 1.5399723052978516, 1.548174500465393, 1.7609120607376099, 1.657958984375, 1.5414093732833862, 1.635523796081543, 1.2564295530319214, 1.5870414972305298, 1.6249055862426758, 1.8685916662216187, 1.610111951828003, 1.6079769134521484]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9737844467163086, 2.0050222873687744, 2.295619487762451, 1.934855341911316, 2.0645077228546143, 1.9957904815673828, 2.532633066177368, 2.0610928535461426, 1.95533287525177, 1.9437354803085327, 1.9616895914077759, 1.7751115560531616, 1.9487274885177612, 1.8280521631240845, 2.1997525691986084, 1.928462266921997]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.756842136383057, 5.0755534172058105, 4.675223350524902, 4.618904113769531, 4.641340732574463, 4.558496475219727, 4.755871295928955, 4.860151290893555, 4.929542064666748, 4.703355312347412, 4.686529636383057, 4.606026649475098, 4.668909072875977, 4.717002868652344, 4.855566024780273, 4.700941562652588]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.5390381813049316, 3.4284744262695312, 3.3854570388793945, 3.28324556350708, 3.488135576248169, 3.5386743545532227, 3.1818225383758545, 3.446695566177368, 3.619346857070923, 3.4680707454681396, 3.361049175262451, 3.0050344467163086, 3.2862966060638428, 3.52581524848938, 3.529527425765991, 3.4606947898864746]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.6979570388793945, 2.7353696823120117, 2.703852891921997, 2.7049689292907715, 2.630628824234009, 2.7194292545318604, 2.813567638397217, 2.464479923248291, 2.6357240676879883, 2.529866933822632, 3.147709608078003, 2.0126254558563232, 2.6845643520355225, 2.580357074737549, 3.0618691444396973, 2.6388537883758545]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.08510327339172363, 0.09606292098760605, 0.10289902985095978, -0.25846385955810547, -0.23235197365283966, -0.09523963928222656, 0.11175672709941864, -0.08370842784643173, 0.07760419696569443, 0.08294735103845596, 0.09258804470300674, 0.059339739382267, 0.08172812312841415, 0.10642852634191513, -1.0640770196914673, 0.11091182380914688]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08085350692272186, 0.050423718988895416, 0.04366113990545273, 0.04877857118844986, 0.07183666527271271, 0.02896878682076931, 0.05238654464483261, 0.092764712870121, 0.02346971072256565, 0.07635092735290527, -0.20013341307640076, 0.04754607751965523, 0.010957622900605202, -0.015332674607634544, 0.03334162384271622, 0.024029040709137917]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08802802860736847, 0.06591196358203888, 0.1057385802268982, 0.07929696887731552, 0.11282775551080704, 0.10413865745067596, 0.07097377628087997, -0.09380540996789932, 0.07756819576025009, 0.10880176723003387, 0.03694159910082817, 0.08373713493347168, -0.2093730866909027, 0.008809995837509632, -0.06042594835162163, 0.10196809470653534]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13019347190856934, 0.13373613357543945, 0.12975963950157166, 0.14082297682762146, 0.14513692259788513, 0.1029273048043251, -0.012013149447739124, 0.1768021583557129, 0.18145886063575745, -0.5578632354736328, 0.04131937772035599, 0.06550292670726776, 0.26599282026290894, -0.30927449464797974, 0.006796729750931263, -0.06915130466222763]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06771626323461533, 0.11052276939153671, 0.07366891205310822, 0.20281152427196503, 0.04779856279492378, -0.06578385829925537, 0.021924495697021484, 0.04930677264928818, -0.10932397842407227, 0.021471787244081497, -0.20194849371910095, 0.14007548987865448, 0.008448409847915173, -0.1975582093000412, 0.13242940604686737, -0.0942176803946495]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07085087150335312, 0.13577666878700256, 0.07608084380626678, 0.14682379364967346, -0.10679225623607635, -0.07204055786132812, 0.06003274768590927, -0.04672689363360405, 0.20880603790283203, -0.09324917942285538, -0.11502895504236221, 0.07771331071853638, -0.21154256165027618, 0.1459803581237793, 0.12786293029785156, 0.07842548191547394]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11148929595947266, -0.01824493333697319, 0.1031179428100586, 0.2972168028354645, 0.18729686737060547, 0.15264925360679626, -0.13340377807617188, -0.0959894210100174, 0.33112668991088867, 0.1972944289445877, -0.544476330280304, 0.23988142609596252, 0.23090973496437073, -0.2667791247367859, 0.302992045879364, 0.21635417640209198]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5485483407974243, 0.2329901158809662, 0.20871791243553162, 0.15815329551696777, -0.9035896062850952, 0.29879000782966614, 0.1964883804321289, -0.4204021394252777, 0.5500367879867554, 0.16850776970386505, -0.25355395674705505, 0.3192405700683594, 0.13881340622901917, 0.2352275848388672, -0.6804901957511902, -0.3171241879463196]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.35227012634277344, 0.31317195296287537, -0.3464599549770355, 0.29636651277542114, 0.38991832733154297, 0.30377712845802307, 0.24082069098949432, -0.26794394850730896, 0.18923263251781464, 0.7886924743652344, 0.10366372764110565, 0.0756450667977333, 0.46445924043655396, -0.517865002155304, -0.5376564264297485, 0.35853996872901917]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3446853756904602, 0.2269987165927887, 0.43829211592674255, 1.0501587390899658, 0.2694816589355469, 0.11961212009191513, 0.5088071227073669, 0.48747119307518005, 0.6594520807266235, -0.0037931441329419613, 0.1504005491733551, 0.6283538937568665, 0.5692761540412903, 0.10843143612146378, 0.763532280921936, 0.6112640500068665]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0138728618621826, 0.7889953851699829, 0.5386268496513367, 0.705950915813446, 0.9070999026298523, 0.4695318341255188, 0.6111419796943665, 0.6990966796875, 0.16767998039722443, 0.42598381638526917, 0.823956310749054, 0.915264904499054, 0.21731586754322052, 0.22568388283252716, 0.9567897915840149, 0.38303622603416443]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.882128119468689, 1.036688208580017, 0.8498199582099915, 0.9890500903129578, 1.144202470779419, 0.28229790925979614, 0.773370087146759, 0.811938464641571, 0.9068023562431335, 1.355950951576233, 0.8477703332901001, 0.9009033441543579, 0.22749614715576172, 0.4364570677280426, -0.08407821506261826, 0.7031204104423523]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.11915016174316406, 0.7814361453056335, 1.2298157215118408, 1.0855591297149658, 0.15678711235523224, 0.6214630007743835, 0.9295371770858765, 1.1787246465682983, 0.43159541487693787, 0.15480566024780273, 0.8892669677734375, 0.850451648235321, 0.669970691204071, 0.8680175542831421, 0.6090148687362671, 0.878369152545929]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6672462224960327, 0.9261810183525085, 1.069677710533142, 0.4046602249145508, 0.990673840045929, -0.23605003952980042, 1.036462426185608, 1.851586937904358, 0.9531921148300171, 0.909008800983429, 1.1944167613983154, 1.102453589439392, 0.034653473645448685, 0.6665428876876831, 0.9969362020492554, 0.5515453219413757]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7623298764228821, 0.7305358648300171, 0.976757824420929, 0.835156261920929, 1.0256226062774658, 1.2979748249053955, 0.0711669921875, 1.2905025482177734, 0.7349914312362671, 0.7178199887275696, -0.47260817885398865, 0.6688634157180786, 0.9407722353935242, 1.0829956531524658, 1.130773901939392, 0.4352966248989105]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.3624981939792633, 1.040032982826233, 0.9367431402206421, 0.947583019733429, 0.7101272344589233, 0.86328125, 1.3815046548843384, 0.9298461675643921, 0.8343505859375, 0.996630847454071, 0.5777557492256165, -0.4054756164550781, 1.0523712635040283, 0.829455554485321, 0.7512573003768921, 1.355320692062378]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6673858761787415, 0.15413913130760193, 0.8586776852607727, 0.8074997067451477, 0.7273700833320618, 0.3780868649482727, 0.16893696784973145, 0.8394020199775696, -1.0694770812988281, 2.257976531982422, -0.8866806030273438, 0.4878730773925781, 0.813854992389679, 0.7839263677597046, 0.7979049682617188, 0.1543678343296051]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.004567337222397327, 0.6849859356880188, 1.507836937904358, 1.2385505437850952, 1.534448266029358, 1.438232421875, 1.3024413585662842, 1.3636963367462158, 1.946997046470642, 1.2252568006515503, 2.1705322265625, 1.5061523914337158, 1.3932727575302124, 1.31201171875, 0.4586513638496399, 1.18865966796875]
Running loglikelihood requests:  22%|██████████▉                                      | 113/504 [06:32<22:13,  3.41s/it]Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3981444835662842, 1.2119262218475342, 1.4315277338027954, 1.6930420398712158, 0.8498474359512329, 1.152734398841858, 1.7784423828125, 1.6551513671875, 2.360302686691284, 1.410375952720642, 1.0851013660430908, 0.705453097820282, 1.1336272954940796, 0.8020713925361633, 1.30535888671875, 1.431420922279358]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.363671898841858, 1.2054443359375, 1.3820984363555908, 1.2704589366912842, 1.4637939929962158, 0.8310470581054688, 1.164544701576233, 1.1512542963027954, 1.828070044517517, 1.343042016029358, 1.4994385242462158, 1.3341553211212158, 1.5888698101043701, 1.4560546875, 1.352783203125, 1.3119995594024658]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0809093713760376, 1.04742431640625, 1.0039489269256592, 1.054834008216858, -0.29992207884788513, 1.28680419921875, 1.1496460437774658, 1.623501181602478, 0.7500839233398438, 1.2214844226837158, 1.1210418939590454, 1.098870873451233, 1.0637085437774658, 1.1852295398712158, 0.7682899236679077, 0.2976362109184265]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.937231421470642, 2.106250047683716, 2.0379881858825684, 1.8557860851287842, 1.832666039466858, 2.004443407058716, 1.7877686023712158, 2.044482469558716, 1.8379395008087158, 1.9390380382537842, 2.195019483566284, 2.4979004859924316, 1.496586561203003, 2.1541991233825684, 1.9819824695587158, 1.913964867591858]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3331298828125, 1.3427002429962158, 1.8350830078125, 1.0027252435684204, 1.3778564929962158, 1.5439453125, 1.266760230064392, 1.3646972179412842, 1.457861304283142, 1.49261474609375, 1.512426733970642, 1.9667479991912842, 1.4600830078125, 1.461645483970642, 1.2028625011444092, 1.602441430091858]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.257275342941284, 1.8261229991912842, 1.0093719959259033, 1.3837890625, 1.589990258216858, 1.4636962413787842, 1.4503173828125, 1.401220679283142, 1.485742211341858, 1.4605712890625, 1.564453125, 1.505407691001892, 1.254638671875, 0.571667492389679, 1.2393310070037842, 1.472021460533142]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.326953172683716, 2.3656249046325684, 2.266845703125, 2.0966796875, 2.492236375808716, 2.187304735183716, 2.4022459983825684, 2.1531739234924316, 2.0855712890625, 2.104931592941284, 2.701855421066284, 2.066601514816284, 2.3141112327575684, 2.4161133766174316, 2.2637696266174316, 2.311962842941284]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.674536108970642, 1.628515601158142, 1.6726806163787842, 1.7759521007537842, 1.2274291515350342, 1.6273682117462158, 1.6747314929962158, 1.984106421470642, 1.698461890220642, 1.581787109375, 1.687536597251892, 1.5126464366912842, 1.6196777820587158, 1.569067358970642, 1.950585961341858, 1.942651391029358]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.738916039466858, 1.769945502281189, 1.6850464344024658, 1.7347290515899658, 1.7487671375274658, 1.8161201477050781, 1.732927680015564, 1.7202942371368408, 1.81170654296875, 1.820550560951233, 2.293597459793091, 1.6945312023162842, 1.638208031654358, 1.75726318359375, 1.8016769886016846, 1.7932027578353882]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5881836414337158, 1.6385345458984375, 1.5910377502441406, 1.6588577032089233, 1.55194091796875, 1.5569374561309814, 1.7622146606445312, 1.6549999713897705, 1.548903465270996, 1.6369140148162842, 1.2382323741912842, 1.5917632579803467, 1.6319878101348877, 1.877587914466858, 1.6076751947402954, 1.6154906749725342]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.954614281654358, 2.0019288063049316, 2.3125367164611816, 1.936425805091858, 2.0693421363830566, 2.00238037109375, 2.5189208984375, 2.0704712867736816, 1.9630858898162842, 1.946679711341858, 1.9644775390625, 1.764990210533142, 1.944915771484375, 1.8326416015625, 2.2090821266174316, 1.93365478515625]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.746923923492432, 5.0616455078125, 4.670702934265137, 4.597283840179443, 4.635937690734863, 4.578051567077637, 4.752587795257568, 4.8414306640625, 4.903466701507568, 4.68719482421875, 4.659350395202637, 4.60400390625, 4.651086330413818, 4.713427543640137, 4.851269721984863, 4.681445121765137]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.459545850753784, 3.372509717941284, 3.3170409202575684, 3.22540283203125, 3.419262647628784, 3.4666504859924316, 3.111132860183716, 3.3877196311950684, 3.540210008621216, 3.4129881858825684, 3.285449266433716, 2.892251491546631, 3.1927123069763184, 3.4447021484375, 3.452685594558716, 3.391369581222534]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.645922899246216, 2.6593995094299316, 2.6370606422424316, 2.632275342941284, 2.5640625953674316, 2.633544921875, 2.739013671875, 2.3719239234924316, 2.5831542015075684, 2.420581102371216, 3.0506834983825684, 1.9277023077011108, 2.632251024246216, 2.5048828125, 2.9942383766174316, 2.574267625808716]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.1137450560927391, 0.1311882585287094, 0.12829235196113586, -0.2544504702091217, -0.14333587884902954, -0.2178667187690735, 0.14186710119247437, -0.2360498160123825, 0.09084831923246384, 0.11847878992557526, 0.1227148249745369, 0.11153992265462875, 0.11282987147569656, 0.12502479553222656, -1.1402641534805298, 0.14152032136917114]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0963955745100975, 0.07860416173934937, 0.053294774144887924, 0.05436885356903076, 0.09736393392086029, 0.04434671998023987, 0.0592048205435276, 0.060761552304029465, 0.010746614076197147, 0.08262171596288681, -0.163518026471138, 0.05510958656668663, 0.009134736843407154, -0.02621728740632534, -0.01766703836619854, 0.011225190944969654]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07669854164123535, 0.06883755326271057, 0.0986723005771637, 0.05130518227815628, 0.10149882733821869, 0.11303547769784927, 0.1166224479675293, -0.05253159627318382, 0.07160074263811111, 0.026494212448596954, -0.001307145575992763, 0.09237303584814072, -0.10490532964468002, 0.032955337315797806, 0.003499157028272748, 0.10102412104606628]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11798959225416183, 0.19235296547412872, 0.11475684493780136, 0.1719581037759781, 0.1209665909409523, 0.16088540852069855, 0.10919419676065445, 0.21840307116508484, 0.17144928872585297, -0.31267985701560974, -0.0734572485089302, 0.14129629731178284, 0.09744396805763245, -0.08588529378175735, -0.16608193516731262, -0.07019196450710297]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.09851601719856262, 0.12218993902206421, 0.10779998451471329, -0.038758955895900726, 0.015773389488458633, -0.09372159093618393, 0.10316577553749084, 0.06948396563529968, 0.00898502767086029, 0.0539601631462574, -0.05740433186292648, 0.21346047520637512, -0.1796601563692093, -0.17489860951900482, 0.11578076332807541, 0.030094938352704048]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09770533442497253, 0.21669994294643402, 0.09155479818582535, 0.006522508338093758, 0.017116619274020195, 0.12380002439022064, 0.03482813760638237, 0.11990999430418015, 0.055640239268541336, -0.0995752140879631, -0.28475627303123474, 0.10119648277759552, -0.0388539619743824, 0.10826288163661957, 0.21251866221427917, 0.07691461592912674]
Layer: gate_5 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.07911395281553268, 0.20468658208847046, 0.08424578607082367, 0.3381524384021759, 0.19336561858654022, 0.07774948328733444, 0.1359531283378601, 0.1736428588628769, 0.36628904938697815, 0.1536892056465149, -0.27262255549430847, 0.2782522737979889, 0.24505653977394104, 0.10583193600177765, 0.10561893880367279, 0.11627014726400375]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.16506285965442657, 0.3832063376903534, 0.45413774251937866, 0.2038796842098236, -0.5959518551826477, 0.32366299629211426, 0.1849171370267868, -0.020183779299259186, 0.17324291169643402, 0.05057453736662865, 0.11729594320058823, 0.24987447261810303, 0.432975709438324, 0.11339122802019119, -0.07452392578125, -0.16448935866355896]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.41550469398498535, 0.4160482585430145, 0.08009770512580872, 0.46900296211242676, 0.33963194489479065, 0.37865903973579407, 0.3710695803165436, 0.0022034074645489454, 0.1470864713191986, 0.32098886370658875, 0.07817020267248154, -0.07602677494287491, 0.4764857292175293, 0.0181658286601305, 0.0972142219543457, 0.1653396338224411]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.5415652990341187, 0.7722191214561462, 0.4482715427875519, 0.586722731590271, 0.5592371225357056, 0.5379815101623535, 0.9684352278709412, 0.7127731442451477, 0.6726735234260559, 0.2759568393230438, 0.7972542643547058, 0.8272405862808228, 0.7159855961799622, 0.8755343556404114, 0.8303852081298828, 0.7798312306404114]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9978085160255432, 1.0519267320632935, 0.7020056247711182, 1.0257468223571777, 1.1017528772354126, 0.4466940462589264, 0.7430949807167053, 0.8285242319107056, 1.0456939935684204, 1.247940182685852, 0.7771349549293518, 0.9550351500511169, 0.9096595048904419, 0.5527767539024353, 0.6845898032188416, 0.8996635675430298]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.1682965755462646, 1.126269817352295, 1.0808151960372925, 1.1993046998977661, 1.3234202861785889, 0.9566145539283752, 1.0774662494659424, 1.196677565574646, 1.174301028251648, 1.2474603652954102, 1.2226301431655884, 1.4263217449188232, 1.2536344528198242, 1.053511619567871, 0.7515661716461182, 1.3202203512191772]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.8554948568344116, 1.4416519403457642, 1.2373937368392944, 1.3520891666412354, 1.1399425268173218, 1.0700668096542358, 0.8943577408790588, 1.066187858581543, 1.1466809511184692, 0.5901627540588379, 1.1564711332321167, 1.1438310146331787, 1.2509212493896484, 1.0592201948165894, 0.7985701560974121, 1.16796875]
Layer: gate_12 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.617138385772705, 1.2913227081298828, 1.5821633338928223, 1.4210113286972046, 1.4706294536590576, 0.7755932807922363, 1.5031323432922363, 1.782601237297058, 1.307537317276001, 1.425732135772705, 1.3112962245941162, 1.7903153896331787, 0.9520845413208008, 1.6019740104675293, 1.9419958591461182, 1.3873820304870605]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_14 - Captured router_logits: [1.0646803379058838, 1.1267197132110596, 1.103773593902588, 1.2094389200210571, 1.209248423576355, 1.624320149421692, 1.004581093788147, 0.7292764782905579, 1.1645599603652954, 1.0387492179870605, 0.8855158686637878, 1.0782140493392944, 1.0794224739074707, 1.3037539720535278, 1.3268721103668213, 1.0765557289123535]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [1.2549350261688232, 1.3136792182922363, 1.5590360164642334, 1.3042083978652954, 1.5884172916412354, 1.3452240228652954, 1.276282787322998, 1.379299283027649, 1.2201749086380005, 1.3686983585357666, 1.3776041269302368, 0.3309740722179413, 1.5464880466461182, 1.2070189714431763, 1.2120307683944702, 1.5751999616622925]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9034385681152344, 0.7371610999107361, 0.990169882774353, 0.7577579617500305, 0.8237795829772949, 0.8961465954780579, 0.19672130048274994, 1.0072720050811768, -0.09424442052841187, 1.4379161596298218, 0.24698394536972046, 0.8998293876647949, 1.1066234111785889, 0.8742691278457642, 0.9738662242889404, 0.8466247916221619]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.452125072479248, 1.28422212600708, 1.859424114227295, 2.159400701522827, 1.8928974866867065, 1.8794713020324707, 1.8013094663619995, 2.000515937805176, 1.8700618743896484, 1.9486874341964722, 2.0772037506103516, 1.7717546224594116, 1.705208659172058, 1.647331953048706, 1.6320586204528809, 1.8199071884155273]
Layer: gate_17 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.993293046951294, 1.850874662399292, 1.939847707748413, 2.036040782928467, 1.601513385772705, 1.9888954162597656, 1.9886006116867065, 2.362175703048706, 2.438187837600708, 1.9683077335357666, 1.954181432723999, 1.9666064977645874, 2.055842161178589, 1.8485610485076904, 1.9414799213409424, 1.9017295837402344]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.229264974594116, 1.9566136598587036, 1.861930012702942, 1.8993955850601196, 2.057586431503296, 1.8218663930892944, 2.0321836471557617, 2.0276999473571777, 2.04642653465271, 2.0463714599609375, 2.061861276626587, 2.062229871749878, 2.2457988262176514, 2.081245183944702, 2.0464327335357666, 2.0012528896331787]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1482785940170288, 1.3215285539627075, 1.5129963159561157, 1.1829488277435303, 0.4686258137226105, 1.389568567276001, 1.5316184759140015, 2.2213940620422363, 1.0945239067077637, 1.4131964445114136, 1.4602619409561157, 1.2061468362808228, 1.2317585945129395, 1.359080195426941, 1.3848332166671753, 0.9204685091972351]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.4347729682922363, 2.513512134552002, 2.3468947410583496, 2.3781447410583496, 2.4207448959350586, 2.4007468223571777, 2.3570656776428223, 2.423152446746826, 2.3542158603668213, 2.600972890853882, 2.6310436725616455, 2.8205580711364746, 2.1719977855682373, 2.6986536979675293, 2.6749212741851807, 2.468062162399292]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Running loglikelihood requests:  23%|███████████▍                                     | 117/504 [06:45<21:53,  3.39s/it]Layer: gate_22 - Captured router_logits: [1.8954647779464722, 1.705680012702942, 1.9066675901412964, 1.5761964321136475, 1.970543384552002, 1.8920990228652954, 1.790880560874939, 1.8284443616867065, 1.8929097652435303, 2.0229461193084717, 1.713394284248352, 1.961993932723999, 1.958382487297058, 1.895636796951294, 1.8232606649398804, 1.9952584505081177]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.0817487239837646, 1.8347091674804688, 1.7628734111785889, 1.758819818496704, 1.7315988540649414, 1.7085052728652954, 1.6738895177841187, 1.8922710418701172, 1.741548776626587, 1.6063777208328247, 1.8575078248977661, 1.666783332824707, 1.724867343902588, 1.2652860879898071, 1.8510711193084717, 1.8584905862808228]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.740123748779297, 2.803410053253174, 2.638512134552002, 2.706564426422119, 3.0530169010162354, 2.8297464847564697, 2.834365129470825, 2.6650943756103516, 2.9286065101623535, 2.8314661979675293, 2.7553067207336426, 2.669123411178589, 2.6883351802825928, 2.8485653400421143, 2.8641903400421143, 2.9258058071136475]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.8464770317077637, 1.7398043870925903, 1.6293238401412964, 1.798619270324707, 1.4842582941055298, 1.7095125913619995, 1.756756067276001, 1.8221551179885864, 1.7346943616867065, 1.7022160291671753, 1.6795032024383545, 1.6449488401412964, 1.6640379428863525, 1.7821835279464722, 1.5808274745941162, 2.3549036979675293]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.0818593502044678, 1.721995234489441, 1.7938777208328247, 1.8392295837402344, 1.752217173576355, 1.7550499439239502, 1.7728962898254395, 1.910444974899292, 1.852314829826355, 1.8974732160568237, 1.9755859375, 1.715381145477295, 1.7916052341461182, 1.7431455850601196, 1.7502117156982422, 1.7516030073165894]
Layer: gate_26 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5004360675811768, 1.522125005722046, 1.543346643447876, 1.6075271368026733, 1.6360615491867065, 1.51806640625, 1.6039739847183228, 1.6093519926071167, 1.6336919069290161, 1.7272319793701172, 1.462386965751648, 1.5773050785064697, 1.5029234886169434, 1.5466817617416382, 1.6241800785064697, 1.5035192966461182]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1997838020324707, 2.259974479675293, 2.398468255996704, 2.2523584365844727, 2.206423282623291, 2.225493907928467, 2.294811248779297, 2.26767635345459, 2.389150857925415, 2.305056095123291, 2.159198045730591, 2.189391613006592, 2.2091686725616455, 2.120676040649414, 2.36470627784729, 2.2876744270324707]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.692020416259766, 5.826257705688477, 5.511693954467773, 5.79763650894165, 5.686246871948242, 5.638610363006592, 5.701896667480469, 6.00675630569458, 5.91425895690918, 5.76658296585083, 5.706809997558594, 5.606082916259766, 5.836956977844238, 5.643917083740234, 5.748722553253174, 5.6598615646362305]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.046482086181641, 3.9600038528442383, 3.935927629470825, 3.9007468223571777, 4.003635883331299, 3.891509532928467, 4.126974582672119, 3.9549429416656494, 3.985812187194824, 4.04373025894165, 4.01076078414917, 3.8550050258636475, 4.087291717529297, 3.934330701828003, 4.021029949188232, 4.039738178253174]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.03896427154541, 3.032036066055298, 2.799577474594116, 2.6168925762176514, 2.7801198959350586, 3.0026042461395264, 2.775009870529175, 2.915069818496704, 2.8654677867889404, 2.9969289302825928, 2.8254716396331787, 2.800560235977173, 3.040339946746826, 2.8994693756103516, 2.988011121749878, 3.0699193477630615]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_0 - Captured router_logits: [0.08277091383934021, 0.09365374594926834, 0.09959004074335098, -0.25892090797424316, -0.23726370930671692, -0.09511292725801468, 0.10957017540931702, -0.07239536941051483, 0.07387007772922516, 0.07952463626861572, 0.0904301106929779, 0.057942815124988556, 0.07955466955900192, 0.10266003012657166, -1.0573500394821167, 0.10806427896022797]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07988211512565613, 0.051611173897981644, 0.0409846268594265, 0.04798080772161484, 0.07193794846534729, 0.02919943816959858, 0.052227333188056946, 0.09629697352647781, 0.02576923929154873, 0.07587994635105133, -0.19823643565177917, 0.04554712399840355, 0.012715513817965984, -0.0152740478515625, 0.033082447946071625, 0.024570489302277565]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08900913596153259, 0.06593704223632812, 0.10607585310935974, 0.07920707762241364, 0.11456222087144852, 0.10303444415330887, 0.06791811436414719, -0.09114324301481247, 0.07471255213022232, 0.1116841658949852, 0.03475162759423256, 0.08564599603414536, -0.20981967449188232, 0.008472400717437267, -0.06412865966558456, 0.10118678957223892]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12805652618408203, 0.13775010406970978, 0.12671445310115814, 0.1389283537864685, 0.14273877441883087, 0.10397060215473175, -0.02472912333905697, 0.17144036293029785, 0.17822879552841187, -0.5412306189537048, 0.044773172587156296, 0.06352584064006805, 0.27074217796325684, -0.3113096356391907, 0.009967491962015629, -0.06677188724279404]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06784708052873611, 0.11341018229722977, 0.06934414058923721, 0.20624907314777374, 0.045234356075525284, -0.06305936723947525, 0.01989268697798252, 0.049616459757089615, -0.11120723187923431, 0.024234220385551453, -0.20282292366027832, 0.13910804688930511, 0.02067018859088421, -0.19538529217243195, 0.13056758046150208, -0.08809498697519302]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.0706140324473381, 0.1370353400707245, 0.07657177001237869, 0.15311244130134583, -0.11345240473747253, -0.06592034548521042, 0.053559694439172745, -0.04830692708492279, 0.22178909182548523, -0.09183065593242645, -0.11083447188138962, 0.07580627501010895, -0.21198277175426483, 0.14869847893714905, 0.12726876139640808, 0.07900845259428024]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11374995112419128, -0.03018687479197979, 0.10520047694444656, 0.29440152645111084, 0.19101373851299286, 0.15260785818099976, -0.13592010736465454, -0.09843435138463974, 0.32593387365341187, 0.19985482096672058, -0.5427259802818298, 0.23350246250629425, 0.23181074857711792, -0.2592938542366028, 0.32018226385116577, 0.22357235848903656]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5532594919204712, 0.22608515620231628, 0.20938575267791748, 0.14974582195281982, -0.8975822329521179, 0.2847731411457062, 0.19288213551044464, -0.4279961884021759, 0.552162230014801, 0.17258189618587494, -0.26104238629341125, 0.32200247049331665, 0.13802726566791534, 0.2388426512479782, -0.685904860496521, -0.3094766438007355]
Layer: gate_7 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3500654101371765, 0.31422069668769836, -0.34841689467430115, 0.3046495020389557, 0.38318774104118347, 0.29037418961524963, 0.23385457694530487, -0.2711261212825775, 0.18611806631088257, 0.7971421480178833, 0.09792543947696686, 0.08124988526105881, 0.4638763964176178, -0.5037906765937805, -0.5223596096038818, 0.35685575008392334]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3427848815917969, 0.251075804233551, 0.4417122006416321, 1.0709420442581177, 0.2820626497268677, 0.15433017909526825, 0.51365727186203, 0.49471336603164673, 0.6771420836448669, 0.004931563977152109, 0.16660860180854797, 0.6276103258132935, 0.550330400466919, 0.15996739268302917, 0.7728524804115295, 0.6105818748474121]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0056612491607666, 0.7891485095024109, 0.5611088871955872, 0.7123101949691772, 0.8929443359375, 0.47829049825668335, 0.6160773634910583, 0.7068347334861755, 0.21713697910308838, 0.43119263648986816, 0.8244460225105286, 0.9167434573173523, 0.22291651368141174, 0.24461393058300018, 0.9725145101547241, 0.3926423192024231]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.8834850192070007, 1.031394362449646, 0.8520446419715881, 0.9874100685119629, 1.1156612634658813, 0.2786189615726471, 0.7613072395324707, 0.799801230430603, 0.8905850648880005, 1.384470820426941, 0.8249220252037048, 0.896793007850647, 0.25120002031326294, 0.4195592999458313, -0.0784570500254631, 0.6870063543319702]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.12499333173036575, 0.7559852600097656, 1.197130560874939, 1.0797464847564697, 0.15670181810855865, 0.6178461909294128, 0.9340236783027649, 1.192482590675354, 0.43244290351867676, 0.14693734049797058, 0.8576944470405579, 0.8249803185462952, 0.6594232320785522, 0.8559048175811768, 0.5834347009658813, 0.8428409695625305]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6251358985900879, 0.8854596614837646, 1.015027642250061, 0.35639330744743347, 0.9605720639228821, -0.28370964527130127, 0.9923778772354126, 1.812843918800354, 0.9125270247459412, 0.8501498699188232, 1.15774667263031, 1.0275894403457642, 0.005102385301142931, 0.6084940433502197, 1.0138193368911743, 0.5221384763717651]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6996855139732361, 0.6992402672767639, 0.9458745121955872, 0.80137699842453, 0.9888401627540588, 1.2267160415649414, 0.09145599603652954, 1.3054771423339844, 0.7275636196136475, 0.6925539970397949, -0.4708883464336395, 0.6140123009681702, 0.9102191925048828, 1.0351930856704712, 1.0701159238815308, 0.4016134440898895]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.32829928398132324, 0.9782269597053528, 0.9030562043190002, 0.9505699872970581, 0.6759240627288818, 0.8205336332321167, 1.3492224216461182, 0.9029088020324707, 0.8104117512702942, 0.9464303255081177, 0.5256349444389343, -0.4292771518230438, 1.019424557685852, 0.7544835805892944, 0.7199353575706482, 1.2708152532577515]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6421313285827637, 0.12722519040107727, 0.8058897852897644, 0.8066222071647644, 0.7326648831367493, 0.354251503944397, 0.14785248041152954, 0.761873185634613, -1.0883854627609253, 2.1945478916168213, -0.8898188471794128, 0.46849972009658813, 0.7771472334861755, 0.7503777146339417, 0.7768416404724121, 0.11101604253053665]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.045113712549209595, 0.6234418749809265, 1.4265183210372925, 1.1469634771347046, 1.4557291269302368, 1.3800609111785889, 1.2413522005081177, 1.28515625, 1.893180012702942, 1.124267578125, 2.075963020324707, 1.4152048826217651, 1.327004075050354, 1.237857460975647, 0.3697317838668823, 1.0927488803863525]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3536999225616455, 1.1319526433944702, 1.3913636207580566, 1.6946492195129395, 0.7978185415267944, 1.1153265237808228, 1.7455041408538818, 1.620381236076355, 2.3073654174804688, 1.3813139200210571, 1.0581514835357666, 0.6436652541160583, 1.0767338275909424, 0.7366064190864563, 1.2559576034545898, 1.3889052867889404]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3334561586380005, 1.1884212493896484, 1.3708480596542358, 1.248181939125061, 1.4309896230697632, 0.8197935223579407, 1.1530746221542358, 1.1243151426315308, 1.8045216798782349, 1.3192437887191772, 1.4653228521347046, 1.3065546751022339, 1.5367377996444702, 1.4239140748977661, 1.3199931383132935, 1.271988034248352]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0738191604614258, 1.0198137760162354, 1.000945806503296, 1.0589300394058228, -0.2745979428291321, 1.2488023042678833, 1.1192511320114136, 1.6303761005401611, 0.754337728023529, 1.2179392576217651, 1.1004692316055298, 1.1053766012191772, 1.0457326173782349, 1.1608195304870605, 0.7663697004318237, 0.29327392578125]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.928974986076355, 2.0967471599578857, 2.03916072845459, 1.8618563413619995, 1.8233834505081177, 2.0017197132110596, 1.797218918800354, 2.043926954269409, 1.8307783603668213, 1.933200716972351, 2.1964426040649414, 2.5076651573181152, 1.5192978382110596, 2.1403794288635254, 1.9821147918701172, 1.9143081903457642]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.317413568496704, 1.3423742055892944, 1.8523977994918823, 1.0051822662353516, 1.3751965761184692, 1.5354019403457642, 1.267025351524353, 1.3530856370925903, 1.4352643489837646, 1.4788718223571777, 1.5049381256103516, 1.9663423299789429, 1.4341588020324707, 1.4391705989837646, 1.197529673576355, 1.5737887620925903]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.2501964569091797, 1.7966784238815308, 1.0049611330032349, 1.3865712881088257, 1.5742924213409424, 1.4373279809951782, 1.40939462184906, 1.3677525520324707, 1.4655808210372925, 1.4330286979675293, 1.5359669923782349, 1.4596846103668213, 1.26095712184906, 0.6044389009475708, 1.2275267839431763, 1.43826162815094]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3092079162597656, 2.3470911979675293, 2.2737324237823486, 2.0656938552856445, 2.4974448680877686, 2.157134532928467, 2.3603577613830566, 2.1414601802825928, 2.0860111713409424, 2.0748329162597656, 2.725776433944702, 2.0531396865844727, 2.298889636993408, 2.404972553253174, 2.239435911178589, 2.2943692207336426]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6746511459350586, 1.644482135772705, 1.6751179695129395, 1.7754029035568237, 1.2500613927841187, 1.638119101524353, 1.6749213933944702, 1.9790929555892944, 1.6913325786590576, 1.576036810874939, 1.6839131116867065, 1.509974479675293, 1.6140183210372925, 1.5814907550811768, 1.965138554573059, 1.9510613679885864]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  24%|███████████▊                                     | 121/504 [06:59<21:47,  3.42s/it]Layer: gate_26 - Captured router_logits: [1.7212927341461182, 1.7607494592666626, 1.6650820970535278, 1.6960864067077637, 1.7170425653457642, 1.7802611589431763, 1.6905354261398315, 1.6840851306915283, 1.775719165802002, 1.8139679431915283, 2.2676150798797607, 1.6631412506103516, 1.6038841009140015, 1.7169443368911743, 1.7792131900787354, 1.7651197910308838]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5652453899383545, 1.624884843826294, 1.5673478841781616, 1.647430181503296, 1.5329697132110596, 1.5282105207443237, 1.7426373958587646, 1.6378511190414429, 1.5263699293136597, 1.614337682723999, 1.2376670837402344, 1.5728737115859985, 1.6080728769302368, 1.8540822267532349, 1.587027668952942, 1.5966366529464722]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9412096738815308, 1.9770784378051758, 2.2693960666656494, 1.910107135772705, 2.039111614227295, 1.989976406097412, 2.501793384552002, 2.0519604682922363, 1.9340360164642334, 1.9192463159561157, 1.937893033027649, 1.7495086193084717, 1.9116917848587036, 1.8080532550811768, 2.17435622215271, 1.906385064125061]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.737126350402832, 5.059600830078125, 4.680964946746826, 4.603048801422119, 4.635023593902588, 4.554269790649414, 4.740959167480469, 4.829033851623535, 4.9094438552856445, 4.688888072967529, 4.657380104064941, 4.588295936584473, 4.663405418395996, 4.720027446746826, 4.850211143493652, 4.678041458129883]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.520317316055298, 3.4369595050811768, 3.3680474758148193, 3.282127618789673, 3.47513747215271, 3.509139060974121, 3.172384738922119, 3.4444034099578857, 3.589057683944702, 3.47503924369812, 3.348712682723999, 2.9676783084869385, 3.280255079269409, 3.4906396865844727, 3.5134875774383545, 3.4353625774383545]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.6650943756103516, 2.6845273971557617, 2.6763954162597656, 2.6148781776428223, 2.598123073577881, 2.6726365089416504, 2.732999324798584, 2.4021718502044678, 2.5949292182922363, 2.4610602855682373, 3.082301378250122, 1.9439904689788818, 2.648855209350586, 2.5439023971557617, 3.0038325786590576, 2.586625337600708]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.08778167515993118, 0.10616642981767654, 0.10716553777456284, -0.26129287481307983, -0.24024292826652527, -0.12887349724769592, 0.1229838952422142, -0.13724251091480255, 0.06932728737592697, 0.09061062335968018, 0.1015472412109375, 0.0798531323671341, 0.08885500580072403, 0.10300572216510773, -1.1363633871078491, 0.11850043386220932]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08443319797515869, 0.057276878505945206, 0.04003182053565979, 0.03909667581319809, 0.07721170783042908, 0.022505827248096466, 0.04998735710978508, 0.07660976052284241, 0.02055230177938938, 0.0672031119465828, -0.1913849115371704, 0.056607119739055634, 0.002339259721338749, -0.03923200070858002, 0.020048614591360092, 0.04377700388431549]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09562469273805618, 0.054249271750450134, 0.09261657297611237, 0.06921615451574326, 0.10424172878265381, 0.11329028755426407, 0.08080850541591644, -0.09055083245038986, 0.07755354791879654, 0.10870803892612457, 0.00976499356329441, 0.0911051407456398, -0.18334455788135529, 0.021402735263109207, -0.030574046075344086, 0.1060982495546341]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13435129821300507, 0.1697329431772232, 0.11612674593925476, 0.12322492897510529, 0.14541471004486084, 0.11460253596305847, 0.03457087650895119, 0.17874543368816376, 0.1913405954837799, -0.44313621520996094, 0.000613753218203783, 0.09209877997636795, 0.1953982263803482, -0.22098886966705322, -0.006312667857855558, -0.05780870094895363]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06480169296264648, 0.10442687571048737, 0.08827955275774002, 0.16493254899978638, 0.0457005575299263, -0.04755926504731178, 0.03548693656921387, 0.03942025452852249, -0.09509889781475067, 0.02619963325560093, -0.17427286505699158, 0.1533341109752655, -0.01334801409393549, -0.20709101855754852, 0.1320023238658905, -0.03241695836186409]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06348851323127747, 0.12090763449668884, 0.06366654485464096, 0.1414523720741272, -0.041340626776218414, -0.041981108486652374, 0.10148251056671143, 0.0026551263872534037, 0.15006455779075623, -0.09203596413135529, -0.0810418576002121, 0.08612432330846786, -0.17997965216636658, 0.13187195360660553, 0.12117476016283035, 0.053607042878866196]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08935236185789108, 0.0022775442339479923, 0.1770107001066208, 0.338113009929657, 0.20031091570854187, 0.14696137607097626, -0.10150204598903656, -0.07092984765768051, 0.45092812180519104, 0.15718117356300354, -0.502334475517273, 0.31226563453674316, 0.2102147936820984, -0.15170618891716003, 0.2285643219947815, 0.20144769549369812]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.4194849133491516, 0.23070749640464783, 0.29211434721946716, 0.15299083292484283, -0.9083190560340881, 0.3076479136943817, 0.18272705376148224, -0.25854066014289856, 0.474417120218277, 0.1842212975025177, -0.11871765553951263, 0.3088448941707611, 0.29049623012542725, 0.15654341876506805, -0.4864044189453125, -0.07342685014009476]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.22995130717754364, 0.3600265681743622, -0.19690175354480743, 0.3305065333843231, 0.5291009545326233, 0.2903450131416321, 0.26801490783691406, -0.2491839975118637, 0.16784881055355072, 0.7493702173233032, 0.004412025213241577, 0.34847575426101685, 0.3586899936199188, -0.4244559705257416, -0.30091774463653564, 0.33246782422065735]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.43882235884666443, 0.20304881036281586, 0.44147738814353943, 0.9469282031059265, 0.4128062129020691, 0.39377561211586, 0.645706832408905, 0.5422876477241516, 0.627731442451477, 0.12230978906154633, 0.39472854137420654, 0.6797870397567749, 0.7298630475997925, 0.5061144232749939, 0.7331931591033936, 0.6633782982826233]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.977871835231781, 0.8161978721618652, 0.5962458252906799, 0.8016272187232971, 1.0652146339416504, 0.6539011001586914, 0.655837893486023, 0.6822377443313599, 0.31713828444480896, 0.7896751761436462, 0.7577036619186401, 0.9359574317932129, 0.33453795313835144, 0.26770681142807007, 0.8182035088539124, 0.4318769872188568]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0022859573364258, 1.0712144374847412, 0.9513211846351624, 1.1533141136169434, 1.3075580596923828, 0.669878363609314, 0.9028880000114441, 1.2083934545516968, 1.078460931777954, 1.4003719091415405, 0.8732677102088928, 1.0257762670516968, 0.6293475031852722, 0.40563011169433594, -0.08013322949409485, 0.7714066505432129]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.2064858227968216, 0.8866903781890869, 1.2120321989059448, 1.143469214439392, 0.3277930021286011, 0.7732990980148315, 0.9858927130699158, 1.2731856107711792, 0.8809359669685364, 0.4815495014190674, 0.6632276177406311, 0.8699057102203369, 1.0094585418701172, 0.8307747840881348, 0.5554092526435852, 0.9167745113372803]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7065904140472412, 0.9058207869529724, 0.7992931008338928, 0.4819219410419464, 1.1523686647415161, 0.043285977095365524, 1.077768087387085, 2.250671863555908, 0.9675059914588928, 0.8770028948783875, 1.2418018579483032, 1.1308966875076294, -0.15512725710868835, 0.9067544341087341, 1.1548069715499878, 0.6594045758247375]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6726229786872864, 0.7494761347770691, 0.9267989993095398, 0.7947691082954407, 0.8876020312309265, 0.8817010521888733, 0.3751731812953949, 1.3144149780273438, 0.8491895198822021, 0.9279512763023376, 0.0399799719452858, 0.7102859616279602, 1.0090643167495728, 1.0093177556991577, 1.0471487045288086, 0.6260597705841064]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.3546735346317291, 1.0592842102050781, 0.9270874857902527, 0.9020887017250061, 0.5551151633262634, 0.8144904375076294, 1.214786171913147, 0.9847481846809387, 0.8385375142097473, 0.9308444261550903, 0.6318126320838928, -0.4326366186141968, 0.7890461683273315, 0.8161885738372803, 0.9537221193313599, 1.3902190923690796]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.6321220397949219, 0.1785779893398285, 0.7707254886627197, 0.7525755167007446, 0.6415914297103882, 0.41896241903305054, 0.25671154260635376, 0.7701696157455444, -0.8627433776855469, 2.549860715866089, -0.7552673816680908, 0.6972615718841553, 0.8224957585334778, 0.8416584730148315, 0.7865868806838989, 0.41179609298706055]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.07687125355005264, 0.5232796669006348, 1.2687848806381226, 1.1445059776306152, 1.2550382614135742, 1.3261345624923706, 1.3254379034042358, 1.2593426704406738, 1.7086049318313599, 1.3213963508605957, 1.9651920795440674, 1.2938395738601685, 1.361175775527954, 1.1393156051635742, 0.6261787414550781, 1.2256481647491455]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.2893859148025513, 1.2250945568084717, 1.3221598863601685, 1.591983437538147, 0.8858673572540283, 1.1329617500305176, 1.743431568145752, 1.5814590454101562, 1.9795886278152466, 1.3008558750152588, 1.038339376449585, 0.8269369602203369, 1.0844664573669434, 1.0784730911254883, 1.346822738647461, 1.3451184034347534]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.1792925596237183, 1.1247388124465942, 1.0835987329483032, 1.086273431777954, 1.3226512670516968, 0.7973158359527588, 0.8892223834991455, 1.0704524517059326, 1.7945611476898193, 1.3232483863830566, 1.3083573579788208, 1.247026801109314, 1.7077466249465942, 1.2358429431915283, 1.1613878011703491, 1.2032991647720337]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8914600610733032, 1.0209307670593262, 1.0095292329788208, 0.8766685724258423, 0.03519396111369133, 1.1845422983169556, 1.1226239204406738, 1.812835931777954, 0.8432394862174988, 1.17984139919281, 1.199529767036438, 1.1262083053588867, 1.0491764545440674, 1.3348925113677979, 1.053074598312378, 0.4027542769908905]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.9236539602279663, 2.0596139430999756, 1.930931568145752, 1.8005200624465942, 1.762962818145752, 1.908811092376709, 1.7644680738449097, 1.9830065965652466, 1.819292426109314, 1.8731837272644043, 2.1947152614593506, 2.581608295440674, 1.717941403388977, 2.2216858863830566, 2.078373908996582, 1.9484723806381226]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3289958238601685, 1.2755647897720337, 1.7815237045288086, 1.0542272329330444, 1.4215266704559326, 1.491565465927124, 1.2634727954864502, 1.3528811931610107, 1.4142863750457764, 1.5316729545593262, 1.5157618522644043, 2.168516159057617, 1.4507365226745605, 1.4798094034194946, 1.1546452045440674, 1.5554587841033936]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.59590220451355, 1.7901697158813477, 1.2087153196334839, 1.4209544658660889, 1.5772541761398315, 1.4870309829711914, 1.4216511249542236, 1.4259366989135742, 1.5417371988296509, 1.476649522781372, 1.543424367904663, 1.5094577074050903, 1.3733640909194946, 0.876172661781311, 1.4741740226745605, 1.5062508583068848]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3412680625915527, 2.3265576362609863, 2.2217233180999756, 2.140195846557617, 2.6981489658355713, 2.269456624984741, 2.5903849601745605, 2.197924852371216, 2.2205910682678223, 2.2083001136779785, 2.782428741455078, 2.196929693222046, 2.2398860454559326, 2.4107813835144043, 2.320387125015259, 2.3560659885406494]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6358603239059448, 1.7282869815826416, 1.4532867670059204, 1.6413092613220215, 1.4493929147720337, 1.5379117727279663, 1.5603104829788208, 1.8500945568084717, 1.5410032272338867, 1.5062931776046753, 1.533327579498291, 1.422186017036438, 1.5068461894989014, 1.4056528806686401, 1.705034613609314, 1.8966803550720215]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9115371704101562, 1.7659200429916382, 1.7245471477508545, 1.8113865852355957, 1.7363121509552002, 1.782069444656372, 1.7196643352508545, 1.7776702642440796, 1.809579610824585, 1.8870266675949097, 2.2309792041778564, 1.6540130376815796, 1.718625545501709, 1.8452801704406738, 1.8564329147338867, 1.7725790739059448]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6019108295440674, 1.6675550937652588, 1.6196942329406738, 1.7440845966339111, 1.646649956703186, 1.6101524829864502, 1.756425380706787, 1.71835196018219, 1.6311143636703491, 1.7360855340957642, 1.470292568206787, 1.7318371534347534, 1.6464967727661133, 1.8691219091415405, 1.5871939659118652, 1.616176724433899]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.032655715942383, 2.115060329437256, 2.4784162044525146, 2.0154664516448975, 2.134641170501709, 2.024482488632202, 2.5302112102508545, 2.088549852371216, 2.0038564205169678, 2.024653434753418, 1.9360231161117554, 1.8532854318618774, 1.9470168352127075, 1.8287345170974731, 2.329866647720337, 2.0834431648254395]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.3995819091796875, 5.751492977142334, 5.413913249969482, 5.392839431762695, 5.303219318389893, 5.231936931610107, 5.387440204620361, 5.811654090881348, 5.602109909057617, 5.3680830001831055, 5.426154613494873, 5.293093204498291, 5.472979545593262, 5.466461181640625, 5.58434534072876, 5.3935112953186035]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  25%|████████████▏                                    | 125/504 [07:12<21:14,  3.36s/it]Layer: gate_30 - Captured router_logits: [3.9790380001068115, 3.7828173637390137, 3.7927448749542236, 3.635051727294922, 3.8748879432678223, 3.818589448928833, 3.58600115776062, 3.868730068206787, 3.8979475498199463, 3.8751492500305176, 3.696233034133911, 3.458122968673706, 3.7904775142669678, 3.832740306854248, 3.8869924545288086, 3.957451105117798]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.963674306869507, 3.0721535682678223, 2.900726556777954, 2.805732488632202, 2.843824625015259, 3.0042545795440674, 2.9465067386627197, 2.8524458408355713, 2.914435625076294, 2.9140002727508545, 3.233628511428833, 2.3755955696105957, 2.9879329204559326, 2.9256818294525146, 3.19431734085083, 2.9953722953796387]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06973475962877274, 0.08425159752368927, 0.0962948426604271, -0.2767420709133148, -0.2592501640319824, -0.0698482096195221, 0.09625516086816788, -0.05391845852136612, 0.061637237668037415, 0.06997011601924896, 0.07221435755491257, 0.04172046482563019, 0.07506061345338821, 0.10571545362472534, -1.0240297317504883, 0.10697897523641586]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.07618607580661774, 0.032316725701093674, 0.02767176553606987, 0.04389378800988197, 0.0640743151307106, 0.01474010106176138, 0.049500349909067154, 0.09905356168746948, 0.06901344656944275, 0.06835748255252838, -0.1909843236207962, 0.05945640057325363, 0.006909770425409079, -0.019156867638230324, 0.04635777696967125, 0.03857092186808586]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09182404726743698, 0.03651443123817444, 0.0731019526720047, 0.06190958246588707, 0.14650151133537292, 0.08854667842388153, 0.07613406330347061, -0.08913569152355194, 0.07905133068561554, 0.14091971516609192, 0.03737901151180267, 0.08327213674783707, -0.2288377285003662, 0.015568345785140991, -0.023326702415943146, 0.10026166588068008]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1261238306760788, 0.1391253024339676, 0.12251973152160645, 0.13059771060943604, 0.1291123926639557, 0.11258451640605927, 0.014641152694821358, 0.1596234291791916, 0.1844082772731781, -0.5295615196228027, 0.039922431111335754, 0.07045169919729233, 0.30038392543792725, -0.3177224397659302, 0.09849708527326584, -0.06149016320705414]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.05502892658114433, 0.09938935190439224, 0.06305824965238571, 0.2829316258430481, 0.07017239183187485, -0.06177860498428345, 0.036514367908239365, 0.027580101042985916, -0.11459577083587646, -0.0030864099971950054, -0.2684284746646881, 0.11675621569156647, 0.0987921953201294, -0.23150871694087982, 0.14485935866832733, -0.10722868144512177]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.05959059298038483, 0.13958199322223663, 0.0678127184510231, 0.24675607681274414, -0.17527367174625397, -0.11344515532255173, 0.08181668817996979, -0.02613992989063263, 0.23510520160198212, -0.09373931586742401, -0.1934850811958313, 0.1166418269276619, -0.25103554129600525, 0.19691181182861328, 0.12475792318582535, 0.09236150234937668]
Layer: gate_5 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.10193953663110733, 0.0061893584206700325, 0.1654621809720993, 0.2843007743358612, 0.17990052700042725, 0.18228130042552948, -0.15414851903915405, -0.055571917444467545, 0.36322295665740967, 0.17526933550834656, -0.5586559772491455, 0.2648179531097412, 0.2820800840854645, -0.30376842617988586, 0.3385069668292999, 0.2162490338087082]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5954400897026062, 0.22050929069519043, 0.22446097433567047, 0.17090976238250732, -0.9022378325462341, 0.27662214636802673, 0.20463119447231293, -0.40402910113334656, 0.6685094237327576, 0.1906053125858307, -0.1998806893825531, 0.2827244997024536, 0.0723235085606575, 0.3046378791332245, -0.6687451601028442, -0.40373390913009644]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.31883347034454346, 0.2717928886413574, -0.24695512652397156, 0.2877933979034424, 0.4235992431640625, 0.4010419249534607, 0.2676486074924469, -0.3263116776943207, 0.1780868023633957, 0.9620893001556396, 0.11329247057437897, 0.21315917372703552, 0.5288306474685669, -0.4888571500778198, -0.4945313632488251, 0.47124022245407104]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.45419371128082275, 0.2639178931713104, 0.5223133563995361, 1.2076140642166138, 0.4017256200313568, 0.21839481592178345, 0.6909794211387634, 0.6347467303276062, 1.0807515382766724, 0.2082996964454651, 0.2335014045238495, 0.720482587814331, 0.5652603507041931, 0.1028604805469513, 0.8348097205162048, 0.6944052577018738]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0458275079727173, 0.9190335273742676, 0.5243006348609924, 0.6817729473114014, 0.8490265607833862, 0.7882471084594727, 0.7451218962669373, 0.757591962814331, 0.11878848820924759, 0.5360119342803955, 0.7926663160324097, 1.05458664894104, 0.22693973779678345, 0.16592249274253845, 1.2116535902023315, 0.5265197157859802]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [1.0969065427780151, 1.409419059753418, 0.9052041172981262, 1.3709057569503784, 1.382431149482727, 0.2518385350704193, 0.9774445295333862, 0.9537959694862366, 1.0382182598114014, 1.737424373626709, 0.899533748626709, 1.0864505767822266, 0.34820792078971863, 0.5628762245178223, 0.11078707873821259, 0.8402249217033386]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.19863241910934448, 0.9075226783752441, 1.3715285062789917, 1.29682457447052, 0.4903564453125, 0.6204007267951965, 1.3079960346221924, 1.570410132408142, 0.4411672353744507, 0.0948389396071434, 0.9938004016876221, 0.9005166292190552, 0.9387821555137634, 0.9028981924057007, 0.7498362064361572, 1.0314390659332275]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.9763482809066772, 1.1750472784042358, 1.2612966299057007, 0.5770291090011597, 1.2552293539047241, -0.12518703937530518, 1.2525768280029297, 2.302948474884033, 1.360861897468567, 1.0561617612838745, 1.7385081052780151, 1.3501511812210083, 0.3640916347503662, 1.0031927824020386, 1.194266676902771, 0.8079244494438171]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.9117313623428345, 0.8794229030609131, 1.1982232332229614, 0.9793598651885986, 1.152104377746582, 1.6004551649093628, 0.5470025539398193, 1.9045497179031372, 0.8299300670623779, 0.8825801610946655, -0.24929198622703552, 0.9631473422050476, 1.0207970142364502, 1.1746408939361572, 1.2313319444656372, 0.6325094699859619]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.542927622795105, 1.2570942640304565, 1.079221248626709, 1.0529108047485352, 0.939257800579071, 0.979296863079071, 1.915062665939331, 1.1452243328094482, 1.1081652641296387, 1.0643397569656372, 0.8291614055633545, -0.225091353058815, 1.2513892650604248, 0.9642640948295593, 1.021755337715149, 1.6888388395309448]
Running loglikelihood requests:  26%|████████████▌                                    | 129/504 [07:25<20:53,  3.34s/it]Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6288648843765259, 0.18305832147598267, 0.7423434257507324, 0.8254725337028503, 0.7490108609199524, 0.5105106234550476, 0.7201197743415833, 0.8390089273452759, -0.7870514988899231, 2.158128023147583, -0.6824691295623779, 0.585640013217926, 0.8237020969390869, 0.8493825793266296, 0.8001449108123779, 0.5047394633293152]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.2173324078321457, 0.8625184893608093, 1.4919795989990234, 1.4145318269729614, 1.5761845111846924, 1.4833921194076538, 1.6191579103469849, 1.4713551998138428, 2.067399263381958, 1.365262746810913, 2.5627267360687256, 1.760009765625, 1.5470514297485352, 1.425075650215149, 0.6242077350616455, 1.340087890625]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.586794376373291, 1.3314768075942993, 1.5061618089675903, 1.9803427457809448, 1.0067367553710938, 1.2761719226837158, 2.2829384803771973, 1.8719757795333862, 2.655317544937134, 1.5234123468399048, 1.306930422782898, 0.9782624244689941, 1.2872810363769531, 0.9097294211387634, 1.598122477531433, 1.6715725660324097]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.6034400463104248, 1.4314515590667725, 1.8473790884017944, 1.472996473312378, 1.7806199789047241, 1.2059885263442993, 1.4876385927200317, 1.4334867000579834, 2.301663398742676, 1.5071320533752441, 1.8217867612838745, 1.6091481447219849, 1.8070060014724731, 1.6594631671905518, 1.6519404649734497, 1.6373236179351807]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.4002078771591187, 1.1019657850265503, 1.0433279275894165, 1.023432731628418, -0.013912864960730076, 1.4557459354400635, 1.237701654434204, 1.9807419776916504, 0.7750295400619507, 1.3572958707809448, 1.2519657611846924, 1.2069997787475586, 1.196975827217102, 1.277053952217102, 0.9570391178131104, 0.410626620054245]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.1689515113830566, 2.3504536151885986, 2.2878527641296387, 2.078200578689575, 2.1097781658172607, 2.240020275115967, 2.072026252746582, 2.354435443878174, 2.0524697303771973, 2.241431474685669, 2.4530746936798096, 2.7893145084381104, 1.7973569631576538, 2.5327117443084717, 2.2688255310058594, 2.2142136096954346]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4920614957809448, 1.517237901687622, 2.200000047683716, 1.1439075469970703, 1.5371975898742676, 1.677394151687622, 1.3782762289047241, 1.485861897468567, 1.5892388820648193, 1.6732232570648193, 1.7068548202514648, 2.21885085105896, 1.5604335069656372, 1.6540826559066772, 1.3135206699371338, 1.7415322065353394]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4648184776306152, 1.916532278060913, 1.188845157623291, 1.549294352531433, 1.7561240196228027, 1.5088709592819214, 1.4613407850265503, 1.4373739957809448, 1.5442665815353394, 1.5395917892456055, 1.5637600421905518, 1.5853452682495117, 1.2867439985275269, 0.7182837724685669, 1.3475050926208496, 1.5462197065353394]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.5306451320648193, 2.504082679748535, 2.463407278060913, 2.180746078491211, 2.6629536151885986, 2.2944555282592773, 2.543145179748535, 2.2147176265716553, 2.1965222358703613, 2.2486391067504883, 3.0097782611846924, 2.2323083877563477, 2.434526205062866, 2.553729772567749, 2.3584678173065186, 2.3598790168762207]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7879031896591187, 1.7582913637161255, 1.669531226158142, 1.9088962078094482, 1.3833417892456055, 1.7317540645599365, 1.7908265590667725, 2.116406202316284, 1.7867439985275269, 1.6765625476837158, 1.7450857162475586, 1.6044102907180786, 1.7113155126571655, 1.5971269607543945, 2.2816531658172607, 2.082686424255371]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_26 - Captured router_logits: [1.8134324550628662, 2.0324738025665283, 1.8135584592819214, 1.8047630786895752, 1.868901252746582, 1.966382622718811, 1.7745845317840576, 1.8241305351257324, 1.878985047340393, 1.9094380140304565, 2.554926872253418, 1.7355027198791504, 1.8131804466247559, 1.8867565393447876, 1.959619164466858, 1.8862966299057007]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6806671619415283, 1.7392468452453613, 1.66460120677948, 1.7208480834960938, 1.6028351783752441, 1.6257716417312622, 1.888252854347229, 1.7590222358703613, 1.6299378871917725, 1.7177544832229614, 1.3451108932495117, 1.6820974349975586, 1.762966275215149, 2.103046178817749, 1.6536353826522827, 1.691922903060913]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1080896854400635, 2.1931955814361572, 2.4300150871276855, 2.060332775115967, 2.1958417892456055, 2.133946657180786, 2.7251007556915283, 2.219682455062866, 2.0961694717407227, 2.0533769130706787, 2.0357863903045654, 1.88671875, 1.980909824371338, 1.9100050926208496, 2.3975303173065186, 2.02323579788208]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.940776348114014, 5.498286247253418, 4.943699359893799, 4.799067497253418, 4.883492946624756, 4.803225994110107, 4.939566612243652, 5.081401348114014, 5.134576797485352, 4.948185443878174, 4.896799564361572, 4.857963562011719, 4.892199993133545, 5.071723937988281, 5.145312309265137, 4.892590522766113]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.5359628200531006, 3.401411294937134, 3.3575100898742676, 3.1995842456817627, 3.4447832107543945, 3.459778308868408, 3.105405807495117, 3.5537803173065186, 3.5894153118133545, 3.3888609409332275, 3.260307550430298, 2.9066405296325684, 3.1714844703674316, 3.4635331630706787, 3.465121030807495, 3.485546827316284]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.544102907180786, 2.6839213371276855, 2.718447685241699, 2.6491432189941406, 2.6056954860687256, 2.6127519607543945, 2.6613407135009766, 2.288029193878174, 2.6006553173065186, 2.358996868133545, 3.1693549156188965, 1.7420378923416138, 2.5828628540039062, 2.4791834354400635, 3.071068525314331, 2.5672378540039062]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09143300354480743, 0.11079109460115433, 0.10802682489156723, -0.2747626304626465, -0.24761447310447693, -0.1358131319284439, 0.12640370428562164, -0.15146379172801971, 0.07078982889652252, 0.09482163190841675, 0.10027769207954407, 0.09034768491983414, 0.08997244387865067, 0.10462317615747452, -1.1489574909210205, 0.12064064294099808]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08396898955106735, 0.05031384900212288, 0.03403787314891815, 0.03561229258775711, 0.08189590275287628, 0.021319229155778885, 0.05135662481188774, 0.07299864292144775, 0.012865884229540825, 0.06669745594263077, -0.19769287109375, 0.05665358155965805, 0.0078047714196145535, -0.04078540578484535, 0.021014399826526642, 0.03947902098298073]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09402664005756378, 0.06400983035564423, 0.08906317502260208, 0.06464754790067673, 0.09735769033432007, 0.10758130252361298, 0.07361367344856262, -0.09374762326478958, 0.07700417190790176, 0.0927993506193161, 0.007946262136101723, 0.0895533412694931, -0.18015894293785095, 0.03230002149939537, -0.028943445533514023, 0.10630746185779572]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13507966697216034, 0.1713758260011673, 0.12150861322879791, 0.12784883379936218, 0.1350456327199936, 0.12694713473320007, 0.06401517987251282, 0.1831854283809662, 0.19030940532684326, -0.43936970829963684, -0.010205058380961418, 0.09339877218008041, 0.18169115483760834, -0.19855648279190063, -0.02808172069489956, -0.06492532789707184]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07256247848272324, 0.10150498151779175, 0.08256196230649948, 0.15356287360191345, 0.04674938693642616, -0.06384995579719543, 0.04356285184621811, 0.0337979719042778, -0.07130888104438782, 0.03515050187706947, -0.14938949048519135, 0.1623266637325287, -0.01830524578690529, -0.20420579612255096, 0.12415362894535065, -0.013643636368215084]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07823465019464493, 0.12353932112455368, 0.05905181169509888, 0.13390904664993286, -0.03417414054274559, -0.024034177884459496, 0.0948050394654274, 0.016201168298721313, 0.13694168627262115, -0.09212516248226166, -0.11797937005758286, 0.09209152311086655, -0.16868898272514343, 0.1392473429441452, 0.12522576749324799, 0.05977447330951691]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.0742514505982399, 0.04411835968494415, 0.18660904467105865, 0.3181186020374298, 0.19135016202926636, 0.140311598777771, -0.08278883993625641, -0.07066741585731506, 0.4812558591365814, 0.15681110322475433, -0.4925045669078827, 0.3209556043148041, 0.19771744310855865, -0.11052723973989487, 0.2144828885793686, 0.19555971026420593]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3763590157032013, 0.24247270822525024, 0.2813372015953064, 0.12148429453372955, -0.8854944705963135, 0.3183732330799103, 0.17462871968746185, -0.21032418310642242, 0.4647844135761261, 0.2007582038640976, -0.08045067638158798, 0.26833194494247437, 0.29897743463516235, 0.16677767038345337, -0.4193945527076721, -0.05640718713402748]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.1961594671010971, 0.35052409768104553, -0.14678341150283813, 0.3241937756538391, 0.5320585370063782, 0.3159528374671936, 0.2772151529788971, -0.2035750299692154, 0.1589672565460205, 0.7524063587188721, -0.0033379097003489733, 0.3892386257648468, 0.36724376678466797, -0.3673383593559265, -0.20494772493839264, 0.3308994174003601]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.45466357469558716, 0.255582332611084, 0.41966623067855835, 0.9174598455429077, 0.41704729199409485, 0.47295087575912476, 0.7069797515869141, 0.535273551940918, 0.6453306674957275, 0.1593049317598343, 0.49877098202705383, 0.6883687376976013, 0.7943059802055359, 0.5850972533226013, 0.7314738631248474, 0.6856356263160706]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9339805841445923, 0.8206390738487244, 0.6002942323684692, 0.8229568004608154, 1.0896376371383667, 0.7035712599754333, 0.6708191633224487, 0.67138671875, 0.3690401613712311, 0.9103580117225647, 0.7391816973686218, 0.9444580078125, 0.39880847930908203, 0.2792282998561859, 0.8879449963569641, 0.49177610874176025]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0246740579605103, 1.0921353101730347, 0.937138557434082, 1.293588638305664, 1.3831802606582642, 0.8360845446586609, 0.9178609251976013, 1.2722469568252563, 1.0860769748687744, 1.421494483947754, 0.9261109828948975, 1.1458280086517334, 0.7478558421134949, 0.5359152555465698, 0.024491645395755768, 0.8661665320396423]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.3144626319408417, 0.9751880168914795, 1.2651304006576538, 1.185166358947754, 0.47067418694496155, 0.8073603510856628, 1.0552939176559448, 1.2961742877960205, 0.9502349495887756, 0.6065784692764282, 0.7484751343727112, 0.8849431872367859, 1.0900911092758179, 0.8323736786842346, 0.5893911123275757, 0.9935064911842346]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8286945223808289, 0.9447734951972961, 0.9274773597717285, 0.6102008819580078, 1.2350789308547974, 0.18316328525543213, 1.129286766052246, 2.3620383739471436, 1.026982307434082, 0.9540571570396423, 1.3398183584213257, 1.2253576517105103, -0.010748330503702164, 1.003342866897583, 1.1622742414474487, 0.7656725645065308]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7681187391281128, 0.8273957371711731, 0.9685471057891846, 0.8923435211181641, 0.9656554460525513, 1.0784242153167725, 0.5801328420639038, 1.48685622215271, 0.8409661650657654, 1.0473600625991821, 0.19681142270565033, 0.8422819972038269, 1.1208559274673462, 1.1009790897369385, 1.1341948509216309, 0.7397841215133667]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.48575422167778015, 1.1552480459213257, 1.039252758026123, 0.9293957948684692, 0.6980630159378052, 0.9325030446052551, 1.366382122039795, 1.0641487836837769, 0.9646915793418884, 1.0167664289474487, 0.7659143209457397, -0.2774701714515686, 0.9250757694244385, 0.8969853520393372, 1.0459238290786743, 1.6317012310028076]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7490527629852295, 0.29510557651519775, 0.8556479215621948, 0.7582246661186218, 0.6352015733718872, 0.5069383978843689, 0.4092591404914856, 0.8578492999076843, -0.7062249183654785, 2.6794464588165283, -0.6033971309661865, 0.787360429763794, 0.8855392932891846, 0.9230069518089294, 0.8437817096710205, 0.599496603012085]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.03756595030426979, 0.6184074282646179, 1.354365348815918, 1.2504228353500366, 1.3186510801315308, 1.3854504823684692, 1.4272270202636719, 1.3639914989471436, 1.788403034210205, 1.432978630065918, 2.0614726543426514, 1.3558555841445923, 1.3945938348770142, 1.1216533184051514, 0.7247516512870789, 1.3384534120559692]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3283532857894897, 1.2742998600006104, 1.3371232748031616, 1.6366933584213257, 0.9688261151313782, 1.2006075382232666, 1.8093546628952026, 1.6557426452636719, 2.0515358448028564, 1.3318537473678589, 1.121088981628418, 0.9455090761184692, 1.1670873165130615, 1.2008000612258911, 1.4200993776321411, 1.4140371084213257]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.2913581132888794, 1.2164291143417358, 1.1652165651321411, 1.1860160827636719, 1.4112722873687744, 0.9363213181495667, 1.0041154623031616, 1.1764661073684692, 1.9032949209213257, 1.3891538381576538, 1.4059202671051025, 1.3397676944732666, 1.8302303552627563, 1.3515371084213257, 1.2522321939468384, 1.2941482067108154]
Running loglikelihood requests:  26%|████████████▉                                    | 133/504 [07:38<20:28,  3.31s/it]Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.9042033553123474, 1.081904411315918, 1.083794116973877, 0.9163152575492859, 0.11622560024261475, 1.2476156949996948, 1.1737520694732666, 1.913339614868164, 0.8233917951583862, 1.2310141324996948, 1.2359983921051025, 1.1754721403121948, 1.1185065507888794, 1.3895342350006104, 1.127619743347168, 0.46826884150505066]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0002284049987793, 2.15688419342041, 1.9841467142105103, 1.9120839834213257, 1.8601866960525513, 1.9909192323684692, 1.872920036315918, 2.0774147510528564, 1.8883421421051025, 1.9840072393417358, 2.2875912189483643, 2.67578125, 1.8585633039474487, 2.3335530757904053, 2.196200370788574, 2.045073986053467]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3892298936843872, 1.2851816415786743, 1.8167359828948975, 1.1222922801971436, 1.4691051244735718, 1.5244520902633667, 1.3312703371047974, 1.3650568723678589, 1.438032627105713, 1.5966416597366333, 1.5253145694732666, 2.206371784210205, 1.496474266052246, 1.5042359828948975, 1.2134994268417358, 1.6159700155258179]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.6299209594726562, 1.8287084102630615, 1.2771313190460205, 1.4445008039474487, 1.5555498600006104, 1.518744945526123, 1.4475699663162231, 1.4777547121047974, 1.5552709102630615, 1.4582741260528564, 1.5810420513153076, 1.5189415216445923, 1.4461241960525513, 1.0171306133270264, 1.5701602697372437, 1.5488027334213257]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3527801036834717, 2.3576502799987793, 2.2140321731567383, 2.160663604736328, 2.7567977905273438, 2.3231027126312256, 2.650517463684082, 2.2400059700012207, 2.2854607105255127, 2.2749593257904053, 2.7400567531585693, 2.2608563899993896, 2.258979320526123, 2.4391233921051025, 2.3556716442108154, 2.4185774326324463]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6514559984207153, 1.7573305368423462, 1.4679383039474487, 1.6605620384216309, 1.5191380977630615, 1.522017002105713, 1.5721768140792847, 1.8820008039474487, 1.5526835918426514, 1.5357142686843872, 1.5346616506576538, 1.4542664289474487, 1.5249848365783691, 1.427201747894287, 1.6776835918426514, 1.9365360736846924]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9309303760528564, 1.7339913845062256, 1.7295936346054077, 1.8224050998687744, 1.744184970855713, 1.7814528942108154, 1.7062703371047974, 1.7781871557235718, 1.7974631786346436, 1.8934721946716309, 2.229175090789795, 1.6600326299667358, 1.724609375, 1.8605798482894897, 1.8733956813812256, 1.7727819681167603]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6212586164474487, 1.6780387163162231, 1.622973918914795, 1.7511693239212036, 1.6724869012832642, 1.6250951290130615, 1.7493817806243896, 1.7295777797698975, 1.6464241743087769, 1.7521750926971436, 1.523310661315918, 1.7483259439468384, 1.668501377105713, 1.88525390625, 1.6013532876968384, 1.6353775262832642]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0248770713806152, 2.1119368076324463, 2.4763309955596924, 2.0167665481567383, 2.1123158931732178, 2.023700714111328, 2.513481616973877, 2.0933210849761963, 1.9988205432891846, 2.0289924144744873, 1.9428013563156128, 1.8714234828948975, 1.9421418905258179, 1.839057445526123, 2.362114429473877, 2.083286762237549]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.398539066314697, 5.7422380447387695, 5.437652111053467, 5.384588241577148, 5.282163143157959, 5.195616722106934, 5.384029865264893, 5.8039774894714355, 5.5675225257873535, 5.349482536315918, 5.406097888946533, 5.294135570526123, 5.471134185791016, 5.458806991577148, 5.584415435791016, 5.357903957366943]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.9285967350006104, 3.746220588684082, 3.7625811100006104, 3.585151195526123, 3.839311122894287, 3.7988533973693848, 3.5512187480926514, 3.8062093257904053, 3.854278087615967, 3.820122241973877, 3.660942554473877, 3.458268642425537, 3.7694473266601562, 3.7941229343414307, 3.8505985736846924, 3.891528606414795]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1077516078948975, 3.1982040405273438, 3.0535714626312256, 2.8885958194732666, 2.981736898422241, 3.1710634231567383, 3.0375406742095947, 3.005580425262451, 3.04164981842041, 3.0837559700012207, 3.3378653526306152, 2.4881274700164795, 3.095018148422241, 3.062347888946533, 3.3271102905273438, 3.130580425262451]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09139923751354218, 0.10959798842668533, 0.10391289740800858, -0.23848272860050201, -0.19956910610198975, -0.13769303262233734, 0.12458033114671707, -0.18336665630340576, 0.058841731399297714, 0.09373345226049423, 0.10724498331546783, 0.08773025870323181, 0.09244383871555328, 0.10618670284748077, -1.1053229570388794, 0.11994151026010513]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08150967955589294, 0.062132276594638824, 0.03151489794254303, 0.04231073334813118, 0.07309885323047638, 0.01971377246081829, 0.03848502039909363, 0.06290650367736816, 0.010905401781201363, 0.06262405216693878, -0.17579537630081177, 0.04383500665426254, -0.008527582511305809, -0.02197420410811901, -0.0019346089102327824, 0.022269409149885178]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07236728072166443, 0.05233853682875633, 0.08881606161594391, 0.05572971701622009, 0.10237398743629456, 0.10887839645147324, 0.07713495939970016, -0.07890656590461731, 0.06648112833499908, 0.09669618308544159, -0.009397432208061218, 0.0903296023607254, -0.14630047976970673, 0.036290355026721954, -0.014176307246088982, 0.1001756340265274]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13895921409130096, 0.15578965842723846, 0.1071602925658226, 0.15354642271995544, 0.13075117766857147, 0.10532904416322708, 0.06923209875822067, 0.17263218760490417, 0.1718456745147705, -0.3731059432029724, -0.0029642675071954727, 0.08107725530862808, 0.16850736737251282, -0.16293741762638092, -0.034856028854846954, -0.05021548643708229]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06760597229003906, 0.07338006049394608, 0.09025628119707108, 0.1317957043647766, 0.052365388721227646, -0.0019784902688115835, 0.026577752083539963, 0.06686267256736755, -0.08067797124385834, -0.020753934979438782, -0.11506375670433044, 0.10589902102947235, -0.03574609011411667, -0.11861573159694672, 0.11688187718391418, -0.0036854930222034454]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.03483244776725769, 0.15432380139827728, 0.061568766832351685, 0.1559830754995346, -0.05921252444386482, -0.0004676719836425036, 0.08631277829408646, -0.018384413793683052, 0.13274170458316803, -0.06495587527751923, -0.20286421477794647, 0.09956265985965729, -0.12662045657634735, 0.1340179443359375, 0.09932416677474976, 0.08301400393247604]
Layer: gate_5 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08164205402135849, 0.0035376858431845903, 0.2355470508337021, 0.3093862235546112, 0.19827944040298462, 0.10851235687732697, -0.05371192842721939, -0.06821213662624359, 0.5289193391799927, 0.15949861705303192, -0.42281511425971985, 0.29312649369239807, 0.18576525151729584, -0.16029784083366394, 0.18459735810756683, 0.1646309494972229]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3518633246421814, 0.18948057293891907, 0.30413034558296204, 0.11908404529094696, -0.7878434062004089, 0.3223682641983032, 0.1669096201658249, -0.13893602788448334, 0.39185860753059387, 0.29479819536209106, -0.1192074567079544, 0.23430395126342773, 0.25668811798095703, 0.16379404067993164, -0.39396172761917114, -0.17817668616771698]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.23631583154201508, 0.32123029232025146, -0.2087511271238327, 0.2816511392593384, 0.529961109161377, 0.25520622730255127, 0.26075923442840576, -0.22343067824840546, 0.1886967271566391, 0.8096400499343872, -0.003888018662109971, 0.264756441116333, 0.3460911214351654, -0.377061128616333, -0.350893497467041, 0.3237651586532593]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.41511911153793335, 0.2523484528064728, 0.42793768644332886, 0.9059678316116333, 0.42606860399246216, 0.40696775913238525, 0.6404950022697449, 0.5093597769737244, 0.6836869120597839, 0.2717907428741455, 0.3505895137786865, 0.6633522510528564, 0.771003246307373, 0.6141994595527649, 0.6485516428947449, 0.6607142686843872]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9287148714065552, 0.7883237600326538, 0.6043431758880615, 0.7942877411842346, 1.0991384983062744, 0.7645283341407776, 0.6604511141777039, 0.6517413258552551, 0.3252577483654022, 0.9465205073356628, 0.7089463472366333, 0.9200455546379089, 0.3523757755756378, 0.30561086535453796, 0.8770567774772644, 0.4290577173233032]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.018110752105713, 1.0761290788650513, 0.9390980005264282, 1.3675029277801514, 1.3441716432571411, 0.7988427877426147, 0.9207271933555603, 1.2992736101150513, 1.0790635347366333, 1.4461939334869385, 0.925961971282959, 1.0697734355926514, 0.721572995185852, 0.6144680380821228, 0.10467628389596939, 0.8792391419410706]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.3027002811431885, 0.9707023501396179, 1.3014851808547974, 1.1839488744735718, 0.5405011773109436, 0.7683359384536743, 1.1546028852462769, 1.2881619930267334, 0.9047225117683411, 0.5183687806129456, 0.7737212777137756, 0.8964589834213257, 1.1526354551315308, 0.8531097769737244, 0.6500450372695923, 0.9906402230262756]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8385754823684692, 0.9346464276313782, 0.9250853061676025, 0.5730658173561096, 1.211723804473877, 0.12013304233551025, 1.0912134647369385, 2.422940254211426, 0.9767463803291321, 0.9802182912826538, 1.2814275026321411, 1.2432148456573486, 0.015282371081411839, 1.1661661863327026, 1.0994001626968384, 0.8402606844902039]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7853496670722961, 0.8360516428947449, 0.9372907280921936, 0.9075309634208679, 0.9582551121711731, 1.0319552421569824, 0.6397395730018616, 1.4792143106460571, 0.8278142809867859, 1.0459245443344116, 0.12965770065784454, 0.8157649636268616, 1.1123331785202026, 1.0681374073028564, 1.093369483947754, 0.7110516428947449]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.46571648120880127, 1.1609805822372437, 1.0266081094741821, 0.9369863271713257, 0.7004172801971436, 0.9304103851318359, 1.4105168581008911, 1.0739396810531616, 0.933784008026123, 1.0188844203948975, 0.7577070593833923, -0.2423313707113266, 0.9320801496505737, 0.9330293536186218, 1.1288554668426514, 1.6592415571212769]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7327976226806641, 0.3957309424877167, 0.8448296189308167, 0.7985871434211731, 0.6133444905281067, 0.4865821599960327, 0.3780628442764282, 0.8385509252548218, -0.7098943591117859, 2.6616780757904053, -0.7386795878410339, 0.7471455931663513, 0.8679262399673462, 0.9283875226974487, 0.8194754719734192, 0.5109926462173462]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.056208621710538864, 0.6401644349098206, 1.3528053760528564, 1.141859769821167, 1.3705483675003052, 1.3572396039962769, 1.4083870649337769, 1.3336355686187744, 1.7088326215744019, 1.5148100852966309, 1.9905070066452026, 1.3133960962295532, 1.3694400787353516, 1.1135571002960205, 0.6975299715995789, 1.2567931413650513]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3729454278945923, 1.3120942115783691, 1.4069697856903076, 1.6102373600006104, 1.017080307006836, 1.2840654850006104, 1.800908088684082, 1.6684253215789795, 2.005174398422241, 1.3506747484207153, 1.1265250444412231, 0.9314769506454468, 1.2008769512176514, 1.115973949432373, 1.4565492868423462, 1.4384639263153076]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3137682676315308, 1.2288073301315308, 1.1839362382888794, 1.2314072847366333, 1.4443233013153076, 0.9701910614967346, 1.0474647283554077, 1.2112672328948975, 1.9155399799346924, 1.4039291143417358, 1.4015320539474487, 1.375063419342041, 1.786069393157959, 1.3714488744735718, 1.3158482313156128, 1.3222782611846924]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8413105607032776, 1.0869965553283691, 1.0150415897369385, 0.9002082943916321, 0.09611847996711731, 1.2096818685531616, 1.1431995630264282, 1.920654296875, 0.789741039276123, 1.221933364868164, 1.1946276426315308, 1.1346895694732666, 1.097402572631836, 1.3432554006576538, 1.0522433519363403, 0.41294920444488525]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.9116274118423462, 2.078125, 1.9357243776321411, 1.8498250246047974, 1.798447608947754, 1.9438413381576538, 1.7743760347366333, 2.043349266052246, 1.8172686100006104, 1.8698508739471436, 2.1982040405273438, 2.564072608947754, 1.7041523456573486, 2.2616679668426514, 2.1093242168426514, 2.0038554668426514]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.423041820526123, 1.2877181768417358, 1.8428622484207153, 1.1497231721878052, 1.4905387163162231, 1.5475598573684692, 1.325448989868164, 1.3773589134216309, 1.5659496784210205, 1.5456321239471436, 1.561713695526123, 2.2375710010528564, 1.514838695526123, 1.5616122484207153, 1.2054078578948975, 1.6418932676315308]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4577414989471436, 1.8191964626312256, 1.2786024808883667, 1.511972427368164, 1.608842372894287, 1.5309202671051025, 1.4478996992111206, 1.486454963684082, 1.5579849481582642, 1.4592887163162231, 1.5868760347366333, 1.5418907403945923, 1.415419578552246, 0.9424692392349243, 1.5153206586837769, 1.5348771810531616]
Running loglikelihood requests:  27%|█████████████▎                                   | 137/504 [07:51<20:03,  3.28s/it]Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.319145679473877, 2.3328936100006104, 2.2065999507904053, 2.1279423236846924, 2.7728793621063232, 2.320819854736328, 2.6078531742095947, 2.1883623600006104, 2.2292511463165283, 2.2321934700012207, 2.673701286315918, 2.1898844242095947, 2.232954502105713, 2.397676467895508, 2.3287336826324463, 2.355316638946533]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6372767686843872, 1.741806983947754, 1.4659851789474487, 1.625532627105713, 1.4291642904281616, 1.4956625699996948, 1.5615108013153076, 1.8488737344741821, 1.5422838926315308, 1.4961191415786743, 1.5686510801315308, 1.4102576971054077, 1.4934557676315308, 1.4179179668426514, 1.6137378215789795, 1.9275568723678589]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.8926289081573486, 1.674990177154541, 1.6536880731582642, 1.762758731842041, 1.7021293640136719, 1.7395716905593872, 1.652216911315918, 1.7487634420394897, 1.7400449514389038, 1.8547078371047974, 2.151899814605713, 1.6286779642105103, 1.6625659465789795, 1.775010108947754, 1.8162540197372437, 1.7598512172698975]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6070287227630615, 1.6739580631256104, 1.6171587705612183, 1.7408558130264282, 1.6302379369735718, 1.5927845239639282, 1.7252086400985718, 1.7025630474090576, 1.6022108793258667, 1.710258960723877, 1.4719079732894897, 1.7073229551315308, 1.6331485509872437, 1.8039075136184692, 1.5844155550003052, 1.6097031831741333]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0067343711853027, 2.079672336578369, 2.4311587810516357, 1.9862266778945923, 2.1019556522369385, 2.015498161315918, 2.5075840950012207, 2.099076747894287, 1.9598214626312256, 1.9924918413162231, 1.9145444631576538, 1.828251838684082, 1.9414823055267334, 1.8183085918426514, 2.2941102981567383, 2.0605216026306152]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.454291820526123, 5.762936115264893, 5.458705425262451, 5.455052852630615, 5.324320316314697, 5.242694854736328, 5.5030436515808105, 5.813717365264893, 5.565391540527344, 5.3917412757873535, 5.440239429473877, 5.340655326843262, 5.436333179473877, 5.479859828948975, 5.600192546844482, 5.390016078948975]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.987837314605713, 3.827744483947754, 3.8031656742095947, 3.6141183376312256, 3.792841911315918, 3.829038143157959, 3.6472833156585693, 3.7835328578948975, 3.8496522903442383, 3.8772194385528564, 3.74741268157959, 3.496854782104492, 3.818930149078369, 3.784864664077759, 3.9129464626312256, 3.9245922565460205]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.0460126399993896, 3.1211698055267334, 3.016284465789795, 2.824370861053467, 2.9840199947357178, 3.0852272510528564, 2.9725546836853027, 2.92905330657959, 2.970956802368164, 3.0097274780273438, 3.201425552368164, 2.4358701705932617, 3.0818283557891846, 3.026278495788574, 3.228642463684082, 3.0775163173675537]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09252092242240906, 0.10894551128149033, 0.10971608012914658, -0.26249006390571594, -0.24718600511550903, -0.11920225620269775, 0.12451640516519547, -0.17156453430652618, 0.06985986977815628, 0.0936642587184906, 0.10360917448997498, 0.08613836020231247, 0.09293036162853241, 0.10677342861890793, -1.1567862033843994, 0.12136472016572952]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08819300681352615, 0.05160602182149887, 0.03611481189727783, 0.03982606157660484, 0.07779195159673691, 0.02198639325797558, 0.044283896684646606, 0.06937707960605621, 0.01097864843904972, 0.06997720897197723, -0.18924160301685333, 0.062272462993860245, 0.001270107226446271, -0.03241106495261192, 0.02312980592250824, 0.040236279368400574]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09480547159910202, 0.06109190359711647, 0.09303874522447586, 0.06186852976679802, 0.09965415298938751, 0.10577800124883652, 0.06369160860776901, -0.09293749183416367, 0.07755067199468613, 0.10394807159900665, 0.013192868791520596, 0.08728066086769104, -0.18325386941432953, 0.03453153744339943, -0.027860354632139206, 0.10902075469493866]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.1364039033651352, 0.1607947200536728, 0.12356218695640564, 0.11858858913183212, 0.1349005550146103, 0.11258891969919205, 0.057771120220422745, 0.1780112236738205, 0.18363544344902039, -0.4207783639431, 0.003162458771839738, 0.08412359654903412, 0.18062616884708405, -0.198267862200737, -0.009656121022999287, -0.05282912030816078]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07375051826238632, 0.1075630933046341, 0.08678823709487915, 0.15994322299957275, 0.05148245766758919, -0.049816008657217026, 0.039824992418289185, 0.0360785573720932, -0.07315217703580856, 0.01959916576743126, -0.1669694483280182, 0.14422158896923065, -0.020949818193912506, -0.2010687291622162, 0.13195820152759552, -0.017665090039372444]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06953706592321396, 0.12647855281829834, 0.07264275848865509, 0.13364943861961365, -0.02985716238617897, -0.0337049700319767, 0.10571912676095963, 0.004523881711065769, 0.14686276018619537, -0.0967836081981659, -0.12624303996562958, 0.09708554297685623, -0.1840922087430954, 0.13815023005008698, 0.12221826612949371, 0.06212875247001648]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08365516364574432, 0.022856269031763077, 0.19980835914611816, 0.3267098367214203, 0.20837701857089996, 0.13602198660373688, -0.0774073377251625, -0.07177405059337616, 0.4544362723827362, 0.16348056495189667, -0.5049970746040344, 0.3279779851436615, 0.19986580312252045, -0.12044230848550797, 0.2065507471561432, 0.19614125788211823]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3691009283065796, 0.22089160978794098, 0.28019964694976807, 0.13471515476703644, -0.9410520195960999, 0.3352189362049103, 0.18544594943523407, -0.2387176752090454, 0.4438793659210205, 0.20914503931999207, -0.1101543977856636, 0.2789011299610138, 0.26786234974861145, 0.16811296343803406, -0.455326646566391, -0.07309259474277496]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.1986178159713745, 0.33734968304634094, -0.16868840157985687, 0.31176817417144775, 0.5450048446655273, 0.2940863370895386, 0.28362807631492615, -0.2028072625398636, 0.17620261013507843, 0.7382892370223999, 0.004314746707677841, 0.38329699635505676, 0.3768126964569092, -0.3876917362213135, -0.20727653801441193, 0.32780346274375916]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.45029962062835693, 0.22754664719104767, 0.42974135279655457, 0.9073286652565002, 0.4132756292819977, 0.4710535705089569, 0.695197582244873, 0.5513525009155273, 0.652336597442627, 0.14154690504074097, 0.4468200206756592, 0.6983187794685364, 0.8031677007675171, 0.5480701923370361, 0.755990207195282, 0.694961428642273]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9763486981391907, 0.8238581418991089, 0.5989922285079956, 0.8436223268508911, 1.098374366760254, 0.6633018255233765, 0.6554553508758545, 0.6962443590164185, 0.35341349244117737, 0.8542221188545227, 0.753044605255127, 0.9431180357933044, 0.3729727864265442, 0.27476829290390015, 0.8460491895675659, 0.4606470763683319]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0042508840560913, 1.0727012157440186, 0.950559139251709, 1.191413402557373, 1.3705480098724365, 0.7741633653640747, 0.9195452928543091, 1.270884394645691, 1.082442045211792, 1.3908659219741821, 0.9311156272888184, 1.0655605792999268, 0.6669250726699829, 0.4463012218475342, -0.05287489667534828, 0.8063390254974365]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.2498077154159546, 0.9359096884727478, 1.2450724840164185, 1.138442039489746, 0.36215031147003174, 0.7635234594345093, 0.9924867153167725, 1.229878306388855, 0.9142256379127502, 0.5876684188842773, 0.7333298325538635, 0.8610855937004089, 1.0400900840759277, 0.8592665195465088, 0.5748863220214844, 0.9369638562202454]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7568846344947815, 0.9254812598228455, 0.8535730838775635, 0.49801456928253174, 1.1777408123016357, 0.18883319199085236, 1.1011348962783813, 2.2777011394500732, 0.9776156544685364, 0.9017118811607361, 1.2789713144302368, 1.2049249410629272, -0.11128664016723633, 0.9432227611541748, 1.104358196258545, 0.7154113054275513]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7405678629875183, 0.7646915316581726, 0.960312008857727, 0.8628440499305725, 0.9425168633460999, 0.9889534115791321, 0.4548690915107727, 1.3472955226898193, 0.8528837561607361, 0.9888365268707275, 0.1585286408662796, 0.7754496932029724, 1.1094484329223633, 1.0883246660232544, 1.1251914501190186, 0.6777020692825317]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4258514642715454, 1.133310317993164, 1.0013786554336548, 0.9261961579322815, 0.6178905963897705, 0.8725745677947998, 1.2835677862167358, 1.0473345518112183, 0.9251940250396729, 0.9932470321655273, 0.7030851244926453, -0.3882410526275635, 0.8727421164512634, 0.8473307490348816, 1.0163909196853638, 1.5263340473175049]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7178584337234497, 0.24312236905097961, 0.8212395906448364, 0.7470623254776001, 0.6682671308517456, 0.4592432677745819, 0.315671443939209, 0.885617733001709, -0.8189968466758728, 2.630431652069092, -0.6900985836982727, 0.7403109669685364, 0.8735064268112183, 0.8919079303741455, 0.8319610357284546, 0.4699435830116272]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.043360091745853424, 0.5857707262039185, 1.3531709909439087, 1.2131744623184204, 1.324320912361145, 1.3869229555130005, 1.4129902124404907, 1.3278696537017822, 1.7703291177749634, 1.3530999422073364, 2.022027015686035, 1.344975471496582, 1.3907644748687744, 1.1814303398132324, 0.7044224143028259, 1.341709852218628]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3315716981887817, 1.2760672569274902, 1.356831431388855, 1.6553564071655273, 0.9297451376914978, 1.1702154874801636, 1.7529616355895996, 1.6553819179534912, 2.0413284301757812, 1.3372139930725098, 1.097887635231018, 0.9011358022689819, 1.1458520889282227, 1.2039400339126587, 1.4144326448440552, 1.382301926612854]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3061044216156006, 1.2178053855895996, 1.153071403503418, 1.2079886198043823, 1.4379340410232544, 0.9305475950241089, 1.0023361444473267, 1.1828023195266724, 1.902490496635437, 1.4011437892913818, 1.4294066429138184, 1.3805019855499268, 1.8366141319274902, 1.3627195358276367, 1.2629059553146362, 1.322865605354309]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8910606503486633, 1.0740145444869995, 1.0818908214569092, 0.9160938262939453, 0.06217966601252556, 1.2489851713180542, 1.1855723857879639, 1.8354012966156006, 0.8433953523635864, 1.2461639642715454, 1.2666462659835815, 1.183558702468872, 1.1095919609069824, 1.4291512966156006, 1.118682622909546, 0.44139838218688965]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.9948043823242188, 2.1534926891326904, 1.9949448108673096, 1.891978144645691, 1.8438776731491089, 1.9990042448043823, 1.8512433767318726, 2.0558619499206543, 1.884063482284546, 1.9619460105895996, 2.2752246856689453, 2.6264808177948, 1.8118393421173096, 2.330601453781128, 2.1836957931518555, 2.0265777111053467]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3855187892913818, 1.303257703781128, 1.8055044412612915, 1.0806063413619995, 1.4683414697647095, 1.5257352590560913, 1.3369332551956177, 1.3798763751983643, 1.4384957551956177, 1.6116217374801636, 1.5386539697647095, 2.1796364784240723, 1.4987235069274902, 1.5082465410232544, 1.2067759037017822, 1.630131721496582]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.605877161026001, 1.8219082355499268, 1.2590786218643188, 1.4459060430526733, 1.5782781839370728, 1.5263736248016357, 1.4544334411621094, 1.4756178855895996, 1.575838327407837, 1.497057557106018, 1.5817759037017822, 1.5490260124206543, 1.4590610265731812, 0.945611298084259, 1.572265625, 1.5517258644104004]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3826849460601807, 2.3820722103118896, 2.2262561321258545, 2.18581485748291, 2.7365705966949463, 2.3288910388946533, 2.645961046218872, 2.2507147789001465, 2.2922539710998535, 2.2743821144104004, 2.744485378265381, 2.2782373428344727, 2.2705013751983643, 2.4566993713378906, 2.360396146774292, 2.4128880500793457]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.645769476890564, 1.7096610069274902, 1.4420572519302368, 1.6314913034439087, 1.4540058374404907, 1.5140548944473267, 1.5403262376785278, 1.8473626375198364, 1.5310043096542358, 1.4995914697647095, 1.524280071258545, 1.4208282232284546, 1.5017744302749634, 1.3944802284240723, 1.622931957244873, 1.9003396034240723]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9120041131973267, 1.722180724143982, 1.7138687372207642, 1.798974871635437, 1.7335261106491089, 1.7736353874206543, 1.695322036743164, 1.7652994394302368, 1.7884258031845093, 1.8890994787216187, 2.1881191730499268, 1.6394633054733276, 1.7051547765731812, 1.8360012769699097, 1.8706469535827637, 1.7623291015625]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Running loglikelihood requests:  28%|█████████████▋                                   | 141/504 [08:04<19:43,  3.26s/it]Layer: gate_27 - Captured router_logits: [1.6017699241638184, 1.6661955118179321, 1.621479868888855, 1.7464001178741455, 1.662495493888855, 1.6130694150924683, 1.7382166385650635, 1.7122268676757812, 1.633575201034546, 1.7309410572052002, 1.4922130107879639, 1.734987735748291, 1.6525416374206543, 1.8632014989852905, 1.5916372537612915, 1.6220033168792725]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.051776885986328, 2.134382724761963, 2.5016212463378906, 2.050203561782837, 2.1283955574035645, 2.0455985069274902, 2.5206801891326904, 2.1095919609069824, 2.029118061065674, 2.0630106925964355, 1.9693244695663452, 1.8946844339370728, 1.976051926612854, 1.8595975637435913, 2.3810253143310547, 2.101855993270874]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.495404243469238, 5.798866271972656, 5.50168514251709, 5.468265056610107, 5.370965957641602, 5.304483413696289, 5.461601257324219, 5.877604007720947, 5.651092529296875, 5.459788799285889, 5.5202717781066895, 5.393075942993164, 5.5677595138549805, 5.5420241355896, 5.65456485748291, 5.470639228820801]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.9484527111053467, 3.7625739574432373, 3.7752373218536377, 3.6095283031463623, 3.847560405731201, 3.7963387966156006, 3.579529285430908, 3.838184118270874, 3.851179599761963, 3.8585197925567627, 3.694087028503418, 3.458441734313965, 3.7885773181915283, 3.8166615962982178, 3.868349075317383, 3.9146242141723633]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1362335681915283, 3.2163500785827637, 3.057547092437744, 2.9464869499206543, 3.001225471496582, 3.1828534603118896, 3.1008987426757812, 2.989506721496582, 3.063521146774292, 3.074193239212036, 3.328993082046509, 2.5368101596832275, 3.1174938678741455, 3.0717933177948, 3.3282783031463623, 3.1551265716552734]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09615817666053772, 0.11410221457481384, 0.10984978079795837, -0.25016504526138306, -0.22889980673789978, -0.14502781629562378, 0.12753938138484955, -0.1567254364490509, 0.08235559612512589, 0.10016380995512009, 0.10520718991756439, 0.09289059042930603, 0.09659255295991898, 0.1070418581366539, -1.1064517498016357, 0.12436836957931519]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0904986709356308, 0.060960881412029266, 0.04163059592247009, 0.04162845015525818, 0.08998263627290726, 0.026045046746730804, 0.05694713070988655, 0.07125087827444077, 0.01251180563122034, 0.0719461441040039, -0.18267300724983215, 0.054492849856615067, 0.009790408425033092, -0.03327655792236328, 0.01333377230912447, 0.044384945183992386]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09528160095214844, 0.06185014545917511, 0.09128600358963013, 0.06553655862808228, 0.10128442943096161, 0.11273334175348282, 0.07437324523925781, -0.08539380878210068, 0.07399465888738632, 0.08795773237943649, 0.0043237772770226, 0.08701515197753906, -0.16162149608135223, 0.03075364977121353, -0.020772332325577736, 0.11174453049898148]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13318905234336853, 0.17428569495677948, 0.1226641982793808, 0.1406925618648529, 0.14811424911022186, 0.12038034945726395, 0.05727115273475647, 0.18682260811328888, 0.1820443868637085, -0.40639013051986694, -0.017064746469259262, 0.1092388778924942, 0.16248953342437744, -0.191200852394104, -0.039268895983695984, -0.06249262019991875]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07524839043617249, 0.10659604519605637, 0.09381218999624252, 0.12647870182991028, 0.04476077854633331, -0.047056399285793304, 0.04496965929865837, 0.05134602636098862, -0.05461893603205681, 0.027425866574048996, -0.1390037089586258, 0.15776726603507996, -0.04307897388935089, -0.1858324259519577, 0.12270184606313705, -0.02219190075993538]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07941356301307678, 0.12394975125789642, 0.06557776033878326, 0.10894022136926651, -0.036672089248895645, -0.005171022843569517, 0.1335812658071518, 0.038343679159879684, 0.11456579715013504, -0.08628945797681808, -0.11831077933311462, 0.09351489692926407, -0.14897486567497253, 0.13221479952335358, 0.1332259476184845, 0.0635833740234375]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.0802856981754303, 0.042571019381284714, 0.16145043075084686, 0.3347870707511902, 0.17901591956615448, 0.13055026531219482, -0.06868217140436172, -0.04792459309101105, 0.49117520451545715, 0.15657444298267365, -0.47918057441711426, 0.3328857421875, 0.19484108686447144, -0.10421798378229141, 0.15940937399864197, 0.18963080644607544]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3459697961807251, 0.2523966431617737, 0.31982341408729553, 0.10400159657001495, -0.8663274049758911, 0.3306993246078491, 0.18180054426193237, -0.12828083336353302, 0.39844873547554016, 0.1943182647228241, -0.03214560076594353, 0.2893030345439911, 0.35163864493370056, 0.1375907063484192, -0.3619886636734009, -0.03644963353872299]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.19857124984264374, 0.34701818227767944, -0.08493232727050781, 0.3392041325569153, 0.5186237692832947, 0.3009161651134491, 0.2908017039299011, -0.17516829073429108, 0.15564948320388794, 0.6724279522895813, -0.005439356900751591, 0.45670661330223083, 0.3500109314918518, -0.3249991536140442, -0.20206241309642792, 0.3104725778102875]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.47549158334732056, 0.2473789006471634, 0.48882293701171875, 0.8056464195251465, 0.4599866271018982, 0.45212554931640625, 0.7092525959014893, 0.5744050741195679, 0.5546009540557861, 0.18626323342323303, 0.5371696352958679, 0.7193089723587036, 0.9171592593193054, 0.5953598022460938, 0.7708997130393982, 0.7346833944320679]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9682095050811768, 0.8573190569877625, 0.66180419921875, 0.8743253946304321, 1.099037528038025, 0.7266086935997009, 0.6703988909721375, 0.6805805563926697, 0.39571863412857056, 0.9128667116165161, 0.7551366090774536, 0.9425113201141357, 0.6072010397911072, 0.2679736614227295, 0.7349833250045776, 0.48721957206726074]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0529013872146606, 1.1239351034164429, 0.9736841917037964, 1.3610743284225464, 1.4311652183532715, 0.8297584652900696, 0.9634367227554321, 1.3940943479537964, 1.0649220943450928, 1.3787328004837036, 0.9991198182106018, 1.1694400310516357, 0.8018072247505188, 0.5553683042526245, 0.041524987667798996, 0.8928134441375732]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.3920113444328308, 0.9879439473152161, 1.2555124759674072, 1.2831517457962036, 0.5171095132827759, 0.8320344686508179, 1.0691704750061035, 1.29791259765625, 1.0281167030334473, 0.7323684692382812, 0.727689266204834, 0.8945441246032715, 1.1580361127853394, 0.8897640705108643, 0.6672885417938232, 1.0340640544891357]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7822675108909607, 0.9157028198242188, 0.8729039430618286, 0.6532966494560242, 1.236572265625, 0.2874852120876312, 1.1334292888641357, 2.2845652103424072, 1.023521065711975, 0.9668225646018982, 1.30224609375, 1.303017020225525, -0.055005524307489395, 1.03581964969635, 1.122572660446167, 0.8238437175750732]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7075340151786804, 0.7929494976997375, 0.9080874919891357, 0.8456083536148071, 0.9073036313056946, 0.8470756411552429, 0.5749688148498535, 1.225366234779358, 0.7233782410621643, 1.0306733846664429, 0.20578444004058838, 0.7696918845176697, 1.0924216508865356, 1.0328304767608643, 1.0991339683532715, 0.7039433717727661]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4144873321056366, 1.1656044721603394, 1.0059878826141357, 0.8237625956535339, 0.6510668396949768, 0.8584883809089661, 1.146278738975525, 1.0147769451141357, 0.8821957111358643, 0.9424341917037964, 0.7374364137649536, -0.34121403098106384, 0.8108245730400085, 0.8605828285217285, 1.0858732461929321, 1.497191309928894]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8128613829612732, 0.3192140758037567, 0.8619465231895447, 0.7478862404823303, 0.6711313128471375, 0.5266791582107544, 0.4154393970966339, 0.9176025390625, -0.7326459288597107, 2.628931999206543, -0.6326504945755005, 0.7838030457496643, 0.9225496053695679, 0.9547440409660339, 0.8612317442893982, 0.7150798439979553]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.14394819736480713, 0.6285392642021179, 1.3614373207092285, 1.3039357662200928, 1.3394068479537964, 1.3869757652282715, 1.439234733581543, 1.4041683673858643, 1.730076789855957, 1.5483688116073608, 2.0052554607391357, 1.349146842956543, 1.3863292932510376, 1.1194297075271606, 0.7467715740203857, 1.4292312860488892]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3869757652282715, 1.3402806520462036, 1.4082127809524536, 1.652806282043457, 1.049033761024475, 1.3104441165924072, 1.8661338090896606, 1.7105777263641357, 2.02458119392395, 1.3985146284103394, 1.2008763551712036, 1.0543473958969116, 1.3105148077011108, 1.4594907760620117, 1.5132607221603394, 1.448704719543457]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.306884765625, 1.249473214149475, 1.1458162069320679, 1.1893503665924072, 1.413599967956543, 0.9625629782676697, 1.0303634405136108, 1.210487723350525, 1.8761886358261108, 1.375732421875, 1.4110043048858643, 1.3486328125, 1.994654655456543, 1.3489412069320679, 1.245579719543457, 1.2935212850570679]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8721144795417786, 1.071070671081543, 1.0750025510787964, 0.898162841796875, 0.18269066512584686, 1.1927040815353394, 1.2073267698287964, 1.939539909362793, 0.8423959612846375, 1.193153738975525, 1.2753777503967285, 1.098350167274475, 1.0843056440353394, 1.4598067998886108, 1.1946548223495483, 0.5281714200973511]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.06486439704895, 2.195363998413086, 2.0498046875, 2.00675892829895, 1.9240593910217285, 2.0305304527282715, 1.9441817998886108, 2.18318247795105, 1.9652035236358643, 2.0754780769348145, 2.3590667247772217, 2.811985969543457, 1.9109914302825928, 2.419407844543457, 2.2628753185272217, 2.1217620372772217]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4683644771575928, 1.322959542274475, 1.8196957111358643, 1.15966796875, 1.5152652263641357, 1.5699269771575928, 1.3864103555679321, 1.3984888792037964, 1.4977127313613892, 1.6592310667037964, 1.5435855388641357, 2.313939094543457, 1.5580283403396606, 1.5701583623886108, 1.2916709184646606, 1.6719777584075928]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.59626841545105, 1.8044562339782715, 1.3172768354415894, 1.4705489873886108, 1.5037906169891357, 1.5365697145462036, 1.4281070232391357, 1.4986122846603394, 1.5197882652282715, 1.4543328285217285, 1.5878392457962036, 1.4989527463912964, 1.4571726322174072, 1.019461750984192, 1.568699836730957, 1.5466179847717285]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3790090084075928, 2.4196135997772217, 2.25187611579895, 2.2248148918151855, 2.8815789222717285, 2.3811676502227783, 2.7496917247772217, 2.2947163581848145, 2.330103874206543, 2.3689351081848145, 2.6669921875, 2.327096939086914, 2.3074116706848145, 2.474609375, 2.4314863681793213, 2.4538960456848145]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.666015625, 1.833958625793457, 1.4836297035217285, 1.6712582111358643, 1.5416837930679321, 1.5371865034103394, 1.5813502073287964, 1.9009302854537964, 1.5765959024429321, 1.5505242347717285, 1.5714046955108643, 1.4618113040924072, 1.5413240194320679, 1.458701729774475, 1.655479073524475, 1.9781044721603394]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9821006059646606, 1.7391823530197144, 1.7511179447174072, 1.850341796875, 1.7734792232513428, 1.8055387735366821, 1.71942937374115, 1.8259245157241821, 1.8335442543029785, 1.9167190790176392, 2.2184479236602783, 1.6868350505828857, 1.7320621013641357, 1.9325271844863892, 1.8613474369049072, 1.8072870969772339]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6387746334075928, 1.6875545978546143, 1.6423243284225464, 1.7823277711868286, 1.6842361688613892, 1.649658203125, 1.7628945112228394, 1.76849365234375, 1.6538214683532715, 1.795493721961975, 1.5506784915924072, 1.7627081871032715, 1.680137276649475, 1.866563320159912, 1.6326261758804321, 1.6626454591751099]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1247615814208984, 2.2124922275543213, 2.6012091636657715, 2.1160953044891357, 2.2099030017852783, 2.147507429122925, 2.63374400138855, 2.2060353755950928, 2.08271861076355, 2.1226999759674072, 2.0340962409973145, 1.9792351722717285, 2.0498433113098145, 1.9506322145462036, 2.4422800540924072, 2.1870758533477783]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.533203125, 5.865182876586914, 5.599506378173828, 5.5119242668151855, 5.446237564086914, 5.341180324554443, 5.523540496826172, 5.940943717956543, 5.712582111358643, 5.499537467956543, 5.545949935913086, 5.435443878173828, 5.605520248413086, 5.614617824554443, 5.720703125, 5.5422492027282715]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.0930304527282715, 3.9162983894348145, 3.90144419670105, 3.7707648277282715, 3.9544870853424072, 3.937204360961914, 3.7341694831848145, 3.9406352043151855, 3.965346574783325, 3.996389389038086, 3.8069489002227783, 3.6446821689605713, 3.9645609855651855, 3.915013313293457, 4.022100925445557, 4.071725845336914]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  29%|██████████████                                   | 145/504 [08:17<19:29,  3.26s/it]Layer: gate_31 - Captured router_logits: [3.14787220954895, 3.2478413581848145, 3.0848581790924072, 2.87109375, 3.0172183513641357, 3.21345591545105, 3.0241570472717285, 3.03263783454895, 3.0578227043151855, 3.10734486579895, 3.295949935913086, 2.5888671875, 3.1267988681793213, 3.06111216545105, 3.2995476722717285, 3.141550064086914]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.06585993617773056, 0.08090295642614365, 0.09324651211500168, -0.28793033957481384, -0.26755040884017944, -0.08968774974346161, 0.09525616466999054, -0.05206580087542534, 0.06492429226636887, 0.06533382087945938, 0.06898674368858337, 0.03577761724591255, 0.07245068997144699, 0.10365056991577148, -1.0621651411056519, 0.10539757460355759]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.07904218137264252, 0.03395198658108711, 0.02661075070500374, 0.04631989821791649, 0.06364482641220093, 0.021896110847592354, 0.04361308366060257, 0.09823548048734665, 0.06199982389807701, 0.0625004991889, -0.18970811367034912, 0.062137968838214874, 0.0063833813183009624, -0.020803287625312805, 0.041065968573093414, 0.04059518128633499]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09108854830265045, 0.03745228424668312, 0.07838676124811172, 0.059017255902290344, 0.14950712025165558, 0.08304169028997421, 0.08279737830162048, -0.09993041306734085, 0.08348286896944046, 0.1321347951889038, 0.037646520882844925, 0.07886525243520737, -0.21576569974422455, 0.010449660010635853, -0.015881424769759178, 0.0964590385556221]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12787587940692902, 0.14046066999435425, 0.1244378313422203, 0.1394331008195877, 0.13246917724609375, 0.11417830735445023, -0.0063751875422894955, 0.15975826978683472, 0.17724990844726562, -0.5511863827705383, 0.027421047911047935, 0.06947075575590134, 0.3022893965244293, -0.3139708638191223, 0.0914364829659462, -0.0701073631644249]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.05646238848567009, 0.09849869459867477, 0.06150807812809944, 0.285321980714798, 0.07129306346178055, -0.051309484988451004, 0.03427856042981148, 0.013888409361243248, -0.1048266738653183, 0.008886236697435379, -0.2865007221698761, 0.11606718599796295, 0.11678645759820938, -0.24438486993312836, 0.15514172613620758, -0.11648689955472946]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.04630992189049721, 0.14397089183330536, 0.07175583392381668, 0.24166443943977356, -0.13970445096492767, -0.12772409617900848, 0.11757208406925201, -0.011982817202806473, 0.23707500100135803, -0.10380559414625168, -0.15537060797214508, 0.11714563518762589, -0.2572902739048004, 0.19821004569530487, 0.11874976009130478, 0.08784374594688416]
Layer: gate_5 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.1270914077758789, -0.0036793758627027273, 0.17981800436973572, 0.28352174162864685, 0.19271498918533325, 0.14586558938026428, -0.19630733132362366, -0.0694972574710846, 0.37989866733551025, 0.1727154403924942, -0.6152620911598206, 0.27873751521110535, 0.2767077088356018, -0.30151087045669556, 0.3591190278530121, 0.2383776754140854]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5832318663597107, 0.20948068797588348, 0.24181967973709106, 0.1626674234867096, -0.91229248046875, 0.28816643357276917, 0.20353537797927856, -0.441056489944458, 0.6526579260826111, 0.18007077276706696, -0.19371534883975983, 0.26287510991096497, 0.09366502612829208, 0.2903175354003906, -0.6608214378356934, -0.3309840261936188]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.27885356545448303, 0.2750721871852875, -0.24683581292629242, 0.26815274357795715, 0.43585968017578125, 0.35699281096458435, 0.27884534001350403, -0.325813889503479, 0.21922925114631653, 0.9469640851020813, 0.0843740776181221, 0.2603832185268402, 0.5203793048858643, -0.5111017823219299, -0.4808598458766937, 0.46975627541542053]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.482191801071167, 0.2286839783191681, 0.5135000348091125, 1.2338080406188965, 0.4518527090549469, 0.28505003452301025, 0.6912897825241089, 0.6360313296318054, 1.1261106729507446, 0.17605149745941162, 0.23484280705451965, 0.7266524434089661, 0.6036524772644043, 0.044415224343538284, 0.8624845743179321, 0.6742650270462036]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1357080936431885, 0.9301693439483643, 0.5405225157737732, 0.7190262675285339, 0.8833144307136536, 0.8147125244140625, 0.6995384097099304, 0.7823421955108643, 0.13747797906398773, 0.5597373843193054, 0.8161107301712036, 1.0627024173736572, 0.23658591508865356, 0.2274744063615799, 1.2045115232467651, 0.5431309938430786]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [1.0193898677825928, 1.427589774131775, 0.9357010722160339, 1.3441158533096313, 1.3989225625991821, 0.3276647627353668, 1.039345145225525, 1.0061196088790894, 1.0947779417037964, 1.7505782842636108, 0.9427040219306946, 1.0458791255950928, 0.36245205998420715, 0.5536482930183411, 0.09879383444786072, 0.8299014568328857]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.1845281571149826, 0.8643317222595215, 1.3359568119049072, 1.2908614873886108, 0.4458947479724884, 0.5669041872024536, 1.2856967449188232, 1.5812313556671143, 0.4736587107181549, 0.16777320206165314, 0.9127004742622375, 0.8615337014198303, 0.9119294881820679, 0.9135613441467285, 0.7535592913627625, 0.9623380899429321]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.9762155413627625, 1.1761410236358643, 1.2189234495162964, 0.5987677574157715, 1.2164113521575928, -0.0768866315484047, 1.2553261518478394, 2.3367340564727783, 1.3934775590896606, 1.0531712770462036, 1.8088507652282715, 1.3288445472717285, 0.2987108826637268, 0.9507558941841125, 1.1845703125, 0.862776517868042]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8274520039558411, 0.8252981305122375, 1.1497224569320679, 0.8936703205108643, 1.102333426475525, 1.486915946006775, 0.4690021574497223, 1.8877185583114624, 0.8364579081535339, 0.7933598756790161, -0.33288413286209106, 0.8565226197242737, 0.9851126670837402, 1.1133712530136108, 1.2073203325271606, 0.5820256471633911]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.44500452280044556, 1.2435752153396606, 1.0096242427825928, 1.0202701091766357, 0.8414193987846375, 0.9048108458518982, 1.8704215288162231, 1.1057642698287964, 1.0912572145462036, 1.0299650430679321, 0.7583553791046143, -0.38918405771255493, 1.13055419921875, 0.9623766541481018, 1.011744499206543, 1.5847983360290527]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.5816417336463928, 0.031945377588272095, 0.7046051025390625, 0.7415257692337036, 0.7422003746032715, 0.46648749709129333, 0.7474156618118286, 0.7648974061012268, -0.8923265337944031, 2.131591796875, -0.8107399940490723, 0.5799761414527893, 0.7734310626983643, 0.7780472636222839, 0.7712337970733643, 0.4686809480190277]
Running loglikelihood requests:  30%|██████████████▍                                  | 149/504 [08:30<19:30,  3.30s/it]Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.18881064653396606, 0.7869459390640259, 1.4805587530136108, 1.3872809410095215, 1.5356380939483643, 1.4739669561386108, 1.5436289310455322, 1.4214670658111572, 2.035244941711426, 1.3656102418899536, 2.5685136318206787, 1.8162761926651, 1.566431999206543, 1.3996775150299072, 0.5873826742172241, 1.3223888874053955]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6217361688613892, 1.4274002313613892, 1.5856587886810303, 2.001439094543457, 1.0413681268692017, 1.290797233581543, 2.3746917247772217, 1.8844058513641357, 2.68415904045105, 1.6273128986358643, 1.325465202331543, 1.0059633255004883, 1.3464258909225464, 0.9699482321739197, 1.6446083784103394, 1.630859375]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.646201729774475, 1.4789011478424072, 1.90234375, 1.4783999919891357, 1.8058439493179321, 1.171107292175293, 1.462363839149475, 1.4313771724700928, 2.3868730068206787, 1.558799386024475, 1.8803582191467285, 1.655234932899475, 1.7897820472717285, 1.694849967956543, 1.6989617347717285, 1.693809151649475]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.391563057899475, 1.0604441165924072, 1.0100547075271606, 0.9468094706535339, -0.09312117844820023, 1.437230110168457, 1.2110146284103394, 1.8879908323287964, 0.8470844626426697, 1.3526418209075928, 1.239232063293457, 1.1920005083084106, 1.1195261478424072, 1.3018091917037964, 0.9478052854537964, 0.4213832914829254]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.1832854747772217, 2.344675064086914, 2.30216908454895, 2.0711605548858643, 2.030299186706543, 2.20759654045105, 2.0621659755706787, 2.364309310913086, 2.1021535396575928, 2.21818470954895, 2.4611945152282715, 2.78572154045105, 1.7579152584075928, 2.5119757652282715, 2.25837779045105, 2.199270248413086]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4690070152282715, 1.4904913902282715, 2.1951582431793213, 1.0703542232513428, 1.5039833784103394, 1.657020926475525, 1.340736746788025, 1.4986636638641357, 1.552708625793457, 1.6667351722717285, 1.6861379146575928, 2.2258942127227783, 1.5509611368179321, 1.6648077964782715, 1.2857345342636108, 1.717002511024475]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.54728627204895, 1.9212839603424072, 1.2015509605407715, 1.5762490034103394, 1.7778834104537964, 1.5004369020462036, 1.5520919561386108, 1.453099250793457, 1.5496890544891357, 1.5811060667037964, 1.5598273277282715, 1.62744140625, 1.285606026649475, 0.6538206338882446, 1.3745888471603394, 1.544407844543457]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.5706722736358643, 2.5187602043151855, 2.4864308834075928, 2.2068769931793213, 2.6652960777282715, 2.2946135997772217, 2.5144941806793213, 2.2423417568206787, 2.1823089122772217, 2.2509765625, 3.00138783454895, 2.2413651943206787, 2.448190689086914, 2.5474917888641357, 2.3694489002227783, 2.32755970954895]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7691457271575928, 1.733604073524475, 1.6363589763641357, 1.882606863975525, 1.3482730388641357, 1.7793482542037964, 1.7828433513641357, 2.1032586097717285, 1.7722039222717285, 1.653114676475525, 1.6995785236358643, 1.564658761024475, 1.7219109535217285, 1.5406558513641357, 2.262746810913086, 2.0489566326141357]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_26 - Captured router_logits: [1.8277523517608643, 2.0615875720977783, 1.7757247686386108, 1.8103927373886108, 1.8022974729537964, 1.9721310138702393, 1.8875949382781982, 1.8346267938613892, 1.8928322792053223, 1.9112035036087036, 2.5721435546875, 1.7625218629837036, 1.8249640464782715, 1.9089484214782715, 1.9696133136749268, 1.8833585977554321]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6461052894592285, 1.7447577714920044, 1.6639853715896606, 1.707798957824707, 1.578651785850525, 1.6162470579147339, 1.8796792030334473, 1.7334827184677124, 1.6021535396575928, 1.7095433473587036, 1.2907072305679321, 1.6529059410095215, 1.740086555480957, 2.1445329189300537, 1.6625301837921143, 1.6262046098709106]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1312448978424072, 2.2199320793151855, 2.4466359615325928, 2.072214126586914, 2.2286183834075928, 2.1402909755706787, 2.74031138420105, 2.23640513420105, 2.138132095336914, 2.0905120372772217, 2.0759918689727783, 1.8853310346603394, 1.99658203125, 1.9219263792037964, 2.4256784915924072, 2.0559723377227783]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.031815528869629, 5.5915398597717285, 5.019634246826172, 4.867091178894043, 4.9443488121032715, 4.988821029663086, 4.9987664222717285, 5.1623406410217285, 5.223941326141357, 5.022409439086914, 4.9900031089782715, 4.9484734535217285, 5.010028839111328, 5.171926498413086, 5.222707748413086, 5.0325608253479]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.6107113361358643, 3.4694437980651855, 3.40974497795105, 3.2918121814727783, 3.5138261318206787, 3.502415657043457, 3.1542904376983643, 3.6409332752227783, 3.682565689086914, 3.46931529045105, 3.3224456310272217, 2.89372181892395, 3.2072947025299072, 3.531275749206543, 3.5226664543151855, 3.53938364982605]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.677271842956543, 2.7698910236358643, 2.8111636638641357, 2.7388980388641357, 2.6915605068206787, 2.696751594543457, 2.7325246334075928, 2.344778060913086, 2.6839535236358643, 2.4205129146575928, 3.2478413581848145, 1.8849358558654785, 2.6950042247772217, 2.5744242668151855, 3.2198808193206787, 2.6731598377227783]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09951522946357727, 0.1190042644739151, 0.11736058443784714, -0.2682138979434967, -0.24365641176700592, -0.12694275379180908, 0.13202646374702454, -0.17552724480628967, 0.07662510126829147, 0.10335815697908401, 0.10962890833616257, 0.09354329109191895, 0.09814000129699707, 0.11106842011213303, -1.1782160997390747, 0.12904632091522217]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09304586052894592, 0.05666086822748184, 0.04559361934661865, 0.04669053480029106, 0.08367513120174408, 0.024185968562960625, 0.05090830475091934, 0.07531891018152237, 0.022652791813015938, 0.0753268450498581, -0.19447855651378632, 0.0624426007270813, -0.002904612338170409, -0.033744100481271744, 0.01999865286052227, 0.041668739169836044]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.10105082392692566, 0.06187751889228821, 0.09274434298276901, 0.06243143603205681, 0.1046847552061081, 0.11636199802160263, 0.07424771785736084, -0.09489715844392776, 0.07942090183496475, 0.10868634283542633, 0.013018290512263775, 0.08861193060874939, -0.1794140636920929, 0.033756356686353683, -0.020562438294291496, 0.11036895960569382]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1352994292974472, 0.1648712158203125, 0.12354512512683868, 0.1353200227022171, 0.1407432109117508, 0.11305348575115204, 0.0531025193631649, 0.18256007134914398, 0.19621342420578003, -0.43285930156707764, -0.009966430254280567, 0.09178818762302399, 0.17887939512729645, -0.20551839470863342, -0.020245157182216644, -0.060501404106616974]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0687674954533577, 0.1178010031580925, 0.08987879753112793, 0.15010294318199158, 0.04767429456114769, -0.06467223912477493, 0.042461395263671875, 0.043799083679914474, -0.08580907434225082, 0.03439290449023247, -0.16401895880699158, 0.1533329337835312, -0.024027710780501366, -0.20430633425712585, 0.1359826624393463, -0.0075268554501235485]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.08419616520404816, 0.1210983544588089, 0.07368692755699158, 0.1255601942539215, -0.019858907908201218, -0.03243977949023247, 0.12489550560712814, 0.011441548354923725, 0.13700862228870392, -0.09504710137844086, -0.11414062231779099, 0.09465952217578888, -0.1834843009710312, 0.12675446271896362, 0.11542053520679474, 0.052149709314107895]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.0865059643983841, 0.01941772550344467, 0.19160450994968414, 0.33466553688049316, 0.20577372610569, 0.1642368584871292, -0.1014932245016098, -0.07592245936393738, 0.4148643910884857, 0.1554967314004898, -0.5356632471084595, 0.31643933057785034, 0.2091483622789383, -0.12830626964569092, 0.19320984184741974, 0.1924675554037094]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.38044026494026184, 0.2370641529560089, 0.286458283662796, 0.15283386409282684, -0.955357551574707, 0.3146595358848572, 0.19655802845954895, -0.24326105415821075, 0.4575439989566803, 0.19072997570037842, -0.12244222313165665, 0.29976195096969604, 0.2838529944419861, 0.15308603644371033, -0.49672770500183105, -0.09040242433547974]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.19598270952701569, 0.35544678568840027, -0.19694742560386658, 0.3174237012863159, 0.554582953453064, 0.2985331118106842, 0.282627671957016, -0.2453397661447525, 0.18352162837982178, 0.7275392413139343, -0.0044157919473946095, 0.3760451376438141, 0.34875813126564026, -0.43068644404411316, -0.25245076417922974, 0.3163449168205261]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.44039225578308105, 0.21379190683364868, 0.4514670670032501, 0.8720296025276184, 0.3895391821861267, 0.3992730677127838, 0.6761637330055237, 0.5298762917518616, 0.5864697098731995, 0.09894175082445145, 0.42711424827575684, 0.6985481977462769, 0.770311713218689, 0.5029587149620056, 0.7323795557022095, 0.6612207293510437]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9343652129173279, 0.7976301908493042, 0.5976595282554626, 0.8182812333106995, 1.074324131011963, 0.6383776664733887, 0.6391829252243042, 0.6752343773841858, 0.3629903197288513, 0.832928478717804, 0.7497330904006958, 0.9188899993896484, 0.3817073702812195, 0.2581537961959839, 0.7968182563781738, 0.45764079689979553]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0330045223236084, 1.0631558895111084, 0.9636197686195374, 1.1901921033859253, 1.3499234914779663, 0.7520166039466858, 0.9338932037353516, 1.2884504795074463, 1.1093717813491821, 1.3659570217132568, 0.8953515887260437, 1.119570255279541, 0.7101106643676758, 0.4779919385910034, -0.017822062596678734, 0.8189452886581421]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.23727518320083618, 0.9428141117095947, 1.2186002731323242, 1.1204557418823242, 0.39275309443473816, 0.7845898270606995, 0.9901778101921082, 1.2134212255477905, 0.9285008907318115, 0.5692490935325623, 0.6753641963005066, 0.8674869537353516, 1.0571223497390747, 0.8166341185569763, 0.559827446937561, 0.9309700727462769]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7602079510688782, 0.8868098855018616, 0.8305627703666687, 0.5015246868133545, 1.1435025930404663, 0.16601969301700592, 1.0722135305404663, 2.2294270992279053, 0.9369531273841858, 0.8785611987113953, 1.211757779121399, 1.1555012464523315, -0.11290527135133743, 0.9507861137390137, 1.0658398866653442, 0.6907698512077332]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7141808867454529, 0.7454671263694763, 0.9276887774467468, 0.8332014679908752, 0.9072916507720947, 0.9297342896461487, 0.4397884011268616, 1.2907867431640625, 0.8011751174926758, 0.9611914157867432, 0.16373310983181, 0.7409944534301758, 1.0607608556747437, 1.0275260210037231, 1.049830675125122, 0.6548774242401123]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.42577168345451355, 1.0934375524520874, 0.9786978960037231, 0.8768228888511658, 0.6102001667022705, 0.8565885424613953, 1.2249324321746826, 1.0317187309265137, 0.8852213621139526, 0.9565885663032532, 0.6849690675735474, -0.3944539427757263, 0.8179548978805542, 0.8503385186195374, 0.9863671660423279, 1.503662109375]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7189192771911621, 0.21719248592853546, 0.8131266236305237, 0.7178515791893005, 0.6247879862785339, 0.4576672315597534, 0.3111881613731384, 0.8815014362335205, -0.8329362273216248, 2.589322805404663, -0.6473429203033447, 0.7462223172187805, 0.8641145825386047, 0.8958919048309326, 0.8280208110809326, 0.49337732791900635]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.027342122048139572, 0.5446801781654358, 1.3341275453567505, 1.1847572326660156, 1.2811068296432495, 1.3580468893051147, 1.3812109231948853, 1.2949023246765137, 1.730442762374878, 1.3594938516616821, 1.9758983850479126, 1.3332942724227905, 1.3665006160736084, 1.1824674606323242, 0.7168766260147095, 1.293154239654541]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3461588621139526, 1.3141926527023315, 1.369713544845581, 1.6508593559265137, 0.9380533695220947, 1.1723698377609253, 1.7793749570846558, 1.6491667032241821, 2.0130176544189453, 1.3488801717758179, 1.119140625, 0.9262849688529968, 1.1512939929962158, 1.2433974742889404, 1.4295573234558105, 1.3912239074707031]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.2440104484558105, 1.142910122871399, 1.1053515672683716, 1.1432422399520874, 1.3725260496139526, 0.8618546724319458, 0.9167863130569458, 1.1073404550552368, 1.8306770324707031, 1.3717317581176758, 1.3534114360809326, 1.3083072900772095, 1.8046679496765137, 1.290026068687439, 1.19196617603302, 1.2617838382720947]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8765909671783447, 1.0380533933639526, 1.0712890625, 0.8922428488731384, 0.030432026833295822, 1.2147656679153442, 1.1500911712646484, 1.8480533361434937, 0.8599625825881958, 1.2176432609558105, 1.240390658378601, 1.1759390830993652, 1.082838535308838, 1.3976562023162842, 1.0727978944778442, 0.3952506482601166]
Running loglikelihood requests:  30%|██████████████▊                                  | 153/504 [08:43<19:14,  3.29s/it]Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.9703385829925537, 2.140364646911621, 1.9772917032241821, 1.8698958158493042, 1.8124219179153442, 1.9932552576065063, 1.8283072710037231, 2.0464844703674316, 1.8617708683013916, 1.9247525930404663, 2.255625009536743, 2.6555728912353516, 1.7817057371139526, 2.31166672706604, 2.169114589691162, 2.0174219608306885]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.35212242603302, 1.2813801765441895, 1.7910937070846558, 1.0460807085037231, 1.4457552433013916, 1.5038541555404663, 1.3022786378860474, 1.3302865028381348, 1.4216406345367432, 1.5504426956176758, 1.532057285308838, 2.170781135559082, 1.4731770753860474, 1.4835416078567505, 1.1658658981323242, 1.5854426622390747]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.6319010257720947, 1.7991666793823242, 1.2424169778823853, 1.46247398853302, 1.544895887374878, 1.5222656726837158, 1.4274219274520874, 1.4528906345367432, 1.5588932037353516, 1.4743489027023315, 1.5766927003860474, 1.5406379699707031, 1.4469531774520874, 0.9167293310165405, 1.5540364980697632, 1.5280468463897705]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.4060938358306885, 2.369947910308838, 2.2403645515441895, 2.2024478912353516, 2.756406307220459, 2.324843645095825, 2.6553125381469727, 2.2432291507720947, 2.2855730056762695, 2.2659895420074463, 2.7160415649414062, 2.266770839691162, 2.2755208015441895, 2.4579687118530273, 2.359114646911621, 2.4021875858306885]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6266406774520874, 1.715833306312561, 1.4344791173934937, 1.6227344274520874, 1.4333853721618652, 1.4891666173934937, 1.5374739170074463, 1.8133593797683716, 1.517734408378601, 1.484765648841858, 1.5116145610809326, 1.3995572328567505, 1.4855729341506958, 1.3671094179153442, 1.6183854341506958, 1.8746875524520874]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.912109375, 1.727716088294983, 1.7014323472976685, 1.798020839691162, 1.7296744585037231, 1.7674674987792969, 1.6868358850479126, 1.7626302242279053, 1.7886865139007568, 1.8837499618530273, 2.18345046043396, 1.6324543952941895, 1.6971354484558105, 1.8208203315734863, 1.8840495347976685, 1.762180209159851]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.593824863433838, 1.656040072441101, 1.606276035308838, 1.7381770610809326, 1.6462630033493042, 1.604489803314209, 1.7313679456710815, 1.6988314390182495, 1.62255859375, 1.7161848545074463, 1.4764323234558105, 1.7289843559265137, 1.640882134437561, 1.8452441692352295, 1.5749739408493042, 1.6079427003860474]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.048203229904175, 2.1189582347869873, 2.5082812309265137, 2.0230729579925537, 2.109921932220459, 2.0436198711395264, 2.5009896755218506, 2.1011979579925537, 2.0099740028381348, 2.0321874618530273, 1.938489556312561, 1.86286461353302, 1.9689388275146484, 1.8343489170074463, 2.355130195617676, 2.089609384536743]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.452916622161865, 5.755624771118164, 5.463749885559082, 5.439426898956299, 5.341875076293945, 5.266302108764648, 5.4510416984558105, 5.861093521118164, 5.635000228881836, 5.436302185058594, 5.494765758514404, 5.35770845413208, 5.509101390838623, 5.5043230056762695, 5.621874809265137, 5.427708148956299]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.9636459350585938, 3.7589974403381348, 3.7825846672058105, 3.6272785663604736, 3.8707079887390137, 3.7890706062316895, 3.583750009536743, 3.8442447185516357, 3.8477604389190674, 3.8807055950164795, 3.715930938720703, 3.485504627227783, 3.822578191757202, 3.821152448654175, 3.8717708587646484, 3.9347264766693115]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.0873959064483643, 3.1861979961395264, 3.0059375762939453, 2.9005730152130127, 2.9608333110809326, 3.1027603149414062, 3.0597915649414062, 2.958385467529297, 3.01953125, 3.042851448059082, 3.275989532470703, 2.5174756050109863, 3.073437452316284, 3.0411198139190674, 3.271718740463257, 3.1204166412353516]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.090274378657341, 0.11001412570476532, 0.10515373945236206, -0.23796623945236206, -0.21845167875289917, -0.11979618668556213, 0.12438585609197617, -0.1782996654510498, 0.07639402151107788, 0.09548033773899078, 0.10209778696298599, 0.09018727391958237, 0.09315396100282669, 0.10485025495290756, -1.092334270477295, 0.11951830238103867]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09102010726928711, 0.06368303298950195, 0.04128342121839523, 0.04185141623020172, 0.08231287449598312, 0.027470530942082405, 0.05625912919640541, 0.06841825693845749, 0.0054046837612986565, 0.07323844730854034, -0.1758149415254593, 0.06207853928208351, 0.0015439795097336173, -0.02671184204518795, 0.012339790351688862, 0.04565306752920151]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09775517880916595, 0.0492391400039196, 0.09124504774808884, 0.06092829257249832, 0.1002316027879715, 0.11227616667747498, 0.0697150006890297, -0.08470696210861206, 0.07951139658689499, 0.0983048528432846, 0.004270867444574833, 0.08745124191045761, -0.15977446734905243, 0.030274122953414917, -0.029078131541609764, 0.11106596142053604]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.135952427983284, 0.18003641068935394, 0.11758770793676376, 0.13560214638710022, 0.14532040059566498, 0.12372530251741409, 0.048795148730278015, 0.17833946645259857, 0.18609127402305603, -0.3871079087257385, -0.01375605445355177, 0.10312750190496445, 0.1547507494688034, -0.1842639148235321, -0.029318585991859436, -0.05799228325486183]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07904154807329178, 0.09790581464767456, 0.09887618571519852, 0.12253709137439728, 0.04689732566475868, -0.03982602804899216, 0.04714873805642128, 0.05204671993851662, -0.05263442546129227, 0.02326780930161476, -0.14327062666416168, 0.15288670361042023, -0.047746747732162476, -0.18708550930023193, 0.12311497330665588, -0.014836868271231651]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07828086614608765, 0.12556689977645874, 0.062402814626693726, 0.11297228187322617, -0.03203506022691727, -0.009506942704319954, 0.14018277823925018, 0.039691388607025146, 0.10512036085128784, -0.08026696741580963, -0.12730254232883453, 0.0950237512588501, -0.15208767354488373, 0.13297338783740997, 0.12721897661685944, 0.06032206490635872]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08010680228471756, 0.031453587114810944, 0.17167510092258453, 0.3387967348098755, 0.1832088977098465, 0.12385804951190948, -0.07636603713035583, -0.044678349047899246, 0.48123323917388916, 0.15530046820640564, -0.47881343960762024, 0.3443673253059387, 0.19510522484779358, -0.10804267227649689, 0.14564257860183716, 0.17760349810123444]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.31400489807128906, 0.24349279701709747, 0.3242509067058563, 0.11311458051204681, -0.8676049113273621, 0.3347390294075012, 0.1819603443145752, -0.13572415709495544, 0.3801848292350769, 0.19531987607479095, -0.03800257667899132, 0.29030194878578186, 0.35518690943717957, 0.1297813206911087, -0.375718891620636, -0.033275630325078964]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.1845000833272934, 0.3347628712654114, -0.08158884942531586, 0.3343587815761566, 0.5380351543426514, 0.2877156436443329, 0.2944657504558563, -0.18757414817810059, 0.1487891674041748, 0.6477863788604736, -0.005355937406420708, 0.4566507041454315, 0.34510505199432373, -0.3247975707054138, -0.1979660987854004, 0.3003630042076111]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.47939202189445496, 0.22862540185451508, 0.505878210067749, 0.813814103603363, 0.47772666811943054, 0.47166043519973755, 0.7070115804672241, 0.5701453685760498, 0.5581820607185364, 0.20468036830425262, 0.5286373496055603, 0.7275325059890747, 0.9398414492607117, 0.6065346002578735, 0.7674208283424377, 0.7379928827285767]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9766706228256226, 0.8509726524353027, 0.698045551776886, 0.8858732581138611, 1.1279903650283813, 0.7392774820327759, 0.6899495720863342, 0.6852060556411743, 0.41796013712882996, 0.9311552047729492, 0.7728115916252136, 0.9392745494842529, 0.6225102543830872, 0.2908853590488434, 0.7465680837631226, 0.4979477524757385]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.078125, 1.124408483505249, 1.022179126739502, 1.3709807395935059, 1.4541736841201782, 0.8700442910194397, 0.9931509494781494, 1.4513161182403564, 1.1077102422714233, 1.3968514204025269, 1.030948519706726, 1.2011390924453735, 0.8586270213127136, 0.5870650410652161, 0.05201936140656471, 0.9213826060295105]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.38587161898612976, 0.9973160624504089, 1.2716809511184692, 1.259202003479004, 0.5056504607200623, 0.8282462358474731, 1.0457632541656494, 1.2709928750991821, 1.0240195989608765, 0.731024444103241, 0.7010620832443237, 0.9026321172714233, 1.1629679203033447, 0.878277063369751, 0.6949077844619751, 1.0290085077285767]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8033545613288879, 0.8945791721343994, 0.866741418838501, 0.6293306350708008, 1.2404309511184692, 0.31681138277053833, 1.1260749101638794, 2.257524013519287, 1.0049155950546265, 0.9673081040382385, 1.276321291923523, 1.306548833847046, -0.06285064667463303, 1.0611809492111206, 1.1312869787216187, 0.8356929421424866]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7355625033378601, 0.8034811615943909, 0.9099661707878113, 0.8394996523857117, 0.9162646532058716, 0.8289422392845154, 0.5775560140609741, 1.1984939575195312, 0.7534343600273132, 1.0368832349777222, 0.23977066576480865, 0.7924411296844482, 1.1156014204025269, 1.045079231262207, 1.1014182567596436, 0.7094792127609253]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.43733614683151245, 1.1425912380218506, 1.0161689519882202, 0.8221869468688965, 0.639940083026886, 0.8686818480491638, 1.1519906520843506, 1.0180631875991821, 0.8793519139289856, 0.9413537979125977, 0.7412011027336121, -0.3441866636276245, 0.8002558946609497, 0.8533845543861389, 1.0831586122512817, 1.500647783279419]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8301109075546265, 0.3293786644935608, 0.8827486038208008, 0.773532509803772, 0.6658911108970642, 0.5617532134056091, 0.40061038732528687, 0.9350340366363525, -0.6992875933647156, 2.6659305095672607, -0.6073094606399536, 0.833206057548523, 0.938129186630249, 0.9578898549079895, 0.8803415894508362, 0.7284049987792969]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.14237986505031586, 0.6415749192237854, 1.3558489084243774, 1.3140336275100708, 1.330078125, 1.3804398775100708, 1.4523122310638428, 1.3977820873260498, 1.6802314519882202, 1.5441746711730957, 2.003460645675659, 1.3329390287399292, 1.3494938611984253, 1.1267597675323486, 0.7843611836433411, 1.434898018836975]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3989880084991455, 1.34765625, 1.4181653261184692, 1.669751524925232, 1.0834863185882568, 1.3158557415008545, 1.8607120513916016, 1.7399853467941284, 2.030961513519287, 1.4146654605865479, 1.195594310760498, 1.0850673913955688, 1.318896770477295, 1.4828473329544067, 1.538040041923523, 1.4693005084991455]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3343383073806763, 1.2735686302185059, 1.162279725074768, 1.2237049341201782, 1.4441851377487183, 1.0080803632736206, 1.061693787574768, 1.2445470094680786, 1.9017342329025269, 1.4217700958251953, 1.4178638458251953, 1.3808987140655518, 1.9922399520874023, 1.3820784091949463, 1.268115520477295, 1.2996538877487183]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8863775134086609, 1.1141201257705688, 1.1116033792495728, 0.9276622533798218, 0.22474731504917145, 1.2152467966079712, 1.250694751739502, 1.9742980003356934, 0.8652425408363342, 1.1975539922714233, 1.2948038578033447, 1.1414573192596436, 1.118747353553772, 1.486786961555481, 1.2344413995742798, 0.5522706508636475]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0719902515411377, 2.213899850845337, 2.0487101078033447, 2.0169358253479004, 1.9323878288269043, 2.035182476043701, 1.9676488637924194, 2.1833577156066895, 1.9753565788269043, 2.0871434211730957, 2.3613674640655518, 2.809694766998291, 1.9424680471420288, 2.4294776916503906, 2.290189743041992, 2.1380295753479004]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4509752988815308, 1.301803708076477, 1.7994441986083984, 1.133002519607544, 1.4974045753479004, 1.5474255084991455, 1.3819867372512817, 1.3777527809143066, 1.479839563369751, 1.6476771831512451, 1.5196361541748047, 2.277003049850464, 1.5536388158798218, 1.5523017644882202, 1.2787988185882568, 1.6440855264663696]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5871171951293945, 1.799391746520996, 1.323478102684021, 1.454776644706726, 1.5012977123260498, 1.5363621711730957, 1.4276950359344482, 1.4974570274353027, 1.5192756652832031, 1.434177041053772, 1.581008791923523, 1.4939178228378296, 1.4687631130218506, 1.0304172039031982, 1.570892572402954, 1.5541632175445557]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3775691986083984, 2.39909291267395, 2.2202181816101074, 2.2305474281311035, 2.8858535289764404, 2.391883373260498, 2.7446517944335938, 2.28408145904541, 2.3326079845428467, 2.3666107654571533, 2.6550440788269043, 2.33137583732605, 2.2708683013916016, 2.4580013751983643, 2.4305789470672607, 2.4611735343933105]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  31%|███████████████▎                                 | 157/504 [08:56<18:53,  3.27s/it]Layer: gate_25 - Captured router_logits: [1.6580588817596436, 1.829514503479004, 1.4742292165756226, 1.650928020477295, 1.5315645933151245, 1.514930248260498, 1.567336916923523, 1.8794567584991455, 1.550152063369751, 1.5265309810638428, 1.5558017492294312, 1.4400691986083984, 1.5185874700546265, 1.4386534690856934, 1.6340970993041992, 1.9530987739562988]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9754351377487183, 1.7260725498199463, 1.7512321472167969, 1.8479446172714233, 1.759605050086975, 1.7887872457504272, 1.7089499235153198, 1.805465817451477, 1.8255016803741455, 1.9188109636306763, 2.2175259590148926, 1.67015540599823, 1.7266149520874023, 1.9274722337722778, 1.8715198040008545, 1.7954363822937012]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.635932207107544, 1.6982946395874023, 1.646907091140747, 1.785880446434021, 1.6933953762054443, 1.6545065641403198, 1.7723560333251953, 1.767407774925232, 1.654880166053772, 1.80848228931427, 1.5595113039016724, 1.7673028707504272, 1.6704461574554443, 1.8540629148483276, 1.6386325359344482, 1.6616308689117432]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1281721591949463, 2.2181363105773926, 2.6089818477630615, 2.1278510093688965, 2.209181070327759, 2.13759708404541, 2.642702579498291, 2.19856333732605, 2.0726916790008545, 2.12587833404541, 2.03579044342041, 1.980862021446228, 2.049588441848755, 1.9414849281311035, 2.443110227584839, 2.1895089149475098]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.594379425048828, 5.919672966003418, 5.632864952087402, 5.564702033996582, 5.4861578941345215, 5.388527870178223, 5.5750837326049805, 6.009018421173096, 5.7696099281311035, 5.550912380218506, 5.5900797843933105, 5.476824760437012, 5.650429725646973, 5.638632774353027, 5.752621650695801, 5.584521770477295]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.185166835784912, 4.000629425048828, 3.984820604324341, 3.8495535850524902, 4.027317523956299, 4.014891147613525, 3.819054126739502, 4.027212619781494, 4.035923004150391, 4.076132774353027, 3.8911492824554443, 3.7278249263763428, 4.050846576690674, 3.9920694828033447, 4.099884510040283, 4.1659746170043945]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.143613576889038, 3.246591806411743, 3.070784330368042, 2.891254186630249, 3.004404306411743, 3.1991400718688965, 3.0225985050201416, 3.0263476371765137, 3.0626049041748047, 3.0990982055664062, 3.2980809211730957, 2.5672876834869385, 3.1162960529327393, 3.0828964710235596, 3.303271770477295, 3.1369547843933105]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.08961702138185501, 0.10957776755094528, 0.10770129412412643, -0.23412472009658813, -0.2186046838760376, -0.1246480718255043, 0.12398862093687057, -0.17573700845241547, 0.07394767552614212, 0.09520485997200012, 0.10040682554244995, 0.08300933241844177, 0.09343427419662476, 0.10114523768424988, -1.0563850402832031, 0.11691223084926605]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09008302539587021, 0.06309571117162704, 0.0389314703643322, 0.04183409363031387, 0.07965508103370667, 0.02619711309671402, 0.04893384873867035, 0.06383407115936279, 0.02098984643816948, 0.06980562955141068, -0.16819609701633453, 0.05720576271414757, 0.004326993133872747, -0.03580528497695923, 0.015065019950270653, 0.04843812808394432]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09803807735443115, 0.05727376416325569, 0.08974536508321762, 0.05822856351733208, 0.10140786319971085, 0.1074908971786499, 0.061296604573726654, -0.07475239783525467, 0.07357063889503479, 0.10214944928884506, 0.01380738615989685, 0.08866503089666367, -0.1571348011493683, 0.03075992688536644, -0.019908316433429718, 0.10876280814409256]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13516367971897125, 0.17545129358768463, 0.11645809561014175, 0.1395527422428131, 0.14385883510112762, 0.1192195788025856, 0.054738499224185944, 0.17491698265075684, 0.18330824375152588, -0.38170772790908813, -0.017794307321310043, 0.11357751488685608, 0.17046299576759338, -0.1881009340286255, -0.0211015734821558, -0.04894271865487099]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0823131799697876, 0.10534780472517014, 0.1056136041879654, 0.12321195751428604, 0.043511103838682175, -0.03500017896294594, 0.05648025497794151, 0.05079302564263344, -0.061908286064863205, 0.017986169084906578, -0.15077120065689087, 0.15282578766345978, -0.045734379440546036, -0.18402473628520966, 0.12534143030643463, -0.016018247231841087]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07767647504806519, 0.12703847885131836, 0.07278677821159363, 0.10716698318719864, -0.034982770681381226, -0.004077680874615908, 0.14228011667728424, 0.04307566210627556, 0.11972793191671371, -0.08104363083839417, -0.13143450021743774, 0.09875263273715973, -0.1556849181652069, 0.1329161375761032, 0.13279283046722412, 0.06264060735702515]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08613627403974533, 0.04306352883577347, 0.16686171293258667, 0.3487577438354492, 0.19118627905845642, 0.13259314000606537, -0.07005463540554047, -0.03636737912893295, 0.467252641916275, 0.159445121884346, -0.4846797585487366, 0.34871402382850647, 0.20350268483161926, -0.10648258775472641, 0.15561732649803162, 0.19220729172229767]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.33535847067832947, 0.24893227219581604, 0.32330936193466187, 0.11877938359975815, -0.8904736638069153, 0.33850210905075073, 0.1853480041027069, -0.15095509588718414, 0.3925158977508545, 0.1978268176317215, -0.0488811731338501, 0.30065426230430603, 0.34370002150535583, 0.13011963665485382, -0.3860771656036377, -0.055483005940914154]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.18491199612617493, 0.33623120188713074, -0.08077587932348251, 0.32684525847435, 0.5189069509506226, 0.2995114028453827, 0.29464229941368103, -0.17507176101207733, 0.15464526414871216, 0.6492831707000732, -0.007151354104280472, 0.4400155544281006, 0.3388606309890747, -0.32798880338668823, -0.18243767321109772, 0.295226126909256]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.45515307784080505, 0.20490115880966187, 0.5063312649726868, 0.7826525568962097, 0.446575790643692, 0.4454665184020996, 0.6952283382415771, 0.5609630346298218, 0.5174593329429626, 0.18099576234817505, 0.5272171497344971, 0.7177603244781494, 0.9289169907569885, 0.6037261486053467, 0.7781105637550354, 0.7133264541625977]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.975041925907135, 0.8495897054672241, 0.6737412810325623, 0.886522114276886, 1.0915478467941284, 0.7146561145782471, 0.6816848516464233, 0.6839542388916016, 0.41507798433303833, 0.9654770493507385, 0.7589594721794128, 0.9531118869781494, 0.6451538801193237, 0.3008631765842438, 0.6913142800331116, 0.5086223483085632]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0622706413269043, 1.1079773902893066, 0.9845191836357117, 1.3492259979248047, 1.4353960752487183, 0.803071916103363, 0.9787712097167969, 1.4244048595428467, 1.0810219049453735, 1.3352689743041992, 1.0196208953857422, 1.1715866327285767, 0.8215594291687012, 0.5712435841560364, 0.01877507008612156, 0.9013704657554626]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.42058220505714417, 1.01551353931427, 1.2428035736083984, 1.273044228553772, 0.5417431592941284, 0.8414265513420105, 1.0677727460861206, 1.2652744054794312, 1.0353598594665527, 0.7472808361053467, 0.7097790837287903, 0.8924863934516907, 1.1801855564117432, 0.8865090012550354, 0.6775672435760498, 1.0099490880966187]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8342233896255493, 0.9268495440483093, 0.846101701259613, 0.6359121799468994, 1.2091809511184692, 0.337200790643692, 1.1368433237075806, 2.2854185104370117, 1.010788083076477, 0.9644635915756226, 1.270678162574768, 1.3009647130966187, -0.06494253128767014, 1.0484331846237183, 1.145207166671753, 0.8402900695800781]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7004001140594482, 0.7935434579849243, 0.9206690192222595, 0.8516263961791992, 0.9152880907058716, 0.771844208240509, 0.5819925665855408, 1.153568983078003, 0.7388449311256409, 1.0079567432403564, 0.242776557803154, 0.7568359375, 1.0749101638793945, 1.0283071994781494, 1.1041054725646973, 0.7053329348564148]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4546179175376892, 1.167182207107544, 1.0220414400100708, 0.8181103467941284, 0.6685913801193237, 0.8775560855865479, 1.1275511980056763, 1.0302799940109253, 0.8977558612823486, 0.9506737589836121, 0.7487530708312988, -0.3583828806877136, 0.8068104386329651, 0.8562158942222595, 1.0833158493041992, 1.5033384561538696]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8367264270782471, 0.34069639444351196, 0.8822111487388611, 0.7566753625869751, 0.6687626242637634, 0.561542272567749, 0.43608421087265015, 0.9837998747825623, -0.6888943910598755, 2.6251049041748047, -0.5526127219200134, 0.8019888401031494, 0.9328203797340393, 0.9811438322067261, 0.8817442059516907, 0.7858370542526245]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.19039158523082733, 0.6316040754318237, 1.3899827003479004, 1.3559815883636475, 1.370634913444519, 1.4233430624008179, 1.4789482355117798, 1.4235659837722778, 1.7152763605117798, 1.6045606136322021, 2.040858268737793, 1.350238561630249, 1.3868154287338257, 1.1788309812545776, 0.812710165977478, 1.461822271347046]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4171559810638428, 1.3583263158798218, 1.4412096738815308, 1.6758337020874023, 1.063529372215271, 1.3319132328033447, 1.8984899520874023, 1.7640520334243774, 2.0478451251983643, 1.4184144735336304, 1.231143832206726, 1.135406255722046, 1.3512282371520996, 1.5365946292877197, 1.5556575059890747, 1.481727123260498]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3484690189361572, 1.2873846292495728, 1.1599596738815308, 1.2296298742294312, 1.444683313369751, 1.0032541751861572, 1.0603306293487549, 1.253512978553772, 1.8942428827285767, 1.4147441387176514, 1.4242607355117798, 1.3876625299453735, 2.0353922843933105, 1.3793781995773315, 1.2748401165008545, 1.3127752542495728]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8800114393234253, 1.143600583076477, 1.145271897315979, 0.9590122103691101, 0.17769156396389008, 1.2393102645874023, 1.2767276763916016, 1.9853286743164062, 0.8890684247016907, 1.2265756130218506, 1.3362520933151245, 1.1534711122512817, 1.133441686630249, 1.496329665184021, 1.2405309677124023, 0.5568380951881409]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0832371711730957, 2.2217648029327393, 2.0511744022369385, 2.020632266998291, 1.941248893737793, 2.0754246711730957, 1.9727087020874023, 2.2027580738067627, 1.963428020477295, 2.0849673748016357, 2.3736891746520996, 2.8215184211730957, 1.9316537380218506, 2.404676914215088, 2.299024820327759, 2.1488044261932373]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4731281995773315, 1.3403156995773315, 1.8167470693588257, 1.1440986394882202, 1.528759479522705, 1.5810612440109253, 1.41319739818573, 1.4041526317596436, 1.5073144435882568, 1.6608378887176514, 1.5346319675445557, 2.300440549850464, 1.571702003479004, 1.579619288444519, 1.2964686155319214, 1.6729499101638794]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5655148029327393, 1.8113988637924194, 1.3155171871185303, 1.4648699760437012, 1.4806522130966187, 1.5542680025100708, 1.4324663877487183, 1.4987154006958008, 1.527251958847046, 1.4250471591949463, 1.5907875299453735, 1.5078387260437012, 1.4894216060638428, 1.0179762840270996, 1.5714037418365479, 1.5623688697814941]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3941380977630615, 2.4452600479125977, 2.24934458732605, 2.2661492824554443, 2.9147965908050537, 2.422189712524414, 2.7686660289764404, 2.315279006958008, 2.3623111248016357, 2.388108253479004, 2.665320873260498, 2.3616819381713867, 2.304372787475586, 2.497640609741211, 2.452495813369751, 2.489985227584839]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6511116027832031, 1.8324769735336304, 1.472498893737793, 1.6474937200546265, 1.5226247310638428, 1.5149695873260498, 1.5664324760437012, 1.8917261362075806, 1.5542417764663696, 1.5173028707504272, 1.5614644289016724, 1.443424940109253, 1.5340813398361206, 1.4417732954025269, 1.6304267644882202, 1.9611209630966187]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.988045334815979, 1.725395917892456, 1.7583892345428467, 1.8458473682403564, 1.7691248655319214, 1.8006551265716553, 1.7059563398361206, 1.8082791566848755, 1.8295341730117798, 1.9094090461730957, 2.209141731262207, 1.671226143836975, 1.7233641147613525, 1.907665729522705, 1.8629732131958008, 1.8107205629348755]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6514884233474731, 1.6961743831634521, 1.6591600179672241, 1.7942533493041992, 1.7023646831512451, 1.652190923690796, 1.772212266921997, 1.7694493532180786, 1.659150242805481, 1.8102060556411743, 1.5596424341201782, 1.7805814743041992, 1.6824729442596436, 1.853685975074768, 1.6342675685882568, 1.6674673557281494]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1554110050201416, 2.2469589710235596, 2.642669916152954, 2.157351016998291, 2.219352960586548, 2.1691484451293945, 2.664036273956299, 2.2321465015411377, 2.101903200149536, 2.1559877395629883, 2.0567848682403564, 2.002621650695801, 2.0721476078033447, 1.9684354066848755, 2.4554319381713867, 2.2111473083496094]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  32%|███████████████▋                                 | 161/504 [09:09<18:36,  3.25s/it]Layer: gate_29 - Captured router_logits: [5.682676315307617, 5.987626075744629, 5.717176914215088, 5.666409015655518, 5.574638366699219, 5.487154006958008, 5.645763397216797, 6.097393989562988, 5.865195274353027, 5.662961483001709, 5.707424640655518, 5.572042942047119, 5.739814758300781, 5.7330641746521, 5.841443061828613, 5.669777870178223]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.179320335388184, 3.999436378479004, 4.00410270690918, 3.856625556945801, 4.0236406326293945, 4.010842323303223, 3.818890333175659, 4.0574140548706055, 4.031118869781494, 4.076801300048828, 3.8929319381713867, 3.7469310760498047, 4.06685209274292, 4.009477138519287, 4.098285675048828, 4.1522321701049805]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1472315788269043, 3.2256972789764404, 3.0606648921966553, 2.908924102783203, 3.005295753479004, 3.168755292892456, 3.0727243423461914, 3.011115789413452, 3.049077272415161, 3.1000680923461914, 3.2824559211730957, 2.647650957107544, 3.1022965908050537, 3.065934419631958, 3.2905306816101074, 3.1530516147613525]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.0970083624124527, 0.1105596050620079, 0.11369246989488602, -0.28267163038253784, -0.21556009352207184, -0.1266557276248932, 0.124574214220047, -0.11773908138275146, 0.09455174952745438, 0.09367094188928604, 0.10753165930509567, 0.07998698204755783, 0.09305531531572342, 0.11332733184099197, -1.1451833248138428, 0.12307432293891907]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09501191973686218, 0.04858224466443062, 0.04410834610462189, 0.056540340185165405, 0.0800304040312767, 0.029271427541971207, 0.048881083726882935, 0.08694908767938614, 0.023318910971283913, 0.08135950565338135, -0.19528526067733765, 0.04564031958580017, 0.023849794641137123, -0.017626052722334862, 0.0317038968205452, 0.012599407695233822]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09223835915327072, 0.07122687250375748, 0.09067396819591522, 0.07282835990190506, 0.10920152068138123, 0.10671362280845642, 0.09249027818441391, -0.08990847319364548, 0.0839458703994751, 0.09287574142217636, 0.02668045274913311, 0.07543996721506119, -0.1804584264755249, 0.01894819736480713, -0.03407525643706322, 0.11395212262868881]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13110871613025665, 0.14141471683979034, 0.12903599441051483, 0.15419815480709076, 0.12269996851682663, 0.11761121451854706, 0.04834025353193283, 0.1975400745868683, 0.1996452808380127, -0.5258584022521973, -0.0015227554831653833, 0.08964251726865768, 0.22387047111988068, -0.24428972601890564, -0.03132081404328346, -0.07193556427955627]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06792383641004562, 0.10502481460571289, 0.08572029322385788, 0.15776051580905914, 0.042283281683921814, -0.07823165506124496, 0.06309279054403305, 0.03961773216724396, -0.010538190603256226, 0.030432887375354767, -0.1908036768436432, 0.1332649439573288, -0.019142508506774902, -0.2108692228794098, 0.12899990379810333, -0.023763617500662804]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06345700472593307, 0.1164354681968689, 0.06608704477548599, 0.12899376451969147, -0.044626325368881226, -0.05080680176615715, 0.06028061360120773, 0.05361180752515793, 0.18271468579769135, -0.08669767528772354, -0.11978010833263397, 0.0730486512184143, -0.19150400161743164, 0.12765134871006012, 0.1592019647359848, 0.05768314003944397]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.09175345301628113, 0.04081754386425018, 0.11315262317657471, 0.32002413272857666, 0.19783705472946167, 0.15504255890846252, -0.11266516149044037, -0.0344490222632885, 0.32381483912467957, 0.14117227494716644, -0.4826590418815613, 0.22686424851417542, 0.2118477076292038, -0.18442468345165253, 0.24163080751895905, 0.22471076250076294]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5037235617637634, 0.2445153295993805, 0.28689494729042053, 0.12506744265556335, -0.865131139755249, 0.3020268380641937, 0.18438874185085297, -0.25146812200546265, 0.4837459623813629, 0.20690426230430603, -0.088039830327034, 0.25512757897377014, 0.2168162316083908, 0.21020784974098206, -0.5339273810386658, -0.3544680178165436]
Layer: gate_7 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3398173153400421, 0.31002870202064514, -0.16006284952163696, 0.2611612379550934, 0.3950798511505127, 0.35993456840515137, 0.33895525336265564, -0.34716105461120605, 0.1518327295780182, 0.7808260321617126, 0.05434811860322952, 0.12871740758419037, 0.41883552074432373, -0.4623316824436188, -0.4374840259552002, 0.3626778721809387]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.483184814453125, 0.2145785093307495, 0.3758030831813812, 1.0134637355804443, 0.44508475065231323, 0.1393134593963623, 0.6401830315589905, 0.5199523568153381, 0.6994905471801758, 0.17504636943340302, 0.2680979371070862, 0.7048519849777222, 0.6123999357223511, 0.16218024492263794, 0.7642650008201599, 0.5903680920600891]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9727947115898132, 0.858690083026886, 0.7075785398483276, 0.7469097375869751, 0.898634135723114, 0.6222339868545532, 0.790179967880249, 0.7198314070701599, 0.2973671853542328, 0.5850944519042969, 0.8048546314239502, 0.8873676061630249, 0.3303808569908142, 0.29275667667388916, 0.8713354468345642, 0.6169081330299377]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9517871141433716, 1.1019757986068726, 0.8070797324180603, 1.053844928741455, 1.074771761894226, 0.24792316555976868, 0.9275557994842529, 0.8658185005187988, 0.9376245141029358, 1.3115168809890747, 0.8228446245193481, 0.9078311920166016, 0.42928093671798706, 0.5415840148925781, 0.12441422790288925, 0.8626717329025269]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.40136903524398804, 0.9522926211357117, 1.080248475074768, 1.1550309658050537, 0.4747793674468994, 0.6853371262550354, 1.047919511795044, 1.260581612586975, 0.5985621213912964, 0.1522560864686966, 0.8620851039886475, 0.8260276913642883, 0.9757510423660278, 0.8894255757331848, 0.7072999477386475, 0.963585376739502]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8354369401931763, 0.9534100890159607, 0.9768410325050354, 0.7322027087211609, 1.0597145557403564, -0.20649422705173492, 1.0976955890655518, 1.8084626197814941, 1.0022971630096436, 0.9769753813743591, 1.214638113975525, 1.1064518690109253, 0.21319293975830078, 0.9089904427528381, 0.9559424519538879, 0.7262732982635498]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.717848002910614, 0.8256328105926514, 0.9781387448310852, 0.9099956750869751, 0.9586697816848755, 0.8491743206977844, 0.4260237514972687, 1.2970384359359741, 0.7801390886306763, 0.7744337320327759, -0.22119100391864777, 0.7512107491493225, 0.8553335666656494, 1.0698144435882568, 1.1501022577285767, 0.7706118822097778]
Running loglikelihood requests:  33%|████████████████                                 | 165/504 [09:23<18:37,  3.30s/it]Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.45548877120018005, 1.1326944828033447, 0.9383782744407654, 0.9692218899726868, 0.7793738842010498, 0.8762452602386475, 1.2639824151992798, 1.1968854665756226, 0.9355599880218506, 0.998872697353363, 0.7494379878044128, -0.31937527656555176, 1.0460803508758545, 1.106163501739502, 0.8691012859344482, 1.189254879951477]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.7134149670600891, 0.2942269444465637, 0.7861918210983276, 0.8738431930541992, 0.8577463030815125, 0.702772319316864, 0.39848461747169495, 0.8200552463531494, -0.6997315883636475, 2.0292837619781494, -0.5373393893241882, 0.5757895112037659, 0.8996696472167969, 0.8440908193588257, 0.8286493420600891, 0.4746933579444885]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.16644839942455292, 0.6826218366622925, 1.4244179725646973, 1.232467770576477, 1.4607932567596436, 1.416553020477295, 1.5050073862075806, 1.5002720355987549, 1.8758388757705688, 1.3551344871520996, 2.0344090461730957, 1.4499003887176514, 1.9868106842041016, 1.3346660137176514, 0.636676549911499, 1.2625707387924194]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4851090908050537, 1.3246644735336304, 1.5190281867980957, 1.789219856262207, 1.0482186079025269, 1.2704250812530518, 1.8869284391403198, 1.7885382175445557, 2.3329226970672607, 1.436713457107544, 1.3062502145767212, 0.9308467507362366, 1.3808577060699463, 0.9203112125396729, 1.7289875745773315, 1.5554478168487549]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.5648857355117798, 1.3855128288269043, 1.5373715162277222, 1.4945732355117798, 1.6156145334243774, 1.1643214225769043, 1.3996893167495728, 1.3738988637924194, 1.993839144706726, 1.5806679725646973, 1.6236498355865479, 1.5864094495773315, 1.7310389280319214, 1.572200059890747, 1.47265625, 1.5963060855865479]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.141685128211975, 1.0863701105117798, 1.1423618793487549, 1.0634773969650269, 0.08093343675136566, 1.3466272354125977, 1.1935690641403198, 1.9719582796096802, 0.9942323565483093, 1.288970708847046, 1.227493166923523, 1.2324349880218506, 1.1653602123260498, 1.375, 1.0015445947647095, 0.6920620799064636]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.057204246520996, 2.2457005977630615, 2.171743869781494, 2.0189545154571533, 2.0769453048706055, 2.1016411781311035, 1.9994494915008545, 2.2530410289764404, 1.9709521532058716, 2.131685256958008, 2.342334270477295, 2.5100672245025635, 1.7886766195297241, 2.261115789413452, 2.1738674640655518, 2.0606648921966553]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3709888458251953, 1.2944107055664062, 1.7983430624008179, 1.021041989326477, 1.430945873260498, 1.4863412380218506, 1.3011876344680786, 1.4252831935882568, 1.3997220993041992, 1.4937998056411743, 1.5044043064117432, 1.9658137559890747, 1.4723416566848755, 1.4423238039016724, 1.227465271949768, 1.545092225074768]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.123715400695801, 1.6954436302185059, 1.0915379524230957, 1.3279938697814941, 1.5119808912277222, 1.3673971891403198, 1.2887216806411743, 1.4269086122512817, 1.3912410736083984, 1.4673867225646973, 1.5242763757705688, 1.4460465908050537, 1.258743166923523, 0.7039135694503784, 1.2553350925445557, 1.493812918663025]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.333420753479004, 2.329540729522705, 2.2199559211730957, 2.086409330368042, 2.4414849281311035, 2.1723992824554443, 2.4364514350891113, 2.2085256576538086, 2.1161651611328125, 2.1826236248016357, 2.6560401916503906, 2.1788485050201416, 2.175597667694092, 2.400954246520996, 2.2878565788269043, 2.2674076557159424]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7260119915008545, 1.710308313369751, 1.6220899820327759, 1.7809616327285767, 1.3821570873260498, 1.669227123260498, 1.698589563369751, 1.9410392045974731, 1.6937657594680786, 1.6668152809143066, 1.6584521532058716, 1.651321291923523, 1.671481728553772, 1.5636796951293945, 1.9108641147613525, 1.9683829545974731]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.7539455890655518, 1.7820831537246704, 1.768272876739502, 1.7307308912277222, 1.763232707977295, 1.774001121520996, 1.6678694486618042, 1.705962896347046, 1.7861738204956055, 1.876625418663025, 2.213736057281494, 1.678127646446228, 1.7351746559143066, 1.7350435256958008, 1.7938076257705688, 1.744175910949707]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.511127233505249, 1.5931322574615479, 1.5527609586715698, 1.6236138343811035, 1.5196623802185059, 1.62180757522583, 1.6766971349716187, 1.6111150979995728, 1.5200181007385254, 1.6161519289016724, 1.3085675239562988, 1.6781554222106934, 1.5610253810882568, 1.8303337097167969, 1.5427066087722778, 1.5476090908050537]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9800492525100708, 2.0357329845428467, 2.3208892345428467, 2.0410025119781494, 2.080641746520996, 2.010080337524414, 2.48264479637146, 2.068359375, 2.0337142944335938, 2.017958164215088, 1.991165041923523, 1.8736891746520996, 1.9467413425445557, 1.8638842105865479, 2.2480862140655518, 1.961566686630249]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.8450608253479, 5.1966233253479, 4.828649520874023, 4.7970452308654785, 4.901203155517578, 4.6911702156066895, 4.836357116699219, 5.054713726043701, 5.036283493041992, 4.920433044433594, 5.0197672843933105, 4.7703962326049805, 4.854931354522705, 4.900167942047119, 4.988255023956299, 4.815855503082275]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.77561354637146, 3.658924102783203, 3.6147756576538086, 3.5474190711975098, 3.7845795154571533, 3.7760066986083984, 3.51344895362854, 3.67903208732605, 3.825345993041992, 3.811084270477295, 3.5933568477630615, 3.3813295364379883, 3.6483588218688965, 3.7449140548706055, 3.7808306217193604, 3.7589595317840576]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.715184450149536, 2.851064443588257, 2.6795825958251953, 2.6867659091949463, 2.606595993041992, 2.7891411781311035, 2.794253349304199, 2.5505714416503906, 2.8022232055664062, 2.6535496711730957, 3.09375, 2.2196545600891113, 2.7452023029327393, 2.6946046352386475, 3.030804395675659, 2.745490789413452]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.08891069889068604, 0.10673105716705322, 0.10467248409986496, -0.25618454813957214, -0.2384004294872284, -0.12181709706783295, 0.12317249923944473, -0.12796588242053986, 0.07354762405157089, 0.09024728089570999, 0.09721879661083221, 0.0820230022072792, 0.09308645129203796, 0.10405617952346802, -1.113037109375, 0.11566027998924255]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0834735631942749, 0.05437735095620155, 0.03641556575894356, 0.04111988842487335, 0.0748860090970993, 0.02200254239141941, 0.050126951187849045, 0.07686295360326767, 0.01546900998800993, 0.06733456254005432, -0.19140668213367462, 0.0504329651594162, 0.006580520421266556, -0.0374302975833416, 0.018583890050649643, 0.04443928971886635]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09198229014873505, 0.055661872029304504, 0.09160114079713821, 0.07191304862499237, 0.10414835065603256, 0.10947702080011368, 0.07392563670873642, -0.08717191219329834, 0.07389535009860992, 0.10256855189800262, 0.011308773420751095, 0.08648127317428589, -0.17102380096912384, 0.020383525639772415, -0.017662202939391136, 0.1136917918920517]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.1335788369178772, 0.17695081233978271, 0.12327204644680023, 0.13365808129310608, 0.14625199139118195, 0.11357023566961288, 0.041317399591207504, 0.1863735318183899, 0.19127067923545837, -0.43677541613578796, 0.00038889292045496404, 0.09845083951950073, 0.19312261044979095, -0.2170173078775406, -0.012645412236452103, -0.058685302734375]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06259284168481827, 0.10643216967582703, 0.08620879799127579, 0.1579625904560089, 0.04620928317308426, -0.051032815128564835, 0.02928202785551548, 0.043007928878068924, -0.09060937166213989, 0.03468487784266472, -0.1628405600786209, 0.15344619750976562, -0.01155069749802351, -0.19602391123771667, 0.1297207623720169, -0.03687482327222824]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07198462635278702, 0.11334042996168137, 0.06505141407251358, 0.1340852677822113, -0.03511397913098335, -0.032540399581193924, 0.11136846244335175, 0.004870646633207798, 0.14622995257377625, -0.10193298757076263, -0.0734640583395958, 0.08651434630155563, -0.19070971012115479, 0.12610848248004913, 0.1292947232723236, 0.05688948184251785]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08690766990184784, 0.01491443533450365, 0.18718327581882477, 0.3271375000476837, 0.18995913863182068, 0.15146198868751526, -0.08963064104318619, -0.07404770702123642, 0.4415336847305298, 0.15834437310695648, -0.5079362392425537, 0.30795183777809143, 0.2098722755908966, -0.14553740620613098, 0.21591511368751526, 0.19807660579681396]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.40766164660453796, 0.24754472076892853, 0.2840545177459717, 0.14798159897327423, -0.9109556674957275, 0.33054888248443604, 0.1753457486629486, -0.2340203821659088, 0.4609600901603699, 0.1899496614933014, -0.10708628594875336, 0.30391135811805725, 0.3068448603153229, 0.1579543501138687, -0.47597378492355347, -0.08351299911737442]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2246161848306656, 0.34676679968833923, -0.17744269967079163, 0.32258522510528564, 0.5180614590644836, 0.2917150557041168, 0.27135199308395386, -0.2170839011669159, 0.1612749844789505, 0.7427988648414612, -0.00673283776268363, 0.3686082065105438, 0.3523642122745514, -0.40529653429985046, -0.2782229781150818, 0.3216382563114166]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.42798182368278503, 0.2340058982372284, 0.43576791882514954, 0.8718541860580444, 0.37970712780952454, 0.34119704365730286, 0.6422150135040283, 0.5168753862380981, 0.5872547030448914, 0.08240240812301636, 0.41723138093948364, 0.666259765625, 0.7541701793670654, 0.5320920944213867, 0.7308399081230164, 0.6566789150238037]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9478437900543213, 0.8210482001304626, 0.6010494828224182, 0.8098408579826355, 1.0200607776641846, 0.6348563432693481, 0.6416956186294556, 0.6839962601661682, 0.33344578742980957, 0.821635901927948, 0.7385616898536682, 0.9197618365287781, 0.38363203406333923, 0.25879967212677, 0.7544106245040894, 0.4354095458984375]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9883175492286682, 1.0290032625198364, 0.9313502907752991, 1.145263671875, 1.2918766736984253, 0.6174675226211548, 0.8577443957328796, 1.2010531425476074, 1.0235875844955444, 1.325919508934021, 0.8710525035858154, 1.0076475143432617, 0.6051710247993469, 0.3796706199645996, -0.09229237586259842, 0.7549331188201904]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.23186472058296204, 0.8689934015274048, 1.1841281652450562, 1.1135319471359253, 0.32186126708984375, 0.7600812911987305, 0.9638630747795105, 1.1877408027648926, 0.8333936333656311, 0.4949769675731659, 0.6618384122848511, 0.8489099740982056, 1.000471830368042, 0.8125197887420654, 0.5341566205024719, 0.9069758057594299]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7069025635719299, 0.9146695733070374, 0.8236298561096191, 0.50042724609375, 1.1764806509017944, 0.056352149695158005, 1.097359299659729, 2.2756810188293457, 0.9776380658149719, 0.889107346534729, 1.2209670543670654, 1.1674672365188599, -0.1669200211763382, 0.9154788851737976, 1.1233636140823364, 0.6551620960235596]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6656362414360046, 0.7458265423774719, 0.9316208362579346, 0.8035658001899719, 0.9011494517326355, 0.8717440962791443, 0.38205862045288086, 1.2482819557189941, 0.839546799659729, 0.930350661277771, 0.05355504900217056, 0.6965001821517944, 0.9824202060699463, 1.0195180177688599, 1.0691379308700562, 0.6045697331428528]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.366352379322052, 1.0998073816299438, 0.9457876086235046, 0.8839870095252991, 0.5811907649040222, 0.8212758898735046, 1.182661771774292, 1.013777494430542, 0.8644821643829346, 0.9539431929588318, 0.6614252328872681, -0.4641847312450409, 0.8031723499298096, 0.8154230713844299, 0.9604756236076355, 1.4078518152236938]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.6751003861427307, 0.1922718733549118, 0.8109576106071472, 0.7534047961235046, 0.6480502486228943, 0.43895331025123596, 0.3212971091270447, 0.8393917679786682, -0.9089594483375549, 2.60693359375, -0.7362576127052307, 0.6709033846855164, 0.8514173626899719, 0.9041550159454346, 0.8265150189399719, 0.48704633116722107]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.02716909907758236, 0.5466061234474182, 1.3243770599365234, 1.2356189489364624, 1.3083957433700562, 1.3931324481964111, 1.3904534578323364, 1.3019953966140747, 1.7574363946914673, 1.3908922672271729, 2.030445098876953, 1.3284878730773926, 1.4041863679885864, 1.1889532804489136, 0.6659306883811951, 1.3116884231567383]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.343314528465271, 1.2953441143035889, 1.368373155593872, 1.6261613368988037, 0.8989225029945374, 1.1689716577529907, 1.7766574621200562, 1.6323374509811401, 2.0333120822906494, 1.324113130569458, 1.0834788084030151, 0.9013177156448364, 1.1418027877807617, 1.164462685585022, 1.4038481712341309, 1.3804634809494019]
Running loglikelihood requests:  34%|████████████████▍                                | 169/504 [09:36<18:17,  3.28s/it]Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.2278821468353271, 1.1408625841140747, 1.1160526275634766, 1.1119616031646729, 1.3651024103164673, 0.8226613402366638, 0.9308652877807617, 1.1057194471359253, 1.7994483709335327, 1.3600876331329346, 1.3555479049682617, 1.271048903465271, 1.7983134984970093, 1.2757865190505981, 1.1899677515029907, 1.251517653465271]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.9055621027946472, 1.0457466840744019, 1.0192805528640747, 0.9005357623100281, -0.01003162283450365, 1.2081265449523926, 1.1411793231964111, 1.8486789464950562, 0.8526701927185059, 1.2101589441299438, 1.23046875, 1.1260161399841309, 1.0624867677688599, 1.3660526275634766, 1.0668821334838867, 0.4217850863933563]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.963919997215271, 2.0956239700317383, 1.9713101387023926, 1.8614864349365234, 1.7968486547470093, 1.9616237878799438, 1.8149018287658691, 2.0317513942718506, 1.8450168371200562, 1.9339237213134766, 2.2361698150634766, 2.6171348094940186, 1.7571659088134766, 2.269188165664673, 2.1233372688293457, 1.9939295053482056]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.362845778465271, 1.2894055843353271, 1.8015730381011963, 1.0639549493789673, 1.4494826793670654, 1.5093433856964111, 1.2939716577529907, 1.3552839756011963, 1.435441255569458, 1.5815562009811401, 1.5251003503799438, 2.1898226737976074, 1.4742398262023926, 1.497123122215271, 1.1854479312896729, 1.5975770950317383]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.595069646835327, 1.7847603559494019, 1.2112443447113037, 1.4284206628799438, 1.5340477228164673, 1.4766944646835327, 1.419539213180542, 1.4223500490188599, 1.5241501331329346, 1.4253853559494019, 1.5422033071517944, 1.5032860040664673, 1.3680914640426636, 0.8783559799194336, 1.4896537065505981, 1.5026129484176636]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.295660972595215, 2.3279666900634766, 2.202702760696411, 2.129117488861084, 2.7173774242401123, 2.2641470432281494, 2.591796875, 2.205394744873047, 2.217562198638916, 2.2109901905059814, 2.7095649242401123, 2.191511869430542, 2.232421875, 2.3972761631011963, 2.3075907230377197, 2.338312864303589]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6181377172470093, 1.720597505569458, 1.4437289237976074, 1.6294736862182617, 1.4327359199523926, 1.5077333450317383, 1.5349847078323364, 1.8319520950317383, 1.5138962268829346, 1.4856154918670654, 1.513619065284729, 1.4173352718353271, 1.4974398612976074, 1.3897804021835327, 1.6498891115188599, 1.8890941143035889]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.8852275609970093, 1.7233420610427856, 1.6878299713134766, 1.7715107202529907, 1.6949330568313599, 1.7433454990386963, 1.6813074350357056, 1.7559913396835327, 1.7598018646240234, 1.8537631034851074, 2.178351402282715, 1.6239516735076904, 1.6768501996994019, 1.8212627172470093, 1.8190324306488037, 1.7375530004501343]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.581246018409729, 1.6331007480621338, 1.5930407047271729, 1.7220128774642944, 1.6211564540863037, 1.5876810550689697, 1.7259727716445923, 1.69696044921875, 1.6026479005813599, 1.7145204544067383, 1.4515414237976074, 1.7142630815505981, 1.6295132637023926, 1.8623888492584229, 1.563057541847229, 1.5992761850357056]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9889938831329346, 2.072932004928589, 2.456886053085327, 1.9660314321517944, 2.0704972743988037, 1.9850348234176636, 2.4644081592559814, 2.0460567474365234, 1.9729136228561401, 1.9681693315505981, 1.8938632011413574, 1.8215001821517944, 1.9140888452529907, 1.7901182174682617, 2.303605318069458, 2.042229652404785]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.35890007019043, 5.691036701202393, 5.364653587341309, 5.333324432373047, 5.264331817626953, 5.193570613861084, 5.362911701202393, 5.768317222595215, 5.55811882019043, 5.328705787658691, 5.396589756011963, 5.2832560539245605, 5.4240922927856445, 5.413112163543701, 5.543866157531738, 5.344066619873047]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.902531862258911, 3.6912808418273926, 3.698755979537964, 3.5233583450317383, 3.78989839553833, 3.7748725414276123, 3.4909305572509766, 3.7744667530059814, 3.8091018199920654, 3.7879738807678223, 3.6217007637023926, 3.3912336826324463, 3.7157890796661377, 3.7640480995178223, 3.7959017753601074, 3.8618526458740234]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.0367398262023926, 3.138566255569458, 2.9769320487976074, 2.8755805492401123, 2.916226863861084, 3.0522592067718506, 3.0122992992401123, 2.8951117992401123, 2.9539167881011963, 2.969304323196411, 3.2999367713928223, 2.442176580429077, 3.027027130126953, 2.9845070838928223, 3.2761824131011963, 3.072371244430542]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.10106083005666733, 0.12100749462842941, 0.11506066471338272, -0.2433749884366989, -0.22150561213493347, -0.13697601854801178, 0.13022400438785553, -0.18643416464328766, 0.08375824242830276, 0.10587393492460251, 0.11015971004962921, 0.09592614322900772, 0.10112264007329941, 0.11100681126117706, -1.1252524852752686, 0.12817589938640594]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09737340360879898, 0.06605195999145508, 0.046668123453855515, 0.04542244225740433, 0.08768852055072784, 0.029714934527873993, 0.05301598832011223, 0.06890988349914551, 0.014868366532027721, 0.07531172037124634, -0.17958173155784607, 0.06104286387562752, -0.001084853196516633, -0.03243445232510567, 0.012491589412093163, 0.046409089118242264]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.10287122428417206, 0.05862535908818245, 0.09086927771568298, 0.06216368451714516, 0.10129386186599731, 0.1163148432970047, 0.07311918586492538, -0.08497131615877151, 0.08158692717552185, 0.09488989412784576, 0.004355294164270163, 0.08483269065618515, -0.15699529647827148, 0.03501763567328453, -0.01911749877035618, 0.1138363778591156]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13770070672035217, 0.17229394614696503, 0.12498494982719421, 0.1446492224931717, 0.14675529301166534, 0.119905486702919, 0.06691134721040726, 0.18516966700553894, 0.1876278817653656, -0.38667431473731995, -0.0288725346326828, 0.10958540439605713, 0.15058265626430511, -0.17659629881381989, -0.041453875601291656, -0.06351850181818008]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08029953390359879, 0.10805215686559677, 0.10027702152729034, 0.11208441853523254, 0.04325372353196144, -0.04764764383435249, 0.04948020353913307, 0.055146146565675735, -0.05361710116267204, 0.03113260120153427, -0.14656098186969757, 0.15618564188480377, -0.049101121723651886, -0.18880406022071838, 0.12369127571582794, -0.003782518906518817]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07943180948495865, 0.12412703037261963, 0.06304999440908432, 0.10305941849946976, -0.017606565728783607, -0.00203741155564785, 0.14724606275558472, 0.052816275507211685, 0.10200448334217072, -0.08241884410381317, -0.11591904610395432, 0.09250210225582123, -0.15905414521694183, 0.1241208016872406, 0.12575198709964752, 0.05548061802983284]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08596324175596237, 0.0354730524122715, 0.17311438918113708, 0.34809303283691406, 0.18391485512256622, 0.1387959122657776, -0.07778152078390121, -0.0420677550137043, 0.45660141110420227, 0.15595188736915588, -0.4980801045894623, 0.3367358446121216, 0.19351714849472046, -0.10931230336427689, 0.1214788556098938, 0.18116645514965057]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3270825743675232, 0.2521713078022003, 0.3235751688480377, 0.12477174401283264, -0.8831861615180969, 0.32632747292518616, 0.19023209810256958, -0.11133559793233871, 0.38866788148880005, 0.20376524329185486, -0.06315887719392776, 0.29123443365097046, 0.34772807359695435, 0.11537056416273117, -0.4154675602912903, -0.0860421285033226]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.18552850186824799, 0.333587646484375, -0.09260009229183197, 0.32407093048095703, 0.5242712497711182, 0.2863873243331909, 0.282522588968277, -0.19854901731014252, 0.143280029296875, 0.6229655146598816, -0.01235982682555914, 0.43341177701950073, 0.3154471218585968, -0.35861724615097046, -0.22915638983249664, 0.30013519525527954]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4539623558521271, 0.18900054693222046, 0.515413224697113, 0.7659770846366882, 0.4294755458831787, 0.39761829376220703, 0.686337411403656, 0.5576636791229248, 0.5119230151176453, 0.15558578073978424, 0.5058726668357849, 0.7246425747871399, 0.9025986790657043, 0.5209213495254517, 0.7401745915412903, 0.7235796451568604]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9592368006706238, 0.8423349857330322, 0.6480023860931396, 0.8529443144798279, 1.0716278553009033, 0.6849705576896667, 0.6656867861747742, 0.662674069404602, 0.3606013059616089, 0.8385964632034302, 0.7388725280761719, 0.909269392490387, 0.5920895934104919, 0.23606406152248383, 0.6902108788490295, 0.45684564113616943]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0431804656982422, 1.0897773504257202, 0.9934497475624084, 1.290680170059204, 1.4113852977752686, 0.7491754293441772, 0.9424093961715698, 1.356896996498108, 1.0860238075256348, 1.323062777519226, 0.9760841727256775, 1.1397215127944946, 0.7544779777526855, 0.5097498297691345, -0.0226462222635746, 0.8559038639068604]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.3227929472923279, 0.9663152098655701, 1.2417224645614624, 1.217594027519226, 0.4652996361255646, 0.7877338528633118, 0.9947991371154785, 1.2188297510147095, 0.9629637002944946, 0.6320350170135498, 0.6623834371566772, 0.8922592401504517, 1.1158820390701294, 0.8745548725128174, 0.6436601281166077, 0.9910846948623657]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7138617634773254, 0.8549538850784302, 0.7746098041534424, 0.5478441119194031, 1.1598507165908813, 0.21231846511363983, 1.0997023582458496, 2.1188881397247314, 0.9641594886779785, 0.8987364768981934, 1.1981823444366455, 1.2310667037963867, -0.14352728426456451, 0.9803782105445862, 1.1209251880645752, 0.743751585483551]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6756604313850403, 0.753524661064148, 0.8891634941101074, 0.7890293002128601, 0.8646032810211182, 0.7080124020576477, 0.47767773270606995, 1.0820897817611694, 0.7205303907394409, 0.9616217613220215, 0.17223498225212097, 0.7170127630233765, 1.0188204050064087, 0.9920745491981506, 1.0358206033706665, 0.654901385307312]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.3659829795360565, 1.1339551210403442, 0.9558420777320862, 0.8302574753761292, 0.5854758024215698, 0.7928292155265808, 1.1030024290084839, 0.996678352355957, 0.8503401279449463, 0.9217022657394409, 0.6798834800720215, -0.4379842281341553, 0.7190782427787781, 0.8114636540412903, 1.0518441200256348, 1.3727525472640991]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.73517906665802, 0.24174758791923523, 0.8215631246566772, 0.7114955186843872, 0.6662012338638306, 0.48364508152008057, 0.3290203809738159, 0.8436055183410645, -0.8197959661483765, 2.510117769241333, -0.7139328122138977, 0.712722897529602, 0.8822013139724731, 0.9300163388252258, 0.8176219463348389, 0.5901493430137634]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.04246074706315994, 0.5596828460693359, 1.3095437288284302, 1.224589467048645, 1.2610809803009033, 1.3331207036972046, 1.3803411722183228, 1.3541135787963867, 1.64261794090271, 1.4878013134002686, 1.897640347480774, 1.2783435583114624, 1.3587679862976074, 1.1407113075256348, 0.7045765519142151, 1.3406143188476562]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3361234664916992, 1.3101615905761719, 1.3942655324935913, 1.5723320245742798, 0.9731191992759705, 1.211701512336731, 1.7679102420806885, 1.6455941200256348, 1.8875824213027954, 1.3415443897247314, 1.1063822507858276, 0.9608046412467957, 1.221795916557312, 1.358839750289917, 1.437048316001892, 1.404496192932129]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.2269079685211182, 1.1842713356018066, 1.0577168464660645, 1.1270992755889893, 1.3530240058898926, 0.8755580186843872, 0.9398849606513977, 1.1221699714660645, 1.7783468961715698, 1.3615540266036987, 1.3111448287963867, 1.2944568395614624, 1.8729671239852905, 1.2757227420806885, 1.1741735935211182, 1.228728175163269]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8119228482246399, 1.0403380393981934, 1.0353953838348389, 0.8717265129089355, 0.13427318632602692, 1.1292915344238281, 1.1625080108642578, 1.9080902338027954, 0.8449275493621826, 1.141016960144043, 1.2343617677688599, 1.0891661643981934, 1.0464763641357422, 1.404469609260559, 1.1321697235107422, 0.4511951208114624]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.9926342964172363, 2.1202964782714844, 1.9603794813156128, 1.9088807106018066, 1.8272215127944946, 1.953889012336731, 1.8548309803009033, 2.1005260944366455, 1.8964179754257202, 1.9637011289596558, 2.272188663482666, 2.7086522579193115, 1.816226840019226, 2.3370003700256348, 2.1651785373687744, 2.0418527126312256]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3859481811523438, 1.2393757104873657, 1.7295918464660645, 1.0628321170806885, 1.437127947807312, 1.4972363710403442, 1.2969813346862793, 1.3343164920806885, 1.4193239212036133, 1.547326683998108, 1.4689359664916992, 2.2108047008514404, 1.4950042963027954, 1.5137382745742798, 1.1933726072311401, 1.587093472480774]
Running loglikelihood requests:  34%|████████████████▊                                | 173/504 [09:48<17:57,  3.25s/it]Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.508397102355957, 1.7430444955825806, 1.2553112506866455, 1.4452261924743652, 1.4473187923431396, 1.4913504123687744, 1.3816399574279785, 1.4464950561523438, 1.475014567375183, 1.3969194889068604, 1.5407034158706665, 1.458804965019226, 1.4013206958770752, 0.9525561928749084, 1.4897793531417847, 1.4931308031082153]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.315981149673462, 2.3335659503936768, 2.1814944744110107, 2.1950998306274414, 2.830888509750366, 2.3397109508514404, 2.7191884517669678, 2.249760866165161, 2.2793898582458496, 2.3178677558898926, 2.5549399852752686, 2.2611076831817627, 2.231053352355957, 2.3989555835723877, 2.383397102355957, 2.389960765838623]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6017218828201294, 1.7800906896591187, 1.4253029823303223, 1.6037049293518066, 1.4821693897247314, 1.473536491394043, 1.521285057067871, 1.8158283233642578, 1.506404161453247, 1.4890186786651611, 1.5026805400848389, 1.3853635787963867, 1.4826676845550537, 1.3838489055633545, 1.562127947807312, 1.9058231115341187]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9325108528137207, 1.679646372795105, 1.7183314561843872, 1.8020766973495483, 1.7186204195022583, 1.7485700845718384, 1.6761698722839355, 1.7775928974151611, 1.7900656461715698, 1.8804807662963867, 2.165437698364258, 1.642661213874817, 1.683676838874817, 1.8737244606018066, 1.831466555595398, 1.7662286758422852]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.586761236190796, 1.6563363075256348, 1.6162009239196777, 1.7471400499343872, 1.6513539552688599, 1.6105774641036987, 1.7294490337371826, 1.7313324213027954, 1.6210074424743652, 1.7593470811843872, 1.522029161453247, 1.7480601072311401, 1.6187021732330322, 1.7975716590881348, 1.600213885307312, 1.6252508163452148]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.081453323364258, 2.160458564758301, 2.5727241039276123, 2.078749418258667, 2.1568546295166016, 2.099224090576172, 2.5796115398406982, 2.1457271575927734, 2.0351977348327637, 2.074099063873291, 1.9844746589660645, 1.9268043041229248, 2.014641761779785, 1.8962053060531616, 2.390522003173828, 2.159508466720581]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.540284633636475, 5.83779764175415, 5.6109161376953125, 5.529815196990967, 5.433221817016602, 5.348692417144775, 5.519664287567139, 5.955463409423828, 5.690688610076904, 5.504171848297119, 5.547645568847656, 5.4470133781433105, 5.588754177093506, 5.607195854187012, 5.693930625915527, 5.533801078796387]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.158588409423828, 3.9690158367156982, 3.980282783508301, 3.8294403553009033, 4.028193950653076, 3.98038911819458, 3.8052072525024414, 4.015996932983398, 4.009061336517334, 4.085485935211182, 3.889136791229248, 3.7012882232666016, 4.042420864105225, 3.9752869606018066, 4.079134941101074, 4.129597187042236]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.186995029449463, 3.2891952991485596, 3.0956101417541504, 2.925117015838623, 3.045041561126709, 3.237032413482666, 3.0622341632843018, 3.0655159950256348, 3.0984268188476562, 3.1435680389404297, 3.3022959232330322, 2.6048343181610107, 3.1659491062164307, 3.123790979385376, 3.3179209232330322, 3.1887755393981934]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09454198181629181, 0.11404523998498917, 0.1096452921628952, -0.23850896954536438, -0.21836622059345245, -0.13013868033885956, 0.1257607340812683, -0.1921689808368683, 0.07534382492303848, 0.09826675802469254, 0.10383963584899902, 0.09000496566295624, 0.0957605317234993, 0.10620459169149399, -1.0895339250564575, 0.12123918533325195]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09505899250507355, 0.06403417885303497, 0.0409979447722435, 0.044791124761104584, 0.08467943966388702, 0.03150298073887825, 0.0565081350505352, 0.06509733200073242, 0.005311847664415836, 0.07296429574489594, -0.17219291627407074, 0.061179693788290024, 0.003439225722104311, -0.02888004668056965, 0.017837142571806908, 0.048903945833444595]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.10307051241397858, 0.05986112356185913, 0.09350830316543579, 0.06169099360704422, 0.09805139899253845, 0.11329066753387451, 0.06524337083101273, -0.07867725938558578, 0.08378614485263824, 0.09059495478868484, 0.004899235442280769, 0.08704981207847595, -0.15143705904483795, 0.035604581236839294, -0.024285783991217613, 0.11624271422624588]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.14116843044757843, 0.17813678085803986, 0.11908084899187088, 0.142745703458786, 0.14278417825698853, 0.1263454109430313, 0.07082119584083557, 0.1835215538740158, 0.1775386482477188, -0.3761727213859558, -0.02012939378619194, 0.11455099284648895, 0.14221738278865814, -0.16356916725635529, -0.04328739643096924, -0.05559903010725975]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08529915660619736, 0.10741940140724182, 0.09992728382349014, 0.1057678759098053, 0.0475304052233696, -0.04042595252394676, 0.04836304858326912, 0.05195822939276695, -0.04175883159041405, 0.023997129872441292, -0.13287875056266785, 0.15909290313720703, -0.057349685579538345, -0.18731895089149475, 0.12299615144729614, -0.01064453087747097]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07296610623598099, 0.12773647904396057, 0.0699281096458435, 0.09049598127603531, -0.02695649303495884, 0.0024207804817706347, 0.1316225230693817, 0.046653904020786285, 0.09936817735433578, -0.07962919771671295, -0.1298971176147461, 0.1000889241695404, -0.14800794422626495, 0.1343430131673813, 0.13302843272686005, 0.06791342049837112]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08176279813051224, 0.04124240204691887, 0.1778445541858673, 0.34015777707099915, 0.18506385385990143, 0.1197405606508255, -0.06688253581523895, -0.040171023458242416, 0.4714515507221222, 0.1628536880016327, -0.4732009470462799, 0.33872345089912415, 0.19472992420196533, -0.101903036236763, 0.12417013198137283, 0.17462347447872162]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.31767672300338745, 0.2578105926513672, 0.33051273226737976, 0.11247844994068146, -0.876991868019104, 0.3402882516384125, 0.18952271342277527, -0.10577939450740814, 0.3706825375556946, 0.2009100615978241, -0.022496454417705536, 0.2830665409564972, 0.3560159504413605, 0.12714548408985138, -0.3517813980579376, -0.05143074691295624]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.18913203477859497, 0.3285343647003174, -0.05956484004855156, 0.32603001594543457, 0.5251431465148926, 0.298747718334198, 0.2941818833351135, -0.16583651304244995, 0.1457424759864807, 0.6085766553878784, -0.010527091100811958, 0.45636680722236633, 0.34017038345336914, -0.3129862844944, -0.16613958775997162, 0.2813495397567749]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4799198508262634, 0.21837958693504333, 0.5043928623199463, 0.7317416667938232, 0.45149093866348267, 0.45682036876678467, 0.712314784526825, 0.566200852394104, 0.5248833894729614, 0.19423428177833557, 0.5548731088638306, 0.7234846353530884, 0.9590245485305786, 0.6010363101959229, 0.7558997869491577, 0.7471039891242981]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.979995608329773, 0.8558796048164368, 0.670605480670929, 0.8880522847175598, 1.0986943244934082, 0.7088804244995117, 0.6806758642196655, 0.674609363079071, 0.3940829634666443, 0.9539024829864502, 0.7492052912712097, 0.9378973841667175, 0.6399473547935486, 0.2734564542770386, 0.7049728631973267, 0.4948968291282654]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.064271330833435, 1.1229896545410156, 0.9933863282203674, 1.3817988634109497, 1.4679821729660034, 0.8480755090713501, 0.9828798770904541, 1.440611481666565, 1.0770978927612305, 1.3397225141525269, 1.0340180397033691, 1.2126885652542114, 0.8452460169792175, 0.5852968692779541, 0.03922329470515251, 0.919423520565033]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.43467196822166443, 1.0373114347457886, 1.2678744792938232, 1.274111032485962, 0.5667059421539307, 0.85205078125, 1.0559191703796387, 1.2539029121398926, 1.045007348060608, 0.7727568745613098, 0.7225619554519653, 0.8944234848022461, 1.1984374523162842, 0.8703866004943848, 0.696713387966156, 1.0462418794631958]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8347049951553345, 0.9106369614601135, 0.8699662685394287, 0.6607245206832886, 1.2550915479660034, 0.35938340425491333, 1.1432920694351196, 2.2523436546325684, 1.0217739343643188, 0.9777478575706482, 1.28435480594635, 1.3338900804519653, -0.03404288366436958, 1.066264033317566, 1.152280569076538, 0.8668349981307983]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7084607481956482, 0.8088732361793518, 0.9142476916313171, 0.8395979404449463, 0.907421886920929, 0.8000328540802002, 0.6130573153495789, 1.1320220232009888, 0.7213530540466309, 1.0377744436264038, 0.2802313566207886, 0.7823764085769653, 1.0949808359146118, 1.0463025569915771, 1.1004445552825928, 0.7260758876800537]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.46766525506973267, 1.1866782903671265, 1.0377222299575806, 0.8386785984039307, 0.6772544980049133, 0.8823612332344055, 1.1635198593139648, 1.0513739585876465, 0.915463387966156, 0.9822198152542114, 0.7667800188064575, -0.3423689305782318, 0.830161452293396, 0.8691002130508423, 1.1268318891525269, 1.5389270782470703]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8244216442108154, 0.3340891897678375, 0.8927818536758423, 0.734667956829071, 0.6419748067855835, 0.5663641691207886, 0.4363502264022827, 0.9236429333686829, -0.6836745738983154, 2.6202046871185303, -0.5583125948905945, 0.8193767666816711, 0.9310749173164368, 0.9678407907485962, 0.8608633875846863, 0.7565774917602539]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.2265627086162567, 0.6652440428733826, 1.3990706205368042, 1.3791149854660034, 1.362284541130066, 1.415274739265442, 1.485479474067688, 1.4441272020339966, 1.6787244081497192, 1.5960533618927002, 2.0195178985595703, 1.3805058002471924, 1.396909475326538, 1.1540813446044922, 0.8454303741455078, 1.4807987213134766]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4181573390960693, 1.3719558715820312, 1.445854663848877, 1.6396820545196533, 1.0793237686157227, 1.3175511360168457, 1.844423532485962, 1.7391703128814697, 1.9872037172317505, 1.3992726802825928, 1.2385371923446655, 1.1353800296783447, 1.3297817707061768, 1.5420342683792114, 1.5351293087005615, 1.477666974067688]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3525726795196533, 1.2816541194915771, 1.1658135652542114, 1.2360721826553345, 1.4506465196609497, 1.0106360912322998, 1.0711172819137573, 1.263617992401123, 1.8872575759887695, 1.418480634689331, 1.4238415956497192, 1.4013066291809082, 2.0374865531921387, 1.391136884689331, 1.2820178270339966, 1.320770502090454]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8619226813316345, 1.1203663349151611, 1.1028690338134766, 0.9405761957168579, 0.19763930141925812, 1.2198882102966309, 1.2550241947174072, 1.9842638969421387, 0.8634210228919983, 1.220029592514038, 1.2976967096328735, 1.1256263256072998, 1.1305630207061768, 1.4817079305648804, 1.232797384262085, 0.5528400540351868]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0633082389831543, 2.1879310607910156, 2.0338361263275146, 2.0011584758758545, 1.9192618131637573, 2.028125047683716, 1.9551993608474731, 2.1766703128814697, 1.9506195783615112, 2.0632543563842773, 2.3464438915252686, 2.7703664302825928, 1.9100619554519653, 2.414978504180908, 2.258539915084839, 2.115086317062378]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4544180631637573, 1.3029364347457886, 1.7619612216949463, 1.1181294918060303, 1.49396550655365, 1.5532327890396118, 1.371605634689331, 1.3860721588134766, 1.4782596826553345, 1.6318964958190918, 1.500565767288208, 2.25905179977417, 1.5567349195480347, 1.5586206912994385, 1.2665948867797852, 1.649057149887085]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5359914302825928, 1.787958025932312, 1.3326529264450073, 1.4742321968078613, 1.4889682531356812, 1.5427262783050537, 1.4360452890396118, 1.5189385414123535, 1.5312095880508423, 1.4180293083190918, 1.5968480110168457, 1.492133617401123, 1.475511908531189, 1.04131281375885, 1.5699790716171265, 1.5555496215820312]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.336961269378662, 2.39294171333313, 2.226104497909546, 2.234375, 2.8843750953674316, 2.391702651977539, 2.7607219219207764, 2.2884159088134766, 2.326023817062378, 2.3688578605651855, 2.5955820083618164, 2.3241918087005615, 2.2760236263275146, 2.443103551864624, 2.422952651977539, 2.4488415718078613]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.625996708869934, 1.8161368370056152, 1.449730634689331, 1.6224945783615112, 1.523154616355896, 1.487069010734558, 1.5486125946044922, 1.8391163349151611, 1.5429013967514038, 1.506815791130066, 1.528394341468811, 1.426400899887085, 1.5115840435028076, 1.4192079305648804, 1.564736008644104, 1.9511314630508423]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Running loglikelihood requests:  35%|█████████████████▏                               | 177/504 [10:01<17:37,  3.23s/it]Layer: gate_26 - Captured router_logits: [1.9726831912994385, 1.705019235610962, 1.7397629022598267, 1.833135724067688, 1.7476226091384888, 1.7844945192337036, 1.6960264444351196, 1.7937803268432617, 1.813383936882019, 1.909536600112915, 2.1722049713134766, 1.6679418087005615, 1.7151939868927002, 1.9011719226837158, 1.837372064590454, 1.788500189781189]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.626562476158142, 1.671693205833435, 1.6392039060592651, 1.7755640745162964, 1.6821995973587036, 1.6431472301483154, 1.744042992591858, 1.7607792615890503, 1.6511045694351196, 1.788759469985962, 1.545662760734558, 1.7662311792373657, 1.6555495262145996, 1.8100687265396118, 1.623632788658142, 1.6462217569351196]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1166608333587646, 2.2024381160736084, 2.6067585945129395, 2.12463641166687, 2.1963632106781006, 2.135753870010376, 2.606004238128662, 2.192312002182007, 2.0726022720336914, 2.1205146312713623, 2.0291757583618164, 1.964277982711792, 2.0496902465820312, 1.9407596588134766, 2.430152177810669, 2.2020137310028076]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.614223957061768, 5.905818939208984, 5.673383712768555, 5.594881534576416, 5.50409460067749, 5.410021781921387, 5.592133522033691, 6.005280017852783, 5.781142234802246, 5.584536552429199, 5.610775947570801, 5.5095906257629395, 5.666433334350586, 5.677801609039307, 5.776939868927002, 5.5809807777404785]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.13744592666626, 3.9633889198303223, 3.9522898197174072, 3.7968435287475586, 3.983243465423584, 3.9697468280792236, 3.775639772415161, 3.984321117401123, 3.9735519886016846, 4.054256439208984, 3.850592613220215, 3.6720383167266846, 4.001700401306152, 3.9503233432769775, 4.062715530395508, 4.10749626159668]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1970367431640625, 3.284536600112915, 3.127424478530884, 2.9122304916381836, 3.0666487216949463, 3.2526400089263916, 3.0639548301696777, 3.074784517288208, 3.1179418563842773, 3.1524245738983154, 3.281142234802246, 2.642547845840454, 3.1604526042938232, 3.1288254261016846, 3.311099052429199, 3.1957435607910156]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.08800111711025238, 0.10141338407993317, 0.10299494862556458, -0.2575029134750366, -0.21015582978725433, -0.15205320715904236, 0.11382635682821274, -0.15179316699504852, 0.09237910062074661, 0.08503812551498413, 0.09429536759853363, 0.10355924069881439, 0.0913204550743103, 0.10589020699262619, -1.049521803855896, 0.11977580934762955]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.09740479290485382, 0.062872014939785, 0.03767077997326851, 0.05345482751727104, 0.0847601518034935, 0.039705369621515274, 0.04645691066980362, 0.07311280071735382, 0.027019238099455833, 0.06490197032690048, -0.18304717540740967, 0.05809073522686958, 0.014476670883595943, -0.019875625148415565, 0.01161622628569603, 0.018991878256201744]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07628894597291946, 0.055425263941287994, 0.09453314542770386, 0.0519772544503212, 0.12480637431144714, 0.085136778652668, 0.07193371653556824, -0.07260321080684662, 0.08366909623146057, 0.08674661070108414, 0.010488681495189667, 0.08065111935138702, -0.1552218794822693, 0.024526596069335938, 0.001093371189199388, 0.10483798384666443]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12095673382282257, 0.15744461119174957, 0.1301100105047226, 0.15407545864582062, 0.11680424213409424, 0.1495528668165207, 0.05914390832185745, 0.1606316864490509, 0.17914776504039764, -0.4617987275123596, -0.02788972482085228, 0.1363350749015808, 0.22020789980888367, -0.23005497455596924, -0.01940886490046978, -0.03132176771759987]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07954321056604385, 0.07194624096155167, 0.08310920745134354, 0.156600221991539, 0.05096713826060295, -0.05806095525622368, 0.041571203619241714, 0.04321199655532837, -0.07657154649496078, 0.02440395951271057, -0.166066974401474, 0.11899203807115555, -0.03179195150732994, -0.1365421712398529, 0.13787958025932312, -0.0013388798106461763]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07853440940380096, 0.14110112190246582, 0.07005715370178223, 0.1576620191335678, -0.08097986876964569, 0.010806011036038399, 0.08627603203058243, 0.031218746677041054, 0.16962817311286926, -0.051768019795417786, -0.18229222297668457, 0.0926956981420517, -0.17319640517234802, 0.13858263194561005, 0.15601222217082977, 0.08364260196685791]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.12472492456436157, 0.06726452708244324, 0.15624363720417023, 0.29075801372528076, 0.18032057583332062, 0.13169650733470917, -0.049174342304468155, 0.0031069000251591206, 0.3008904755115509, 0.15389035642147064, -0.4563594460487366, 0.29313433170318604, 0.19863785803318024, -0.14743715524673462, 0.23209522664546967, 0.21775265038013458]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.4049912095069885, 0.25513899326324463, 0.31203192472457886, 0.14089013636112213, -0.6507088541984558, 0.3056282699108124, 0.14370916783809662, 9.176319872494787e-05, 0.4436212480068207, 0.1916375458240509, -0.042771123349666595, 0.22657638788223267, 0.22698764503002167, 0.19808581471443176, -0.4255034327507019, -0.28142258524894714]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.257497102022171, 0.2773761749267578, -0.06944895535707474, 0.28341764211654663, 0.43906208872795105, 0.41074198484420776, 0.2958782911300659, -0.26594194769859314, 0.19123435020446777, 0.858154296875, 0.14234714210033417, 0.1201719120144844, 0.48664382100105286, -0.3655790090560913, -0.23010759055614471, 0.36303678154945374]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.6009972095489502, 0.40343984961509705, 0.35299620032310486, 0.9419459700584412, 0.45106831192970276, 0.43247154355049133, 0.7303458452224731, 0.5817753076553345, 0.8471662998199463, 0.37410256266593933, 0.3650798797607422, 0.6698781251907349, 0.7636024355888367, 0.5292798280715942, 0.7251481413841248, 0.6407260298728943]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9681455492973328, 0.8719221353530884, 0.5774877071380615, 0.765342116355896, 0.9253636598587036, 0.6523012518882751, 0.6632357835769653, 0.6730199456214905, 0.3336939215660095, 0.764011561870575, 0.7846174836158752, 0.9403960108757019, 0.5753093957901001, 0.39933788776397705, 1.0721065998077393, 0.6326702237129211]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [1.061264157295227, 1.1967672109603882, 0.911328136920929, 1.423359990119934, 1.4991446733474731, 0.5999636054039001, 0.9542093276977539, 0.9906553030014038, 1.0009698867797852, 1.4911099672317505, 0.9088530540466309, 1.1115672588348389, 0.5787554383277893, 0.734912097454071, 0.2984621226787567, 1.0022902488708496]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.5066099166870117, 1.027929663658142, 1.332051396369934, 1.2522629499435425, 0.7458529472351074, 0.7137434482574463, 1.1797633171081543, 1.2922077178955078, 0.6993370056152344, 0.39276471734046936, 1.0279582738876343, 0.8819100260734558, 1.1653623580932617, 0.9077653288841248, 0.8233162760734558, 0.9911907315254211]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.122373342514038, 1.0800410509109497, 1.387133002281189, 0.9593379497528076, 1.292631983757019, 0.15176033973693848, 1.2553070783615112, 2.2504849433898926, 1.247588872909546, 1.1557650566101074, 1.4874528646469116, 1.435829758644104, 0.5632509589195251, 1.3669652938842773, 1.4407192468643188, 0.9439284801483154]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.9517375826835632, 0.8416958451271057, 1.0694302320480347, 1.0217807292938232, 1.0842537879943848, 1.3315201997756958, 0.8097917437553406, 1.6308071613311768, 0.7786250710487366, 1.031664252281189, 0.0363255999982357, 0.9377458095550537, 1.0967700481414795, 1.119795560836792, 1.2034331560134888, 0.8385581970214844]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.709488034248352, 1.3371565341949463, 1.1673760414123535, 1.0449084043502808, 0.9868029356002808, 1.1004984378814697, 1.6979424953460693, 1.115975260734558, 1.0113415718078613, 1.0778555870056152, 1.0144476890563965, 0.08851329237222672, 1.320562481880188, 0.9837823510169983, 1.016958475112915, 1.865308403968811]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7199302911758423, 0.44280827045440674, 0.8268386125564575, 0.6833740472793579, 0.6812474727630615, 0.6959118843078613, 0.5771408677101135, 0.8461745977401733, -0.47937095165252686, 2.2335903644561768, -0.5286293029785156, 0.7152941226959229, 0.8859577178955078, 1.0015220642089844, 0.846450686454773, 0.7035467624664307]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.6401965022087097, 1.1599129438400269, 1.6477370262145996, 1.7170594930648804, 1.6804687976837158, 1.6054956912994385, 1.7373921871185303, 1.6509429216384888, 1.9163389205932617, 1.7402209043502808, 2.5004403591156006, 1.745043158531189, 1.532639503479004, 1.380502462387085, 1.002557635307312, 1.6333175897598267]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6352909803390503, 1.434051752090454, 1.6695514917373657, 1.9265356063842773, 1.315895676612854, 1.481465458869934, 2.1861259937286377, 2.0272629261016846, 2.63671875, 1.671875, 1.5318729877471924, 1.426210641860962, 1.6689587831497192, 1.2812942266464233, 1.8080010414123535, 1.7229795455932617]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.863227367401123, 1.6960399150848389, 1.8321659564971924, 1.6681842803955078, 1.893534541130066, 1.5572737455368042, 1.7790948152542114, 1.7267106771469116, 2.3140759468078613, 1.6371498107910156, 1.9288792610168457, 1.8492995500564575, 2.2682044506073, 1.8385506868362427, 1.83771550655365, 1.7915409803390503]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2766602039337158, 1.2867591381072998, 1.3020743131637573, 1.0713765621185303, 0.4190818965435028, 1.4720096588134766, 1.3822871446609497, 2.295114755630493, 1.1029044389724731, 1.438348650932312, 1.3729121685028076, 1.283340334892273, 1.2551723718643188, 1.4529094696044922, 1.2426669597625732, 0.8243349194526672]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.3127424716949463, 2.4952585697174072, 2.3768858909606934, 2.271578550338745, 2.255549669265747, 2.4092671871185303, 2.4126884937286377, 2.461853504180908, 2.197683095932007, 2.4305765628814697, 2.610452651977539, 2.88938570022583, 2.1387929916381836, 2.8273167610168457, 2.53782320022583, 2.469719886779785]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.8880118131637573, 1.645150899887085, 2.190786600112915, 1.4453394412994385, 1.9080549478530884, 1.8224138021469116, 1.7299299240112305, 1.6569504737854004, 1.8018049001693726, 1.95703125, 1.8020473718643188, 2.3041486740112305, 1.8669989109039307, 1.8301724195480347, 1.6734644174575806, 1.9579472541809082]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.359779119491577, 1.8667833805084229, 1.4957098960876465, 1.6877424716949463, 1.718184232711792, 1.6095366477966309, 1.6320850849151611, 1.5797144174575806, 1.5985722541809082, 1.5639547109603882, 1.659482717514038, 1.583647608757019, 1.5220770835876465, 1.073188304901123, 1.5715382099151611, 1.6949622631072998]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.5945582389831543, 2.6099677085876465, 2.4637930393218994, 2.3922953605651855, 2.943319082260132, 2.5430495738983154, 2.750053882598877, 2.543750047683716, 2.544100284576416, 2.637176752090454, 2.88938570022583, 2.4913253784179688, 2.472252130508423, 2.6321120262145996, 2.62623929977417, 2.6202046871185303]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7582974433898926, 1.7885775566101074, 1.6415948867797852, 1.805361032485962, 1.5199757814407349, 1.653017282485962, 1.7027747631072998, 1.9293642044067383, 1.7019935846328735, 1.6536368131637573, 1.6782597303390503, 1.61115300655365, 1.7577855587005615, 1.5579471588134766, 1.8566540479660034, 2.088550567626953]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.8585667610168457, 1.8171488046646118, 1.7463901042938232, 1.8015625476837158, 1.8294854164123535, 1.7964860200881958, 1.85578191280365, 1.7771955728530884, 1.7844347953796387, 1.8970232009887695, 2.278273105621338, 1.738227367401123, 1.795312523841858, 1.8139008283615112, 1.7865031957626343, 1.7814562320709229]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6080886125564575, 1.6827998161315918, 1.6581560373306274, 1.731064796447754, 1.629862666130066, 1.5998914241790771, 1.7731828689575195, 1.7078495025634766, 1.6867777109146118, 1.7483162879943848, 1.5029902458190918, 1.6446928977966309, 1.6557785272598267, 1.8368113040924072, 1.6630792617797852, 1.6441035270690918]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.212796449661255, 2.3411099910736084, 2.510681629180908, 2.2571659088134766, 2.2441136837005615, 2.219423532485962, 2.6528286933898926, 2.305253267288208, 2.2141971588134766, 2.220958948135376, 2.264143228530884, 2.085829734802246, 2.1388468742370605, 2.147979497909546, 2.4210667610168457, 2.171659469604492]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.25533390045166, 5.5864763259887695, 5.156303882598877, 5.24536657333374, 5.2990031242370605, 5.165948390960693, 5.238685131072998, 5.44337272644043, 5.415625095367432, 5.2528557777404785, 5.250215530395508, 5.141648769378662, 5.300942897796631, 5.278448104858398, 5.375269412994385, 5.279364109039307]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  36%|█████████████████▌                               | 181/504 [10:15<17:34,  3.26s/it]Layer: gate_30 - Captured router_logits: [4.027585983276367, 3.8827316761016846, 3.849757432937622, 3.835913896560669, 3.8338093757629395, 3.888416051864624, 3.7488913536071777, 3.956465482711792, 3.9488821029663086, 4.008701324462891, 3.835721969604492, 3.6398942470550537, 3.884341239929199, 3.89684796333313, 3.9288253784179688, 3.961408853530884]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.901885747909546, 3.0237338542938232, 2.878448247909546, 2.7649784088134766, 2.813685417175293, 2.955118417739868, 2.8474137783050537, 2.734375, 2.8744611740112305, 2.899218797683716, 3.119288682937622, 2.4330921173095703, 2.9885776042938232, 2.886260747909546, 3.1649246215820312, 2.9413793087005615]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09069935232400894, 0.10946422070264816, 0.10665043443441391, -0.23220103979110718, -0.20854218304157257, -0.13881614804267883, 0.12390348315238953, -0.1796620637178421, 0.07172732800245285, 0.09537437558174133, 0.10401736199855804, 0.08855310827493668, 0.09131521731615067, 0.10135719180107117, -1.0699734687805176, 0.11956575512886047]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09336789697408676, 0.06736377626657486, 0.04272662103176117, 0.041766420006752014, 0.08307170867919922, 0.02797073870897293, 0.0545605830848217, 0.06689596176147461, 0.01548987627029419, 0.0696367546916008, -0.17748048901557922, 0.058978769928216934, 0.004707018379122019, -0.035980358719825745, 0.012080801650881767, 0.048184555023908615]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09981759637594223, 0.05333397164940834, 0.09019409120082855, 0.06188056245446205, 0.09800592809915543, 0.11481262743473053, 0.07225629687309265, -0.07615438848733902, 0.07839366793632507, 0.10031965374946594, -0.004114654380828142, 0.09123770147562027, -0.15801747143268585, 0.034884851425886154, -0.022081904113292694, 0.11388932168483734]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13801278173923492, 0.18081028759479523, 0.1193084716796875, 0.13747014105319977, 0.14436425268650055, 0.11801041662693024, 0.05315685272216797, 0.18603675067424774, 0.1839754283428192, -0.37734901905059814, -0.01679229736328125, 0.11549445986747742, 0.15123578906059265, -0.174005925655365, -0.025855805724859238, -0.04847293347120285]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07789155840873718, 0.10259347409009933, 0.10078806430101395, 0.11138831079006195, 0.041950251907110214, -0.0277396310120821, 0.04626660794019699, 0.057069141417741776, -0.05310143530368805, 0.015046225860714912, -0.13728898763656616, 0.15604236721992493, -0.04999521002173424, -0.18086205422878265, 0.13166703283786774, -0.01273515447974205]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06833478808403015, 0.1305413842201233, 0.06889719516038895, 0.1018393337726593, -0.03107537142932415, -0.0046246848069131374, 0.1345486044883728, 0.049048952758312225, 0.1057189553976059, -0.06364038586616516, -0.10560454428195953, 0.09174135327339172, -0.13795693218708038, 0.12836933135986328, 0.13089942932128906, 0.06801480799913406]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08370600640773773, 0.02331087365746498, 0.16581323742866516, 0.3534033000469208, 0.18650181591510773, 0.12177011370658875, -0.07572253793478012, -0.033729977905750275, 0.4862481355667114, 0.16398344933986664, -0.46131473779678345, 0.3467063307762146, 0.19655779004096985, -0.10861672461032867, 0.128423273563385, 0.17271868884563446]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.30803489685058594, 0.24592559039592743, 0.3171554207801819, 0.1259138286113739, -0.8644188642501831, 0.33326512575149536, 0.1951105296611786, -0.1351504921913147, 0.3736577033996582, 0.19076919555664062, -0.030691994354128838, 0.29433950781822205, 0.3660430908203125, 0.12040869146585464, -0.3563661575317383, -0.041855283081531525]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20788781344890594, 0.3381180167198181, -0.10293388366699219, 0.343718945980072, 0.5125876665115356, 0.28214943408966064, 0.2904551923274994, -0.17585499584674835, 0.14920955896377563, 0.6122487187385559, -0.012250476516783237, 0.4691827595233917, 0.3237118124961853, -0.3233053386211395, -0.21626323461532593, 0.2795551121234894]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.45399242639541626, 0.21612167358398438, 0.5369207859039307, 0.7436635494232178, 0.4555117189884186, 0.44067466259002686, 0.6829512119293213, 0.5738050937652588, 0.4740859270095825, 0.22047975659370422, 0.5498767495155334, 0.7391154170036316, 0.9564526081085205, 0.6057741641998291, 0.7659742832183838, 0.7287055253982544]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.007546305656433, 0.8546820878982544, 0.6717495322227478, 0.8722127079963684, 1.0834723711013794, 0.6907069087028503, 0.6735416054725647, 0.6785142421722412, 0.3832751512527466, 0.9435975551605225, 0.7649332880973816, 0.9386562705039978, 0.6571375727653503, 0.2772924602031708, 0.6415730714797974, 0.4772016704082489]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0737168788909912, 1.1125073432922363, 1.0216200351715088, 1.3140673637390137, 1.4256591796875, 0.7780448198318481, 0.9820963740348816, 1.3817681074142456, 1.0721944570541382, 1.3306206464767456, 0.9920289516448975, 1.14666748046875, 0.7861277461051941, 0.5480154752731323, -0.012241787277162075, 0.8828532099723816]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.387225478887558, 0.9773830771446228, 1.2191026210784912, 1.2596435546875, 0.5195460915565491, 0.8241738080978394, 1.0413877964019775, 1.2426046133041382, 0.9895684719085693, 0.7027373909950256, 0.6796908974647522, 0.9127333164215088, 1.1480848789215088, 0.9002821445465088, 0.6744808554649353, 1.0049099922180176]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7533543705940247, 0.8905928134918213, 0.8010172247886658, 0.5814738869667053, 1.1817762851715088, 0.24299494922161102, 1.1196967363357544, 2.1713595390319824, 0.9664306640625, 0.9159477949142456, 1.1883680820465088, 1.2445068359375, -0.11816152185201645, 0.9950782060623169, 1.0991020202636719, 0.7620643973350525]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6642608642578125, 0.7739851474761963, 0.9041103720664978, 0.8020985722541809, 0.8770548701286316, 0.6817749738693237, 0.5335235595703125, 1.0418689250946045, 0.7379286289215088, 0.9722764492034912, 0.17556804418563843, 0.6943461298942566, 1.02587890625, 1.0030076503753662, 1.0537855625152588, 0.6475898027420044]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.3835979104042053, 1.1341146230697632, 0.9884372353553772, 0.8516913652420044, 0.6201087236404419, 0.8361748456954956, 1.0715010166168213, 1.0100911855697632, 0.8758544921875, 0.945648193359375, 0.7013922929763794, -0.4149678647518158, 0.749212920665741, 0.8154568076133728, 1.0671658515930176, 1.4041146039962769]
Running loglikelihood requests:  37%|█████████████████▉                               | 185/504 [10:27<17:17,  3.25s/it]Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7799254059791565, 0.2828841805458069, 0.8414933681488037, 0.7575581669807434, 0.685546875, 0.4933963418006897, 0.3557285666465759, 0.90742027759552, -0.764166533946991, 2.510023355484009, -0.6638069152832031, 0.7463385462760925, 0.9114040732383728, 0.9427693486213684, 0.8520846962928772, 0.652292013168335]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.12690183520317078, 0.5925987958908081, 1.3540581464767456, 1.2980787754058838, 1.3368734121322632, 1.37469482421875, 1.4253607988357544, 1.3726534843444824, 1.6291097402572632, 1.5541517734527588, 1.9439833164215088, 1.3228793144226074, 1.3480521440505981, 1.1758490800857544, 0.7662498950958252, 1.4142930507659912]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3892822265625, 1.3282334804534912, 1.437135100364685, 1.6329209804534912, 1.0173746347427368, 1.2868516445159912, 1.7950845956802368, 1.7130533456802368, 1.9375, 1.3723958730697632, 1.1666802167892456, 1.0349528789520264, 1.264739990234375, 1.461293339729309, 1.4884439706802368, 1.4466146230697632]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.2841660976409912, 1.2331678867340088, 1.0985243320465088, 1.1786431074142456, 1.4061415195465088, 0.9288398027420044, 0.9966328740119934, 1.1804403066635132, 1.7962782382965088, 1.4059786796569824, 1.3697645664215088, 1.3441976308822632, 1.9452040195465088, 1.3314073085784912, 1.2399359941482544, 1.2711045742034912]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.809014618396759, 1.0654771327972412, 1.0729573965072632, 0.8937810063362122, 0.1113315150141716, 1.16941237449646, 1.1981201171875, 1.8561875820159912, 0.8498992919921875, 1.1688638925552368, 1.2627224922180176, 1.0894029140472412, 1.0563422441482544, 1.4277886152267456, 1.1531405448913574, 0.47721418738365173]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.023736000061035, 2.1401095390319824, 1.9961751699447632, 1.9434678554534912, 1.8607584238052368, 1.9986708164215088, 1.8929579257965088, 2.1217989921569824, 1.9007161855697632, 1.9922689199447632, 2.3019206523895264, 2.738823890686035, 1.8464083671569824, 2.3460285663604736, 2.198431968688965, 2.063530921936035]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4120008945465088, 1.2793511152267456, 1.7291938066482544, 1.0708143711090088, 1.4732801914215088, 1.5321993827819824, 1.3298611640930176, 1.3645291328430176, 1.4567056894302368, 1.591064453125, 1.4779459238052368, 2.223741292953491, 1.5182563066482544, 1.5407172441482544, 1.2285020351409912, 1.6262478828430176]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4912922382354736, 1.7479383945465088, 1.2800869941711426, 1.4305555820465088, 1.4768338203430176, 1.5125325918197632, 1.4070096015930176, 1.47412109375, 1.5036858320236206, 1.417236328125, 1.5638563632965088, 1.4710744619369507, 1.4357774257659912, 0.9819052219390869, 1.5105522871017456, 1.5218099355697632]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.2998046875, 2.341905355453491, 2.1801486015319824, 2.1888020038604736, 2.8156466484069824, 2.336805582046509, 2.6921114921569824, 2.2370333671569824, 2.268500328063965, 2.3020832538604736, 2.5534939765930176, 2.260687828063965, 2.221571207046509, 2.400770425796509, 2.3701171875, 2.392686605453491]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6095106601715088, 1.7811415195465088, 1.4326443672180176, 1.6033257246017456, 1.4836968183517456, 1.4807535409927368, 1.5230712890625, 1.8156874179840088, 1.5146077871322632, 1.4994302988052368, 1.5203315019607544, 1.3967827558517456, 1.48828125, 1.3996853828430176, 1.5504556894302368, 1.9228787422180176]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9542236328125, 1.692065715789795, 1.7161729335784912, 1.8110555410385132, 1.741161823272705, 1.76531982421875, 1.6817643642425537, 1.7820875644683838, 1.8018171787261963, 1.8931511640548706, 2.145984172821045, 1.6547682285308838, 1.6958402395248413, 1.8779703378677368, 1.8278536796569824, 1.7785677909851074]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6238877773284912, 1.6719902753829956, 1.639984130859375, 1.773323893547058, 1.676849365234375, 1.63531494140625, 1.7435981035232544, 1.7559577226638794, 1.6472879648208618, 1.7933824062347412, 1.5339083671569824, 1.7638821601867676, 1.6527913808822632, 1.8137059211730957, 1.6272786855697632, 1.6478619575500488]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1294946670532227, 2.203904867172241, 2.6239216327667236, 2.126556396484375, 2.2165188789367676, 2.141486167907715, 2.6180789470672607, 2.2040202617645264, 2.081817626953125, 2.129987955093384, 2.035169839859009, 1.9746025800704956, 2.0598552227020264, 1.9448835849761963, 2.420576333999634, 2.2078280448913574]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.618597984313965, 5.91845703125, 5.660481929779053, 5.616970539093018, 5.52674674987793, 5.433376789093018, 5.612738609313965, 6.03309440612793, 5.798665523529053, 5.599880695343018, 5.652506351470947, 5.519693851470947, 5.688205242156982, 5.690972328186035, 5.794596195220947, 5.615505695343018]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.192599773406982, 4.008246421813965, 4.017035484313965, 3.85506534576416, 4.028781414031982, 4.026123046875, 3.8396809101104736, 4.042697429656982, 4.04432487487793, 4.108072757720947, 3.908745765686035, 3.7339444160461426, 4.071817874908447, 4.01768684387207, 4.115234375, 4.16234016418457]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1398112773895264, 3.2349445819854736, 3.048936605453491, 2.8879122734069824, 3.000868082046509, 3.1843533515930176, 3.041124105453491, 3.0255534648895264, 3.0431857109069824, 3.101318359375, 3.221245765686035, 2.6413557529449463, 3.1239962577819824, 3.077582359313965, 3.2639973163604736, 3.142578125]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.06994400918483734, 0.08390174806118011, 0.08326424658298492, -0.19486522674560547, -0.1851908415555954, -0.12730710208415985, 0.10281891375780106, -0.10098859667778015, 0.08256515115499496, 0.07031329721212387, 0.07876208424568176, 0.06797769665718079, 0.08052629977464676, 0.09127236902713776, -0.9567244052886963, 0.09686660766601562]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07627352327108383, 0.06721793115139008, 0.04004420340061188, 0.043305788189172745, 0.07743385434150696, 0.02331347018480301, 0.0493682362139225, 0.0807841345667839, 0.0283957589417696, 0.06284347921609879, -0.16603702306747437, 0.04216453805565834, 0.013179725967347622, -0.012453979812562466, 0.006039778236299753, 0.03625275194644928]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07436651736497879, 0.054556939750909805, 0.09067109227180481, 0.06454028189182281, 0.12703439593315125, 0.10181108862161636, 0.0716334730386734, -0.0777486190199852, 0.0706353485584259, 0.07425642013549805, 0.00820472463965416, 0.09894832223653793, -0.132382333278656, 0.03743796795606613, -0.010502709075808525, 0.10112910717725754]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11987823992967606, 0.15844875574111938, 0.10716931521892548, 0.14991940557956696, 0.14807796478271484, 0.1270904541015625, 0.04575496166944504, 0.1895701140165329, 0.17933887243270874, -0.37720340490341187, 0.009997580200433731, 0.07663282006978989, 0.2045019567012787, -0.1604427695274353, -0.047890398651361465, -0.030491510406136513]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07505639642477036, 0.0930047556757927, 0.09313678741455078, 0.1260988563299179, 0.04372863098978996, -0.0445827916264534, 0.05489974468946457, 0.06787215173244476, -0.0670340359210968, 0.01576116308569908, -0.10872580111026764, 0.16728857159614563, -0.06583531945943832, -0.08419524133205414, 0.12024757266044617, -0.04188166558742523]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07396809011697769, 0.14837591350078583, 0.06588882952928543, 0.07316483557224274, -0.09714698791503906, 0.0557081438601017, 0.13264888525009155, 0.009116172790527344, 0.1654634177684784, -0.07765769958496094, -0.13132698833942413, 0.11467698961496353, -0.11513180285692215, 0.13627076148986816, 0.1430904120206833, 0.12054602056741714]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08390103280544281, 0.0474565289914608, 0.15443018078804016, 0.3476783037185669, 0.1770661622285843, 0.10093943029642105, -0.023085275664925575, -0.0463680699467659, 0.5347281694412231, 0.15511533617973328, -0.41914960741996765, 0.3275977671146393, 0.1969553679227829, -0.14158016443252563, 0.19650088250637054, 0.18394511938095093]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.38375070691108704, 0.26425594091415405, 0.30082497000694275, 0.1314295083284378, -0.7730204463005066, 0.30558374524116516, 0.15002907812595367, -0.026138518005609512, 0.360478937625885, 0.21940380334854126, -0.05340576171875, 0.24455197155475616, 0.35409674048423767, 0.14379876852035522, -0.3847266435623169, -0.0770823135972023]
Layer: gate_7 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20762719213962555, 0.36368727684020996, -0.14225217700004578, 0.3443790078163147, 0.4537980854511261, 0.2646861672401428, 0.29455989599227905, -0.1916099637746811, 0.2238020896911621, 0.7202704548835754, 0.029203414916992188, 0.3481038510799408, 0.36081525683403015, -0.2415805459022522, -0.274919718503952, 0.32276132702827454]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.38889968395233154, 0.33768123388290405, 0.5418294072151184, 0.8447715044021606, 0.4638879597187042, 0.34229788184165955, 0.6169922947883606, 0.5684543251991272, 0.4851383566856384, 0.23474693298339844, 0.41832834482192993, 0.6572401523590088, 0.953098714351654, 0.6531477570533752, 0.7665134072303772, 0.6931965947151184]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9247440695762634, 0.7809244990348816, 0.6594746708869934, 0.8146023154258728, 1.083301067352295, 0.673119843006134, 0.6189829707145691, 0.6875135898590088, 0.3482615053653717, 0.9115397334098816, 0.6915419101715088, 0.8472697138786316, 0.7171698808670044, 0.2926858365535736, 0.6218993067741394, 0.43295732140541077]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0952080488204956, 1.1055009365081787, 0.9721137285232544, 1.3689507246017456, 1.33612060546875, 0.7371448874473572, 0.8841145634651184, 1.2148946523666382, 0.9900447130203247, 1.3722262382507324, 0.9689466953277588, 0.9907565712928772, 0.6799229383468628, 0.6382583975791931, 0.12306806445121765, 0.8805381059646606]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.32104238867759705, 0.8970472812652588, 1.1126708984375, 1.1948513984680176, 0.4832202196121216, 0.757965087890625, 1.097577452659607, 1.2296617031097412, 0.8380156755447388, 0.6110369563102722, 0.7796401977539062, 0.8352864384651184, 1.0315687656402588, 0.8900485634803772, 0.6524115800857544, 0.9304606318473816]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7141484022140503, 0.9277682900428772, 0.9740278720855713, 0.5447964072227478, 1.1826850175857544, 0.1011878103017807, 1.0874701738357544, 2.231797933578491, 0.974853515625, 0.9023640751838684, 1.1880831718444824, 1.2708468437194824, -0.0028981103096157312, 1.030434489250183, 1.1596713066101074, 0.7318670749664307]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6336949467658997, 0.7462531328201294, 0.8801981806755066, 0.865966796875, 0.8830363154411316, 0.6898998618125916, 0.5995562672615051, 1.1393157243728638, 0.6844685673713684, 0.870792806148529, -0.07851961255073547, 0.6412853598594666, 1.0277031660079956, 0.9612019658088684, 1.0711669921875, 0.4830525815486908]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.3392319083213806, 1.2340766191482544, 0.9296332597732544, 0.8018798828125, 0.6210225224494934, 0.7753431797027588, 1.133890151977539, 0.9338786005973816, 0.7865803837776184, 0.8897976279258728, 0.6742824912071228, -0.2485622763633728, 0.8879360556602478, 0.7765434980392456, 0.9756537675857544, 1.3783111572265625]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7335476279258728, 0.49641916155815125, 0.8094024658203125, 0.7468600869178772, 0.6900604963302612, 0.4934637248516083, 0.337262898683548, 0.9480658769607544, -0.7381536960601807, 2.4578857421875, -0.8003188371658325, 0.7596732378005981, 0.8163655400276184, 0.8998005986213684, 0.8319532871246338, 0.6089698076248169]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.18009227514266968, 0.6506690979003906, 1.2470839023590088, 1.1775681972503662, 1.3528510332107544, 1.2850341796875, 1.2495455741882324, 1.3075833320617676, 1.658233642578125, 1.5985379219055176, 1.8912217617034912, 1.2535874843597412, 1.255676031112671, 1.1107177734375, 0.5300372242927551, 1.3161569833755493]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3184950351715088, 1.237060546875, 1.3521134853363037, 1.5065646171569824, 0.8935987949371338, 1.226318359375, 1.8550347089767456, 1.7222764492034912, 1.9751518964767456, 1.3193631172180176, 1.0656059980392456, 0.8545235395431519, 1.2349480390548706, 1.2065412998199463, 1.335693359375, 1.3264974355697632]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.2370469570159912, 1.1677517890930176, 1.0539008378982544, 1.0787217617034912, 1.2835015058517456, 0.7627885341644287, 0.9560173749923706, 1.0199991464614868, 1.7501220703125, 1.2030843496322632, 1.2608506679534912, 1.1599527597427368, 1.9154798984527588, 1.2109239101409912, 1.1352674961090088, 1.1169975996017456]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]Running loglikelihood requests:  38%|██████████████████▍                              | 189/504 [10:40<16:58,  3.23s/it]
Layer: gate_20 - Captured router_logits: [0.8786994218826294, 1.0756632089614868, 0.9677293300628662, 0.8985286355018616, 0.1008809432387352, 1.1136949062347412, 1.1353353261947632, 1.8481818437576294, 0.7437880039215088, 1.1343790292739868, 1.2981295585632324, 1.0686984062194824, 0.9747178554534912, 1.2883843183517456, 1.0913619995117188, 0.47630774974823]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0039875507354736, 2.210015296936035, 2.008002281188965, 1.9915907382965088, 1.9018282890319824, 2.073296546936035, 1.9214138984680176, 2.193413734436035, 1.9384765625, 2.0251736640930176, 2.3553059101104736, 2.8570964336395264, 1.8197021484375, 2.322265625, 2.159478187561035, 2.1169161796569824]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4300673007965088, 1.2921549081802368, 1.8267958164215088, 1.1754692792892456, 1.49169921875, 1.5297579765319824, 1.3500704765319824, 1.3604058027267456, 1.5495877265930176, 1.5572645664215088, 1.5059407949447632, 2.2529296875, 1.5050455331802368, 1.5514322519302368, 1.2466837167739868, 1.6129556894302368]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.467054605484009, 1.7274848222732544, 1.1986660957336426, 1.3664008378982544, 1.4185112714767456, 1.4723036289215088, 1.3451606035232544, 1.4257540702819824, 1.4128689765930176, 1.3622708320617676, 1.5002983808517456, 1.4169785976409912, 1.32474684715271, 0.9185837507247925, 1.3579440116882324, 1.473388671875]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3434245586395264, 2.3748371601104736, 2.320746421813965, 2.1511502265930176, 2.852484703063965, 2.358018636703491, 2.6623263359069824, 2.25146484375, 2.3106281757354736, 2.2917752265930176, 2.6824543476104736, 2.186903238296509, 2.306260824203491, 2.4140625, 2.3742947578430176, 2.4114582538604736]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6856011152267456, 1.8591036796569824, 1.5445692539215088, 1.7008734941482544, 1.5488823652267456, 1.5932345390319824, 1.6290689706802368, 1.9727647304534912, 1.6266547441482544, 1.55908203125, 1.6952853202819824, 1.4952799081802368, 1.5968695878982544, 1.5896267890930176, 1.8234591484069824, 2.0749239921569824]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9495035409927368, 1.712288737297058, 1.7093505859375, 1.7865939140319824, 1.6816474199295044, 1.7590230703353882, 1.6888478994369507, 1.7987534999847412, 1.750213623046875, 1.8634982109069824, 2.2092623710632324, 1.6261190176010132, 1.66259765625, 1.8699136972427368, 1.7816230058670044, 1.8329306840896606]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6492716073989868, 1.7136197090148926, 1.6206411123275757, 1.7957032918930054, 1.6536593437194824, 1.5955132246017456, 1.7739461660385132, 1.7476670742034912, 1.6195340156555176, 1.7670694589614868, 1.5040689706802368, 1.7311469316482544, 1.6620687246322632, 1.8850750923156738, 1.634765625, 1.6392042636871338]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.99853515625, 2.032118082046509, 2.5235934257507324, 2.0008137226104736, 2.1368408203125, 2.048597574234009, 2.628607749938965, 2.1139864921569824, 1.9737142324447632, 1.98583984375, 1.9173448085784912, 1.871337890625, 1.9413248300552368, 1.8426920175552368, 2.224066734313965, 2.0077311992645264]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.32508659362793, 5.671061038970947, 5.435546875, 5.442220211029053, 5.300238609313965, 5.128743648529053, 5.420681476593018, 5.726291179656982, 5.571451663970947, 5.358072757720947, 5.355143070220947, 5.23817253112793, 5.40418815612793, 5.473578453063965, 5.544921875, 5.431803226470947]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.02392578125, 3.8162434101104736, 3.77001953125, 3.6537814140319824, 3.8382160663604736, 3.8713107109069824, 3.6786296367645264, 3.7985568046569824, 3.8888888359069824, 3.8245441913604736, 3.774522542953491, 3.526646137237549, 3.8046603202819824, 3.858778238296509, 3.8938801288604736, 3.9233670234680176]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.968207359313965, 2.994520425796509, 2.8673503398895264, 2.7330729961395264, 2.866807699203491, 2.909288167953491, 2.8689236640930176, 2.765814781188965, 2.8205838203430176, 2.833116292953491, 3.1946613788604736, 2.30688738822937, 2.919867515563965, 2.808973550796509, 3.1330294609069824, 2.884874105453491]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.08771154284477234, 0.09248002618551254, 0.09372838586568832, -0.176966592669487, -0.13145995140075684, -0.07999123632907867, 0.12438853830099106, -0.1432490348815918, 0.07650770246982574, 0.08462411910295486, 0.08686500042676926, 0.08820846676826477, 0.09052515029907227, 0.101802296936512, -0.8532307744026184, 0.11124250292778015]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08659079670906067, 0.05001889169216156, 0.02627846971154213, 0.061818256974220276, 0.07197104394435883, 0.07014264166355133, 0.04499270021915436, 0.05849605053663254, -0.012299378402531147, 0.0665983110666275, -0.16138924658298492, 0.050330255180597305, 0.03240805119276047, -0.00968972872942686, 0.011649634689092636, 0.024487681686878204]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05999808758497238, 0.08742894232273102, 0.1025381088256836, 0.0552382729947567, 0.08889012783765793, 0.10966873168945312, 0.0862976685166359, -0.0536634661257267, 0.0755881741642952, 0.05875176936388016, 0.009966002777218819, 0.09104304760694504, -0.11248397827148438, 0.013396846130490303, -0.026300350204110146, 0.10176531225442886]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1339644342660904, 0.17776721715927124, 0.15160740911960602, 0.1356726735830307, 0.12466022372245789, 0.12853877246379852, 0.04393937811255455, 0.187112495303154, 0.17018000781536102, -0.4301687777042389, 0.027521662414073944, 0.1394519805908203, 0.13104714453220367, -0.15943019092082977, -0.07198047637939453, -0.02902812510728836]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.09849187731742859, 0.08194070309400558, 0.11507055163383484, 0.04012690484523773, 0.10341951251029968, -0.051421087235212326, 0.0440959669649601, 0.07293552905321121, -0.09563329815864563, 0.011495377868413925, -0.11253605782985687, 0.15448273718357086, -0.07185596972703934, -0.18281783163547516, 0.1346314698457718, -0.08919376879930496]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.02231767401099205, 0.1305137276649475, 0.06877199560403824, 0.06664594262838364, -0.09027586877346039, -0.028432104736566544, 0.11404005438089371, -0.007317529991269112, 0.04023830220103264, -0.08976104855537415, 0.008157942444086075, 0.09425004571676254, -0.10000631213188171, 0.14778964221477509, 0.13517093658447266, 0.06592734903097153]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.16629451513290405, 0.0406680628657341, 0.1347995400428772, 0.3572252094745636, 0.15673235058784485, 0.139169380068779, -0.14362743496894836, -0.11152288317680359, 0.23539479076862335, 0.2446245402097702, -0.3891351521015167, 0.3713917136192322, 0.1846550852060318, -0.18814171850681305, 0.1101626306772232, 0.17513424158096313]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.2831147015094757, 0.25036418437957764, 0.26774194836616516, 0.12734095752239227, -0.6342570185661316, 0.34591928124427795, 0.27296340465545654, -0.34651756286621094, 0.28098487854003906, 0.1732514202594757, -0.08587095141410828, 0.24172909557819366, 0.25199657678604126, 0.15294773876667023, -0.34365251660346985, -0.1584082692861557]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.266509473323822, 0.2742934823036194, -0.29087841510772705, 0.35020679235458374, 0.48209360241889954, 0.4503275454044342, 0.25794538855552673, -0.1523309350013733, 0.20637914538383484, 0.5218260288238525, 0.17004817724227905, 0.192413330078125, 0.4874979555606842, -0.3018896281719208, -0.44872215390205383, 0.308881014585495]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.555720865726471, 0.2629373371601105, 0.40915340185165405, 0.69671630859375, 0.5835091471672058, 0.3872194290161133, 0.6209911704063416, 0.5976223349571228, 0.6665166020393372, 0.22002537548542023, 0.26358306407928467, 0.6268717646598816, 0.8479903936386108, 0.06317901611328125, 0.6330634355545044, 0.7821519374847412]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.01424241065979, 0.8681403398513794, 0.685878336429596, 0.8206380009651184, 0.9048487544059753, 0.5354933142662048, 0.7438609004020691, 0.7153862714767456, 0.22625732421875, 0.3978407084941864, 0.7523193359375, 0.8608059287071228, 0.5182215571403503, 0.3255818784236908, 0.985347330570221, 0.4247334897518158]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.8929850459098816, 1.0655059814453125, 0.9557698369026184, 1.3096482753753662, 1.4043036699295044, 0.6058383584022522, 0.9095594882965088, 1.0013529062271118, 0.9797431230545044, 1.2046304941177368, 0.94842529296875, 1.0933159589767456, 0.5076954960823059, 0.6411327719688416, 0.297505259513855, 0.7819145917892456]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.2735103964805603, 0.8954840898513794, 1.2504068613052368, 1.1099989414215088, 0.5302098393440247, 0.8205786943435669, 1.0417208671569824, 1.1479593515396118, 0.7355796098709106, 0.642805814743042, 0.9516673684120178, 0.9541965126991272, 0.8118269443511963, 0.8846096396446228, 0.8631523847579956, 1.1788465976715088]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.9742160439491272, 0.9452582597732544, 1.4947679042816162, 0.7174797058105469, 1.1997884511947632, 0.21831682324409485, 1.2245551347732544, 1.88427734375, 1.0363633632659912, 1.177001953125, 1.4002617597579956, 1.4100205898284912, 0.6117032170295715, 0.9999393820762634, 1.3145277500152588, 1.05846107006073]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.9756283164024353, 0.9637451171875, 1.0247396230697632, 0.941333532333374, 1.2384711503982544, 1.1640667915344238, 0.5823500156402588, 1.3355441093444824, 0.8456454873085022, 1.0125257968902588, -0.15157529711723328, 1.0419074296951294, 1.297473430633545, 1.2711317539215088, 1.2475314140319824, 0.7785809636116028]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6266657710075378, 1.25994873046875, 1.1785482168197632, 1.0067274570465088, 0.8310021162033081, 1.05517578125, 1.5004068613052368, 1.0146348476409912, 1.0501708984375, 1.1777886152267456, 0.6971986293792725, 0.14023631811141968, 1.259717345237732, 1.0576578378677368, 1.0143228769302368, 1.5193939208984375]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8299560546875, 0.18183135986328125, 1.0499267578125, 0.7941352128982544, 0.6367321014404297, 0.5468088984489441, 0.39402902126312256, 0.9021470546722412, -0.5376917719841003, 2.3237338066101074, -0.8057920932769775, 0.7390098571777344, 0.9981147050857544, 1.0608656406402588, 0.9301418662071228, 0.17605505883693695]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.22426265478134155, 1.097769856452942, 1.5504828691482544, 1.4392852783203125, 1.4911702871322632, 1.5955946445465088, 1.4750162363052368, 1.6029459238052368, 1.7190483808517456, 1.491930603981018, 2.111924886703491, 1.5491129159927368, 1.3836464881896973, 1.1576368808746338, 0.7914611101150513, 1.5650296211242676]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5986328125, 1.360595703125, 1.5865750312805176, 1.7475857734680176, 1.2075601816177368, 1.4729105234146118, 1.5735269784927368, 1.7804903984069824, 2.289198160171509, 1.53515625, 1.4220174551010132, 1.051297903060913, 1.2872077226638794, 1.1809073686599731, 1.590087890625, 1.5585123300552368]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.6338297128677368, 1.5665689706802368, 1.5257636308670044, 1.5554741621017456, 1.8187391757965088, 1.357452392578125, 1.5632730722427368, 1.6276719570159912, 1.9334784746170044, 1.5757242441177368, 1.7759331464767456, 1.6593289375305176, 2.2784016132354736, 1.7774794101715088, 1.7033419609069824, 1.6527777910232544]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0527420043945312, 1.1912367343902588, 1.0823160409927368, 0.9489627480506897, 0.24565379321575165, 1.3666585683822632, 1.2812771797180176, 1.7539266347885132, 0.9855639338493347, 1.3077120780944824, 1.3988443613052368, 1.1615532636642456, 1.2591959238052368, 1.4856228828430176, 1.1269124746322632, 0.7981041669845581]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.24267578125, 2.370008707046509, 2.202582359313965, 2.063530921936035, 2.146294593811035, 2.2270236015319824, 2.0722384452819824, 2.252387046813965, 2.1272244453430176, 2.3449435234069824, 2.4345703125, 2.4509005546569824, 1.9650336503982544, 2.558159828186035, 2.373155355453491, 2.29248046875]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.6104872226715088, 1.4856228828430176, 1.966552734375, 1.2914903163909912, 1.6961805820465088, 1.7549912929534912, 1.6352810859680176, 1.5617947578430176, 1.6264106035232544, 1.8400336503982544, 1.6162109375, 2.039008140563965, 1.65087890625, 1.7394747734069824, 1.5243055820465088, 1.8985731601715088]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5011394023895264, 1.8558213710784912, 1.3510589599609375, 1.5458441972732544, 1.6679959297180176, 1.5896538496017456, 1.5808919668197632, 1.5393880605697632, 1.68115234375, 1.5330404043197632, 1.6223958730697632, 1.5110541582107544, 1.5242241621017456, 1.0414551496505737, 1.5415987968444824, 1.7550455331802368]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  38%|██████████████████▊                              | 193/504 [10:54<16:55,  3.26s/it]Layer: gate_24 - Captured router_logits: [2.2111003398895264, 2.331000328063965, 2.158148765563965, 2.1897244453430176, 2.486219644546509, 2.3777127265930176, 2.50244140625, 2.1969943046569824, 2.0945096015930176, 2.2316622734069824, 2.5244140625, 2.2861328125, 2.26904296875, 2.374457359313965, 2.2932400703430176, 2.347276449203491]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7132161855697632, 1.7196723222732544, 1.5324435234069824, 1.8129340410232544, 1.4412163496017456, 1.5960829257965088, 1.6617025136947632, 1.9058973789215088, 1.6863064765930176, 1.6252713203430176, 1.6415201425552368, 1.6826443672180176, 1.7239041328430176, 1.4919975996017456, 1.6141493320465088, 1.8656684160232544]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.7647840976715088, 1.6876226663589478, 1.7155219316482544, 1.764892578125, 1.6563856601715088, 1.7816766500473022, 1.7222459316253662, 1.6682264804840088, 1.7632819414138794, 1.8989936113357544, 2.1939289569854736, 1.6587320566177368, 1.8256564140319824, 1.8892550468444824, 1.7122523784637451, 1.7333695888519287]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5820261240005493, 1.6057603359222412, 1.6916741132736206, 1.6935153007507324, 1.5648599863052368, 1.6058316230773926, 1.7107747793197632, 1.6839735507965088, 1.5932278633117676, 1.7209811210632324, 1.429443359375, 1.5873734951019287, 1.6055094003677368, 1.7568697929382324, 1.5872056484222412, 1.5959879159927368]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.040418863296509, 2.07666015625, 2.4214680194854736, 2.0176053047180176, 2.099961996078491, 2.069525718688965, 2.285454750061035, 2.139756917953491, 2.0131564140319824, 2.078667640686035, 2.016791343688965, 1.8988443613052368, 1.9968465566635132, 1.9128146171569824, 2.2922635078430176, 2.1308321952819824]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.29139518737793, 5.483669757843018, 5.170844078063965, 5.09324836730957, 5.126058101654053, 5.20176887512207, 5.266058921813965, 5.444661617279053, 5.497884273529053, 5.33349609375, 5.220187664031982, 5.05333137512207, 5.362372398376465, 5.315158367156982, 5.379123210906982, 5.196017742156982]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.773451089859009, 3.639051675796509, 3.6985812187194824, 3.462510824203491, 3.615424156188965, 3.66796875, 3.4643056392669678, 3.6070828437805176, 3.7914631366729736, 3.688720703125, 3.617784261703491, 3.2994587421417236, 3.6458003520965576, 3.630126953125, 3.729600667953491, 3.7522311210632324]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.980360269546509, 3.000623941421509, 2.880425453186035, 2.718370199203491, 2.82568359375, 3.0183918476104736, 2.831597328186035, 2.728949546813965, 2.8825955390930176, 2.8201496601104736, 2.989854574203491, 2.287461996078491, 2.9108073711395264, 2.8987629413604736, 3.2034504413604736, 3.0364582538604736]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09318510442972183, 0.11251574754714966, 0.10961396247148514, -0.23713481426239014, -0.212498277425766, -0.14342188835144043, 0.12747341394424438, -0.17900800704956055, 0.0790226012468338, 0.09851773083209991, 0.10777069628238678, 0.08868958055973053, 0.0948035791516304, 0.10418706387281418, -1.0786781311035156, 0.12184921652078629]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09470554441213608, 0.06975008547306061, 0.04459695890545845, 0.04385911673307419, 0.08397296071052551, 0.030026189982891083, 0.05725225806236267, 0.06559576839208603, 0.014913385733962059, 0.07041052728891373, -0.1758689135313034, 0.057345304638147354, 0.0027008589822798967, -0.03353646397590637, 0.013972942717373371, 0.05106508359313011]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.10032280534505844, 0.05379491299390793, 0.09174605458974838, 0.06196951866149902, 0.10082165151834488, 0.1166023313999176, 0.07679983228445053, -0.07865505665540695, 0.0811968743801117, 0.09472677856683731, 0.0017523865681141615, 0.08938566595315933, -0.15582916140556335, 0.02954460307955742, -0.02857917919754982, 0.1144275814294815]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1376943588256836, 0.18209603428840637, 0.11826756596565247, 0.1405593305826187, 0.14630970358848572, 0.12091131508350372, 0.054771583527326584, 0.18457916378974915, 0.18055009841918945, -0.3753175437450409, -0.02460399642586708, 0.11450440436601639, 0.14890272915363312, -0.17851631343364716, -0.041976820677518845, -0.0557340607047081]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07841267436742783, 0.10450541973114014, 0.10088785737752914, 0.10999922454357147, 0.039187852293252945, -0.03274797648191452, 0.0489303357899189, 0.0538572296500206, -0.054284367710351944, 0.028464363887906075, -0.131681427359581, 0.1613556146621704, -0.0542118102312088, -0.18457143008708954, 0.12612402439117432, -0.012756774201989174]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06781822443008423, 0.12554088234901428, 0.06138504296541214, 0.09233776479959488, -0.01388827245682478, 0.004075697157531977, 0.12733712792396545, 0.051663272082805634, 0.09698662161827087, -0.07122845202684402, -0.08230911195278168, 0.08777815848588943, -0.13874158263206482, 0.1243913546204567, 0.13024261593818665, 0.0582912415266037]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.08363281190395355, 0.03510057553648949, 0.1610572636127472, 0.35365477204322815, 0.18634438514709473, 0.12382389605045319, -0.0778544470667839, -0.03533380851149559, 0.46792489290237427, 0.1572427749633789, -0.4726921021938324, 0.3359539210796356, 0.2017156481742859, -0.09940610080957413, 0.12727035582065582, 0.17191725969314575]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.31289225816726685, 0.24720706045627594, 0.32373046875, 0.12285079061985016, -0.8690595030784607, 0.3226802945137024, 0.19454827904701233, -0.13323931396007538, 0.3525125980377197, 0.19315364956855774, -0.027126071974635124, 0.296348512172699, 0.374057799577713, 0.11316870152950287, -0.3545510768890381, -0.039775073528289795]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21136526763439178, 0.34482842683792114, -0.0943162813782692, 0.3363822400569916, 0.5043262243270874, 0.2856680154800415, 0.2873047888278961, -0.18976940214633942, 0.13970616459846497, 0.5988466739654541, -0.015017849393188953, 0.45845553278923035, 0.3295719027519226, -0.32698431611061096, -0.21055293083190918, 0.2826215922832489]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.462350070476532, 0.20463423430919647, 0.5290975570678711, 0.7199211716651917, 0.4464777112007141, 0.4372757077217102, 0.6790763139724731, 0.5591503381729126, 0.48815855383872986, 0.18525108695030212, 0.5474802255630493, 0.7156427502632141, 0.9436826109886169, 0.5850198268890381, 0.7440313696861267, 0.7256883978843689]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9810594916343689, 0.8400213122367859, 0.6653497815132141, 0.8681367635726929, 1.066180944442749, 0.6978111267089844, 0.6652900576591492, 0.6621981263160706, 0.3742765486240387, 0.9326598644256592, 0.744215726852417, 0.9142477512359619, 0.6379923820495605, 0.2666967511177063, 0.6487941145896912, 0.47806063294410706]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0524543523788452, 1.0895466804504395, 0.9761117696762085, 1.3235417604446411, 1.4130927324295044, 0.7970158457756042, 0.9602887630462646, 1.3838095664978027, 1.0450037717819214, 1.3046875, 0.9972956776618958, 1.1560109853744507, 0.7975741028785706, 0.5392823219299316, -0.013367125764489174, 0.878831148147583]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.39479759335517883, 0.9968449473381042, 1.2140446901321411, 1.2578943967819214, 0.5180719494819641, 0.8142431378364563, 1.0344033241271973, 1.228621482849121, 1.002105712890625, 0.7363771796226501, 0.6875272989273071, 0.8960199952125549, 1.1597192287445068, 0.8569916486740112, 0.6727713346481323, 1.0155566930770874]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7570096254348755, 0.8814885020256042, 0.8184093236923218, 0.6028655767440796, 1.1940150260925293, 0.24568016827106476, 1.1119563579559326, 2.14294695854187, 0.9806941151618958, 0.9277138710021973, 1.2100019454956055, 1.2610085010528564, -0.10498046875, 0.9942908883094788, 1.12411630153656, 0.7751535177230835]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6856454610824585, 0.7888951897621155, 0.8922162055969238, 0.8192539811134338, 0.888337254524231, 0.7750476002693176, 0.5539800524711609, 1.077149748802185, 0.696608304977417, 1.0065388679504395, 0.190120667219162, 0.7244847416877747, 1.0539038181304932, 1.0174484252929688, 1.0708861351013184, 0.6768346428871155]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.38329780101776123, 1.1526305675506592, 0.9769790768623352, 0.7982544898986816, 0.613450288772583, 0.8358896970748901, 1.0820069313049316, 1.0084407329559326, 0.858473539352417, 0.9291958212852478, 0.7181388139724731, -0.3988165259361267, 0.7617357969284058, 0.8171847462654114, 1.073727011680603, 1.3995702266693115]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7983500957489014, 0.2898576557636261, 0.8528754115104675, 0.7440159916877747, 0.6536818146705627, 0.5156383514404297, 0.39954060316085815, 0.905135989189148, -0.7720102071762085, 2.5445120334625244, -0.6695300340652466, 0.7617806196212769, 0.8990657925605774, 0.9605209231376648, 0.8557897210121155, 0.690681517124176]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.13076889514923096, 0.5786867141723633, 1.334476113319397, 1.2958984375, 1.3113117218017578, 1.3704107999801636, 1.4300836324691772, 1.3697962760925293, 1.632279872894287, 1.551788568496704, 1.9584790468215942, 1.3164745569229126, 1.3488504886627197, 1.1190587282180786, 0.7253656983375549, 1.4021490812301636]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3727737665176392, 1.3182978630065918, 1.4227627515792847, 1.604157567024231, 1.0231096744537354, 1.279556393623352, 1.813701868057251, 1.6859155893325806, 1.938251256942749, 1.362147569656372, 1.1635401248931885, 1.0413780212402344, 1.2758549451828003, 1.479388952255249, 1.4931299686431885, 1.439685344696045]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.2821036577224731, 1.225278615951538, 1.1111847162246704, 1.1765460968017578, 1.392837643623352, 0.9275414347648621, 0.9908729195594788, 1.1884628534317017, 1.7996169328689575, 1.3645514249801636, 1.3797531127929688, 1.3349746465682983, 1.9771088361740112, 1.3243553638458252, 1.2249780893325806, 1.2677556276321411]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8351000547409058, 1.068892002105713, 1.0634150505065918, 0.8918678760528564, 0.141508087515831, 1.172694444656372, 1.1981943845748901, 1.9086573123931885, 0.8337965607643127, 1.1745383739471436, 1.266444444656372, 1.0708929300308228, 1.0762401819229126, 1.4625219106674194, 1.1859651803970337, 0.5008015632629395]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0516281127929688, 2.1709463596343994, 2.0228912830352783, 1.9833643436431885, 1.8984375, 2.0182747840881348, 1.9329655170440674, 2.1706185340881348, 1.949328064918518, 2.040783405303955, 2.3368115425109863, 2.7736012935638428, 1.8738117218017578, 2.399803400039673, 2.2321624755859375, 2.1050589084625244]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.425398826599121, 1.280840277671814, 1.7347301244735718, 1.1054584980010986, 1.4629316329956055, 1.524748682975769, 1.330829381942749, 1.3500328063964844, 1.4559931755065918, 1.6035566329956055, 1.4776278734207153, 2.234757423400879, 1.5212794542312622, 1.5342274904251099, 1.2438265085220337, 1.6246448755264282]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5018575191497803, 1.7514750957489014, 1.3041616678237915, 1.437732219696045, 1.464584231376648, 1.5132348537445068, 1.4025212526321411, 1.4784746170043945, 1.4932528734207153, 1.4022618532180786, 1.567225694656372, 1.4618662595748901, 1.4208916425704956, 0.9935345649719238, 1.5215083360671997, 1.511336326599121]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3081839084625244, 2.3651933670043945, 2.212822437286377, 2.2196240425109863, 2.856151580810547, 2.3500874042510986, 2.733336925506592, 2.255845785140991, 2.287724018096924, 2.3322224617004395, 2.5794363021850586, 2.28469181060791, 2.2471864223480225, 2.4281578063964844, 2.3959243297576904, 2.416466236114502]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6113691329956055, 1.8139750957489014, 1.4474431276321411, 1.623688817024231, 1.5071022510528564, 1.4853993654251099, 1.544689655303955, 1.8445968627929688, 1.5370820760726929, 1.5176737308502197, 1.5334489345550537, 1.417477011680603, 1.507320761680603, 1.4177775382995605, 1.5794634819030762, 1.9424989223480225]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9655267000198364, 1.716049075126648, 1.7382675409317017, 1.8323590755462646, 1.7546916007995605, 1.7813985347747803, 1.6980834007263184, 1.8036350011825562, 1.814618706703186, 1.9074143171310425, 2.1610782146453857, 1.6710118055343628, 1.7117979526519775, 1.9073289632797241, 1.8296138048171997, 1.797107219696045]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6226781606674194, 1.6676478385925293, 1.6348066329956055, 1.7726060152053833, 1.6703555583953857, 1.632962703704834, 1.7456464767456055, 1.7524516582489014, 1.6460916996002197, 1.7898205518722534, 1.5326430797576904, 1.7549716234207153, 1.6540237665176392, 1.8116225004196167, 1.623046875, 1.6440088748931885]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  39%|███████████████████▏                             | 197/504 [11:06<16:36,  3.25s/it]Layer: gate_28 - Captured router_logits: [2.1247406005859375, 2.206293821334839, 2.608637571334839, 2.1163747310638428, 2.1933867931365967, 2.1345608234405518, 2.604393243789673, 2.189521312713623, 2.070793867111206, 2.107783794403076, 2.02234148979187, 1.9617160558700562, 2.048971652984619, 1.9368853569030762, 2.407332420349121, 2.190126657485962]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.60910177230835, 5.922749042510986, 5.660238265991211, 5.61300802230835, 5.518383979797363, 5.431927680969238, 5.611833572387695, 6.032069683074951, 5.800262451171875, 5.593531608581543, 5.627458572387695, 5.513658046722412, 5.680807590484619, 5.6867899894714355, 5.792285919189453, 5.614455699920654]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.185123443603516, 4.001584529876709, 3.9921329021453857, 3.855801582336426, 4.034664630889893, 4.0064191818237305, 3.8329856395721436, 4.0349650382995605, 4.0285868644714355, 4.099923610687256, 3.8969078063964844, 3.7359046936035156, 4.065935134887695, 3.9923512935638428, 4.100906848907471, 4.147966384887695]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1667394638061523, 3.2655704021453857, 3.093312978744507, 2.893465995788574, 3.029884099960327, 3.2189137935638428, 3.050863265991211, 3.065723419189453, 3.0762128829956055, 3.1305179595947266, 3.260216236114502, 2.6561834812164307, 3.1599650382995605, 3.105168342590332, 3.30031681060791, 3.166630268096924]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.06858745962381363, 0.08695391565561295, 0.09551441669464111, -0.2886548936367035, -0.26651790738105774, -0.09038802236318588, 0.09851372987031937, -0.04940817132592201, 0.06107050180435181, 0.06969916075468063, 0.07319798320531845, 0.03741436451673508, 0.07235506922006607, 0.10639435797929764, -1.0373278856277466, 0.10826553404331207]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.07891920208930969, 0.03420661762356758, 0.02374054118990898, 0.04473579674959183, 0.06581655889749527, 0.022815557196736336, 0.05227138102054596, 0.0978870540857315, 0.06115925312042236, 0.06408867239952087, -0.19697986543178558, 0.05555050075054169, 0.012195774354040623, -0.025748860090970993, 0.039633262902498245, 0.038450002670288086]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09308096021413803, 0.02890382707118988, 0.07280518114566803, 0.06848286092281342, 0.14173750579357147, 0.08683232963085175, 0.08189501613378525, -0.10228659957647324, 0.0870961844921112, 0.1376619040966034, 0.035165321081876755, 0.07463093847036362, -0.2310672104358673, 0.01703120954334736, -0.016939282417297363, 0.10476124286651611]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13113078474998474, 0.14265206456184387, 0.12698698043823242, 0.14123626053333282, 0.12809187173843384, 0.10954423248767853, -0.003894192399457097, 0.16753920912742615, 0.1907222718000412, -0.5477581024169922, 0.024998271837830544, 0.08101861923933029, 0.29172199964523315, -0.3172069489955902, 0.09411962330341339, -0.07232751697301865]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.055843379348516464, 0.09109032899141312, 0.061864253133535385, 0.27795612812042236, 0.06272066384553909, -0.045712344348430634, 0.037279196083545685, 0.01780409924685955, -0.10780579596757889, 0.0005279754404909909, -0.278572142124176, 0.13119037449359894, 0.1182393953204155, -0.24819588661193848, 0.13895848393440247, -0.11855737864971161]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.0543191023170948, 0.14369462430477142, 0.06825309991836548, 0.2554517686367035, -0.14990469813346863, -0.128871351480484, 0.10283911973237991, 0.0018950775265693665, 0.2278928905725479, -0.0982164517045021, -0.15415795147418976, 0.11520586162805557, -0.24856844544410706, 0.19995886087417603, 0.11940076947212219, 0.07943528145551682]
Layer: gate_5 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.10828186571598053, 0.015960372984409332, 0.17703375220298767, 0.27723202109336853, 0.18726609647274017, 0.15886640548706055, -0.2029106765985489, -0.07176795601844788, 0.37381941080093384, 0.17234791815280914, -0.5742377638816833, 0.2876468598842621, 0.26574963331222534, -0.29895105957984924, 0.33173930644989014, 0.22551433742046356]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5717442631721497, 0.22960267961025238, 0.2617185413837433, 0.14636720716953278, -0.8585384488105774, 0.27871468663215637, 0.198917418718338, -0.4001029431819916, 0.6681017875671387, 0.18784166872501373, -0.1630786806344986, 0.25502708554267883, 0.1328330934047699, 0.28734710812568665, -0.6246619820594788, -0.32101866602897644]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.27513155341148376, 0.27984321117401123, -0.20553578436374664, 0.2835180163383484, 0.4393745958805084, 0.4092714488506317, 0.27008774876594543, -0.3462635278701782, 0.17576096951961517, 0.9400105476379395, 0.11596444994211197, 0.27584710717201233, 0.5085022449493408, -0.4719223380088806, -0.4777558743953705, 0.47940725088119507]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.48972538113594055, 0.2163306325674057, 0.4891109764575958, 1.1747585535049438, 0.47432512044906616, 0.2383754700422287, 0.6863535642623901, 0.6181470155715942, 1.082579255104065, 0.2295890748500824, 0.25722408294677734, 0.7154310345649719, 0.6249898076057434, 0.037522535771131516, 0.8133536577224731, 0.6721823215484619]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0625956058502197, 0.9405936002731323, 0.5583513379096985, 0.714584231376648, 0.8908674120903015, 0.8286540508270264, 0.7324321269989014, 0.7586525082588196, 0.13613273203372955, 0.5589949488639832, 0.8070640563964844, 1.070158839225769, 0.3236835300922394, 0.1977790892124176, 1.1948131322860718, 0.554124116897583]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [1.012463092803955, 1.4166200160980225, 0.9027398228645325, 1.3682562112808228, 1.4177006483078003, 0.31102511286735535, 1.0461920499801636, 0.9925835728645325, 1.0579655170440674, 1.7213518619537354, 0.9226261973381042, 1.0514847040176392, 0.35569795966148376, 0.5620253682136536, 0.12357399612665176, 0.839890718460083]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.2291620373725891, 0.9100879430770874, 1.3779501914978027, 1.3113527297973633, 0.5062535405158997, 0.5662782192230225, 1.3064485788345337, 1.5619946718215942, 0.49212026596069336, 0.17208713293075562, 0.9151824712753296, 0.8718585968017578, 0.9379916787147522, 0.9025622606277466, 0.7986915707588196, 1.0032097101211548]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.9426764845848083, 1.1481369733810425, 1.2198289632797241, 0.6103827357292175, 1.213641881942749, -0.12660910189151764, 1.239701747894287, 2.2911932468414307, 1.3551682233810425, 1.0199273824691772, 1.7736355066299438, 1.3342711925506592, 0.34246954321861267, 0.9600189924240112, 1.1454805135726929, 0.8664320111274719]
Running loglikelihood requests:  40%|███████████████████▌                             | 201/504 [11:19<16:25,  3.25s/it]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8929776549339294, 0.8245943784713745, 1.1542422771453857, 0.9415291547775269, 1.0989128351211548, 1.591733694076538, 0.514325737953186, 1.8601628541946411, 0.8038918972015381, 0.8529351353645325, -0.29656746983528137, 0.9341673851013184, 0.9776867628097534, 1.129090666770935, 1.190279483795166, 0.61081862449646]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.5304509997367859, 1.2645869255065918, 1.0620629787445068, 1.0291261672973633, 0.8852590322494507, 0.9607053399085999, 1.909533143043518, 1.1486287117004395, 1.1273765563964844, 1.0508631467819214, 0.8207143545150757, -0.28779035806655884, 1.1839796304702759, 0.9614018797874451, 1.042217493057251, 1.7137118577957153]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6232009530067444, 0.1046716645359993, 0.7238394021987915, 0.7646211385726929, 0.7232760190963745, 0.5054213404655457, 0.7500973343849182, 0.7796792984008789, -0.8461214303970337, 2.234306812286377, -0.7500429749488831, 0.6237888932228088, 0.8158189654350281, 0.8306986093521118, 0.8002690672874451, 0.5075840353965759]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.26927345991134644, 0.8622466325759888, 1.5476057529449463, 1.4404501914978027, 1.5611205101013184, 1.516826868057251, 1.610348105430603, 1.496762990951538, 2.0889227390289307, 1.4537242650985718, 2.641512870788574, 1.8385684490203857, 1.5838433504104614, 1.4267646074295044, 0.6535713076591492, 1.372180461883545]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6364729404449463, 1.4382648468017578, 1.6219063997268677, 2.035374879837036, 1.102767825126648, 1.3492406606674194, 2.3582823276519775, 1.9048842191696167, 2.7421329021453857, 1.609866738319397, 1.3701820373535156, 1.0618183612823486, 1.3761234283447266, 1.0167160034179688, 1.6564958095550537, 1.6881829500198364]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.6630654335021973, 1.4924060106277466, 1.928144097328186, 1.4979922771453857, 1.8317034244537354, 1.2416155338287354, 1.5264559984207153, 1.4720416069030762, 2.3949272632598877, 1.5857189893722534, 1.907151460647583, 1.6730222702026367, 1.8432856798171997, 1.7304277420043945, 1.7203617095947266, 1.7382949590682983]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.4163178205490112, 1.0790605545043945, 1.0277398824691772, 0.9915285110473633, -0.016680950298905373, 1.4654310941696167, 1.240466594696045, 1.9621436595916748, 0.8213891386985779, 1.396375060081482, 1.2617324590682983, 1.2072737216949463, 1.1842902898788452, 1.3133741617202759, 0.9796731472015381, 0.4549230933189392]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.2316980361938477, 2.40187931060791, 2.3416738510131836, 2.1319384574890137, 2.117624521255493, 2.25437068939209, 2.0976016521453857, 2.393793821334839, 2.1232516765594482, 2.286877155303955, 2.4999454021453857, 2.8380136489868164, 1.8309112787246704, 2.5907998085021973, 2.3309385776519775, 2.2585227489471436]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.5389806032180786, 1.5255681276321411, 2.2509286403656006, 1.1728242635726929, 1.5624727010726929, 1.7203344106674194, 1.406509518623352, 1.5293651819229126, 1.6245629787445068, 1.7250055074691772, 1.748688817024231, 2.2731096744537354, 1.6201103925704956, 1.740876317024231, 1.3642646074295044, 1.7819875478744507]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.570995330810547, 1.9831457138061523, 1.2730687856674194, 1.6169689893722534, 1.8557692766189575, 1.5811843872070312, 1.551190972328186, 1.5149967670440674, 1.616668462753296, 1.634915828704834, 1.6348339319229126, 1.659404993057251, 1.346454381942749, 0.7685956358909607, 1.464024305343628, 1.5928212404251099]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.6260926723480225, 2.5790536403656006, 2.532014846801758, 2.2492897510528564, 2.7424607276916504, 2.3810641765594482, 2.5922749042510986, 2.3170344829559326, 2.232135057449341, 2.3240275382995605, 3.075557231903076, 2.2909746170043945, 2.4819164276123047, 2.608173131942749, 2.4398491382598877, 2.3877294063568115]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.8167613744735718, 1.7828344106674194, 1.6763821840286255, 1.9294143915176392, 1.3987926244735718, 1.7503005266189575, 1.8358008861541748, 2.1312828063964844, 1.8091400861740112, 1.6947661638259888, 1.7239947319030762, 1.6298350095748901, 1.7538243532180786, 1.5902807712554932, 2.3072824478149414, 2.094569444656372]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_26 - Captured router_logits: [1.854676604270935, 2.0700204372406006, 1.8264586925506592, 1.8357735872268677, 1.8181135654449463, 2.0078892707824707, 1.841095209121704, 1.8593477010726929, 1.917499303817749, 1.949328064918518, 2.623429298400879, 1.7817007303237915, 1.8518630266189575, 1.9182692766189575, 1.987774133682251, 1.9107376337051392]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.663802981376648, 1.7612526416778564, 1.6763241291046143, 1.726456642150879, 1.5967274904251099, 1.6439589262008667, 1.8995404243469238, 1.766082525253296, 1.6181777715682983, 1.7265352010726929, 1.3294907808303833, 1.6660395860671997, 1.7577954530715942, 2.156243085861206, 1.6578923463821411, 1.6815587282180786]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.12546443939209, 2.2307145595550537, 2.435833692550659, 2.053922653198242, 2.217985153198242, 2.143793821334839, 2.7463395595550537, 2.233091115951538, 2.1169416904449463, 2.0716238021850586, 2.064521312713623, 1.8771306276321411, 1.9621530771255493, 1.9084899425506592, 2.4030539989471436, 2.0527753829956055]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.041521072387695, 5.622377395629883, 5.017373085021973, 4.833984375, 4.987215995788574, 4.931162357330322, 4.973229885101318, 5.174415588378906, 5.2170562744140625, 5.023000240325928, 4.952633380889893, 4.936079502105713, 4.988936901092529, 5.190614223480225, 5.218312740325928, 5.001283645629883]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.5497705936431885, 3.407533884048462, 3.350524425506592, 3.2096126079559326, 3.4771361351013184, 3.4512128829956055, 3.1244332790374756, 3.5720880031585693, 3.6580255031585693, 3.435533285140991, 3.2532780170440674, 2.865772247314453, 3.195025682449341, 3.473776340484619, 3.490002155303955, 3.5015432834625244]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.6776113510131836, 2.8050153255462646, 2.8023383617401123, 2.716346263885498, 2.6601288318634033, 2.7260162830352783, 2.6958587169647217, 2.3729512691497803, 2.7040536403656006, 2.437417984008789, 3.2565560340881348, 1.8487762212753296, 2.708096504211426, 2.6006884574890137, 3.2361233234405518, 2.700939655303955]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09698183089494705, 0.11581031233072281, 0.11004114151000977, -0.231603741645813, -0.21454408764839172, -0.1361341029405594, 0.12786021828651428, -0.19814695417881012, 0.07945666462182999, 0.10044877231121063, 0.10664822161197662, 0.09116607159376144, 0.09753066301345825, 0.10884391516447067, -1.0978847742080688, 0.12454505264759064]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0974930003285408, 0.06991313397884369, 0.043665215373039246, 0.045278724282979965, 0.08760233223438263, 0.032203592360019684, 0.0528053380548954, 0.06409162282943726, 0.010531404986977577, 0.07380744069814682, -0.16860604286193848, 0.0606948621571064, 0.0032149511389434338, -0.025296650826931, 0.012202093377709389, 0.04680979996919632]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09648554027080536, 0.06014197692275047, 0.09232665598392487, 0.05990876629948616, 0.09951987862586975, 0.11601895838975906, 0.06291858851909637, -0.07955694198608398, 0.08091963082551956, 0.08761540055274963, 0.00464220903813839, 0.08499486744403839, -0.14835351705551147, 0.040035221725702286, -0.016778716817498207, 0.11633452028036118]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.1405353993177414, 0.16965889930725098, 0.12106820940971375, 0.13712354004383087, 0.14294151961803436, 0.11678075790405273, 0.07511956244707108, 0.18405042588710785, 0.18050995469093323, -0.375956654548645, -0.01735476218163967, 0.11104205250740051, 0.13903051614761353, -0.1545695811510086, -0.04479028284549713, -0.05472342669963837]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0841226801276207, 0.10113785415887833, 0.09801937639713287, 0.09840858727693558, 0.050700653344392776, -0.03791489824652672, 0.05839208513498306, 0.053134024143218994, -0.041160691529512405, 0.024593597277998924, -0.12955670058727264, 0.14811009168624878, -0.06144708767533302, -0.17681430280208588, 0.12243609130382538, 0.009851766750216484]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07265704870223999, 0.13114485144615173, 0.07299306988716125, 0.09356740862131119, -0.011943844147026539, 0.01179769542068243, 0.13650469481945038, 0.060622356832027435, 0.09932497888803482, -0.08089473843574524, -0.14063285291194916, 0.10316678136587143, -0.14722393453121185, 0.13095623254776, 0.14050552248954773, 0.06285528093576431]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.07499067485332489, 0.05479755625128746, 0.17686279118061066, 0.3450503647327423, 0.18734416365623474, 0.11454234272241592, -0.058880530297756195, -0.03426171839237213, 0.47609591484069824, 0.15564830601215363, -0.47594261169433594, 0.35108017921447754, 0.1880982369184494, -0.08781487494707108, 0.12348158657550812, 0.175115704536438]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.29911738634109497, 0.25526657700538635, 0.33444279432296753, 0.10293763130903244, -0.8821476101875305, 0.33622807264328003, 0.18498462438583374, -0.09136659651994705, 0.35494673252105713, 0.20230859518051147, -0.010273358784615993, 0.28593066334724426, 0.36312222480773926, 0.12227809429168701, -0.3367379903793335, -0.0353800468146801]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.17698512971401215, 0.3336312472820282, -0.0558227114379406, 0.3255331814289093, 0.5277610421180725, 0.29100435972213745, 0.30031871795654297, -0.16234788298606873, 0.14403928816318512, 0.5963048338890076, -0.016221066936850548, 0.47413063049316406, 0.3422202169895172, -0.31667181849479675, -0.16663376986980438, 0.2789492905139923]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4753260016441345, 0.20292934775352478, 0.501319408416748, 0.7253782749176025, 0.4612695276737213, 0.46995165944099426, 0.7189820408821106, 0.5778825879096985, 0.5159773826599121, 0.19432121515274048, 0.560592770576477, 0.7342018485069275, 0.9847961664199829, 0.5950278639793396, 0.7576255202293396, 0.7551944851875305]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.981224536895752, 0.8528022766113281, 0.674946665763855, 0.8850564956665039, 1.1031337976455688, 0.717505931854248, 0.6720775961875916, 0.668183445930481, 0.3893696665763855, 0.928015947341919, 0.745955228805542, 0.9221763014793396, 0.6578810811042786, 0.26844993233680725, 0.679897665977478, 0.4917447865009308]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.051972508430481, 1.1115740537643433, 0.9837101101875305, 1.3593922853469849, 1.4571974277496338, 0.8304391503334045, 0.9882189035415649, 1.4217053651809692, 1.071543574333191, 1.309088945388794, 1.029379963874817, 1.1774365901947021, 0.8284868597984314, 0.5583794713020325, 0.018336519598960876, 0.9038258790969849]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.416584849357605, 1.017093300819397, 1.2481715679168701, 1.2680352926254272, 0.5363137722015381, 0.8327584862709045, 1.0347216129302979, 1.2307285070419312, 1.044938325881958, 0.7690819501876831, 0.7015631794929504, 0.893409252166748, 1.1850066184997559, 0.8741342425346375, 0.6880795955657959, 1.0363613367080688]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.791857123374939, 0.867850661277771, 0.8409410715103149, 0.6399523019790649, 1.207578420639038, 0.36264535784721375, 1.118295669555664, 2.16519832611084, 0.9745262861251831, 0.9483045339584351, 1.230191707611084, 1.3203125, -0.06017287075519562, 1.050431489944458, 1.1171433925628662, 0.831091046333313]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7172098159790039, 0.7940448522567749, 0.8966575264930725, 0.8342562317848206, 0.8967891335487366, 0.7838225960731506, 0.593888521194458, 1.0836268663406372, 0.7208347320556641, 1.046990990638733, 0.273486852645874, 0.76631760597229, 1.1095775365829468, 1.0445894002914429, 1.0991661548614502, 0.7291481494903564]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4306577742099762, 1.1864334344863892, 1.0294908285140991, 0.8172720074653625, 0.6484912037849426, 0.8548662066459656, 1.0850872993469238, 1.0347683429718018, 0.8959441781044006, 0.9592475891113281, 0.7528465986251831, -0.35670676827430725, 0.778761088848114, 0.8506482839584351, 1.111314296722412, 1.485326886177063]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8159283399581909, 0.3344873785972595, 0.864495038986206, 0.7203429937362671, 0.6419773101806641, 0.5504297614097595, 0.41977187991142273, 0.9300147294998169, -0.7040186524391174, 2.618461847305298, -0.5787397027015686, 0.8072769641876221, 0.9197002649307251, 0.9417387247085571, 0.8577335476875305, 0.733325719833374]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.19578303396701813, 0.6331406235694885, 1.362159252166748, 1.340751051902771, 1.3232214450836182, 1.3851535320281982, 1.451310396194458, 1.411208987236023, 1.6323484182357788, 1.5796105861663818, 1.9673093557357788, 1.3400342464447021, 1.3733150959014893, 1.119971752166748, 0.8138207197189331, 1.4694876670837402]
Running loglikelihood requests:  41%|███████████████████▉                             | 205/504 [11:32<16:07,  3.24s/it]Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3970800638198853, 1.3776873350143433, 1.4390029907226562, 1.6419824361801147, 1.0767675638198853, 1.307831883430481, 1.8338596820831299, 1.7420213222503662, 1.9676141738891602, 1.403257966041565, 1.2293778657913208, 1.129957675933838, 1.342634916305542, 1.5606131553649902, 1.5369292497634888, 1.4756205081939697]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3455229997634888, 1.2734375, 1.1396691799163818, 1.223625898361206, 1.452072262763977, 1.001023292541504, 1.055518627166748, 1.2455673217773438, 1.8691960573196411, 1.4269170761108398, 1.418744444847107, 1.4026762247085571, 2.0426363945007324, 1.3776041269302368, 1.2697666883468628, 1.3176805973052979]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8340844511985779, 1.084954023361206, 1.0960217714309692, 0.9256877303123474, 0.20819568634033203, 1.187638521194458, 1.244971752166748, 1.950697422027588, 0.8430712819099426, 1.1977781057357788, 1.2987865209579468, 1.1009184122085571, 1.1007590293884277, 1.4845689535140991, 1.214249849319458, 0.5285477042198181]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0483155250549316, 2.179992198944092, 2.0296154022216797, 2.00390625, 1.919160008430481, 2.0275931358337402, 1.9454233646392822, 2.178413152694702, 1.954593300819397, 2.059175491333008, 2.351839542388916, 2.769835948944092, 1.914616584777832, 2.4280805587768555, 2.2540171146392822, 2.1156914234161377]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.461574673652649, 1.2945201396942139, 1.757452368736267, 1.1232408285140991, 1.4973126649856567, 1.5487034320831299, 1.3751939535140991, 1.387688398361206, 1.4769504070281982, 1.6343916654586792, 1.4884752035140991, 2.26058292388916, 1.554604411125183, 1.5586491823196411, 1.273866891860962, 1.6560560464859009]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5037953853607178, 1.769946813583374, 1.3382785320281982, 1.472157597541809, 1.481369137763977, 1.5467088222503662, 1.4317375421524048, 1.519171118736267, 1.5187348127365112, 1.420060396194458, 1.595633864402771, 1.4846866130828857, 1.4751496315002441, 1.0575041770935059, 1.565356969833374, 1.545960783958435]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3279032707214355, 2.3888518810272217, 2.2097461223602295, 2.2415225505828857, 2.890625, 2.401928186416626, 2.750554084777832, 2.29227614402771, 2.3226397037506104, 2.3749167919158936, 2.5660459995269775, 2.3261303901672363, 2.2802250385284424, 2.4483044147491455, 2.425919771194458, 2.4435393810272217]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6251939535140991, 1.814605474472046, 1.443650245666504, 1.6245428323745728, 1.5320534706115723, 1.4912316799163818, 1.5449773073196411, 1.8221687078475952, 1.5361120700836182, 1.515985131263733, 1.5327599048614502, 1.4186890125274658, 1.508588194847107, 1.4197971820831299, 1.5456006526947021, 1.9496066570281982]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9875470399856567, 1.7073308229446411, 1.7537261247634888, 1.8451628684997559, 1.7580358982086182, 1.78243088722229, 1.7074226140975952, 1.8113676309585571, 1.8200994729995728, 1.9254816770553589, 2.1532649993896484, 1.6807186603546143, 1.7250525951385498, 1.9022468328475952, 1.8350509405136108, 1.7959918975830078]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6254571676254272, 1.6707113981246948, 1.6435374021530151, 1.7804725170135498, 1.682343602180481, 1.6411514282226562, 1.739815354347229, 1.764634609222412, 1.6549098491668701, 1.7934188842773438, 1.5645500421524048, 1.7646760940551758, 1.6572058200836182, 1.805937647819519, 1.630997896194458, 1.6518138647079468]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.143824815750122, 2.230884313583374, 2.624861478805542, 2.148700714111328, 2.2136178016662598, 2.1620123386383057, 2.624086618423462, 2.2157163619995117, 2.102754592895508, 2.1449191570281982, 2.0490567684173584, 2.0017454624176025, 2.0739831924438477, 1.9675449132919312, 2.449159860610962, 2.2203569412231445]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.648492813110352, 5.925642967224121, 5.697916507720947, 5.632091999053955, 5.5323028564453125, 5.462987422943115, 5.628546237945557, 6.060394287109375, 5.810061931610107, 5.622673034667969, 5.665503025054932, 5.542664051055908, 5.702958583831787, 5.704122543334961, 5.813054084777832, 5.639960289001465]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.167885780334473, 3.980441093444824, 3.98323917388916, 3.812344551086426, 4.007646083831787, 3.988308906555176, 3.8068692684173584, 3.9997782707214355, 3.996924877166748, 4.086020469665527, 3.879210948944092, 3.700268030166626, 4.0413618087768555, 3.978501796722412, 4.0875444412231445, 4.117990970611572]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.243461847305298, 3.3279032707214355, 3.1520943641662598, 2.9565048217773438, 3.097074508666992, 3.298426389694214, 3.1045544147491455, 3.12777042388916, 3.143561601638794, 3.191987991333008, 3.2883975505828857, 2.684741735458374, 3.218916177749634, 3.17691707611084, 3.3378214836120605, 3.234929084777832]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.08800511807203293, 0.10891766846179962, 0.10217425972223282, -0.22919221222400665, -0.208116352558136, -0.14011329412460327, 0.11839199811220169, -0.1679258942604065, 0.075672447681427, 0.09230101108551025, 0.09614454954862595, 0.08484054356813431, 0.09052850306034088, 0.1028265431523323, -1.0676805973052979, 0.11760197579860687]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09410831332206726, 0.06293027848005295, 0.04057009145617485, 0.04297279566526413, 0.08432482928037643, 0.030529266223311424, 0.05267975106835365, 0.06706859916448593, 0.00720440736040473, 0.06548222154378891, -0.1686086505651474, 0.06050889194011688, 0.004852836020290852, -0.0274713933467865, 0.012355899438261986, 0.04948744550347328]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.10207967460155487, 0.058623671531677246, 0.09348537772893906, 0.06473655253648758, 0.10171465575695038, 0.10578424483537674, 0.07147606462240219, -0.07607339322566986, 0.07627663016319275, 0.08725927770137787, -0.00017061977996490896, 0.09046013653278351, -0.1499408632516861, 0.027916062623262405, -0.020581834018230438, 0.11386454850435257]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.139715313911438, 0.18185365200042725, 0.11979350447654724, 0.14736343920230865, 0.14384688436985016, 0.12757883965969086, 0.062365077435970306, 0.19127163290977478, 0.17750181257724762, -0.372456431388855, -0.027547389268875122, 0.11570620536804199, 0.16260024905204773, -0.17150554060935974, -0.046930406242609024, -0.061060480773448944]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08293060213327408, 0.10104900598526001, 0.09485025703907013, 0.1140727624297142, 0.045063965022563934, -0.031115539371967316, 0.04511537030339241, 0.052605487406253815, -0.04121561348438263, 0.027796318754553795, -0.14177140593528748, 0.15854975581169128, -0.04790567234158516, -0.18581001460552216, 0.12695010006427765, -0.02767965942621231]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.0702647790312767, 0.12349533289670944, 0.06168998405337334, 0.09661973267793655, -0.04223935678601265, 0.006676531862467527, 0.13052259385585785, 0.04599983990192413, 0.10964662581682205, -0.07467997819185257, -0.10983654856681824, 0.094146728515625, -0.14806826412677765, 0.13225343823432922, 0.1387370228767395, 0.06475906074047089]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.0790782943367958, 0.03225838020443916, 0.15192818641662598, 0.33538082242012024, 0.16638962924480438, 0.122417151927948, -0.07572341710329056, -0.04020896553993225, 0.4986655116081238, 0.1521260142326355, -0.4805707037448883, 0.3553813099861145, 0.18841855227947235, -0.10565575212240219, 0.14310243725776672, 0.17808911204338074]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.327772319316864, 0.2488960325717926, 0.308577299118042, 0.10616903007030487, -0.8738260269165039, 0.3388831913471222, 0.18395650386810303, -0.11966180801391602, 0.3728707432746887, 0.20103271305561066, -0.01808718405663967, 0.2811257541179657, 0.36192938685417175, 0.120892733335495, -0.34870705008506775, -0.05305556580424309]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.18894758820533752, 0.3095131814479828, -0.06733839213848114, 0.33159881830215454, 0.5176023840904236, 0.2897074818611145, 0.2894980311393738, -0.15005676448345184, 0.13453100621700287, 0.6237528920173645, -0.007955781184136868, 0.4760153591632843, 0.3293500244617462, -0.3044572174549103, -0.1858406811952591, 0.3055841028690338]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.45083552598953247, 0.21047692000865936, 0.4967023730278015, 0.7604443430900574, 0.44544798135757446, 0.4245994985103607, 0.686840295791626, 0.559220552444458, 0.5094316601753235, 0.18406309187412262, 0.5681420564651489, 0.7208693623542786, 0.9857653975486755, 0.585224986076355, 0.7749612331390381, 0.7366882562637329]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.982768177986145, 0.8624986410140991, 0.6671215891838074, 0.8846548199653625, 1.0644115209579468, 0.7057968974113464, 0.6690457463264465, 0.6697902679443359, 0.3897847831249237, 0.933024525642395, 0.749653697013855, 0.9339954853057861, 0.6680089831352234, 0.25349587202072144, 0.6522239446640015, 0.481010764837265]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0410572290420532, 1.1160882711410522, 0.9742076396942139, 1.3233425617218018, 1.414284110069275, 0.7599383592605591, 0.9199010729789734, 1.394669771194458, 1.0303514003753662, 1.332512617111206, 0.9865121245384216, 1.1406042575836182, 0.7569525837898254, 0.5153611898422241, -0.03031456097960472, 0.8825008273124695]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.4063149392604828, 0.9733107686042786, 1.2189923524856567, 1.2623282670974731, 0.48259827494621277, 0.8008730411529541, 1.0394598245620728, 1.227040410041809, 0.9737629890441895, 0.711132287979126, 0.6910842061042786, 0.8787261843681335, 1.1413284540176392, 0.8839830160140991, 0.6642148494720459, 1.0071614980697632]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7603422403335571, 0.9010459780693054, 0.8191842436790466, 0.6101091504096985, 1.2163397073745728, 0.2833191454410553, 1.1336159706115723, 2.260721445083618, 1.000824213027954, 0.9595522880554199, 1.221326470375061, 1.3042442798614502, -0.13768838346004486, 1.006384015083313, 1.1125531196594238, 0.7971338629722595]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6398441195487976, 0.75313401222229, 0.8764232993125916, 0.7797607779502869, 0.8385277986526489, 0.6495519280433655, 0.47902554273605347, 1.0442695617675781, 0.7007632255554199, 0.9700486063957214, 0.1618202179670334, 0.6764201521873474, 1.0161045789718628, 0.9797276854515076, 1.0535378456115723, 0.6308091878890991]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.3906293213367462, 1.1890513896942139, 0.98661208152771, 0.8409726619720459, 0.6151183843612671, 0.8347808718681335, 1.077728509902954, 0.9947362542152405, 0.8833804130554199, 0.9433870911598206, 0.7010229825973511, -0.4268067181110382, 0.7694205641746521, 0.8152427077293396, 1.0789422988891602, 1.4188319444656372]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.787610650062561, 0.29850974678993225, 0.8535173535346985, 0.7572653293609619, 0.7177128195762634, 0.52430260181427, 0.4010312855243683, 0.9218801856040955, -0.8377105593681335, 2.595412254333496, -0.6807664632797241, 0.740676760673523, 0.9194508790969849, 0.983259916305542, 0.8526360392570496, 0.6832331418991089]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.14164571464061737, 0.6041324734687805, 1.3831102848052979, 1.3191558122634888, 1.3540973663330078, 1.401498794555664, 1.4593722820281982, 1.4169714450836182, 1.708014726638794, 1.5964373350143433, 2.0074663162231445, 1.341021180152893, 1.3891419172286987, 1.1727477312088013, 0.7305561900138855, 1.4667692184448242]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3925365209579468, 1.3486673831939697, 1.4314947128295898, 1.6307346820831299, 1.0072098970413208, 1.2988697290420532, 1.845633864402771, 1.7090259790420532, 1.9959136247634888, 1.373808741569519, 1.163079023361206, 1.0307079553604126, 1.277558445930481, 1.4575797319412231, 1.5078401565551758, 1.456477165222168]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.301335334777832, 1.2668439149856567, 1.1355136632919312, 1.189453125, 1.425227165222168, 0.9328163266181946, 1.0305850505828857, 1.2042677402496338, 1.8241218328475952, 1.4059175252914429, 1.3993240594863892, 1.352532148361206, 2.0226340293884277, 1.3603723049163818, 1.248116135597229, 1.2816932201385498]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8540082573890686, 1.072556495666504, 1.0496799945831299, 0.9038640260696411, 0.09638717025518417, 1.1584680080413818, 1.1998697519302368, 1.9207148551940918, 0.8579984903335571, 1.1789394617080688, 1.2702792882919312, 1.084406852722168, 1.0745234489440918, 1.4467531442642212, 1.1420187950134277, 0.4947120249271393]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0878214836120605, 2.1963374614715576, 2.0567097663879395, 2.015625, 1.9177193641662598, 2.033189296722412, 1.9482214450836182, 2.19575572013855, 1.9552028179168701, 2.0667665004730225, 2.3651928901672363, 2.8291778564453125, 1.8936446905136108, 2.4153923988342285, 2.2418272495269775, 2.1176862716674805]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Running loglikelihood requests:  41%|████████████████████▎                            | 209/504 [11:45<15:51,  3.22s/it]Layer: gate_22 - Captured router_logits: [1.437084436416626, 1.282856822013855, 1.7854887247085571, 1.0766428709030151, 1.473708987236023, 1.5408354997634888, 1.3444702625274658, 1.3732270002365112, 1.470495343208313, 1.623254656791687, 1.508311152458191, 2.2845189571380615, 1.534048080444336, 1.550504207611084, 1.2498338222503662, 1.6451407670974731]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.539090156555176, 1.7755707502365112, 1.2637073993682861, 1.454094648361206, 1.4594275951385498, 1.5123004913330078, 1.394184947013855, 1.472240686416626, 1.479145884513855, 1.4156277179718018, 1.559785008430481, 1.481175184249878, 1.4116383790969849, 0.9654272794723511, 1.5105794668197632, 1.514987826347351]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3397605419158936, 2.3890180587768555, 2.2241246700286865, 2.2108821868896484, 2.8694591522216797, 2.364250898361206, 2.737034559249878, 2.277925491333008, 2.2955451011657715, 2.3446364402770996, 2.589871406555176, 2.3026373386383057, 2.260749101638794, 2.4378323554992676, 2.403590440750122, 2.42508864402771]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.647800326347351, 1.835161805152893, 1.4878933429718018, 1.6567624807357788, 1.518561601638794, 1.519448161125183, 1.5677776336669922, 1.9076628684997559, 1.55594801902771, 1.5423592329025269, 1.564321517944336, 1.4492741823196411, 1.5479000806808472, 1.456117033958435, 1.6373836994171143, 1.9855386018753052]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9637079238891602, 1.7153164148330688, 1.7519046068191528, 1.82686448097229, 1.75211763381958, 1.7814888954162598, 1.7029224634170532, 1.8106437921524048, 1.8162122964859009, 1.908486247062683, 2.194154977798462, 1.6704188585281372, 1.7047734260559082, 1.9189798831939697, 1.845973253250122, 1.7905490398406982]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6145555973052979, 1.6841685771942139, 1.628920078277588, 1.773728370666504, 1.662245273590088, 1.6331241130828857, 1.7388041019439697, 1.750353217124939, 1.638675332069397, 1.7793176174163818, 1.5358211994171143, 1.756635069847107, 1.645376205444336, 1.8500785827636719, 1.6154906749725342, 1.646009922027588]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0871427059173584, 2.1712169647216797, 2.5914576053619385, 2.0891876220703125, 2.1772356033325195, 2.1078097820281982, 2.6004786491394043, 2.1600730419158936, 2.035680055618286, 2.07694935798645, 1.9940402507781982, 1.939605474472046, 2.0132977962493896, 1.9097545146942139, 2.405857801437378, 2.159940004348755]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.581560134887695, 5.8655805587768555, 5.633089542388916, 5.562943458557129, 5.478058338165283, 5.367464542388916, 5.545323371887207, 5.951850414276123, 5.727837085723877, 5.559341907501221, 5.584108829498291, 5.478501796722412, 5.632535457611084, 5.658466339111328, 5.734042644500732, 5.570700168609619]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.175808906555176, 3.974900245666504, 3.987699508666992, 3.817185401916504, 4.015237331390381, 3.9905529022216797, 3.805366277694702, 4.011081695556641, 4.030003547668457, 4.076961517333984, 3.876939296722412, 3.702134609222412, 4.027731418609619, 3.9676694869995117, 4.082225322723389, 4.1299591064453125]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1539227962493896, 3.247617483139038, 3.0642175674438477, 2.8834218978881836, 3.0071475505828857, 3.1946475505828857, 3.02299427986145, 3.004460334777832, 3.0540781021118164, 3.097379207611084, 3.254487991333008, 2.592839241027832, 3.1296541690826416, 3.07856822013855, 3.300476551055908, 3.1415669918060303]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.11490452289581299, 0.1392076164484024, 0.1311405748128891, -0.193341925740242, -0.15087111294269562, -0.15144726634025574, 0.14539137482643127, -0.2093077301979065, 0.09973451495170593, 0.12261612713336945, 0.11660095304250717, 0.12145108729600906, 0.1039586141705513, 0.13022467494010925, -1.091935396194458, 0.1352362185716629]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09456274658441544, 0.07175727188587189, 0.03986109793186188, 0.06250225752592087, 0.11167377233505249, 0.06211458146572113, 0.0617765337228775, 0.05528659746050835, -0.008712295442819595, 0.07464264333248138, -0.13875217735767365, 0.05596477538347244, 0.018011566251516342, 0.0029545777942985296, 0.008252894505858421, 0.006246147211641073]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06598273664712906, 0.10111431032419205, 0.12516990303993225, 0.06546474993228912, 0.102749764919281, 0.1163787841796875, 0.11002150177955627, -0.06342743337154388, 0.08543092757463455, 0.02809694968163967, 0.024088867008686066, 0.09438805282115936, -0.16438975930213928, 0.07994312047958374, -0.02455739863216877, 0.1128518283367157]
Layer: gate_2 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.15592187643051147, 0.20341329276561737, 0.160946786403656, 0.13977159559726715, 0.13956165313720703, 0.17577411234378815, 0.146295428276062, 0.2252572774887085, 0.18910227715969086, -0.5270676016807556, -0.02905619703233242, 0.13035573065280914, 0.10879354178905487, -0.20114579796791077, -0.16485953330993652, -0.03894162178039551]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.11439696699380875, 0.10691206157207489, 0.12714877724647522, 0.020944366231560707, 0.024887843057513237, -0.08619825541973114, 0.10441210865974426, 0.11144553869962692, -0.0763794407248497, 0.02907654270529747, -0.07119253277778625, 0.18352633714675903, -0.18611763417720795, -0.14482863247394562, 0.11582913994789124, -0.027108483016490936]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.19860953092575073, 0.15519103407859802, 0.11431213468313217, 0.030842017382383347, -0.07395296543836594, 0.0777769684791565, 0.08265502005815506, 0.054492056369781494, 0.07156632095575333, -0.08317425101995468, -0.2464638501405716, 0.10574957728385925, -0.036902353167533875, 0.11770802736282349, 0.16273373365402222, 0.05566709116101265]
Layer: gate_5 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.19072437286376953, 0.21804052591323853, 0.06287448853254318, 0.36714422702789307, 0.1981547474861145, 0.19550512731075287, 0.03514564409852028, -0.012714792042970657, 0.15950991213321686, 0.24488744139671326, -0.2981511056423187, 0.2968502938747406, 0.21176818013191223, -0.028850527480244637, 0.03202246129512787, 0.18114173412322998]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.20613542199134827, 0.4587832987308502, 0.37318259477615356, 0.2561301290988922, -0.6727229952812195, 0.3247426450252533, 0.23761765658855438, -0.2406715750694275, 0.17269188165664673, -0.015401637181639671, -0.01494365744292736, 0.3109806180000305, 0.30172598361968994, 0.1623903065919876, -0.25221630930900574, -0.3595970571041107]
Layer: gate_7 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.48376983404159546, 0.4803122580051422, -0.15437565743923187, 0.4175739586353302, 0.46619951725006104, 0.4900854527950287, 0.3414081633090973, 0.049006685614585876, 0.2210443764925003, 0.2784653306007385, 0.2385479062795639, -0.2117941528558731, 0.4895002245903015, -0.038046978414058685, -0.016474513337016106, 0.1701761931180954]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.829002857208252, 0.6704577803611755, 0.557811975479126, 0.680799126625061, 0.7625115513801575, 0.4030540883541107, 1.128790259361267, 0.7085791826248169, 0.628576397895813, 0.1860942393541336, 0.5889416337013245, 0.8010236620903015, 0.791184663772583, 0.6312028765678406, 0.8917331695556641, 0.8699994683265686]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.069632887840271, 1.0163661241531372, 0.944751501083374, 1.0583997964859009, 1.1557998657226562, 0.3667236864566803, 0.8113658428192139, 0.8492699861526489, 1.0596718788146973, 0.75813889503479, 0.9069979786872864, 0.9772031307220459, 0.7023943066596985, 0.6425662040710449, 0.7926374077796936, 0.8309230804443359]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0837593078613281, 1.151462435722351, 1.078277349472046, 1.113779902458191, 1.3592780828475952, 0.7193343639373779, 1.1838291883468628, 1.5644947290420532, 1.1757605075836182, 1.259917974472046, 1.328111171722412, 1.475502848625183, 1.1377053260803223, 0.9634148478507996, 0.912594199180603, 1.042421579360962]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.6852187514305115, 1.4612007141113281, 1.3234984874725342, 1.306709885597229, 0.8487527370452881, 1.3604762554168701, 0.8590953350067139, 1.1341526508331299, 1.1962025165557861, 0.7722600698471069, 1.0853557586669922, 1.221880555152893, 1.2477490901947021, 1.122686743736267, 1.011019229888916, 1.1989140510559082]
Layer: gate_12 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.6425504684448242, 1.3871897459030151, 1.693989634513855, 1.3548870086669922, 1.6212599277496338, 1.111899495124817, 1.573373794555664, 1.934230923652649, 1.4191323518753052, 1.6243350505828857, 1.5771207809448242, 1.896969199180603, 1.2879630327224731, 1.70759916305542, 1.742464542388916, 1.787240982055664]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.4980988502502441, 1.2364528179168701, 1.265278697013855, 1.3502534627914429, 1.3306807279586792, 1.864693284034729, 1.1661748886108398, 1.0586985349655151, 1.2770216464996338, 1.182399034500122, 0.988227903842926, 1.476974606513977, 1.3696825504302979, 1.407219648361206, 1.4932540655136108, 1.367530345916748]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [1.4561620950698853, 1.316835641860962, 1.6256787776947021, 1.271498203277588, 1.4681318998336792, 1.3788508176803589, 1.334126353263855, 1.422429084777832, 1.1525030136108398, 1.3389571905136108, 1.306370496749878, 0.3852192759513855, 1.4538730382919312, 1.3785737752914429, 1.3853613138198853, 1.5254442691802979]
Layer: gate_15 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9708347320556641, 0.4818398654460907, 0.9834503531455994, 0.8664810657501221, 0.734761118888855, 0.7758442759513855, 0.04244670644402504, 1.1258587837219238, -0.163305401802063, 1.6437996625900269, 0.3547079861164093, 0.881466269493103, 1.2355386018753052, 0.866527795791626, 0.8818636536598206, 0.4956965744495392]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.443060278892517, 1.4129128456115723, 1.989804983139038, 2.3288469314575195, 2.1755595207214355, 2.0107767581939697, 2.14730167388916, 2.1911985874176025, 2.1529531478881836, 1.7675399780273438, 2.3450520038604736, 2.035142421722412, 2.017608404159546, 1.8999404907226562, 2.0768489837646484, 2.0589053630828857]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.985012173652649, 1.8931738138198853, 1.936447262763977, 2.1006760597229004, 1.5405724048614502, 2.14018177986145, 1.869847059249878, 2.32995343208313, 2.4676971435546875, 2.0453789234161377, 2.2419798374176025, 1.8460839986801147, 1.9683067798614502, 1.9040908813476562, 2.1697418689727783, 1.9808566570281982]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.3720080852508545, 2.182042360305786, 2.133920669555664, 2.2263131141662598, 2.2768173217773438, 2.067317008972168, 2.3108930587768555, 2.4836270809173584, 2.3294548988342285, 2.3648881912231445, 2.3138020038604736, 2.261552572250366, 2.5677638053894043, 2.3052968978881836, 2.3234708309173584, 2.2537124156951904]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2985303401947021, 1.53055739402771, 1.527426838874817, 1.7381045818328857, 0.5148683190345764, 1.5420960187911987, 1.5977116823196411, 2.0488903522491455, 1.290520429611206, 1.497908353805542, 1.548897385597229, 1.3809771537780762, 1.376191258430481, 1.6066323518753052, 1.547261118888855, 1.1372407674789429]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.633643627166748, 2.8012521266937256, 2.662123203277588, 2.706449508666992, 2.7522716522216797, 2.7097740173339844, 2.747340440750122, 2.6071033477783203, 2.6221742630004883, 2.8522274494171143, 2.860483169555664, 2.66683292388916, 2.4197418689727783, 2.9475841522216797, 2.9046430587768555, 2.755596160888672]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [2.009086847305298, 1.819536805152893, 2.1000664234161377, 1.578485131263733, 2.1623449325561523, 2.007978677749634, 2.05260968208313, 1.932956576347351, 2.0660459995269775, 2.1083221435546875, 1.8466311693191528, 2.0405585765838623, 2.026484966278076, 1.9677526950836182, 1.979914665222168, 2.2092199325561523]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.3050754070281982, 2.0185892581939697, 1.8396636247634888, 1.8767729997634888, 1.9629875421524048, 1.927748203277588, 1.8506205081939697, 1.9721298217773438, 1.9032303094863892, 1.8372949361801147, 2.0582334995269775, 1.8192874193191528, 2.01606822013855, 1.3203661441802979, 1.9891400337219238, 2.0524158477783203]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.7106051445007324, 2.794935703277588, 2.6206228733062744, 2.6623449325561523, 2.793938398361206, 2.871509313583374, 2.80330228805542, 2.8425865173339844, 2.739527940750122, 2.7395832538604736, 2.745789051055908, 2.7644059658050537, 2.6970856189727783, 2.7790892124176025, 2.797595262527466, 2.93888521194458]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_25 - Captured router_logits: [1.9885859489440918, 1.916999101638794, 1.955368995666504, 2.0187277793884277, 1.6129765510559082, 1.838042974472046, 1.9175808429718018, 2.001412868499756, 1.939937949180603, 1.914893627166748, 1.968833088874817, 1.979277491569519, 1.9330673217773438, 1.905474305152893, 1.6696864366531372, 2.127659559249878]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Running loglikelihood requests:  42%|████████████████████▋                            | 213/504 [11:58<15:46,  3.25s/it]Layer: gate_26 - Captured router_logits: [1.9140348434448242, 1.8259155750274658, 1.967780351638794, 1.9246176481246948, 2.004875898361206, 1.88456392288208, 1.8905748128890991, 1.813331127166748, 1.892311453819275, 1.936170220375061, 2.065561294555664, 1.8820090293884277, 1.940076470375061, 1.8874529600143433, 1.8554294109344482, 1.893580675125122]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.553823471069336, 1.53865385055542, 1.6438871622085571, 1.6025390625, 1.5982657670974731, 1.545379877090454, 1.653323769569397, 1.6009011268615723, 1.621166467666626, 1.6978404521942139, 1.3926750421524048, 1.6250605583190918, 1.542137622833252, 1.51408052444458, 1.6212149858474731, 1.585591197013855]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.2737977504730225, 2.2813053131103516, 2.4944591522216797, 2.367464542388916, 2.2959885597229004, 2.3020002841949463, 2.3129987716674805, 2.3880486488342285, 2.315298080444336, 2.419381618499756, 2.3418660163879395, 2.172595262527466, 2.3093485832214355, 2.2859597206115723, 2.3909852504730225, 2.309314012527466]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.482823371887207, 5.599733829498291, 5.332668304443359, 5.515957355499268, 5.550310134887695, 5.3685173988342285, 5.582834720611572, 5.500443458557129, 5.691101551055908, 5.557956695556641, 5.422595500946045, 5.3666887283325195, 5.391234397888184, 5.439661026000977, 5.544769287109375, 5.343805313110352]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.7654032707214355, 3.7782301902770996, 3.7017951011657715, 3.7415521144866943, 3.683012008666992, 3.6556127071380615, 3.5900654792785645, 3.704122304916382, 3.7768173217773438, 3.7301084995269775, 3.7278783321380615, 3.3501858711242676, 3.67345929145813, 3.7242770195007324, 3.75390625, 3.6684327125549316]
Layer: gate_30 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.0232436656951904, 2.899850368499756, 2.869847059249878, 2.5920324325561523, 2.8545544147491455, 2.9174423217773438, 2.771332025527954, 2.6661264896392822, 2.8641955852508545, 2.933898448944092, 2.6960604190826416, 2.1472322940826416, 2.8968584537506104, 2.9031472206115723, 2.8701517581939697, 3.0091145038604736]
Layer: gate_31 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09709799289703369, 0.11741801351308823, 0.11222032457590103, -0.2352578341960907, -0.20724956691265106, -0.13280721008777618, 0.12981736660003662, -0.2029767781496048, 0.07707176357507706, 0.1031423807144165, 0.10930994153022766, 0.09769199788570404, 0.09853139519691467, 0.1101427897810936, -1.089990258216858, 0.12584011256694794]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09720388054847717, 0.06780827045440674, 0.0439927764236927, 0.045972250401973724, 0.08703754842281342, 0.030433300882577896, 0.05799176171422005, 0.06282158195972443, 0.00855887308716774, 0.07609745860099792, -0.1715577244758606, 0.0660802572965622, 0.0028258732054382563, -0.02899453230202198, 0.011250890791416168, 0.04607369378209114]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09804071485996246, 0.054001644253730774, 0.09027952700853348, 0.05837405100464821, 0.09800894558429718, 0.11488298326730728, 0.0700911357998848, -0.07808925211429596, 0.08137740939855576, 0.09195572882890701, 0.0021399566903710365, 0.08602812886238098, -0.14793439209461212, 0.04235203564167023, -0.017469869926571846, 0.11441279947757721]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.14143937826156616, 0.1764678955078125, 0.11583862453699112, 0.13730789721012115, 0.1396569460630417, 0.1214260682463646, 0.06677769124507904, 0.18214426934719086, 0.18303483724594116, -0.3597133159637451, -0.01797964982688427, 0.11447056382894516, 0.13646697998046875, -0.17007534205913544, -0.03964364156126976, -0.04937133938074112]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08207964152097702, 0.10075151920318604, 0.1040092185139656, 0.09953787922859192, 0.04606473073363304, -0.03373478353023529, 0.05400319769978523, 0.06069919094443321, -0.04574933648109436, 0.01890476793050766, -0.12848463654518127, 0.15788933634757996, -0.06104850769042969, -0.18236394226551056, 0.12242366373538971, -0.0071775163523852825]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07522648572921753, 0.12793317437171936, 0.06063935533165932, 0.0905313789844513, -0.0039313179440796375, 0.006426783744245768, 0.1349748820066452, 0.06115351989865303, 0.08533099293708801, -0.07672178745269775, -0.10391148179769516, 0.09047960489988327, -0.13758043944835663, 0.1221989244222641, 0.12359259277582169, 0.05785427242517471]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.07872030884027481, 0.035288646817207336, 0.1742207109928131, 0.3520398736000061, 0.19104963541030884, 0.12006568908691406, -0.0714002326130867, -0.04119567945599556, 0.4535115957260132, 0.15544608235359192, -0.4885820746421814, 0.33907264471054077, 0.19179077446460724, -0.10082746297121048, 0.10208195447921753, 0.16003194451332092]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.28660279512405396, 0.25686487555503845, 0.3272629976272583, 0.11709479987621307, -0.8876804709434509, 0.3411964476108551, 0.19622410833835602, -0.11957135796546936, 0.3281501233577728, 0.19803816080093384, -0.02924216166138649, 0.293417364358902, 0.3607605993747711, 0.11628369241952896, -0.3672446012496948, -0.05653294175863266]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.19323596358299255, 0.3302178382873535, -0.09386269748210907, 0.32529252767562866, 0.5242449045181274, 0.27725720405578613, 0.29388296604156494, -0.1626107394695282, 0.13953900337219238, 0.5521488785743713, -0.01906302385032177, 0.4525641202926636, 0.33630284667015076, -0.33640310168266296, -0.1954583376646042, 0.25983691215515137]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.460839182138443, 0.18287789821624756, 0.5210126638412476, 0.6709063053131104, 0.44241201877593994, 0.44147056341171265, 0.6900233626365662, 0.5642586946487427, 0.4575391411781311, 0.16860787570476532, 0.5339656472206116, 0.7291852831840515, 0.973355770111084, 0.555316150188446, 0.750781238079071, 0.7496093511581421]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.990496814250946, 0.8503627181053162, 0.6748674511909485, 0.8815569281578064, 1.0897042751312256, 0.6725746989250183, 0.6709943413734436, 0.6581612825393677, 0.357147216796875, 0.9018162488937378, 0.7540248036384583, 0.9053937792778015, 0.6571546196937561, 0.2519862651824951, 0.6528871655464172, 0.4663633108139038]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0537667274475098, 1.0778521299362183, 0.9792689681053162, 1.3077985048294067, 1.4329380989074707, 0.8025094270706177, 0.9758021831512451, 1.3973075151443481, 1.0484932661056519, 1.2747697830200195, 1.011007308959961, 1.1513742208480835, 0.8118242621421814, 0.5325790643692017, -0.011831228621304035, 0.8804094791412354]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.4027884304523468, 1.001241683959961, 1.2341866493225098, 1.2580077648162842, 0.520128071308136, 0.8212123513221741, 1.013983130455017, 1.1984200477600098, 1.018466591835022, 0.7479461431503296, 0.686816394329071, 0.9127790331840515, 1.1707484722137451, 0.8788923025131226, 0.6988917589187622, 1.0312360525131226]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7591086030006409, 0.8519505262374878, 0.7909554839134216, 0.5954611897468567, 1.1812570095062256, 0.30987200140953064, 1.109249472618103, 2.127455472946167, 0.9538783431053162, 0.9110282063484192, 1.182352066040039, 1.2825474739074707, -0.10567278414964676, 1.010049819946289, 1.0994001626968384, 0.8004128336906433]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6845415234565735, 0.7879603505134583, 0.8793108463287354, 0.8041760921478271, 0.8812848925590515, 0.7403346300125122, 0.5409123301506042, 1.0157183408737183, 0.7192667126655579, 1.0045723915100098, 0.25309228897094727, 0.7487165331840515, 1.0556082725524902, 1.0231096744537354, 1.0730329751968384, 0.7076363563537598]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.40930676460266113, 1.1691964864730835, 1.0029296875, 0.8113141655921936, 0.6350516080856323, 0.8333635330200195, 1.0528424978256226, 1.017452597618103, 0.8651087880134583, 0.9443080425262451, 0.7305698990821838, -0.4045602083206177, 0.7470548152923584, 0.8268833756446838, 1.0919084548950195, 1.4326947927474976]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8148873448371887, 0.3039380609989166, 0.8610926866531372, 0.7269792556762695, 0.6573451161384583, 0.5486223697662354, 0.39757078886032104, 0.9411777853965759, -0.7267482280731201, 2.5372629165649414, -0.5819675922393799, 0.7812782526016235, 0.9256277680397034, 0.9423130750656128, 0.8548305034637451, 0.7144692540168762]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.22019566595554352, 0.6288644075393677, 1.3627859354019165, 1.3611468076705933, 1.342647910118103, 1.3877511024475098, 1.4584542512893677, 1.412332534790039, 1.6193568706512451, 1.5598493814468384, 1.960574746131897, 1.3522496223449707, 1.3733363151550293, 1.1410592794418335, 0.8448190093040466, 1.471316933631897]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.405664086341858, 1.3712611198425293, 1.4514142274856567, 1.6317801475524902, 1.0454938411712646, 1.293680191040039, 1.8203125, 1.7430803775787354, 1.9370675086975098, 1.3870257139205933, 1.2396937608718872, 1.128722906112671, 1.314355492591858, 1.5765398740768433, 1.524637222290039, 1.4604631662368774]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.327999472618103, 1.2641462087631226, 1.136104941368103, 1.2201590538024902, 1.439090371131897, 0.9860822558403015, 1.0351911783218384, 1.2444754838943481, 1.8382184505462646, 1.4106026887893677, 1.4133371114730835, 1.3945591449737549, 2.0186383724212646, 1.357784628868103, 1.263058066368103, 1.3087890148162842]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8414899706840515, 1.0912597179412842, 1.1064523458480835, 0.9304966330528259, 0.1963447630405426, 1.1944406032562256, 1.231319785118103, 1.9580985307693481, 0.8779698014259338, 1.189355492591858, 1.2966238260269165, 1.0874162912368774, 1.0944056510925293, 1.4772878885269165, 1.2308681011199951, 0.5409231781959534]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0499162673950195, 2.161579132080078, 2.0109095573425293, 1.9780133962631226, 1.899023413658142, 2.0130300521850586, 1.9433035850524902, 2.164564847946167, 1.9461774826049805, 2.047070264816284, 2.3327009677886963, 2.755078077316284, 1.8956193923950195, 2.4138951301574707, 2.2413783073425293, 2.099163055419922]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4590122699737549, 1.2909040451049805, 1.739480972290039, 1.1124389171600342, 1.501562476158142, 1.5448660850524902, 1.3772600889205933, 1.3776506185531616, 1.466573715209961, 1.6335099935531616, 1.4732979536056519, 2.2398996353149414, 1.5547432899475098, 1.5558593273162842, 1.276729941368103, 1.6681640148162842]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4722795486450195, 1.7520647048950195, 1.3384417295455933, 1.4686802625656128, 1.4926897287368774, 1.5310826301574707, 1.4232561588287354, 1.5145926475524902, 1.5232701301574707, 1.4228376150131226, 1.5824497938156128, 1.4874790906906128, 1.4721958637237549, 1.0394481420516968, 1.5673165321350098, 1.5598214864730835]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.303013324737549, 2.3697545528411865, 2.2059710025787354, 2.2340402603149414, 2.8736050128936768, 2.381808042526245, 2.7208147048950195, 2.2725446224212646, 2.314257860183716, 2.3538503646850586, 2.543191909790039, 2.3130581378936768, 2.252734422683716, 2.4344308376312256, 2.410435199737549, 2.4296317100524902]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.599832534790039, 1.799051284790039, 1.4257254600524902, 1.6003347635269165, 1.5010184049606323, 1.4665178060531616, 1.5278599262237549, 1.8063615560531616, 1.516336441040039, 1.4862723350524902, 1.5036271810531616, 1.3999162912368774, 1.4939732551574707, 1.390039086341858, 1.5229352712631226, 1.9204519987106323]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9653459787368774, 1.6959559917449951, 1.7362723350524902, 1.8221819400787354, 1.738671898841858, 1.764070749282837, 1.697079062461853, 1.789937973022461, 1.8022373914718628, 1.905880331993103, 2.125331401824951, 1.6642369031906128, 1.7080496549606323, 1.9022600650787354, 1.8222585916519165, 1.7763092517852783]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6047991514205933, 1.6509242057800293, 1.6219063997268677, 1.758760690689087, 1.6661934852600098, 1.6219203472137451, 1.7219308614730835, 1.7356480360031128, 1.6325091123580933, 1.7774832248687744, 1.5297712087631226, 1.7422363758087158, 1.632449746131897, 1.7864645719528198, 1.6159946918487549, 1.6265206336975098]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1242806911468506, 2.213574171066284, 2.6083006858825684, 2.1290876865386963, 2.2010881900787354, 2.135417938232422, 2.587489604949951, 2.1904401779174805, 2.084263324737549, 2.1266043186187744, 2.029101610183716, 1.9750279188156128, 2.061049222946167, 1.9418247938156128, 2.405322313308716, 2.1961355209350586]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.6209821701049805, 5.893638610839844, 5.6688055992126465, 5.612751007080078, 5.514983177185059, 5.44525671005249, 5.600614070892334, 6.029966354370117, 5.791964054107666, 5.604185104370117, 5.630134105682373, 5.514062404632568, 5.6865234375, 5.66796875, 5.778962135314941, 5.611495494842529]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  43%|█████████████████████                            | 217/504 [12:11<15:30,  3.24s/it]Layer: gate_30 - Captured router_logits: [4.212918758392334, 4.017131805419922, 4.023604869842529, 3.860142230987549, 4.031557083129883, 4.029143333435059, 3.8428432941436768, 4.035323619842529, 4.032672882080078, 4.1248884201049805, 3.916461944580078, 3.739048480987549, 4.086718559265137, 4.0052595138549805, 4.115206241607666, 4.169337749481201]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2284598350524902, 3.3215959072113037, 3.132310152053833, 2.924107074737549, 3.072991132736206, 3.2792410850524902, 3.0789620876312256, 3.1016461849212646, 3.1251673698425293, 3.1814732551574707, 3.2579798698425293, 2.7004430294036865, 3.1888949871063232, 3.159402847290039, 3.3151228427886963, 3.220982074737549]
Layer: gate_31 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09427966922521591, 0.11340820789337158, 0.1057378351688385, -0.22261624038219452, -0.18517759442329407, -0.1291385293006897, 0.1302286833524704, -0.20340734720230103, 0.06628552824258804, 0.09548730403184891, 0.10520441085100174, 0.09473438560962677, 0.09755358099937439, 0.10819227993488312, -1.0965356826782227, 0.11966705322265625]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0824979841709137, 0.06551144272089005, 0.03259337693452835, 0.04747646674513817, 0.07490430027246475, 0.026240643113851547, 0.0502806194126606, 0.05888758972287178, 0.0032041948288679123, 0.06763353943824768, -0.16483454406261444, 0.03683548420667648, -0.003187330439686775, -0.017735237255692482, -0.007159418426454067, 0.022088270634412766]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07025320827960968, 0.04984206706285477, 0.08925359696149826, 0.05538512393832207, 0.09989731758832932, 0.11438558995723724, 0.07978584617376328, -0.07622983306646347, 0.06864262372255325, 0.08887602388858795, -0.012834727764129639, 0.08929777890443802, -0.12812289595603943, 0.037625811994075775, -0.004764145240187645, 0.10941863805055618]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13722734153270721, 0.1637432724237442, 0.11021434515714645, 0.1640753448009491, 0.13373637199401855, 0.1096414253115654, 0.07489924877882004, 0.17763996124267578, 0.1729288399219513, -0.3488188683986664, -0.011944887228310108, 0.09547959268093109, 0.15208479762077332, -0.15060819685459137, -0.054855674505233765, -0.048531949520111084]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06351295113563538, 0.0747063159942627, 0.10193749517202377, 0.10442720353603363, 0.05032513290643692, -0.005600085482001305, 0.02235450968146324, 0.07961630076169968, -0.07329937815666199, -0.02163366973400116, -0.08975768834352493, 0.11101784557104111, -0.05897560343146324, -0.10800785571336746, 0.11193990707397461, -0.011429821141064167]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_5 - Captured router_logits: [0.030107786878943443, 0.15365590155124664, 0.05356372892856598, 0.12594780325889587, -0.05182993412017822, 0.011591849848628044, 0.10337058454751968, -0.005236467812210321, 0.09391339868307114, -0.0621819794178009, -0.1359415352344513, 0.09403596818447113, -0.11624002456665039, 0.11870388686656952, 0.09716796875, 0.08174185454845428]
Layer: gate_5 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08768831193447113, 0.003530378919094801, 0.2297947257757187, 0.3156152069568634, 0.18275946378707886, 0.12196975946426392, -0.04677746444940567, -0.07372673600912094, 0.49295997619628906, 0.15542814135551453, -0.41791412234306335, 0.2885216176509857, 0.19024285674095154, -0.17534741759300232, 0.11895214021205902, 0.14909055829048157]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.2980509102344513, 0.21032138168811798, 0.3045642077922821, 0.13271501660346985, -0.7689806222915649, 0.3382008373737335, 0.17648687958717346, -0.09170466661453247, 0.3501897156238556, 0.28939610719680786, -0.11552901566028595, 0.23857347667217255, 0.2953326404094696, 0.14481830596923828, -0.3874846398830414, -0.21015173196792603]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.24159833788871765, 0.3044580817222595, -0.20329515635967255, 0.2748895585536957, 0.5461583733558655, 0.24949108064174652, 0.2642005681991577, -0.22114618122577667, 0.1751241832971573, 0.7313728332519531, -0.01332750916481018, 0.26922959089279175, 0.3172484338283539, -0.36411070823669434, -0.35778477787971497, 0.27860429883003235]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3964790999889374, 0.22593437135219574, 0.4657582938671112, 0.8066564202308655, 0.40847471356391907, 0.34679386019706726, 0.6340279579162598, 0.48924726247787476, 0.5866515040397644, 0.25237664580345154, 0.36895620822906494, 0.6705682277679443, 0.8057281970977783, 0.6177100539207458, 0.6399628520011902, 0.6537137627601624]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9354195594787598, 0.7891678810119629, 0.5884210467338562, 0.7930530309677124, 1.0447778701782227, 0.7303879261016846, 0.6209514737129211, 0.6252880692481995, 0.3048938810825348, 0.9713039398193359, 0.6881884932518005, 0.8751580715179443, 0.4035969376564026, 0.2779894471168518, 0.7387993931770325, 0.41189420223236084]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9974356293678284, 1.0372165441513062, 0.9202871918678284, 1.3293298482894897, 1.3542251586914062, 0.7734792232513428, 0.9124325513839722, 1.3132587671279907, 1.048343300819397, 1.3359445333480835, 0.9072195291519165, 1.0455611944198608, 0.7149179577827454, 0.5946686267852783, 0.06505908817052841, 0.8486257791519165]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.3064483106136322, 0.9452738761901855, 1.2573137283325195, 1.135847568511963, 0.5508833527565002, 0.7335740923881531, 1.1209654808044434, 1.183003544807434, 0.8728532195091248, 0.5499241352081299, 0.7355109453201294, 0.8912572860717773, 1.155336618423462, 0.8087834715843201, 0.647253692150116, 0.980215847492218]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8181046843528748, 0.9079712629318237, 0.8846426606178284, 0.5525789260864258, 1.1843384504318237, 0.1060979813337326, 1.0785183906555176, 2.342822551727295, 0.929922878742218, 0.9329948425292969, 1.207094430923462, 1.2492834329605103, -0.028241712599992752, 1.1629453897476196, 1.0751882791519165, 0.8231921195983887]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7390576004981995, 0.8265863656997681, 0.9177650213241577, 0.8855770826339722, 0.929006040096283, 0.9075189828872681, 0.6243395805358887, 1.3001348972320557, 0.808786928653717, 1.013623595237732, 0.16228589415550232, 0.7870997190475464, 1.0706593990325928, 1.044402003288269, 1.0599777698516846, 0.6600877642631531]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.49163973331451416, 1.1846897602081299, 1.0310392379760742, 0.9301511645317078, 0.7052643299102783, 0.9269334673881531, 1.3259066343307495, 1.0678956508636475, 0.9333548545837402, 1.0080513954162598, 0.7687724828720093, -0.24572095274925232, 0.8991769552230835, 0.9189523458480835, 1.1659172773361206, 1.6120500564575195]
Running loglikelihood requests:  44%|█████████████████████▍                           | 221/504 [12:24<15:12,  3.23s/it]Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.763081967830658, 0.42423802614212036, 0.8624961376190186, 0.7999013066291809, 0.6400774121284485, 0.49762535095214844, 0.40861818194389343, 0.8737999200820923, -0.6692807674407959, 2.6480722427368164, -0.6744336485862732, 0.7532441020011902, 0.9045603275299072, 0.9724595546722412, 0.8374234437942505, 0.5745739936828613]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.0612463653087616, 0.7169408798217773, 1.3955429792404175, 1.2581284046173096, 1.4362354278564453, 1.402301549911499, 1.4710508584976196, 1.3989152908325195, 1.6997171640396118, 1.6287446022033691, 2.0143182277679443, 1.3906431198120117, 1.3806490898132324, 1.189070224761963, 0.8311082720756531, 1.3725067377090454]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3845548629760742, 1.358335256576538, 1.451978087425232, 1.588944435119629, 1.0404430627822876, 1.3395276069641113, 1.791057825088501, 1.6982351541519165, 1.9728388786315918, 1.363309383392334, 1.1682603359222412, 0.9879117608070374, 1.240189552307129, 1.254529356956482, 1.4973583221435547, 1.4559071063995361]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.2746880054473877, 1.1999634504318237, 1.1304378509521484, 1.215363621711731, 1.3962454795837402, 0.923842191696167, 1.0016369819641113, 1.1847881078720093, 1.810589075088501, 1.3859177827835083, 1.3533750772476196, 1.3421341180801392, 1.8561151027679443, 1.3218300342559814, 1.268126130104065, 1.297380805015564]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8259207010269165, 1.0817993879318237, 1.0012505054473877, 0.8793550133705139, 0.07366833835840225, 1.173413634300232, 1.142549991607666, 1.8782423734664917, 0.8230384588241577, 1.197771430015564, 1.2193682193756104, 1.1046468019485474, 1.070902705192566, 1.3636887073516846, 1.0307239294052124, 0.4201335310935974]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.923336386680603, 2.0885510444641113, 1.9563286304473877, 1.858784794807434, 1.8191883563995361, 1.9641131162643433, 1.7965236902236938, 2.0725607872009277, 1.8577169179916382, 1.9002922773361206, 2.2063286304473877, 2.594677448272705, 1.7023943662643433, 2.2824864387512207, 2.1205036640167236, 2.0271189212799072]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4423335790634155, 1.2791142463684082, 1.8400123119354248, 1.1360372304916382, 1.5026135444641113, 1.5494604110717773, 1.3372442722320557, 1.386212944984436, 1.5682328939437866, 1.5662657022476196, 1.555727243423462, 2.2455036640167236, 1.5294233560562134, 1.5858532190322876, 1.223176121711731, 1.654310941696167]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4262027740478516, 1.7980834245681763, 1.3035142421722412, 1.5373201370239258, 1.5869492292404175, 1.5215545892715454, 1.451157808303833, 1.4956722259521484, 1.5401866436004639, 1.4437668323516846, 1.5862747430801392, 1.5366456508636475, 1.416029691696167, 0.9304002523422241, 1.520430564880371, 1.5430530309677124]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.2767536640167236, 2.298842191696167, 2.184746026992798, 2.112410068511963, 2.8083407878875732, 2.3227856159210205, 2.6205596923828125, 2.189298629760742, 2.2017197608947754, 2.2267873287200928, 2.583858013153076, 2.1701889038085938, 2.213016986846924, 2.3626348972320557, 2.320481061935425, 2.329024314880371]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.5737409591674805, 1.751067876815796, 1.4234768152236938, 1.583436369895935, 1.3914048671722412, 1.4436544179916382, 1.5179575681686401, 1.801820993423462, 1.4893772602081299, 1.4691433906555176, 1.532612919807434, 1.3777259588241577, 1.4612466096878052, 1.3665130138397217, 1.5472683906555176, 1.8721054792404175]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.8845126628875732, 1.6672942638397217, 1.632292628288269, 1.7325764894485474, 1.6769615411758423, 1.7023521661758423, 1.6418668031692505, 1.7478783130645752, 1.7037166357040405, 1.8456188440322876, 2.0989279747009277, 1.601042628288269, 1.6481002569198608, 1.7977321147918701, 1.7828940153121948, 1.7361692190170288]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5729612112045288, 1.6483479738235474, 1.5847288370132446, 1.7220607995986938, 1.5880382061004639, 1.563371181488037, 1.688337802886963, 1.666166067123413, 1.5758750438690186, 1.690457820892334, 1.4300669431686401, 1.6804111003875732, 1.5885580778121948, 1.7476561069488525, 1.5496395826339722, 1.5788135528564453]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0051708221435547, 2.0856564044952393, 2.4391720294952393, 1.9689185619354248, 2.0862467288970947, 1.9992973804473877, 2.4684689044952393, 2.06640625, 1.9499213695526123, 1.962370753288269, 1.8905125856399536, 1.7936432361602783, 1.9367904663085938, 1.7977180480957031, 2.276191473007202, 2.0514276027679443]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.51000452041626, 5.79282808303833, 5.51343297958374, 5.52768087387085, 5.4008259773254395, 5.340602397918701, 5.613084316253662, 5.885678768157959, 5.67783260345459, 5.4771246910095215, 5.489152431488037, 5.4233927726745605, 5.518884658813477, 5.543615341186523, 5.666367053985596, 5.463916301727295]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.081637859344482, 3.885566473007202, 3.8767001628875732, 3.652493953704834, 3.8538527488708496, 3.8666465282440186, 3.69689679145813, 3.8336892127990723, 3.8910865783691406, 3.962792158126831, 3.813567876815796, 3.5378329753875732, 3.8795948028564453, 3.8492300510406494, 3.954951763153076, 3.9913339614868164]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.0895907878875732, 3.1901698112487793, 3.0523831844329834, 2.8674685955047607, 3.0383036136627197, 3.1077451705932617, 3.014669418334961, 2.9646048545837402, 2.9967401027679443, 3.0592963695526123, 3.1878371238708496, 2.4840571880340576, 3.126854658126831, 3.0650010108947754, 3.243367910385132, 3.1255619525909424]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09166441112756729, 0.11196479201316833, 0.10716352611780167, -0.2236960530281067, -0.20290419459342957, -0.12967753410339355, 0.12303321808576584, -0.18337084352970123, 0.07873811572790146, 0.0963384136557579, 0.09974725544452667, 0.08856914192438126, 0.09492271393537521, 0.10342948883771896, -1.0590324401855469, 0.12093762308359146]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09481612592935562, 0.06517525017261505, 0.04586659371852875, 0.0461537130177021, 0.08605724573135376, 0.02975676767528057, 0.05768623948097229, 0.06664259731769562, 0.009394936263561249, 0.07245276123285294, -0.1721699982881546, 0.05810270458459854, -0.0002546448667999357, -0.028293997049331665, 0.00892198272049427, 0.04804190248250961]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.10409457236528397, 0.05498792231082916, 0.09112786501646042, 0.06086166948080063, 0.10076495260000229, 0.11180391162633896, 0.06403467059135437, -0.07765661925077438, 0.07608745247125626, 0.10313255339860916, 0.002990197390317917, 0.08912692219018936, -0.1537678986787796, 0.028072716668248177, -0.019980277866125107, 0.11174541711807251]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.14582459628582, 0.1767781525850296, 0.1226043701171875, 0.14681199193000793, 0.14276123046875, 0.12005902826786041, 0.05736851319670677, 0.18949763476848602, 0.18380704522132874, -0.3676351010799408, -0.022714808583259583, 0.11217587441205978, 0.15528669953346252, -0.1918308436870575, -0.03647911921143532, -0.0557861328125]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08211351186037064, 0.11527650058269501, 0.10139783471822739, 0.09623508155345917, 0.043620288372039795, -0.039657481014728546, 0.04157530888915062, 0.060839004814624786, -0.048376332968473434, 0.018378769978880882, -0.12845461070537567, 0.16046375036239624, -0.055210307240486145, -0.18020348250865936, 0.12986932694911957, -0.017907626926898956]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.08020401000976562, 0.12494659423828125, 0.0746077373623848, 0.08715587854385376, -0.04561139643192291, 0.002292356686666608, 0.15226192772388458, 0.047696761786937714, 0.11549128592014313, -0.07233362644910812, -0.12787750363349915, 0.09484155476093292, -0.14097020030021667, 0.12453173100948334, 0.13267052173614502, 0.06597226113080978]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08038153499364853, 0.037220608443021774, 0.17186284065246582, 0.34789153933525085, 0.17596545815467834, 0.12121018022298813, -0.07112685590982437, -0.018968665972352028, 0.47322988510131836, 0.15332959592342377, -0.4729791283607483, 0.36563563346862793, 0.1905340701341629, -0.09360227733850479, 0.13974153995513916, 0.18038365244865417]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3094111979007721, 0.25414934754371643, 0.3033384680747986, 0.12331448495388031, -0.8735775947570801, 0.34659111499786377, 0.18470034003257751, -0.1220473125576973, 0.36940300464630127, 0.1956074982881546, -0.024994337931275368, 0.28418299555778503, 0.3604919910430908, 0.12322666496038437, -0.3380385637283325, -0.07433684170246124]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.1817401945590973, 0.3107416033744812, -0.08181320130825043, 0.33515045046806335, 0.5270712971687317, 0.28633713722229004, 0.2972162365913391, -0.1544065624475479, 0.15849216282367706, 0.6227678060531616, -0.008226035162806511, 0.47984734177589417, 0.32835853099823, -0.2908155024051666, -0.16798333823680878, 0.28931471705436707]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4492718279361725, 0.22848045825958252, 0.5243362188339233, 0.7278203368186951, 0.4458675682544708, 0.41349682211875916, 0.6937450170516968, 0.5592554211616516, 0.4933241605758667, 0.2077150195837021, 0.5653281807899475, 0.7462140321731567, 0.9719127416610718, 0.6032696962356567, 0.7763176560401917, 0.7304192185401917]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9706305861473083, 0.8546620011329651, 0.6830661296844482, 0.8895776867866516, 1.0557489395141602, 0.6931843161582947, 0.6709320545196533, 0.6734176874160767, 0.4300307035446167, 0.9510684013366699, 0.7432773113250732, 0.9392443895339966, 0.6957841515541077, 0.27894967794418335, 0.6534489989280701, 0.4887544810771942]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0563080310821533, 1.0985673666000366, 0.9900503754615784, 1.3577464818954468, 1.43338143825531, 0.7495999336242676, 0.95754075050354, 1.4175300598144531, 1.05952787399292, 1.29755437374115, 1.00436270236969, 1.1794185638427734, 0.7993159890174866, 0.5627828240394592, 0.027579044923186302, 0.9080227017402649]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.41608506441116333, 1.00731360912323, 1.2290605306625366, 1.2574162483215332, 0.5430514812469482, 0.8410043120384216, 1.0561161041259766, 1.1916362047195435, 0.9804121255874634, 0.7412204742431641, 0.7264112234115601, 0.869989812374115, 1.1680395603179932, 0.8916581869125366, 0.6886322498321533, 1.0098505020141602]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8640224933624268, 0.9461483955383301, 0.8759305477142334, 0.6362958550453186, 1.2380547523498535, 0.36863353848457336, 1.1593071222305298, 2.2654552459716797, 1.0175851583480835, 0.9697760939598083, 1.2408146858215332, 1.3501754999160767, -0.06003382429480553, 1.0485574007034302, 1.1181526184082031, 0.8646110892295837]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6563247442245483, 0.7775538563728333, 0.8855652809143066, 0.8185626268386841, 0.8858059048652649, 0.6207637786865234, 0.5420709252357483, 0.9941682815551758, 0.6743871569633484, 0.9562015533447266, 0.211247980594635, 0.7125120162963867, 1.0187050104141235, 0.9939566254615784, 1.0416808128356934, 0.6432371139526367]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.4288352131843567, 1.18168306350708, 1.001953125, 0.7891403436660767, 0.6557865142822266, 0.8435589075088501, 1.0683492422103882, 1.0218806266784668, 0.8812131881713867, 0.9454257488250732, 0.7238256335258484, -0.38089519739151, 0.7910559773445129, 0.8151324987411499, 1.0917119979858398, 1.414459228515625]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8292749524116516, 0.33385545015335083, 0.8383452892303467, 0.7429057955741882, 0.6960458159446716, 0.52715665102005, 0.4120837152004242, 0.9632762670516968, -0.757939875125885, 2.5778136253356934, -0.6101698875427246, 0.7244043946266174, 0.9358299374580383, 0.96584153175354, 0.84351646900177, 0.7372206449508667]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.15846097469329834, 0.6041118502616882, 1.3605072498321533, 1.3423347473144531, 1.3570680618286133, 1.3656730651855469, 1.4258520603179932, 1.4026551246643066, 1.6363507509231567, 1.6082215309143066, 1.9637539386749268, 1.32374107837677, 1.3558601140975952, 1.1649116277694702, 0.7720741629600525, 1.4564615488052368]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3892946243286133, 1.3502603769302368, 1.42263925075531, 1.6477298736572266, 1.0222804546356201, 1.32273268699646, 1.8292006254196167, 1.7485847473144531, 1.9822661876678467, 1.3734714984893799, 1.21294367313385, 1.0791988372802734, 1.3091952800750732, 1.5397869348526, 1.5136152505874634, 1.4527570009231567]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3185292482376099, 1.2585484981536865, 1.1252830028533936, 1.20677649974823, 1.4228657484054565, 0.9634372591972351, 1.0355737209320068, 1.2180352210998535, 1.8014181852340698, 1.3854025602340698, 1.40599524974823, 1.3639464378356934, 2.0646088123321533, 1.3491847515106201, 1.25360906124115, 1.2906618118286133]
Running loglikelihood requests:  45%|█████████████████████▉                           | 225/504 [12:37<14:56,  3.21s/it]Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.863408625125885, 1.1248301267623901, 1.1017677783966064, 0.9283359050750732, 0.10986969619989395, 1.2088569402694702, 1.2319689989089966, 1.9517698287963867, 0.8402771949768066, 1.2081917524337769, 1.3082399368286133, 1.1264365911483765, 1.0938348770141602, 1.4823652505874634, 1.1566780805587769, 0.49756744503974915]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.090240001678467, 2.21475887298584, 2.06886887550354, 2.032721996307373, 1.9239696264266968, 2.083418369293213, 1.9662024974822998, 2.2073709964752197, 1.9660042524337769, 2.0765397548675537, 2.37890625, 2.826709747314453, 1.9069010019302368, 2.4376697540283203, 2.2771739959716797, 2.156646251678467]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4457371234893799, 1.3179913759231567, 1.775390625, 1.0860011577606201, 1.4974241256713867, 1.548828125, 1.3626230955123901, 1.3620357513427734, 1.48174250125885, 1.62041437625885, 1.5027740001678467, 2.26324725151062, 1.5335427522659302, 1.5525645017623901, 1.2393498420715332, 1.6485223770141602]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.506779432296753, 1.7545289993286133, 1.2521768808364868, 1.4498131275177002, 1.4416043758392334, 1.5211163759231567, 1.3803215026855469, 1.46925950050354, 1.4719626903533936, 1.3814821243286133, 1.546875, 1.47091543674469, 1.43427312374115, 0.9817186594009399, 1.5181336402893066, 1.5056328773498535]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3611865043640137, 2.428272247314453, 2.2342052459716797, 2.2530570030212402, 2.9040420055389404, 2.41253399848938, 2.7650022506713867, 2.305593252182007, 2.3504867553710938, 2.390228748321533, 2.583956003189087, 2.3312952518463135, 2.2947802543640137, 2.479506254196167, 2.4231770038604736, 2.4685802459716797]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6247735023498535, 1.8112545013427734, 1.4569463729858398, 1.6218013763427734, 1.4971411228179932, 1.4862148761749268, 1.5352128744125366, 1.8592901229858398, 1.5269757509231567, 1.5088032484054565, 1.5543195009231567, 1.4134397506713867, 1.51689875125885, 1.4290931224822998, 1.55562162399292, 1.9490772485733032]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9705332517623901, 1.6930055618286133, 1.73896062374115, 1.8126132488250732, 1.7482025623321533, 1.77769935131073, 1.6821997165679932, 1.7992385625839233, 1.8049122095108032, 1.9110761880874634, 2.1837918758392334, 1.6552664041519165, 1.6986526250839233, 1.8973335027694702, 1.8380851745605469, 1.79643976688385]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6324799060821533, 1.6682438850402832, 1.6337978839874268, 1.7706422805786133, 1.6763615608215332, 1.6345665454864502, 1.7422066926956177, 1.7489137649536133, 1.6448674201965332, 1.7785892486572266, 1.5445822477340698, 1.7654410600662231, 1.6632133722305298, 1.8255951404571533, 1.6113777160644531, 1.6492654085159302]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.10648775100708, 2.18172550201416, 2.6302366256713867, 2.1056385040283203, 2.168874502182007, 2.13680362701416, 2.586503505706787, 2.185490369796753, 2.0583956241607666, 2.10207200050354, 2.0002830028533936, 1.9621829986572266, 2.0388360023498535, 1.9194972515106201, 2.4216768741607666, 2.1759228706359863]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.604676246643066, 5.878453254699707, 5.641983509063721, 5.598703384399414, 5.503878116607666, 5.402060508728027, 5.6037139892578125, 6.00203800201416, 5.7844767570495605, 5.585795879364014, 5.642691135406494, 5.4977922439575195, 5.663198947906494, 5.666326999664307, 5.7741169929504395, 5.581465244293213]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.160297870635986, 3.98734712600708, 3.995527744293213, 3.8249316215515137, 4.024679660797119, 3.9918408393859863, 3.8085176944732666, 4.023975372314453, 4.00468111038208, 4.083849906921387, 3.889110565185547, 3.721608877182007, 4.05061149597168, 3.997753143310547, 4.082342624664307, 4.1247878074646]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.155683994293213, 3.2447633743286133, 3.069859504699707, 2.9162704944610596, 3.0217957496643066, 3.163496494293213, 3.0729167461395264, 3.0050384998321533, 3.0544044971466064, 3.105015754699707, 3.251981496810913, 2.6341781616210938, 3.111865997314453, 3.076709747314453, 3.285609245300293, 3.1520040035247803]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.0941786915063858, 0.11496595293283463, 0.10922803729772568, -0.21779660880565643, -0.20613008737564087, -0.13635776937007904, 0.12408469617366791, -0.1751294732093811, 0.08296437561511993, 0.09920702129602432, 0.10325945913791656, 0.08908470720052719, 0.09711988270282745, 0.10646363347768784, -1.0770843029022217, 0.12300945073366165]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0957573652267456, 0.06985752284526825, 0.04401659592986107, 0.04438028484582901, 0.08358441293239594, 0.03180454671382904, 0.05507197603583336, 0.06834344565868378, 0.00802044291049242, 0.06945750117301941, -0.16430853307247162, 0.05753098055720329, 0.0040816147811710835, -0.025020182132720947, 0.013594982214272022, 0.048498667776584625]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09996400028467178, 0.057062916457653046, 0.09202659130096436, 0.06469269841909409, 0.10190298408269882, 0.11335286498069763, 0.06933248788118362, -0.07653474807739258, 0.07803328335285187, 0.09088407456874847, 0.002559063257649541, 0.08816595375537872, -0.14362552762031555, 0.03207625821232796, -0.017784452065825462, 0.11810325086116791]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.14281290769577026, 0.17626908421516418, 0.1179947704076767, 0.1420726329088211, 0.1406751275062561, 0.12120693922042847, 0.06399869918823242, 0.19105719029903412, 0.1819431334733963, -0.3789169490337372, -0.023636087775230408, 0.11319971829652786, 0.14937998354434967, -0.1666288673877716, -0.05504925549030304, -0.05962911993265152]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08261629194021225, 0.108158640563488, 0.10200272500514984, 0.09010916203260422, 0.04124467447400093, -0.03627799451351166, 0.04988744109869003, 0.056200459599494934, -0.04345123842358589, 0.0353144071996212, -0.13096635043621063, 0.15913201868534088, -0.06084224954247475, -0.182296022772789, 0.12163993716239929, -0.013439317233860493]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06584613025188446, 0.12075996398925781, 0.07015044242143631, 0.07209276407957077, -0.03654513508081436, 0.006448341999202967, 0.15405963361263275, 0.04679347202181816, 0.09985330700874329, -0.07183737307786942, -0.10194296389818192, 0.09646517038345337, -0.14310823380947113, 0.12334620207548141, 0.1390494406223297, 0.06521790474653244]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.0788186639547348, 0.05111727863550186, 0.14768558740615845, 0.35328397154808044, 0.16819638013839722, 0.11021801829338074, -0.06852933764457703, -0.024957921355962753, 0.4837947189807892, 0.15206964313983917, -0.4699929654598236, 0.36764103174209595, 0.19764341413974762, -0.08719384670257568, 0.13528531789779663, 0.1788385808467865]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.30859529972076416, 0.25833743810653687, 0.30959680676460266, 0.10840389132499695, -0.8740270137786865, 0.34312963485717773, 0.18342667818069458, -0.10132816433906555, 0.35195544362068176, 0.1772157996892929, -0.00747257424518466, 0.29373860359191895, 0.37695881724357605, 0.11803051829338074, -0.3340596556663513, -0.028192004188895226]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.18146678805351257, 0.31429171562194824, -0.0627671405673027, 0.33932751417160034, 0.5189698934555054, 0.2877500355243683, 0.2928571403026581, -0.14398148655891418, 0.15492258965969086, 0.5983231663703918, -0.013903986662626266, 0.5029844641685486, 0.3369274139404297, -0.30081111192703247, -0.16573183238506317, 0.27608662843704224]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.45707935094833374, 0.22920794785022736, 0.5123524069786072, 0.7217376232147217, 0.44978275895118713, 0.4338191747665405, 0.6993238925933838, 0.5788342356681824, 0.4698098599910736, 0.18703889846801758, 0.596851646900177, 0.7361356616020203, 1.01346755027771, 0.6030817031860352, 0.7750484943389893, 0.7498146891593933]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.982760488986969, 0.8681212663650513, 0.6679580807685852, 0.8862054944038391, 1.0536307096481323, 0.7112528681755066, 0.6645846366882324, 0.6715043187141418, 0.43196719884872437, 0.9650932550430298, 0.7445148825645447, 0.9340428113937378, 0.7109958529472351, 0.2685096859931946, 0.595374345779419, 0.5017796158790588]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0571181774139404, 1.099146842956543, 0.9740391373634338, 1.3470361232757568, 1.4258097410202026, 0.7645704746246338, 0.9283545017242432, 1.418852686882019, 1.017104148864746, 1.308030605316162, 0.9835766553878784, 1.1578466892242432, 0.7893281579017639, 0.523128092288971, -0.025910092517733574, 0.8935546875]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.4326207637786865, 0.9843322038650513, 1.2176522016525269, 1.2700159549713135, 0.5158214569091797, 0.8081482648849487, 1.0397182703018188, 1.2111014127731323, 0.9878112077713013, 0.7495333552360535, 0.7122294902801514, 0.8858206272125244, 1.1647967100143433, 0.893547534942627, 0.6724960207939148, 1.0204578638076782]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7923290133476257, 0.914145827293396, 0.8505634665489197, 0.6389182209968567, 1.2244383096694946, 0.3135411739349365, 1.1494354009628296, 2.2541627883911133, 1.006158709526062, 0.9611727595329285, 1.2427363395690918, 1.331460952758789, -0.09239029884338379, 1.0293948650360107, 1.1359297037124634, 0.8348549008369446]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6421704888343811, 0.7714781165122986, 0.8763828873634338, 0.8032654523849487, 0.8479128479957581, 0.6669833064079285, 0.5215133428573608, 1.000517725944519, 0.7056519389152527, 0.9740747809410095, 0.18884678184986115, 0.7011665105819702, 1.0094342231750488, 0.9926508069038391, 1.0518219470977783, 0.6390875577926636]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.4085198938846588, 1.2023694515228271, 1.0015325546264648, 0.8415759205818176, 0.6371143460273743, 0.8407846689224243, 1.088547945022583, 1.0115761756896973, 0.8928775191307068, 0.9513002038002014, 0.720314621925354, -0.4053906202316284, 0.7883058190345764, 0.8251596689224243, 1.0969719886779785, 1.4103847742080688]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8102118372917175, 0.32023364305496216, 0.8552424311637878, 0.7568323612213135, 0.7229057550430298, 0.5480342507362366, 0.40694767236709595, 0.9393426179885864, -0.7960624098777771, 2.5709540843963623, -0.6481198668479919, 0.7322626113891602, 0.925624430179596, 0.9831988215446472, 0.8604228496551514, 0.704944908618927]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.19167087972164154, 0.6155698299407959, 1.3786567449569702, 1.369439959526062, 1.3669451475143433, 1.399107575416565, 1.4425325393676758, 1.4168567657470703, 1.6710623502731323, 1.6228829622268677, 1.9747947454452515, 1.3373453617095947, 1.3558287620544434, 1.1807353496551514, 0.7756423354148865, 1.4856295585632324]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4163435697555542, 1.3752851486206055, 1.468690276145935, 1.6443032026290894, 1.0344719886779785, 1.3180314302444458, 1.8404995203018188, 1.7393362522125244, 1.9810247421264648, 1.398751139640808, 1.201767086982727, 1.0877115726470947, 1.3163492679595947, 1.5322020053863525, 1.517364263534546, 1.462591290473938]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.2968465089797974, 1.2385379076004028, 1.1294479370117188, 1.1768790483474731, 1.4061074256896973, 0.9179847836494446, 1.0096302032470703, 1.1931134462356567, 1.7727389335632324, 1.3854926824569702, 1.380075216293335, 1.3396583795547485, 2.0573678016662598, 1.3328865766525269, 1.2353872060775757, 1.2709853649139404]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8585751056671143, 1.081147313117981, 1.0606324672698975, 0.89397794008255, 0.13258618116378784, 1.169708013534546, 1.2049212455749512, 1.9022796154022217, 0.8471501469612122, 1.1713190078735352, 1.2724109888076782, 1.0662565231323242, 1.0661495923995972, 1.449532389640808, 1.1557198762893677, 0.5042882561683655]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0857093334198, 2.1856751441955566, 2.0567119121551514, 2.01037859916687, 1.9143476486206055, 2.039661169052124, 1.950387716293335, 2.2004446983337402, 1.9682368040084839, 2.0743613243103027, 2.3621692657470703, 2.8108463287353516, 1.9011461734771729, 2.4194228649139404, 2.241759777069092, 2.12733793258667]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4476220607757568, 1.3004391193389893, 1.7824760675430298, 1.0944414138793945, 1.4947251081466675, 1.5573676824569702, 1.3560247421264648, 1.3858634233474731, 1.4889941215515137, 1.6421362161636353, 1.5005987882614136, 2.266993522644043, 1.5373802185058594, 1.5663206577301025, 1.2529083490371704, 1.650376319885254]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5136005878448486, 1.7844433784484863, 1.2901637554168701, 1.4851163625717163, 1.4720004796981812, 1.537522792816162, 1.4220032691955566, 1.4936131238937378, 1.5016038417816162, 1.4147753715515137, 1.580291986465454, 1.491203784942627, 1.4439010620117188, 1.0149354934692383, 1.5431256294250488, 1.544708013534546]
Running loglikelihood requests:  45%|██████████████████████▎                          | 229/504 [12:49<14:41,  3.21s/it]Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.324988603591919, 2.3979814052581787, 2.252765655517578, 2.241046905517578, 2.9095003604888916, 2.387488603591919, 2.750627279281616, 2.297502279281616, 2.3252737522125244, 2.3699817657470703, 2.5622718334198, 2.313640594482422, 2.2983007431030273, 2.446110963821411, 2.4164576530456543, 2.4450843334198]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6271954774856567, 1.821395993232727, 1.461707353591919, 1.6349366903305054, 1.5195597410202026, 1.495780110359192, 1.549712061882019, 1.860458493232727, 1.5435532331466675, 1.5239222049713135, 1.550467610359192, 1.4256671667099, 1.5241217613220215, 1.4280623197555542, 1.5825729370117188, 1.9572592973709106]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9808893203735352, 1.7288310527801514, 1.7530152797698975, 1.8254163265228271, 1.7448961734771729, 1.7854199409484863, 1.709609866142273, 1.816949725151062, 1.818829894065857, 1.9215834140777588, 2.1726341247558594, 1.6798559427261353, 1.7120637893676758, 1.9170421361923218, 1.8274407386779785, 1.8019537925720215]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6236741542816162, 1.6728194952011108, 1.6369165182113647, 1.7846238613128662, 1.6722055673599243, 1.6327911615371704, 1.7494635581970215, 1.7569642066955566, 1.6513208150863647, 1.7975949048995972, 1.5464757680892944, 1.7640140056610107, 1.6497063636779785, 1.8270317316055298, 1.624123215675354, 1.6501836776733398]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1087353229522705, 2.1891536712646484, 2.618149757385254, 2.104552745819092, 2.187321901321411, 2.1295690536499023, 2.57997465133667, 2.183636426925659, 2.056603193283081, 2.0918681621551514, 2.0064616203308105, 1.9623489379882812, 2.0389626026153564, 1.926251769065857, 2.402144193649292, 2.172192096710205]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.605383396148682, 5.876710891723633, 5.6410813331604, 5.5944342613220215, 5.5131731033325195, 5.407162189483643, 5.605154991149902, 5.984888076782227, 5.785013675689697, 5.585367202758789, 5.616559982299805, 5.494924545288086, 5.665488243103027, 5.675182342529297, 5.766879558563232, 5.585709571838379]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.188326835632324, 3.9993441104888916, 4.012089252471924, 3.83408784866333, 4.039575576782227, 4.0128021240234375, 3.815786123275757, 4.01015043258667, 4.007669925689697, 4.104299545288086, 3.8957571983337402, 3.7256250381469727, 4.045855522155762, 3.9946680068969727, 4.101762294769287, 4.136162757873535]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.181854486465454, 3.287066698074341, 3.0897581577301025, 2.8930771350860596, 3.0394046306610107, 3.2147581577301025, 3.0399179458618164, 3.041001319885254, 3.078296184539795, 3.1284501552581787, 3.23203706741333, 2.652543306350708, 3.157447576522827, 3.120495080947876, 3.292141914367676, 3.1717610359191895]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.0972178652882576, 0.10734385251998901, 0.10736590623855591, -0.24868306517601013, -0.18878641724586487, -0.15025362372398376, 0.1211700439453125, -0.16474013030529022, 0.09807327389717102, 0.09218689054250717, 0.09625734388828278, 0.108738474547863, 0.09488878399133682, 0.11716477572917938, -1.0162309408187866, 0.12552504241466522]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.09784720838069916, 0.06841039657592773, 0.038256485015153885, 0.05635476112365723, 0.08041766285896301, 0.04536553472280502, 0.05509469658136368, 0.07004135102033615, 0.03498879447579384, 0.06345438212156296, -0.1804056614637375, 0.058604609221220016, 0.020636899396777153, -0.018961774185299873, 0.013088087551295757, 0.020622393116354942]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07399237155914307, 0.058199748396873474, 0.08842134475708008, 0.06104228273034096, 0.12375668436288834, 0.0835048109292984, 0.0819985568523407, -0.06951659172773361, 0.09339531511068344, 0.08099554479122162, 0.007871335372328758, 0.076777882874012, -0.1519692987203598, 0.02821645326912403, 0.008361649699509144, 0.1090615838766098]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1199423223733902, 0.16792704164981842, 0.13583841919898987, 0.14668524265289307, 0.1143612265586853, 0.1474943459033966, 0.08464651554822922, 0.1720941960811615, 0.17868292331695557, -0.44901514053344727, -0.03766264393925667, 0.13950543105602264, 0.205595925450325, -0.21708333492279053, -0.029487665742635727, -0.039875224232673645]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08109897375106812, 0.06355196237564087, 0.08551833033561707, 0.14742574095726013, 0.05125560984015465, -0.05029725655913353, 0.054031483829021454, 0.04874848946928978, -0.07427059859037399, 0.025101710110902786, -0.17250128090381622, 0.10838562995195389, -0.03250990808010101, -0.13814368844032288, 0.13645188510417938, -0.00443780142813921]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06449912488460541, 0.15287193655967712, 0.06754700839519501, 0.17456990480422974, -0.08513245731592178, -0.009114842861890793, 0.10954151302576065, 0.03460671007633209, 0.13465085625648499, -0.045711543411016464, -0.18357370793819427, 0.0905442088842392, -0.1852351874113083, 0.13643911480903625, 0.15426930785179138, 0.07511620968580246]
Layer: gate_5 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.12504182755947113, 0.06449723243713379, 0.16871632635593414, 0.2930126190185547, 0.17562176287174225, 0.1377575695514679, -0.07294914871454239, -0.011895395815372467, 0.27620869874954224, 0.16349157691001892, -0.4500238001346588, 0.3026033937931061, 0.20156213641166687, -0.16201314330101013, 0.1937672346830368, 0.20516210794448853]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3270704746246338, 0.2453472912311554, 0.3224382996559143, 0.12684175372123718, -0.5860961079597473, 0.2879273295402527, 0.15052472054958344, -0.0039815413765609264, 0.4183821976184845, 0.18493206799030304, -0.04953203350305557, 0.22518308460712433, 0.24570080637931824, 0.1772380769252777, -0.42485547065734863, -0.27480611205101013]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.25495943427085876, 0.2703603506088257, -0.07612688094377518, 0.28112465143203735, 0.4595586359500885, 0.42646822333335876, 0.2939477562904358, -0.2620876431465149, 0.20526334643363953, 0.7926490902900696, 0.17674756050109863, 0.10534578561782837, 0.48302510380744934, -0.40983280539512634, -0.29534777998924255, 0.3465762138366699]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.6363703608512878, 0.3832972049713135, 0.3197433650493622, 0.8594393730163574, 0.45486027002334595, 0.4298452138900757, 0.7035580277442932, 0.5631771683692932, 0.8232181072235107, 0.39844757318496704, 0.35294580459594727, 0.6852760314941406, 0.7486187815666199, 0.4019300937652588, 0.6863594651222229, 0.6558223366737366]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9911810755729675, 0.8659828305244446, 0.5722050070762634, 0.7643846869468689, 0.9656028747558594, 0.6045327186584473, 0.667373538017273, 0.6750898361206055, 0.253374308347702, 0.736679196357727, 0.7903028130531311, 0.9007577300071716, 0.6208826899528503, 0.41307345032691956, 1.020118236541748, 0.6124757528305054]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [1.0858020782470703, 1.212905764579773, 0.9433451294898987, 1.4197739362716675, 1.5381358861923218, 0.6744465231895447, 1.0161596536636353, 0.9592160582542419, 1.0104072093963623, 1.4577867984771729, 0.9105554223060608, 1.0976775884628296, 0.5918463468551636, 0.7865845561027527, 0.2930634319782257, 1.0195953845977783]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.503311276435852, 0.9859289526939392, 1.3664746284484863, 1.204607605934143, 0.7855786085128784, 0.6551558375358582, 1.1421254873275757, 1.2317625284194946, 0.6877896785736084, 0.3951667845249176, 0.9901577234268188, 0.8720917105674744, 1.1423429250717163, 0.8721059560775757, 0.8582772612571716, 0.9820511937141418]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.2103387117385864, 1.0597057342529297, 1.4433629512786865, 1.0299807786941528, 1.2874799966812134, 0.2295229285955429, 1.2672216892242432, 2.176551103591919, 1.2325929403305054, 1.1807140111923218, 1.5132941007614136, 1.4615932703018188, 0.6305724382400513, 1.385528326034546, 1.2404624223709106, 0.9651502370834351]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.9178224205970764, 0.87701016664505, 1.005873680114746, 0.9583713412284851, 1.083578109741211, 1.3748770952224731, 0.778597891330719, 1.5703071355819702, 0.7718300819396973, 1.0655509233474731, -0.008329293690621853, 0.8674710392951965, 1.1116479635238647, 1.0999640226364136, 1.1195807456970215, 0.7727652192115784]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6359726190567017, 1.268761396408081, 1.135934591293335, 1.0173927545547485, 0.9131411910057068, 1.090713381767273, 1.7211111783981323, 1.0382213592529297, 0.9684221148490906, 1.051451325416565, 0.9737504124641418, 0.06885300576686859, 1.2510981559753418, 0.93744295835495, 0.9944685101509094, 1.9137078523635864]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7667868733406067, 0.420116126537323, 0.84247225522995, 0.6872825622558594, 0.6735038161277771, 0.7066792845726013, 0.5813608765602112, 0.8350999355316162, -0.4518125355243683, 2.372448205947876, -0.6128842830657959, 0.7755265235900879, 0.8968265056610107, 1.0233733654022217, 0.8666956424713135, 0.7297559380531311]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.6130256652832031, 1.1510134935379028, 1.6340670585632324, 1.665420413017273, 1.5894445180892944, 1.5707972049713135, 1.7081717252731323, 1.6237454414367676, 1.8840670585632324, 1.753400206565857, 2.51171875, 1.696082353591919, 1.4863343238830566, 1.3430942296981812, 0.9162896275520325, 1.6278940439224243]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.609175443649292, 1.4186816215515137, 1.6106010675430298, 1.8929060697555542, 1.3485828638076782, 1.4850592613220215, 2.169337272644043, 1.9867130517959595, 2.655651330947876, 1.6677120923995972, 1.4681726694107056, 1.476844072341919, 1.6500056982040405, 1.3059674501419067, 1.8310333490371704, 1.7173243761062622]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.7824475765228271, 1.654282569885254, 1.7830605506896973, 1.6189552545547485, 1.8239620923995972, 1.5287283658981323, 1.7591526508331299, 1.6389285326004028, 2.2471914291381836, 1.612297534942627, 1.8760550022125244, 1.7677634954452515, 2.2897415161132812, 1.810646653175354, 1.7691606283187866, 1.7452384233474731]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.208914875984192, 1.2373260259628296, 1.2436915636062622, 1.0164161920547485, 0.476957231760025, 1.419921875, 1.341996431350708, 2.242009401321411, 1.0913567543029785, 1.4006614685058594, 1.3437000513076782, 1.2422890663146973, 1.2700159549713135, 1.400490403175354, 1.2448302507400513, 0.8467260003089905]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.306084632873535, 2.4990875720977783, 2.366617202758789, 2.2453808784484863, 2.256843090057373, 2.3643932342529297, 2.4393532276153564, 2.450958013534546, 2.1809420585632324, 2.4567461013793945, 2.5859944820404053, 2.8325729370117188, 2.157846689224243, 2.8163206577301025, 2.527315139770508, 2.4573450088500977]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.8968123197555542, 1.5965442657470703, 2.1610400676727295, 1.4362454414367676, 1.8523038625717163, 1.7902600765228271, 1.7146726846694946, 1.6391708850860596, 1.7833884954452515, 1.9661839008331299, 1.7685903310775757, 2.2498860359191895, 1.8570654392242432, 1.8003535270690918, 1.6967666149139404, 1.9191663265228271]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.3496806621551514, 1.8659329414367676, 1.5582088232040405, 1.6884409189224243, 1.7425296306610107, 1.637289047241211, 1.6851619482040405, 1.623032569885254, 1.6001653671264648, 1.5563697814941406, 1.6858748197555542, 1.5780822038650513, 1.5354983806610107, 1.1160398721694946, 1.5860515832901, 1.709854006767273]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.5800068378448486, 2.560903310775757, 2.4155452251434326, 2.378478527069092, 2.923243522644043, 2.5338730812072754, 2.7440693378448486, 2.517392873764038, 2.508610963821411, 2.641423463821411, 2.8029768466949463, 2.4887659549713135, 2.4426321983337402, 2.6066946983337402, 2.592381477355957, 2.5632412433624268]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7331204414367676, 1.7635149955749512, 1.6233177185058594, 1.7957345247268677, 1.5154967308044434, 1.6272810697555542, 1.6822822093963623, 1.8856067657470703, 1.682111144065857, 1.6428489685058594, 1.6323562860488892, 1.5829436779022217, 1.7493442296981812, 1.5193458795547485, 1.791029930114746, 2.0686872005462646]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.8420392274856567, 1.8016598224639893, 1.760834813117981, 1.825872540473938, 1.8500370979309082, 1.7946492433547974, 1.8734256029129028, 1.7798813581466675, 1.78466796875, 1.9121378660202026, 2.2427148818969727, 1.7512260675430298, 1.7968465089797974, 1.8333001136779785, 1.804261565208435, 1.7667913436889648]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Running loglikelihood requests:  46%|██████████████████████▋                          | 233/504 [13:03<14:36,  3.23s/it]Layer: gate_27 - Captured router_logits: [1.5938069820404053, 1.684532880783081, 1.6514633893966675, 1.7318623065948486, 1.615034818649292, 1.6075938940048218, 1.7635008096694946, 1.693917155265808, 1.6836814880371094, 1.7533715963363647, 1.5007983446121216, 1.6424283981323242, 1.6463168859481812, 1.8190971612930298, 1.6641409397125244, 1.6430236101150513]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.2327213287353516, 2.359745740890503, 2.5268447399139404, 2.2688183784484863, 2.27734375, 2.2362282276153564, 2.6315865516662598, 2.3243470191955566, 2.2350308895111084, 2.2330634593963623, 2.2909157276153564, 2.0966012477874756, 2.1811559200286865, 2.1804003715515137, 2.444371461868286, 2.222855806350708]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.333428382873535, 5.689210891723633, 5.238024711608887, 5.307296276092529, 5.388999938964844, 5.259580135345459, 5.313754558563232, 5.511205673217773, 5.51060676574707, 5.325359344482422, 5.303518295288086, 5.23705530166626, 5.399749279022217, 5.369924545288086, 5.456432342529297, 5.349281311035156]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.061587810516357, 3.908759117126465, 3.868955373764038, 3.861191749572754, 3.847855806350708, 3.9104700088500977, 3.80969500541687, 3.9817519187927246, 3.9721572399139404, 4.010207653045654, 3.876368522644043, 3.657019853591919, 3.9203860759735107, 3.8854072093963623, 3.9577155113220215, 4.004640579223633]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.935760736465454, 3.0578525066375732, 2.8947877883911133, 2.7278170585632324, 2.814495801925659, 3.0029938220977783, 2.8049156665802, 2.7848711013793945, 2.918966770172119, 2.943544626235962, 3.12365984916687, 2.449690103530884, 3.0264313220977783, 2.902942419052124, 3.1705634593963623, 2.988252639770508]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09048215299844742, 0.10933589935302734, 0.10668516159057617, -0.24277833104133606, -0.22285550832748413, -0.11918482929468155, 0.12379837036132812, -0.1613597869873047, 0.07289419323205948, 0.09179317206144333, 0.09872113913297653, 0.09106742590665817, 0.09504783898591995, 0.10392020642757416, -1.087998390197754, 0.11629553139209747]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08741737902164459, 0.05786716192960739, 0.038186270743608475, 0.04136860370635986, 0.07744815945625305, 0.02531789243221283, 0.05174841731786728, 0.07107734680175781, 0.009727253578603268, 0.06933616101741791, -0.18587684631347656, 0.0551377572119236, 0.005073322914540768, -0.03812085837125778, 0.015586993657052517, 0.044880349189043045]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09453986585140228, 0.054892007261514664, 0.0902814045548439, 0.0668897032737732, 0.09922801703214645, 0.10844435542821884, 0.0709974616765976, -0.0812622532248497, 0.07376519590616226, 0.10423275828361511, 0.007306323386728764, 0.08604361116886139, -0.16235889494419098, 0.03017585352063179, -0.014440424740314484, 0.1145598441362381]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13459670543670654, 0.17208503186702728, 0.12304283678531647, 0.1307849884033203, 0.13933241367340088, 0.11199434846639633, 0.06270650029182434, 0.18399766087532043, 0.1872376799583435, -0.4002447724342346, -0.004232518840581179, 0.10255151242017746, 0.1725221574306488, -0.18688561022281647, -0.01678433082997799, -0.04541952535510063]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06788522750139236, 0.10563810914754868, 0.09433913230895996, 0.13285334408283234, 0.045160405337810516, -0.046300552785396576, 0.033666498959064484, 0.04474659636616707, -0.0789046585559845, 0.02066982537508011, -0.13679975271224976, 0.1535763442516327, -0.03218314051628113, -0.18398691713809967, 0.13033521175384521, -0.014732585288584232]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07147808372974396, 0.11807834357023239, 0.0692337155342102, 0.11258585005998611, -0.022405512630939484, -0.015193265862762928, 0.12844473123550415, 0.021488750353455544, 0.12646518647670746, -0.08486175537109375, -0.07553369551897049, 0.0903276577591896, -0.16121326386928558, 0.12267410010099411, 0.12876303493976593, 0.060620054602622986]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.08248138427734375, 0.024287279695272446, 0.19628502428531647, 0.3343512713909149, 0.19350522756576538, 0.14298798143863678, -0.08099365234375, -0.0711076408624649, 0.44844862818717957, 0.15440323948860168, -0.4851047396659851, 0.32169532775878906, 0.20217491686344147, -0.12444394826889038, 0.166134774684906, 0.1811889261007309]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.33886852860450745, 0.2544584274291992, 0.29891180992126465, 0.14790242910385132, -0.8890665769577026, 0.3357976973056793, 0.1806849241256714, -0.1935521364212036, 0.4210259020328522, 0.20318594574928284, -0.07713295519351959, 0.30029118061065674, 0.3345646560192108, 0.14355961978435516, -0.4068900942802429, -0.0747344046831131]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20413869619369507, 0.33260560035705566, -0.16222696006298065, 0.3309626877307892, 0.5405740141868591, 0.28414738178253174, 0.28688183426856995, -0.2007419317960739, 0.16417671740055084, 0.6732518672943115, -0.012158562429249287, 0.41162827610969543, 0.33856067061424255, -0.3589450716972351, -0.2132200300693512, 0.29098567366600037]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.44604671001434326, 0.23425090312957764, 0.45658695697784424, 0.7887914180755615, 0.39776813983917236, 0.4137389361858368, 0.6694623231887817, 0.5252110958099365, 0.5214628577232361, 0.12328383326530457, 0.4806477129459381, 0.6920453310012817, 0.8480955958366394, 0.585151195526123, 0.739891529083252, 0.6720222234725952]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9670010805130005, 0.8152107000350952, 0.6144660711288452, 0.8382927179336548, 1.0541013479232788, 0.6327896118164062, 0.6422711610794067, 0.6714369654655457, 0.38151079416275024, 0.8962974548339844, 0.7400764226913452, 0.9126982092857361, 0.470428466796875, 0.26788419485092163, 0.695581316947937, 0.4536806046962738]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.023545265197754, 1.0380895137786865, 0.9435891509056091, 1.1759930849075317, 1.3676228523254395, 0.7502925992012024, 0.9076753258705139, 1.3131606578826904, 1.0448931455612183, 1.296034812927246, 0.9279515743255615, 1.0936709642410278, 0.722583532333374, 0.45831456780433655, -0.03711453452706337, 0.8020234704017639]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.2982572615146637, 0.9329071044921875, 1.196669578552246, 1.131232738494873, 0.41244328022003174, 0.8017793297767639, 0.9694976806640625, 1.1503726243972778, 0.930286169052124, 0.6564613580703735, 0.684740424156189, 0.8667279481887817, 1.0709623098373413, 0.8207505345344543, 0.5861825346946716, 0.93896484375]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7714125514030457, 0.8920323848724365, 0.8314881920814514, 0.5135372281074524, 1.1555606126785278, 0.24504896998405457, 1.0976132154464722, 2.2398035526275635, 0.9550996422767639, 0.8850600123405457, 1.1835650205612183, 1.187629222869873, -0.10198301076889038, 0.971152126789093, 1.1157585382461548, 0.7194945216178894]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6991469264030457, 0.7663394808769226, 0.9382109045982361, 0.839885950088501, 0.9135957360267639, 0.8452494144439697, 0.4855068325996399, 1.165684461593628, 0.7998549342155457, 0.9628744721412659, 0.18541717529296875, 0.7390549778938293, 1.069171667098999, 1.0404268503189087, 1.0856646299362183, 0.654503345489502]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4296971559524536, 1.131850242614746, 1.000129222869873, 0.8738008141517639, 0.6059049963951111, 0.855339527130127, 1.1602307558059692, 1.0332318544387817, 0.8867331147193909, 0.9614976048469543, 0.6847578883171082, -0.3872321546077728, 0.8063677549362183, 0.8218994140625, 1.013901710510254, 1.479262351989746]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7360951900482178, 0.2570093870162964, 0.8260515928268433, 0.7644456028938293, 0.6347562074661255, 0.476269006729126, 0.3357261121273041, 0.9175594449043274, -0.804858922958374, 2.6019287109375, -0.6515673398971558, 0.7187535762786865, 0.8871064782142639, 0.9348288178443909, 0.8341315984725952, 0.557580828666687]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.03927702084183693, 0.5711449980735779, 1.348402976989746, 1.2824513912200928, 1.324204444885254, 1.3974322080612183, 1.4246610403060913, 1.3245633840560913, 1.700432300567627, 1.4495562314987183, 2.010727882385254, 1.3438003063201904, 1.366192102432251, 1.1797162294387817, 0.7511506676673889, 1.394301414489746]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3516629934310913, 1.3377469778060913, 1.3901797533035278, 1.6453067064285278, 0.9546903967857361, 1.2244298458099365, 1.7882870435714722, 1.698357105255127, 2.0131475925445557, 1.3517348766326904, 1.1480326652526855, 0.9832344055175781, 1.1934384107589722, 1.3540712594985962, 1.4445513486862183, 1.4090360403060913]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.273179054260254, 1.194565773010254, 1.1139131784439087, 1.157599925994873, 1.396111011505127, 0.8923211693763733, 0.9657377600669861, 1.1662704944610596, 1.8038833141326904, 1.390854835510254, 1.3760052919387817, 1.326416015625, 1.914320945739746, 1.3085362911224365, 1.2157198190689087, 1.2746151685714722]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8764666318893433, 1.0731416940689087, 1.0589815378189087, 0.8895425200462341, 0.039998672902584076, 1.2068301439285278, 1.1733111143112183, 1.8507510423660278, 0.862189769744873, 1.207146167755127, 1.2583869695663452, 1.1161588430404663, 1.0763872861862183, 1.4126551151275635, 1.0917385816574097, 0.44305419921875]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.9931640625, 2.124483108520508, 2.0011775493621826, 1.9073989391326904, 1.8472541570663452, 2.0, 1.8518784046173096, 2.0607192516326904, 1.8716107606887817, 1.967543601989746, 2.270163059234619, 2.671760082244873, 1.8023035526275635, 2.3341567516326904, 2.1802332401275635, 2.0309340953826904]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.400390625, 1.2900965213775635, 1.785041332244873, 1.0541561841964722, 1.483168601989746, 1.5352137088775635, 1.3434053659439087, 1.386603832244873, 1.4647575616836548, 1.6222426891326904, 1.5165728330612183, 2.219496726989746, 1.5047966241836548, 1.523207664489746, 1.2261029481887817, 1.6426355838775635]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5664637088775635, 1.796128273010254, 1.2813146114349365, 1.45947265625, 1.5479090213775635, 1.5082720518112183, 1.4399126768112183, 1.469468116760254, 1.5489717721939087, 1.4375861883163452, 1.5727826356887817, 1.5136288404464722, 1.4477251768112183, 0.9444553256034851, 1.5756261348724365, 1.5399528741836548]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.296530246734619, 2.3419692516326904, 2.206169605255127, 2.166762351989746, 2.762235641479492, 2.3095128536224365, 2.6394760608673096, 2.2167394161224365, 2.254997730255127, 2.254652976989746, 2.625344753265381, 2.2351791858673096, 2.2505743503570557, 2.410041332244873, 2.336454391479492, 2.3795955181121826]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.596076488494873, 1.7202435731887817, 1.4102424383163452, 1.5989199876785278, 1.4219324588775635, 1.46923828125, 1.499253273010254, 1.8060948848724365, 1.4879653453826904, 1.4618566036224365, 1.4928481578826904, 1.3995577096939087, 1.4695829153060913, 1.357306957244873, 1.5610926151275635, 1.8656939268112183]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9096105098724365, 1.6989039182662964, 1.6834213733673096, 1.7746151685714722, 1.698974609375, 1.739333152770996, 1.6722627878189087, 1.7564338445663452, 1.7489911317825317, 1.8685948848724365, 2.153420925140381, 1.618300437927246, 1.68115234375, 1.838852882385254, 1.8236442804336548, 1.7367759943008423]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5882461071014404, 1.6385408639907837, 1.6063555479049683, 1.740729808807373, 1.6398495435714722, 1.5990394353866577, 1.719714879989624, 1.70263671875, 1.6198946237564087, 1.7310503721237183, 1.481071949005127, 1.7269287109375, 1.6303890943527222, 1.8221076726913452, 1.5699427127838135, 1.6047686338424683]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.024385452270508, 2.1040899753570557, 2.512723922729492, 2.0090763568878174, 2.0910212993621826, 2.0143325328826904, 2.472771167755127, 2.0746495723724365, 1.9958065748214722, 2.0128676891326904, 1.918083667755127, 1.8516199588775635, 1.960197925567627, 1.8202550411224365, 2.3249943256378174, 2.0831801891326904]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.452780246734619, 5.734432220458984, 5.458352565765381, 5.432674407958984, 5.344037055969238, 5.286419868469238, 5.451171875, 5.865205764770508, 5.648322582244873, 5.439539432525635, 5.491527080535889, 5.365923881530762, 5.529627323150635, 5.498506546020508, 5.629078388214111, 5.428423881530762]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.010024070739746, 3.7860610485076904, 3.7947027683258057, 3.602165699005127, 3.8668572902679443, 3.821578025817871, 3.5873520374298096, 3.855497360229492, 3.8585708141326904, 3.893576145172119, 3.708883762359619, 3.4917449951171875, 3.834132432937622, 3.8416926860809326, 3.879976272583008, 3.949714183807373]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  47%|███████████████████████                          | 237/504 [13:15<14:21,  3.23s/it]Layer: gate_31 - Captured router_logits: [3.117072582244873, 3.21875, 3.0502068996429443, 2.9191176891326904, 3.000143527984619, 3.132352828979492, 3.072610378265381, 2.9769933223724365, 3.034294605255127, 3.0684454441070557, 3.2941176891326904, 2.554572582244873, 3.0988051891326904, 3.0593693256378174, 3.302504539489746, 3.1543543338775635]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09382424503564835, 0.11435733735561371, 0.10918193310499191, -0.2325589805841446, -0.20212167501449585, -0.15136758983135223, 0.1267022043466568, -0.1831437349319458, 0.08316512405872345, 0.09818250685930252, 0.1014070063829422, 0.10115256160497665, 0.09608146548271179, 0.10360165685415268, -1.0527416467666626, 0.123687744140625]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0931510329246521, 0.06564911454916, 0.04195070266723633, 0.04188668727874756, 0.09139981865882874, 0.029908422380685806, 0.05716067552566528, 0.06369946897029877, 0.010356618091464043, 0.0684572160243988, -0.17629048228263855, 0.05628591403365135, 0.012183118611574173, -0.03906366601586342, 0.0051421974785625935, 0.04959436506032944]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09955548495054245, 0.06523626297712326, 0.09176615625619888, 0.06468982249498367, 0.0989740863442421, 0.1113518625497818, 0.07318171858787537, -0.07808993011713028, 0.07376577705144882, 0.08296677470207214, 0.00046865263720974326, 0.08898339420557022, -0.15171882510185242, 0.030055459588766098, -0.018306560814380646, 0.11891242861747742]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13975296914577484, 0.18170484900474548, 0.12298083305358887, 0.14889287948608398, 0.1383492797613144, 0.13587793707847595, 0.09009517729282379, 0.19282828271389008, 0.1818677932024002, -0.3742438852787018, -0.035749293863773346, 0.11424152553081512, 0.14356562495231628, -0.16726365685462952, -0.06160975247621536, -0.06402838230133057]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08582726866006851, 0.10968364775180817, 0.10422276705503464, 0.08459780365228653, 0.03884551674127579, -0.04848787561058998, 0.06092902645468712, 0.05734459310770035, -0.029355177655816078, 0.03365291655063629, -0.11563702672719955, 0.16894924640655518, -0.07155165076255798, -0.18221551179885864, 0.1195642277598381, 0.001986204646527767]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07329906523227692, 0.11999192833900452, 0.05880771577358246, 0.06254327297210693, -0.017664212733507156, 0.010907073505222797, 0.150421142578125, 0.07302252948284149, 0.08124507963657379, -0.07516946643590927, -0.05557433143258095, 0.08530460298061371, -0.13895922899246216, 0.11699733138084412, 0.13420195877552032, 0.056068193167448044]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.07738904654979706, 0.06682347506284714, 0.15869209170341492, 0.35201549530029297, 0.16989704966545105, 0.11957287788391113, -0.07128451019525528, -0.0282841008156538, 0.4753352999687195, 0.15262365341186523, -0.47453194856643677, 0.358612984418869, 0.194121852517128, -0.09208064526319504, 0.10551885515451431, 0.1584433913230896]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.2902527451515198, 0.2899937331676483, 0.31848689913749695, 0.12025659531354904, -0.8594260215759277, 0.3403279185295105, 0.18798531591892242, -0.08757998049259186, 0.337465763092041, 0.18457350134849548, -0.012422020547091961, 0.288775771856308, 0.39634954929351807, 0.1089048832654953, -0.34808576107025146, -0.04722663387656212]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.1910512000322342, 0.31898611783981323, -0.06562964618206024, 0.3414229154586792, 0.5225593447685242, 0.2939601242542267, 0.30008354783058167, -0.14537298679351807, 0.1416858285665512, 0.5642800331115723, -0.01390417292714119, 0.47791528701782227, 0.3305846154689789, -0.3008689284324646, -0.15531352162361145, 0.2632426917552948]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4661518931388855, 0.2188873291015625, 0.5205733776092529, 0.6538413763046265, 0.44822919368743896, 0.4158634841442108, 0.7278397083282471, 0.5633590221405029, 0.4392845928668976, 0.18776531517505646, 0.5986173152923584, 0.7374140024185181, 1.0133264064788818, 0.5966143608093262, 0.772096574306488, 0.744504988193512]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0132473707199097, 0.8706418871879578, 0.6766667366027832, 0.8926072716712952, 1.0430598258972168, 0.6605743765830994, 0.6631278395652771, 0.6632972955703735, 0.4079482853412628, 0.9568116068840027, 0.737424910068512, 0.9135701060295105, 0.7165588736534119, 0.25489693880081177, 0.566094696521759, 0.49346786737442017]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0654151439666748, 1.0867364406585693, 0.9942426681518555, 1.3252426385879517, 1.442805528640747, 0.7574116587638855, 0.9594143629074097, 1.421321153640747, 1.0222277641296387, 1.2659966945648193, 0.9966294169425964, 1.1497055292129517, 0.8003312349319458, 0.533913254737854, -0.01703336276113987, 0.8877463340759277]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.42765331268310547, 1.0039973258972168, 1.2103253602981567, 1.2485716342926025, 0.5294421911239624, 0.8000435829162598, 1.0147085189819336, 1.159627914428711, 0.983860194683075, 0.7639262676239014, 0.6981583833694458, 0.8895901441574097, 1.161307692527771, 0.8723108172416687, 0.6759215593338013, 1.0244868993759155]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7548372745513916, 0.8691256046295166, 0.771614670753479, 0.5961148738861084, 1.1780550479888916, 0.30264875292778015, 1.1258599758148193, 2.1675024032592773, 0.9616261720657349, 0.9183039665222168, 1.1500334739685059, 1.304264783859253, -0.14399924874305725, 0.9867343902587891, 1.1225786209106445, 0.7937785983085632]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6146450042724609, 0.7595738768577576, 0.8619658350944519, 0.7649672627449036, 0.8320603966712952, 0.5551704168319702, 0.4833492338657379, 0.8920766115188599, 0.6801721453666687, 0.9495594501495361, 0.19286198914051056, 0.6753804087638855, 0.9879350662231445, 0.9654777646064758, 1.029296875, 0.6158757209777832]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.3725923001766205, 1.1533931493759155, 0.9634809494018555, 0.8094609975814819, 0.5912867188453674, 0.7960660457611084, 0.9842420220375061, 0.9906133413314819, 0.8531658053398132, 0.9140042066574097, 0.6870846152305603, -0.46018368005752563, 0.7212257981300354, 0.7815560698509216, 1.062252163887024, 1.3537297248840332]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7973232269287109, 0.2742649018764496, 0.8417258262634277, 0.7325475811958313, 0.6984503269195557, 0.5092545747756958, 0.39235720038414, 0.9408760666847229, -0.850653350353241, 2.497493028640747, -0.67481929063797, 0.6963236927986145, 0.927559494972229, 0.9674564003944397, 0.8338459134101868, 0.6973694562911987]
Running loglikelihood requests:  48%|███████████████████████▍                         | 241/504 [13:28<14:05,  3.21s/it]Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.1581575721502304, 0.5586807727813721, 1.3296481370925903, 1.32327139377594, 1.3036671876907349, 1.3466213941574097, 1.3993557691574097, 1.3967175483703613, 1.5638337135314941, 1.577946424484253, 1.904675841331482, 1.2987480163574219, 1.3291352987289429, 1.1647621393203735, 0.7321799993515015, 1.4557558298110962]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3706272840499878, 1.3342467546463013, 1.4337117671966553, 1.583984375, 0.9766062498092651, 1.2562237977981567, 1.7696187496185303, 1.6914645433425903, 1.8899545669555664, 1.3325560092926025, 1.1486088037490845, 1.0449601411819458, 1.2640800476074219, 1.5297942161560059, 1.4766499996185303, 1.431815505027771]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.280871033668518, 1.2245656251907349, 1.0922341346740723, 1.180314302444458, 1.3851737976074219, 0.9143631458282471, 0.9934118390083313, 1.1825006008148193, 1.7311683893203735, 1.4000991582870483, 1.3615612983703613, 1.3443913459777832, 2.020143508911133, 1.3192048072814941, 1.2079203128814697, 1.2554949522018433]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8426188230514526, 1.0580106973648071, 1.055598497390747, 0.89007568359375, 0.08936361223459244, 1.1533713340759277, 1.2030229568481445, 1.892129898071289, 0.8653637170791626, 1.167793869972229, 1.2831010818481445, 1.0561085939407349, 1.0712015628814697, 1.4563608169555664, 1.1561752557754517, 0.500184953212738]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.049790143966675, 2.142228364944458, 2.0076959133148193, 1.975411057472229, 1.8756704330444336, 1.9969682693481445, 1.9093983173370361, 2.157153606414795, 1.9173566102981567, 2.0294134616851807, 2.3137826919555664, 2.760406970977783, 1.849230408668518, 2.368644952774048, 2.207526922225952, 2.0755014419555664]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4149079322814941, 1.2397679090499878, 1.713240385055542, 1.0376341342926025, 1.455078125, 1.5125641822814941, 1.3251224756240845, 1.3424965143203735, 1.433389663696289, 1.59567391872406, 1.4470324516296387, 2.224113702774048, 1.5055387020111084, 1.5262651443481445, 1.2174382209777832, 1.6301597356796265]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4699888229370117, 1.7397388219833374, 1.2778137922286987, 1.458211898803711, 1.4306057691574097, 1.4938199520111084, 1.3857421875, 1.4692455530166626, 1.4710383415222168, 1.3696143627166748, 1.5492945909500122, 1.4468501806259155, 1.4234929084777832, 0.9805470705032349, 1.5139050483703613, 1.5158581733703613]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.27740216255188, 2.345848798751831, 2.1866836547851562, 2.188199520111084, 2.864388942718506, 2.349463701248169, 2.729769229888916, 2.241079807281494, 2.27734375, 2.3304569721221924, 2.506063461303711, 2.264575481414795, 2.2623016834259033, 2.3940649032592773, 2.3762826919555664, 2.40630841255188]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.5860540866851807, 1.8081855773925781, 1.4228078126907349, 1.5880072116851807, 1.4835004806518555, 1.454524278640747, 1.509138822555542, 1.8255014419555664, 1.5060780048370361, 1.491633653640747, 1.5204349756240845, 1.4051713943481445, 1.4871443510055542, 1.4018772840499878, 1.5196478366851807, 1.9138293266296387]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9696682691574097, 1.687948226928711, 1.7230206727981567, 1.7998046875, 1.7181050777435303, 1.7514055967330933, 1.6766867637634277, 1.7863999605178833, 1.7841414213180542, 1.8982088565826416, 2.123511552810669, 1.6497488021850586, 1.6890887022018433, 1.9194846153259277, 1.813338041305542, 1.7687023878097534]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6144036054611206, 1.6543333530426025, 1.6210500001907349, 1.7598694562911987, 1.6648632287979126, 1.6219755411148071, 1.719792127609253, 1.746786117553711, 1.6376224756240845, 1.7856080532073975, 1.5311334133148193, 1.7530900239944458, 1.6385990381240845, 1.7962992191314697, 1.6103880405426025, 1.6303765773773193]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.082038640975952, 2.157402276992798, 2.60546875, 2.0864477157592773, 2.155900239944458, 2.1044485569000244, 2.5463356971740723, 2.1607391834259033, 2.0366101264953613, 2.080158233642578, 1.9840598106384277, 1.9351387023925781, 2.0277810096740723, 1.9047486782073975, 2.384497880935669, 2.171670913696289]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.5946245193481445, 5.871035575866699, 5.657765865325928, 5.602874279022217, 5.5079874992370605, 5.398029327392578, 5.592000961303711, 5.994986057281494, 5.778101444244385, 5.585471153259277, 5.61508846282959, 5.4984259605407715, 5.660913944244385, 5.672166347503662, 5.76183557510376, 5.570603847503662]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.190123558044434, 3.9848413467407227, 3.9938783645629883, 3.8210813999176025, 4.024224758148193, 4.006383895874023, 3.824911117553711, 4.0086870193481445, 4.016149520874023, 4.110890865325928, 3.889167547225952, 3.7271158695220947, 4.065677642822266, 3.9860949516296387, 4.08996057510376, 4.146761417388916]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1989855766296387, 3.288712739944458, 3.091592788696289, 2.8910913467407227, 3.0391790866851807, 3.2192747592926025, 3.040112018585205, 3.053025960922241, 3.084130048751831, 3.157357692718506, 3.2385144233703613, 2.671044111251831, 3.157590866088867, 3.124242067337036, 3.2944846153259277, 3.1954290866851807]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09012319147586823, 0.10994891077280045, 0.10304482281208038, -0.22113674879074097, -0.20205266773700714, -0.1423102468252182, 0.1228349581360817, -0.17763598263263702, 0.0811844989657402, 0.09457425773143768, 0.09765932708978653, 0.08649490028619766, 0.09391824156045914, 0.10184495896100998, -1.0522825717926025, 0.11860042065382004]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09644431620836258, 0.06800973415374756, 0.04306650906801224, 0.042458876967430115, 0.0859394371509552, 0.032104406505823135, 0.053969454020261765, 0.061830662190914154, 0.00918206200003624, 0.06870411336421967, -0.16390672326087952, 0.05726589262485504, 0.0066469390876591206, -0.031346820294857025, 0.008666208945214748, 0.052624545991420746]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09987088292837143, 0.05697169154882431, 0.09241630882024765, 0.06395932286977768, 0.10024921596050262, 0.10950834304094315, 0.06607607752084732, -0.07401742786169052, 0.07454299926757812, 0.08544130623340607, 0.001236488576978445, 0.08876870572566986, -0.14159700274467468, 0.030820714309811592, -0.01738169603049755, 0.11365880817174911]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.14262880384922028, 0.18183466792106628, 0.12304504960775375, 0.14348818361759186, 0.1449277549982071, 0.12688098847866058, 0.06596124172210693, 0.18862220644950867, 0.1788714975118637, -0.36396539211273193, -0.030520766973495483, 0.11777690052986145, 0.15065252780914307, -0.16241317987442017, -0.052415989339351654, -0.05771784856915474]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08285095542669296, 0.1052578017115593, 0.10195489972829819, 0.08756984770298004, 0.04215149208903313, -0.031577322632074356, 0.06047319993376732, 0.05719449743628502, -0.043051306158304214, 0.024986494332551956, -0.1252155601978302, 0.1540842205286026, -0.06119070574641228, -0.16983048617839813, 0.12071444094181061, -0.010648357681930065]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07003556191921234, 0.13124346733093262, 0.06469108909368515, 0.07340365648269653, -0.03349919244647026, 0.02010980248451233, 0.14393125474452972, 0.05732419714331627, 0.10133396089076996, -0.07273062318563461, -0.10583166033029556, 0.09624902158975601, -0.143645778298378, 0.12531918287277222, 0.14535431563854218, 0.06365075707435608]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.0753556415438652, 0.0546455942094326, 0.14807823300361633, 0.35093575716018677, 0.16234122216701508, 0.11499695479869843, -0.059793274849653244, -0.022551579400897026, 0.49104464054107666, 0.15360578894615173, -0.4563257098197937, 0.3628881573677063, 0.18850935995578766, -0.07471249997615814, 0.12068016827106476, 0.17409087717533112]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.2962171137332916, 0.266216516494751, 0.3253437876701355, 0.11010238528251648, -0.8463335037231445, 0.344449520111084, 0.18265771865844727, -0.07715298980474472, 0.3481624722480774, 0.1938667893409729, 0.0004019665939267725, 0.28115320205688477, 0.39979371428489685, 0.114399753510952, -0.3089335560798645, -0.051986921578645706]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.18735003471374512, 0.30953797698020935, -0.04476393386721611, 0.33818691968917847, 0.5194000601768494, 0.2952452600002289, 0.29709818959236145, -0.133789524435997, 0.13402090966701508, 0.5888854265213013, -0.014550507999956608, 0.48010528087615967, 0.32611358165740967, -0.2814759314060211, -0.15109390020370483, 0.2780233323574066]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.47106751799583435, 0.2235715538263321, 0.5212766528129578, 0.6988871693611145, 0.45303890109062195, 0.43274974822998047, 0.7007036209106445, 0.566595733165741, 0.475689560174942, 0.20821186900138855, 0.6022675633430481, 0.7273423075675964, 1.021646499633789, 0.6333463191986084, 0.7669951319694519, 0.7606838941574097]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0153380632400513, 0.8813185095787048, 0.6796219348907471, 0.9076929688453674, 1.0618185997009277, 0.7181906700134277, 0.6750032901763916, 0.6736167669296265, 0.4231451153755188, 0.9912674427032471, 0.7573169469833374, 0.9325698018074036, 0.7444148063659668, 0.2673230469226837, 0.6278131008148193, 0.5060288310050964]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0775784254074097, 1.1129013299942017, 0.9738660454750061, 1.3660287857055664, 1.458255648612976, 0.7763261795043945, 0.9626756310462952, 1.4255480766296387, 1.0387600660324097, 1.3016995191574097, 1.023397445678711, 1.1729536056518555, 0.8163488507270813, 0.5527726411819458, -0.005712708458304405, 0.9103019833564758]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.44300955533981323, 1.008963942527771, 1.2203969955444336, 1.2901701927185059, 0.5632802248001099, 0.8163119554519653, 1.0599474906921387, 1.20339834690094, 0.9956243634223938, 0.7859519720077515, 0.7150951623916626, 0.8818359375, 1.181994080543518, 0.878731369972229, 0.6923573017120361, 1.0380713939666748]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8132415413856506, 0.9149078726768494, 0.83587646484375, 0.6445165872573853, 1.233602523803711, 0.354765921831131, 1.1474026441574097, 2.2351038455963135, 1.01461923122406, 0.9717598557472229, 1.2256369590759277, 1.3798390626907349, -0.08501172065734863, 1.0417498350143433, 1.1397459506988525, 0.8529362678527832]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6845101714134216, 0.7999627590179443, 0.9013088941574097, 0.8251042366027832, 0.870306670665741, 0.6591855883598328, 0.5736960768699646, 0.9880097508430481, 0.6843225359916687, 1.0148252248764038, 0.21837513148784637, 0.7311118841171265, 1.0696601867675781, 1.0191668272018433, 1.1127128601074219, 0.6744065880775452]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.41807830333709717, 1.2321594953536987, 1.0126953125, 0.806341826915741, 0.6465936899185181, 0.8561829328536987, 1.0650416612625122, 1.039266586303711, 0.8885261416435242, 0.9544367790222168, 0.7310481071472168, -0.40555810928344727, 0.7809315919876099, 0.8366954326629639, 1.1294746398925781, 1.4314948320388794]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8527773022651672, 0.34356677532196045, 0.8717368841171265, 0.768190324306488, 0.7052439451217651, 0.5699945688247681, 0.4364842176437378, 0.9738842248916626, -0.7773538827896118, 2.6075093746185303, -0.6207858324050903, 0.7409504055976868, 0.9403495192527771, 0.9981853365898132, 0.8612334132194519, 0.7814670205116272]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.21878643333911896, 0.6363197565078735, 1.4054265022277832, 1.399793028831482, 1.3752769231796265, 1.41403329372406, 1.4914441108703613, 1.45703125, 1.6545957326889038, 1.6626924276351929, 2.0057573318481445, 1.3637768030166626, 1.3856146335601807, 1.1635960340499878, 0.79536372423172, 1.5432602167129517]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4233033657073975, 1.3940064907073975, 1.4595892429351807, 1.640829086303711, 1.0640158653259277, 1.3483996391296387, 1.85745108127594, 1.7642840147018433, 1.990861177444458, 1.4078824520111084, 1.2357122898101807, 1.1306753158569336, 1.35498046875, 1.598549723625183, 1.5623542070388794, 1.4984550476074219]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3368120193481445, 1.2773072719573975, 1.1471694707870483, 1.225206971168518, 1.4365962743759155, 0.9633679986000061, 1.0578285455703735, 1.2447819709777832, 1.7929250001907349, 1.4007987976074219, 1.409427523612976, 1.3816901445388794, 2.130159854888916, 1.3789645433425903, 1.259168028831482, 1.3014808893203735]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8602631688117981, 1.094617247581482, 1.0866152048110962, 0.919069230556488, 0.1267622709274292, 1.1748411655426025, 1.2381209135055542, 1.9424155950546265, 0.8603715896606445, 1.1833022832870483, 1.3115962743759155, 1.0822863578796387, 1.1011252403259277, 1.4874067306518555, 1.181308627128601, 0.5243266224861145]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  49%|███████████████████████▊                         | 245/504 [13:41<13:53,  3.22s/it]Layer: gate_21 - Captured router_logits: [2.1233675479888916, 2.237581729888916, 2.091447114944458, 2.067018508911133, 1.9737931489944458, 2.0821187496185303, 2.0035274028778076, 2.243936538696289, 2.0060343742370605, 2.125349760055542, 2.4071829319000244, 2.854069471359253, 1.9397737979888916, 2.469158172607422, 2.293930768966675, 2.1746151447296143]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.492537260055542, 1.320866346359253, 1.786555528640747, 1.1218079328536987, 1.5299382209777832, 1.5729069709777832, 1.3913100957870483, 1.3988747596740723, 1.5149545669555664, 1.673624038696289, 1.5206389427185059, 2.30462908744812, 1.5774253606796265, 1.579728364944458, 1.295300841331482, 1.6973822116851807]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5330281257629395, 1.7889459133148193, 1.330518126487732, 1.4938491582870483, 1.4632112979888916, 1.5487406253814697, 1.4208256006240845, 1.5142840147018433, 1.4954451322555542, 1.4027955532073975, 1.5942747592926025, 1.4815000295639038, 1.4612436294555664, 1.030233383178711, 1.5575261116027832, 1.5534340143203735]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3325560092926025, 2.40625, 2.230060577392578, 2.2533233165740967, 2.920767307281494, 2.4110891819000244, 2.772796154022217, 2.3044543266296387, 2.329524278640747, 2.385727643966675, 2.543260335922241, 2.329465866088867, 2.2835237979888916, 2.453474760055542, 2.435051202774048, 2.461229085922241]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6287896633148193, 1.8396979570388794, 1.4663304090499878, 1.633963942527771, 1.5167618989944458, 1.4925227165222168, 1.5498775243759155, 1.8661381006240845, 1.5410739183425903, 1.5258861780166626, 1.552486538887024, 1.4439715147018433, 1.5292385816574097, 1.4347597360610962, 1.5619752407073975, 1.9749300479888916]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.999795913696289, 1.7144155502319336, 1.757608413696289, 1.8394502401351929, 1.7534207105636597, 1.7839500904083252, 1.7058998346328735, 1.823886752128601, 1.8215514421463013, 1.9303433895111084, 2.167757511138916, 1.6768598556518555, 1.7184293270111084, 1.9366109371185303, 1.8367902040481567, 1.8010919094085693]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6286438703536987, 1.672990083694458, 1.6425708532333374, 1.7842234373092651, 1.6796993017196655, 1.6405739784240723, 1.7490161657333374, 1.76657235622406, 1.654012680053711, 1.798587679862976, 1.5558826923370361, 1.7687441110610962, 1.6532037258148193, 1.8266091346740723, 1.624621033668518, 1.6563993692398071]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.117202043533325, 2.207836627960205, 2.6428258419036865, 2.1276965141296387, 2.193213701248169, 2.150040864944458, 2.607574939727783, 2.2020318508148193, 2.0774471759796143, 2.118455648422241, 2.0263097286224365, 1.985526442527771, 2.0622668266296387, 1.9521338939666748, 2.4200384616851807, 2.2078256607055664]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.6188201904296875, 5.890041828155518, 5.663012981414795, 5.605935096740723, 5.5218048095703125, 5.418376922607422, 5.600163459777832, 5.996851444244385, 5.788129806518555, 5.590601444244385, 5.6209187507629395, 5.520872116088867, 5.682311058044434, 5.688899040222168, 5.781716346740723, 5.595265865325928]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.175198078155518, 3.9894473552703857, 4.00011682510376, 3.8283722400665283, 4.009299278259277, 4.0010786056518555, 3.829509735107422, 4.013234615325928, 4.005946636199951, 4.0974812507629395, 3.886368989944458, 3.7329516410827637, 4.060022354125977, 3.9769413471221924, 4.093400001525879, 4.127725601196289]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1742653846740723, 3.2775769233703613, 3.079582452774048, 2.8833372592926025, 3.0325326919555664, 3.2149603366851807, 3.0345733165740967, 3.039966106414795, 3.0793492794036865, 3.1356401443481445, 3.237931489944458, 2.6532328128814697, 3.1542093753814697, 3.111881971359253, 3.2859725952148438, 3.166861057281494]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.08989453315734863, 0.11037115007638931, 0.10468827188014984, -0.222107395529747, -0.19871759414672852, -0.13582776486873627, 0.12191769480705261, -0.1810648888349533, 0.07898940145969391, 0.09533366560935974, 0.09830019623041153, 0.08886449784040451, 0.09469587355852127, 0.1016131192445755, -1.0398714542388916, 0.11869379132986069]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09575948864221573, 0.06602690368890762, 0.04306463152170181, 0.04225064441561699, 0.08532367646694183, 0.030970104038715363, 0.05660122632980347, 0.06570722162723541, 0.005814851261675358, 0.0712018683552742, -0.16497290134429932, 0.05580850690603256, 0.006497567985206842, -0.030560508370399475, 0.009189107455313206, 0.04825325682759285]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09929805248975754, 0.053204894065856934, 0.09069952368736267, 0.06394437700510025, 0.09612410515546799, 0.10881771147251129, 0.06700947135686874, -0.0740431621670723, 0.07692775130271912, 0.09087622165679932, 0.0010292280931025743, 0.09054349362850189, -0.14555609226226807, 0.03289991244673729, -0.017028694972395897, 0.10995733737945557]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.14322486519813538, 0.17954425513744354, 0.12209673225879669, 0.14781974256038666, 0.14363689720630646, 0.11886715143918991, 0.05676497146487236, 0.19133906066417694, 0.17835770547389984, -0.36069455742836, -0.024476464837789536, 0.12478774040937424, 0.14766214787960052, -0.17703747749328613, -0.04461487755179405, -0.05018570274114609]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08022789657115936, 0.11009449511766434, 0.10286285728216171, 0.08686914294958115, 0.041156940162181854, -0.03259231895208359, 0.045509222894907, 0.05794684588909149, -0.04596038535237312, 0.017638875171542168, -0.11617211997509003, 0.16191761195659637, -0.06250569224357605, -0.17219799757003784, 0.1270812302827835, -0.01670450158417225]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.0755927786231041, 0.1255800575017929, 0.0715702697634697, 0.08774589747190475, -0.03727653995156288, 0.012545173056423664, 0.153533935546875, 0.05542800575494766, 0.10888057202100754, -0.07030156999826431, -0.12274853140115738, 0.09517344832420349, -0.12865300476551056, 0.125122532248497, 0.14000952243804932, 0.06780072301626205]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.07558520883321762, 0.043726108968257904, 0.15400126576423645, 0.3487466871738434, 0.16914698481559753, 0.11452421545982361, -0.059277378022670746, -0.022093018516898155, 0.5030364990234375, 0.14950698614120483, -0.4545818865299225, 0.36108750104904175, 0.1900634765625, -0.082855224609375, 0.12110445648431778, 0.17813441157341003]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.30254557728767395, 0.26608753204345703, 0.3252408504486084, 0.11770512908697128, -0.858223557472229, 0.34787794947624207, 0.1843111366033554, -0.08627364784479141, 0.3487812578678131, 0.18549996614456177, 0.00035117988591082394, 0.29010167717933655, 0.3918536603450775, 0.11673258244991302, -0.29729223251342773, -0.046995703130960464]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.17780320346355438, 0.3117331862449646, -0.055905357003211975, 0.3491731286048889, 0.5293460488319397, 0.28641611337661743, 0.30022281408309937, -0.13569140434265137, 0.14558592438697815, 0.5835466384887695, -0.011662269942462444, 0.4828835129737854, 0.33504384756088257, -0.2667391300201416, -0.14209257066249847, 0.27749326825141907]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4688447415828705, 0.22303111851215363, 0.5222914814949036, 0.6857727766036987, 0.4523552358150482, 0.43834128975868225, 0.6993918418884277, 0.5620882511138916, 0.45808297395706177, 0.20805655419826508, 0.602195143699646, 0.7422166466712952, 1.0188181400299072, 0.6480867862701416, 0.773561418056488, 0.7435649037361145]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9906643629074097, 0.8668959736824036, 0.702272355556488, 0.904486358165741, 1.0700063705444336, 0.6925895810127258, 0.67347651720047, 0.6707381010055542, 0.44955307245254517, 0.9934282302856445, 0.7532649040222168, 0.945381760597229, 0.762139618396759, 0.2859952449798584, 0.5970718860626221, 0.5004622936248779]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0731911659240723, 1.107547640800476, 0.996880829334259, 1.341408371925354, 1.4554789066314697, 0.770919144153595, 0.9594435095787048, 1.4366692304611206, 1.0503840446472168, 1.264888882637024, 1.0137265920639038, 1.159952163696289, 0.8115352988243103, 0.5484493970870972, -0.000342525658197701, 0.89771968126297]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.43020448088645935, 0.9898298978805542, 1.2319117784500122, 1.2602903842926025, 0.5296480655670166, 0.8276148438453674, 1.0115128755569458, 1.1764918565750122, 0.990020751953125, 0.7585293054580688, 0.6943942308425903, 0.8725658655166626, 1.1733835935592651, 0.8968414664268494, 0.6797257661819458, 1.008235216140747]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.802614152431488, 0.898048996925354, 0.8016554713249207, 0.6011605262756348, 1.2023816108703613, 0.33841991424560547, 1.1265158653259277, 2.1903276443481445, 0.9837336540222168, 0.9349930286407471, 1.1930387020111084, 1.3001399040222168, -0.12253399938344955, 1.0186147689819336, 1.1131541728973389, 0.8294714093208313]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6201900839805603, 0.7538816332817078, 0.8675482273101807, 0.7813719511032104, 0.8511908054351807, 0.5757120847702026, 0.53517085313797, 0.928436279296875, 0.689577043056488, 0.9507673978805542, 0.1906164437532425, 0.6605607271194458, 0.9907262921333313, 0.9673434495925903, 1.0368469953536987, 0.6145538687705994]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.3938329815864563, 1.1653305292129517, 0.9849434494972229, 0.8088487982749939, 0.6209990382194519, 0.8202104568481445, 1.020003080368042, 1.0055679082870483, 0.8500466346740723, 0.9266557693481445, 0.6968502402305603, -0.42626407742500305, 0.7457544207572937, 0.7877361178398132, 1.0774545669555664, 1.3781191110610962]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8018090724945068, 0.32694119215011597, 0.8319455981254578, 0.7316530346870422, 0.6949781775474548, 0.5178586840629578, 0.3673027455806732, 0.9294797778129578, -0.7900791168212891, 2.5378382205963135, -0.6455643177032471, 0.7182913422584534, 0.9114097356796265, 0.9666001200675964, 0.8324757814407349, 0.7173042893409729]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.15730491280555725, 0.5606896877288818, 1.333736538887024, 1.3181042671203613, 1.3150215148925781, 1.3643015623092651, 1.405069351196289, 1.371895432472229, 1.5730963945388794, 1.5762046575546265, 1.9142956733703613, 1.2766878604888916, 1.3312004804611206, 1.150499939918518, 0.7362188100814819, 1.4435051679611206]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3824043273925781, 1.3409514427185059, 1.4340602159500122, 1.6142140626907349, 1.0091242790222168, 1.3165665864944458, 1.790840744972229, 1.7342584133148193, 1.9393365383148193, 1.368761658668518, 1.1722904443740845, 1.0699117183685303, 1.3043705224990845, 1.5503339767456055, 1.4979593753814697, 1.4393948316574097]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.2941347360610962, 1.2394036054611206, 1.0922633409500122, 1.1847597360610962, 1.3935401439666748, 0.9241852164268494, 1.0122580528259277, 1.1972874402999878, 1.7381501197814941, 1.3916453123092651, 1.3754955530166626, 1.3448286056518555, 2.0639283657073975, 1.3306902647018433, 1.2296816110610962, 1.2653043270111084]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8248156905174255, 1.1057748794555664, 1.0787371397018433, 0.8958649039268494, 0.10698437690734863, 1.160453200340271, 1.224609375, 1.909806489944458, 0.8592720627784729, 1.176932692527771, 1.2970061302185059, 1.0806975364685059, 1.07321298122406, 1.45510733127594, 1.161450743675232, 0.49994397163391113]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0685925483703613, 2.1765682697296143, 2.0385377407073975, 2.0121850967407227, 1.9052879810333252, 2.0347774028778076, 1.9357800483703613, 2.1684350967407227, 1.943359375, 2.0501983165740967, 2.3456156253814697, 2.7950093746185303, 1.88088858127594, 2.4061334133148193, 2.236736297607422, 2.1191697120666504]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4466242790222168, 1.2891207933425903, 1.7302355766296387, 1.079903244972229, 1.4852787256240845, 1.539266586303711, 1.3546087741851807, 1.3757579326629639, 1.4846081733703613, 1.62698233127594, 1.4730644226074219, 2.245452404022217, 1.5277519226074219, 1.5465835332870483, 1.253731369972229, 1.6401585340499878]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4691436290740967, 1.752550721168518, 1.2892574071884155, 1.4449043273925781, 1.446886658668518, 1.5165578126907349, 1.3870248794555664, 1.482217788696289, 1.4781184196472168, 1.367391586303711, 1.5587395429611206, 1.4461077451705933, 1.4399341344833374, 1.0000118017196655, 1.5253323316574097, 1.5159456729888916]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.305445432662964, 2.392432451248169, 2.2025420665740967, 2.2241721153259277, 2.8923158645629883, 2.3850862979888916, 2.743586778640747, 2.279151201248169, 2.3261427879333496, 2.367828845977783, 2.529151201248169, 2.3020639419555664, 2.256821393966675, 2.4398903846740723, 2.4067163467407227, 2.4412312507629395]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  49%|████████████████████████▏                        | 249/504 [13:54<13:38,  3.21s/it]Layer: gate_25 - Captured router_logits: [1.6070137023925781, 1.8113923072814941, 1.4438258409500122, 1.6061391830444336, 1.499825119972229, 1.4855701923370361, 1.5242682695388794, 1.839289903640747, 1.520792007446289, 1.5022737979888916, 1.532889723777771, 1.40426766872406, 1.5041977167129517, 1.421700119972229, 1.532999038696289, 1.9461432695388794]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.96823251247406, 1.6756008863449097, 1.7200180292129517, 1.8020857572555542, 1.7358945608139038, 1.7560925483703613, 1.675890564918518, 1.7963066101074219, 1.7904480695724487, 1.9008206129074097, 2.14422869682312, 1.6475739479064941, 1.6778218746185303, 1.8959742784500122, 1.8017213344573975, 1.7788923978805542]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6255757808685303, 1.6646318435668945, 1.6353504657745361, 1.7730457782745361, 1.6763060092926025, 1.6309140920639038, 1.7335114479064941, 1.7520861625671387, 1.64871346950531, 1.7890552282333374, 1.5472831726074219, 1.7688899040222168, 1.6491516828536987, 1.8000068664550781, 1.6158757209777832, 1.6492719650268555]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.126636028289795, 2.2133569717407227, 2.6551709175109863, 2.1391091346740723, 2.193049669265747, 2.1580684185028076, 2.616006851196289, 2.208895206451416, 2.089289903640747, 2.1275415420532227, 2.0347774028778076, 1.9870787858963013, 2.0694525241851807, 1.9579932689666748, 2.4348034858703613, 2.207395553588867]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.626340866088867, 5.891266345977783, 5.682136058807373, 5.637943267822266, 5.541569709777832, 5.426888942718506, 5.60949182510376, 6.034689903259277, 5.8023552894592285, 5.611415386199951, 5.66604471206665, 5.534456729888916, 5.70324182510376, 5.687091827392578, 5.789528846740723, 5.619344711303711]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.203066825866699, 4.011427402496338, 4.029355049133301, 3.862151622772217, 4.0343546867370605, 4.0115437507629395, 3.860132932662964, 4.050431251525879, 4.024993419647217, 4.126719951629639, 3.91986346244812, 3.76714825630188, 4.09225606918335, 4.017038822174072, 4.114476680755615, 4.1646857261657715]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1553754806518555, 3.254897356033325, 3.056028366088867, 2.866546154022217, 3.011019229888916, 3.175635576248169, 3.034398317337036, 3.030841827392578, 3.04693341255188, 3.1166045665740967, 3.2248716354370117, 2.6609432697296143, 3.131063461303711, 3.086695432662964, 3.261019229888916, 3.1454057693481445]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09098292142152786, 0.11225589364767075, 0.10886593908071518, -0.2522079646587372, -0.22579090297222137, -0.13794748485088348, 0.12585687637329102, -0.1559811532497406, 0.07846211642026901, 0.09450479596853256, 0.1004677414894104, 0.09868165105581284, 0.09324731677770615, 0.10687301307916641, -1.0959362983703613, 0.12223394960165024]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0848216712474823, 0.0561148077249527, 0.03659379482269287, 0.03708801791071892, 0.08481905609369278, 0.02431015484035015, 0.05388977378606796, 0.06761466711759567, 0.012656083330512047, 0.06738007813692093, -0.1863671988248825, 0.05367711931467056, 0.00985985342413187, -0.04223325476050377, 0.013313463889062405, 0.04171659052371979]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09490353614091873, 0.06234040856361389, 0.09101719409227371, 0.06547923386096954, 0.10002022236585617, 0.10699178278446198, 0.07387850433588028, -0.08695585280656815, 0.07630110532045364, 0.08839353919029236, 0.0025338700506836176, 0.08552662283182144, -0.1655009239912033, 0.03270459547638893, -0.0162811279296875, 0.1127568706870079]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13183581829071045, 0.16668017208576202, 0.1238478496670723, 0.13400666415691376, 0.13428428769111633, 0.12413950264453888, 0.06257857382297516, 0.18575093150138855, 0.18277546763420105, -0.40975815057754517, -0.013737920671701431, 0.10416705906391144, 0.16017641127109528, -0.18341656029224396, -0.038729939609766006, -0.05657913535833359]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07631415873765945, 0.10564234852790833, 0.08905941992998123, 0.12266381084918976, 0.04389253258705139, -0.058081042021512985, 0.04622393846511841, 0.04258722439408302, -0.06451950967311859, 0.034061260521411896, -0.12874045968055725, 0.16427293419837952, -0.039172157645225525, -0.18998005986213684, 0.1244468092918396, -0.0050814044661819935]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06907425820827484, 0.12957438826560974, 0.06734272837638855, 0.10943654924631119, -0.03466939181089401, -0.006405503023415804, 0.12253325432538986, 0.03175775334239006, 0.11456799507141113, -0.07954555004835129, -0.10399935394525528, 0.10078464448451996, -0.16201166808605194, 0.13467134535312653, 0.13411928713321686, 0.06225116178393364]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.07827331870794296, 0.05397341027855873, 0.18094509840011597, 0.34195947647094727, 0.18637096881866455, 0.13975000381469727, -0.07489082217216492, -0.0624837726354599, 0.4760787785053253, 0.15109685063362122, -0.47415754199028015, 0.34829744696617126, 0.20380185544490814, -0.10108526796102524, 0.16633902490139008, 0.19238919019699097]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.35218605399131775, 0.2661947011947632, 0.30606135725975037, 0.1293681114912033, -0.8664072751998901, 0.33431532979011536, 0.17361928522586823, -0.16607984900474548, 0.4194850027561188, 0.20658355951309204, -0.05271501839160919, 0.2855459153652191, 0.35072052478790283, 0.14541694521903992, -0.3842964768409729, -0.04040868952870369]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.18620550632476807, 0.3395860493183136, -0.11710619926452637, 0.3361823260784149, 0.5470444560050964, 0.30471324920654297, 0.27547717094421387, -0.16991230845451355, 0.1607053428888321, 0.6747985482215881, -0.003974003717303276, 0.4289337396621704, 0.35891449451446533, -0.34681040048599243, -0.17864443361759186, 0.3069348633289337]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.47279471158981323, 0.24812044203281403, 0.43085765838623047, 0.8124434947967529, 0.4212459623813629, 0.4546992778778076, 0.7226125001907349, 0.5473122596740723, 0.5631567239761353, 0.15187722444534302, 0.5445601940155029, 0.7000495791435242, 0.9003892540931702, 0.5968655347824097, 0.7317896485328674, 0.705078125]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9671230316162109, 0.8420482873916626, 0.6194030046463013, 0.8616196513175964, 1.0669965744018555, 0.6923267841339111, 0.6553882360458374, 0.6730483174324036, 0.4127853214740753, 0.933670699596405, 0.7418449521064758, 0.9199856519699097, 0.5031974911689758, 0.2651292085647583, 0.7381614446640015, 0.493537575006485]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0548696517944336, 1.0848078727722168, 0.9473385214805603, 1.2703920602798462, 1.410957932472229, 0.8292655348777771, 0.9090011119842529, 1.3460237979888916, 1.0413107872009277, 1.361568570137024, 0.9525565505027771, 1.1512213945388794, 0.7976094484329224, 0.5253767371177673, 0.020086487755179405, 0.8589887619018555]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.3507307767868042, 1.0006119012832642, 1.231510877609253, 1.2014050483703613, 0.5027984976768494, 0.8218684196472168, 1.0181337594985962, 1.2219456434249878, 1.0156545639038086, 0.7122429013252258, 0.7399433255195618, 0.8971257209777832, 1.1218516826629639, 0.8306319713592529, 0.6164168119430542, 1.0038187503814697]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8309376239776611, 0.9216782450675964, 0.9137081503868103, 0.6416234374046326, 1.2341563701629639, 0.32293426990509033, 1.1376953125, 2.344712018966675, 1.0026745796203613, 0.978391706943512, 1.2816144227981567, 1.298201322555542, -0.007713659666478634, 1.0533329248428345, 1.174003005027771, 0.8203024864196777]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7530444860458374, 0.8175249099731445, 0.9395697116851807, 0.8709967136383057, 0.9164674878120422, 0.9245395660400391, 0.5900113582611084, 1.262354850769043, 0.8008723258972168, 1.0575224161148071, 0.2834126353263855, 0.8099547624588013, 1.128763198852539, 1.0712015628814697, 1.1220848560333252, 0.7238897085189819]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4641987681388855, 1.1557835340499878, 1.0364680290222168, 0.8731926083564758, 0.6447316408157349, 0.9063811898231506, 1.210737943649292, 1.0665228366851807, 0.9333459734916687, 0.9809934496879578, 0.7373247146606445, -0.326506644487381, 0.8520171046257019, 0.8637476563453674, 1.0631850957870483, 1.545885682106018]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7927446365356445, 0.31658935546875, 0.8561920523643494, 0.7516615986824036, 0.6107418537139893, 0.5375748872756958, 0.39547592401504517, 0.91136234998703, -0.7309806942939758, 2.667065143585205, -0.5781468749046326, 0.7749515175819397, 0.9051277041435242, 0.9324732422828674, 0.8358427882194519, 0.6245593428611755]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.09376275539398193, 0.6344518065452576, 1.3848531246185303, 1.3283591270446777, 1.326900601387024, 1.4197760820388794, 1.4499183893203735, 1.3864855766296387, 1.737508773803711, 1.511824369430542, 2.030353546142578, 1.382673978805542, 1.400313138961792, 1.1592170000076294, 0.7843505144119263, 1.430728793144226]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3839784860610962, 1.362931489944458, 1.4099540710449219, 1.6575617790222168, 1.0135698318481445, 1.2460792064666748, 1.8250349760055542, 1.711112380027771, 2.0397109985351562, 1.3663129806518555, 1.1759825944900513, 1.0272257328033447, 1.245449185371399, 1.379325270652771, 1.4723355770111084, 1.4522212743759155]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.290767788887024, 1.2071478366851807, 1.1425343751907349, 1.1820924282073975, 1.4072703123092651, 0.9255726337432861, 0.9962723255157471, 1.1856051683425903, 1.8328547477722168, 1.4051713943481445, 1.3917326927185059, 1.3442893028259277, 1.9615496397018433, 1.3447411060333252, 1.2230644226074219, 1.2914237976074219]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8955296874046326, 1.0758804082870483, 1.0680824518203735, 0.9056569337844849, 0.12136407941579819, 1.2207176685333252, 1.1946711540222168, 1.9032073020935059, 0.859610915184021, 1.2215921878814697, 1.2802734375, 1.1246247291564941, 1.125247836112976, 1.4340602159500122, 1.1448915004730225, 0.48355966806411743]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0524721145629883, 2.185838460922241, 2.050373077392578, 1.9807311296463013, 1.9003031253814697, 2.0303170680999756, 1.930285096168518, 2.1226680278778076, 1.9176480770111084, 2.052734375, 2.3376283645629883, 2.743178606033325, 1.9002448320388794, 2.39837908744812, 2.250612258911133, 2.0990264415740967]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.443155288696289, 1.3120335340499878, 1.8272504806518555, 1.133993148803711, 1.50588858127594, 1.5507229566574097, 1.3698402643203735, 1.3889342546463013, 1.4651061296463013, 1.6612348556518555, 1.5324159860610962, 2.2572295665740967, 1.5322411060333252, 1.5390042066574097, 1.256034255027771, 1.6623717546463013]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.604623317718506, 1.8305736780166626, 1.31265127658844, 1.4925519227981567, 1.5522242784500122, 1.5404908657073975, 1.4544367790222168, 1.4997668266296387, 1.56442391872406, 1.4553112983703613, 1.6080923080444336, 1.5183870792388916, 1.4747405052185059, 1.0118753910064697, 1.6005131006240845, 1.566756010055542]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.320662260055542, 2.3664004802703857, 2.232276201248169, 2.181786298751831, 2.815648317337036, 2.334480047225952, 2.6833605766296387, 2.2437033653259277, 2.29498028755188, 2.30083966255188, 2.681494951248169, 2.2697062492370605, 2.276031970977783, 2.4405899047851562, 2.3772153854370117, 2.4240903854370117]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.62890625, 1.7687733173370361, 1.4425722360610962, 1.6343283653259277, 1.4901760816574097, 1.493586778640747, 1.5418610572814941, 1.8477437496185303, 1.5299090147018433, 1.5063549280166626, 1.5182340145111084, 1.4360133409500122, 1.5023903846740723, 1.3971548080444336, 1.6106867790222168, 1.918318510055542]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9592467546463013, 1.7392104864120483, 1.733821153640747, 1.8173390626907349, 1.7301225662231445, 1.7794134616851807, 1.7001405954360962, 1.7948052883148193, 1.797663927078247, 1.9119453430175781, 2.188257932662964, 1.6495634317398071, 1.717948317527771, 1.8864126205444336, 1.858638882637024, 1.7687195539474487]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.617741346359253, 1.6776068210601807, 1.6321401596069336, 1.7742400169372559, 1.6759816408157349, 1.623331069946289, 1.7491109371185303, 1.7328773736953735, 1.6559511423110962, 1.7637519836425781, 1.5318621397018433, 1.7541977167129517, 1.657051682472229, 1.8670954704284668, 1.6035666465759277, 1.6352224349975586]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.049414873123169, 2.139284133911133, 2.5419092178344727, 2.049659013748169, 2.1186158657073975, 2.0472187995910645, 2.5213093757629395, 2.1099143028259277, 2.022810697555542, 2.0526177883148193, 1.954728364944458, 1.891353726387024, 1.9695954322814941, 1.8649429082870483, 2.372981309890747, 2.123404026031494]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  50%|████████████████████████▌                        | 253/504 [14:07<13:23,  3.20s/it]Layer: gate_29 - Captured router_logits: [5.464202404022217, 5.782357692718506, 5.4860076904296875, 5.475104808807373, 5.368470191955566, 5.277110576629639, 5.457439422607422, 5.892082691192627, 5.6539177894592285, 5.449860095977783, 5.478253364562988, 5.368003845214844, 5.56821346282959, 5.5264692306518555, 5.640625, 5.436334133148193]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.05081033706665, 3.829786539077759, 3.8550751209259033, 3.6695430278778076, 3.923959255218506, 3.8760931491851807, 3.652839422225952, 3.8903918266296387, 3.9164092540740967, 3.9411439895629883, 3.750699520111084, 3.5554418563842773, 3.8954787254333496, 3.8617069721221924, 3.9355177879333496, 4.0027546882629395]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.173973798751831, 3.2771105766296387, 3.0936334133148193, 2.9032182693481445, 3.026993989944458, 3.20703125, 3.046525239944458, 3.0350687503814697, 3.0812149047851562, 3.135406970977783, 3.3378615379333496, 2.579735517501831, 3.164470672607422, 3.1177122592926025, 3.355177164077759, 3.210529327392578]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.0963074192404747, 0.11054567247629166, 0.10935769230127335, -0.26021355390548706, -0.2056456059217453, -0.14692489802837372, 0.12324558943510056, -0.15615494549274445, 0.09720128029584885, 0.0909038856625557, 0.0969097912311554, 0.09587796032428741, 0.09640287607908249, 0.1147015392780304, -1.0228224992752075, 0.12618552148342133]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.10267918556928635, 0.06120314076542854, 0.039752595126628876, 0.05675375461578369, 0.08212967216968536, 0.04595108702778816, 0.05162455886602402, 0.07550054788589478, 0.03523900732398033, 0.0659722164273262, -0.193329319357872, 0.0569748617708683, 0.021559474989771843, -0.0220138318836689, 0.011656345799565315, 0.018178779631853104]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0761992484331131, 0.05806417763233185, 0.08953158557415009, 0.05843207612633705, 0.12390567362308502, 0.08906956762075424, 0.08202152699232101, -0.0735781118273735, 0.08739826828241348, 0.08687428385019302, 0.009521251544356346, 0.08477091789245605, -0.16475741565227509, 0.023658111691474915, 0.00865417905151844, 0.11359702795743942]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12600195407867432, 0.161590576171875, 0.13610216975212097, 0.176118865609169, 0.11163213849067688, 0.1500300019979477, 0.06851109117269516, 0.17213451862335205, 0.17959192395210266, -0.4479668438434601, -0.036164626479148865, 0.1367141455411911, 0.20549628138542175, -0.22253522276878357, -0.024425243958830833, -0.04591253027319908]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08030477911233902, 0.0734625831246376, 0.08969081193208694, 0.1503331959247589, 0.04515448585152626, -0.05581420287489891, 0.04984988272190094, 0.05020339787006378, -0.06333073228597641, 0.018839828670024872, -0.178439661860466, 0.1130707710981369, -0.027003630995750427, -0.16118809580802917, 0.13492870330810547, 0.012996935285627842]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07374028116464615, 0.15503354370594025, 0.07144412398338318, 0.1589362472295761, -0.0748351588845253, -0.01752081699669361, 0.10094772279262543, 0.057344742119312286, 0.14495092630386353, -0.04742920771241188, -0.1590152233839035, 0.09040646255016327, -0.18324050307273865, 0.13545896112918854, 0.15749993920326233, 0.07397407293319702]
Layer: gate_5 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11180964857339859, 0.06453046947717667, 0.16089612245559692, 0.30522778630256653, 0.17230923473834991, 0.1273002326488495, -0.0753054991364479, -0.0051193819381296635, 0.2820248305797577, 0.15281350910663605, -0.4575749635696411, 0.31537726521492004, 0.19167834520339966, -0.14780385792255402, 0.19785897433757782, 0.2120981067419052]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.36030498147010803, 0.25330448150634766, 0.3032058775424957, 0.1357538402080536, -0.6063055396080017, 0.29387855529785156, 0.1502164900302887, -0.013025647960603237, 0.43969354033470154, 0.20428653061389923, -0.06382716447114944, 0.23066769540309906, 0.25833362340927124, 0.17580407857894897, -0.4299929141998291, -0.2699216902256012]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.25545844435691833, 0.2686925530433655, -0.08438809216022491, 0.28502753376960754, 0.4690304696559906, 0.43543413281440735, 0.29272064566612244, -0.26024970412254333, 0.19134536385536194, 0.8186792135238647, 0.15120679140090942, 0.13508151471614838, 0.47067609429359436, -0.4033764600753784, -0.2884717285633087, 0.35879576206207275]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.6051021218299866, 0.3754226565361023, 0.34482458233833313, 0.838988184928894, 0.4363878667354584, 0.4255467653274536, 0.6963337659835815, 0.5568530559539795, 0.7849941253662109, 0.38117748498916626, 0.3609600365161896, 0.6811038851737976, 0.8037909865379333, 0.43453747034072876, 0.6939781308174133, 0.6433534026145935]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9821535348892212, 0.8652567267417908, 0.5569956302642822, 0.7747495174407959, 0.9104357957839966, 0.6184366345405579, 0.6562350988388062, 0.6620050072669983, 0.2838870882987976, 0.7453398704528809, 0.7810859680175781, 0.9094890356063843, 0.666509747505188, 0.40395769476890564, 0.983803927898407, 0.6068273782730103]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [1.071292757987976, 1.195493221282959, 0.917715311050415, 1.4015451669692993, 1.543447732925415, 0.6261587142944336, 0.9704347848892212, 0.9566622376441956, 0.9937082529067993, 1.4435681104660034, 0.898184061050415, 1.0747629404067993, 0.5729579925537109, 0.7579094171524048, 0.27543675899505615, 0.9710273742675781]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.4877312481403351, 1.0109509229660034, 1.3324860334396362, 1.2261748313903809, 0.7761794328689575, 0.6717296242713928, 1.1617822647094727, 1.234035849571228, 0.6723777055740356, 0.4379291236400604, 0.9673187136650085, 0.8709744811058044, 1.1528432369232178, 0.8921904563903809, 0.8450620174407959, 0.9905474781990051]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.199308156967163, 1.084811806678772, 1.403551459312439, 0.9781727194786072, 1.304836630821228, 0.24972417950630188, 1.291015625, 2.2234911918640137, 1.2683683633804321, 1.1766161918640137, 1.4987773895263672, 1.489324927330017, 0.6150340437889099, 1.4054672718048096, 1.2582001686096191, 0.9787467122077942]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.9302260875701904, 0.8714366555213928, 1.0145217180252075, 0.9648288488388062, 1.0759482383728027, 1.2090719938278198, 0.7926546931266785, 1.600765347480774, 0.7768722176551819, 1.052074670791626, 0.03514902666211128, 0.9058525562286377, 1.1342477798461914, 1.1027477979660034, 1.1489835977554321, 0.7873954772949219]
Running loglikelihood requests:  51%|████████████████████████▉                        | 257/504 [14:20<13:19,  3.24s/it]Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6444646120071411, 1.2835609912872314, 1.135168194770813, 1.0429836511611938, 0.9426605105400085, 1.0721462965011597, 1.6438523530960083, 1.039957046508789, 0.9950500726699829, 1.0612177848815918, 0.9574319124221802, 0.0543031170964241, 1.219606637954712, 0.9584476351737976, 0.9970479607582092, 1.8585866689682007]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7830055952072144, 0.392424613237381, 0.8553531765937805, 0.7017794251441956, 0.6876146197319031, 0.7100522518157959, 0.6208463311195374, 0.8647945523262024, -0.4401436150074005, 2.397766590118408, -0.569566547870636, 0.7225328087806702, 0.9088889360427856, 1.0305417776107788, 0.8638701438903809, 0.7556450366973877]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.6609055995941162, 1.1770075559616089, 1.6637344360351562, 1.7151107788085938, 1.62941312789917, 1.607913851737976, 1.7283366918563843, 1.6427868604660034, 1.9035812616348267, 1.7723267078399658, 2.4914121627807617, 1.7307072877883911, 1.4973742961883545, 1.3967825174331665, 0.9863169193267822, 1.6712673902511597]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6306655406951904, 1.413913369178772, 1.628265142440796, 1.9201455116271973, 1.3610783815383911, 1.4809160232543945, 2.2039599418640137, 2.003697633743286, 2.631112813949585, 1.682550072669983, 1.5124940872192383, 1.4556819200515747, 1.6610358953475952, 1.3726757764816284, 1.8402612209320068, 1.7358659505844116]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.7655056715011597, 1.642831563949585, 1.7613757848739624, 1.618082046508789, 1.8207299709320068, 1.5180253982543945, 1.7253996133804321, 1.6345717906951904, 2.209409236907959, 1.6000864505767822, 1.8622971773147583, 1.7665493488311768, 2.326857805252075, 1.7957717180252075, 1.7561724185943604, 1.7247436046600342]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.211917757987976, 1.2206435203552246, 1.2397871017456055, 1.014657735824585, 0.4692516624927521, 1.4215768575668335, 1.330570101737976, 2.1780996322631836, 1.030838131904602, 1.392175555229187, 1.355699896812439, 1.2313987016677856, 1.2406816482543945, 1.4224416017532349, 1.1961902379989624, 0.8268092274665833]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.284052848815918, 2.4804985523223877, 2.33492374420166, 2.235269546508789, 2.211146116256714, 2.3626551628112793, 2.393845319747925, 2.4442391395568848, 2.154043436050415, 2.4332656860351562, 2.5793774127960205, 2.838561534881592, 2.1172358989715576, 2.7921040058135986, 2.5031607151031494, 2.430403232574463]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.8385615348815918, 1.6050810813903809, 2.126908302307129, 1.3938454389572144, 1.8215051889419556, 1.7746301889419556, 1.6737834215164185, 1.6316794157028198, 1.739086389541626, 1.9302541017532349, 1.7576335668563843, 2.2737953662872314, 1.8264551162719727, 1.7816972732543945, 1.6459028720855713, 1.8932490348815918]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.3796517848968506, 1.8558564186096191, 1.5154013633728027, 1.6977277994155884, 1.7262344360351562, 1.6176645755767822, 1.6430104970932007, 1.5788108110427856, 1.6032472848892212, 1.542223334312439, 1.6614980697631836, 1.5618886947631836, 1.517175555229187, 1.0798982381820679, 1.5750685930252075, 1.6830271482467651]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.54007625579834, 2.527552366256714, 2.393845319747925, 2.3317031860351562, 2.902731418609619, 2.494274854660034, 2.698115348815918, 2.475071668624878, 2.4654698371887207, 2.5802719593048096, 2.7850072383880615, 2.445908784866333, 2.4089932441711426, 2.5687618255615234, 2.564587354660034, 2.532144546508789]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7179746627807617, 1.7605856657028198, 1.5965529680252075, 1.7744215726852417, 1.4991650581359863, 1.6144441366195679, 1.6573830842971802, 1.888269305229187, 1.6632871627807617, 1.6285781860351562, 1.6204079389572144, 1.568612813949585, 1.7185710668563843, 1.5003280639648438, 1.8001550436019897, 2.057102918624878]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.8412153720855713, 1.8087185621261597, 1.7437976598739624, 1.801526665687561, 1.823413610458374, 1.795902132987976, 1.8449018001556396, 1.777448058128357, 1.7796379327774048, 1.903402328491211, 2.2380127906799316, 1.7271215915679932, 1.7875715494155884, 1.8373240232467651, 1.7875734567642212, 1.7719745635986328]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5963926315307617, 1.6756610870361328, 1.6479296684265137, 1.722723364830017, 1.6075336933135986, 1.5980420112609863, 1.754427433013916, 1.6894484758377075, 1.6662201881408691, 1.7458328008651733, 1.4945729970932007, 1.6322907209396362, 1.6551169157028198, 1.8139350414276123, 1.6513447761535645, 1.628991961479187]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.179821729660034, 2.3019442558288574, 2.505740165710449, 2.208015203475952, 2.2369096279144287, 2.186307191848755, 2.6060054302215576, 2.274123430252075, 2.1875, 2.1811187267303467, 2.2195849418640137, 2.0418953895568848, 2.11617374420166, 2.0990278720855713, 2.4095897674560547, 2.169757843017578]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.266340732574463, 5.605618000030518, 5.164181709289551, 5.19972562789917, 5.2833967208862305, 5.171159267425537, 5.239742279052734, 5.450560569763184, 5.445729732513428, 5.251908302307129, 5.260555744171143, 5.139790058135986, 5.3156609535217285, 5.300155162811279, 5.398020267486572, 5.279282093048096]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.030832767486572, 3.880009651184082, 3.8535306453704834, 3.7917089462280273, 3.8237416744232178, 3.8905653953552246, 3.7393546104431152, 3.9474594593048096, 3.9236044883728027, 4.027433395385742, 3.837845802307129, 3.6078989505767822, 3.8881053924560547, 3.859375, 3.9318344593048096, 3.968914031982422]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.944119691848755, 3.071505308151245, 2.9078006744384766, 2.7462427616119385, 2.828244209289551, 2.989593267440796, 2.835042953491211, 2.772155284881592, 2.916149854660034, 2.922173261642456, 3.114772081375122, 2.41163969039917, 3.019412040710449, 2.9045205116271973, 3.183385133743286, 2.9972269535064697]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09503579139709473, 0.11165269464254379, 0.1098547875881195, -0.25736352801322937, -0.20814256370067596, -0.15022911131381989, 0.1232340857386589, -0.1499713659286499, 0.09572436660528183, 0.09040075540542603, 0.09613142907619476, 0.09444873780012131, 0.09406955540180206, 0.11160586774349213, -1.0217623710632324, 0.12646883726119995]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.10492230951786041, 0.060960784554481506, 0.03799423202872276, 0.05757513642311096, 0.0822276771068573, 0.04739585146307945, 0.051490020006895065, 0.07648169994354248, 0.03418270871043205, 0.06717332452535629, -0.18780399858951569, 0.05717984586954117, 0.020364204421639442, -0.021839259192347527, 0.010020065121352673, 0.019433066248893738]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07707968354225159, 0.05952873080968857, 0.08805753290653229, 0.05518530309200287, 0.12486912310123444, 0.09212810546159744, 0.08256953209638596, -0.07259615510702133, 0.0824270099401474, 0.0833456739783287, 0.009046701714396477, 0.08609771728515625, -0.1618509739637375, 0.032258253544569016, 0.0030249962583184242, 0.11503483355045319]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12562960386276245, 0.16255399584770203, 0.13443104922771454, 0.177226722240448, 0.11358387023210526, 0.15305280685424805, 0.06689999252557755, 0.17015808820724487, 0.18265533447265625, -0.4491351842880249, -0.04058515280485153, 0.14188702404499054, 0.20612716674804688, -0.22160902619361877, -0.027597514912486076, -0.04662962630391121]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0793888121843338, 0.06972471624612808, 0.09144338220357895, 0.1448095440864563, 0.05022113397717476, -0.053348951041698456, 0.04722360521554947, 0.05036632716655731, -0.060917310416698456, 0.021050672978162766, -0.17711275815963745, 0.11521007865667343, -0.03223489597439766, -0.1576615571975708, 0.13560040295124054, 0.013952461071312428]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07129810005426407, 0.1424819976091385, 0.06248608976602554, 0.1523047834634781, -0.06685227900743484, -0.019243445247411728, 0.10275597125291824, 0.05843564495444298, 0.13563573360443115, -0.04822557792067528, -0.10358792543411255, 0.08132118731737137, -0.18578356504440308, 0.12593533098697662, 0.14967651665210724, 0.07058114558458328]
Layer: gate_5 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11725816130638123, 0.06236642599105835, 0.16190274059772491, 0.3117252588272095, 0.1764828860759735, 0.12880577147006989, -0.07493685185909271, -0.0047274078242480755, 0.2766803503036499, 0.15402433276176453, -0.4699716567993164, 0.30728745460510254, 0.19315092265605927, -0.16047786176204681, 0.19574174284934998, 0.20267298817634583]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3574303388595581, 0.2522965669631958, 0.3056729733943939, 0.13746607303619385, -0.6391150951385498, 0.30691811442375183, 0.16230422258377075, -0.02598736062645912, 0.42026039958000183, 0.2052222639322281, -0.07167287915945053, 0.23459942638874054, 0.2577589750289917, 0.17553265392780304, -0.4494920074939728, -0.2663644552230835]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.26476335525512695, 0.2722855806350708, -0.09726327657699585, 0.27318185567855835, 0.45486122369766235, 0.4228290319442749, 0.28169819712638855, -0.2738938629627228, 0.17825810611248016, 0.7986892461776733, 0.13606292009353638, 0.13940852880477905, 0.4583946764469147, -0.4295969009399414, -0.307481974363327, 0.3568110466003418]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.583045482635498, 0.3346785008907318, 0.3720642030239105, 0.8095691204071045, 0.4258175194263458, 0.3867487907409668, 0.6799842119216919, 0.5470181703567505, 0.7592482566833496, 0.35968393087387085, 0.3334026634693146, 0.6671274304389954, 0.8028392791748047, 0.38914841413497925, 0.6816331148147583, 0.629199206829071]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9995840191841125, 0.8625826239585876, 0.5547823309898376, 0.7624474167823792, 0.8796011209487915, 0.6299265027046204, 0.6616999506950378, 0.6637244820594788, 0.2475430965423584, 0.7231398224830627, 0.7764347791671753, 0.9118577241897583, 0.65000319480896, 0.3694455623626709, 0.9388770461082458, 0.5745736956596375]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0458495616912842, 1.1753268241882324, 0.9203275442123413, 1.3857158422470093, 1.5280085802078247, 0.5521324872970581, 0.9727914929389954, 0.9521108865737915, 0.9722505807876587, 1.4120492935180664, 0.886256754398346, 1.0360877513885498, 0.5093259215354919, 0.7121534943580627, 0.22743846476078033, 0.9505408406257629]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.4298870265483856, 0.9669321179389954, 1.3143855333328247, 1.2054086923599243, 0.7221201062202454, 0.6193425059318542, 1.1528986692428589, 1.1910043954849243, 0.5995403528213501, 0.36563703417778015, 0.9208016991615295, 0.8654447197914124, 1.0912485122680664, 0.8773888349533081, 0.8184645175933838, 0.9705829620361328]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.1112980842590332, 1.0442758798599243, 1.3318697214126587, 0.9099440574645996, 1.2298527956008911, 0.14157339930534363, 1.2623647451400757, 2.1521334648132324, 1.205153226852417, 1.1047776937484741, 1.444921851158142, 1.429146647453308, 0.5379272699356079, 1.290114164352417, 1.2245793342590332, 0.9248009324073792]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8667917251586914, 0.8163424134254456, 0.9805513620376587, 0.9027193784713745, 1.0447416305541992, 1.1350172758102417, 0.7259502410888672, 1.479089379310608, 0.7488563060760498, 0.9862380027770996, -0.03711782768368721, 0.8417423963546753, 1.0514488220214844, 1.061673641204834, 1.112376093864441, 0.7157724499702454]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6011695265769958, 1.2881910800933838, 1.108864188194275, 1.0341947078704834, 0.8757455945014954, 1.0281400680541992, 1.5832669734954834, 1.0440053939819336, 0.9766676425933838, 1.044771671295166, 0.9201509952545166, -0.016288405284285545, 1.1804403066635132, 0.9340294599533081, 0.9802433848381042, 1.768868327140808]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7195913195610046, 0.32492923736572266, 0.8170926570892334, 0.6613764762878418, 0.6883572936058044, 0.6530752182006836, 0.5599135160446167, 0.8356971144676208, -0.5505915880203247, 2.280686616897583, -0.668630838394165, 0.6675170660018921, 0.8714017271995544, 1.0201096534729004, 0.8412147164344788, 0.6632112860679626]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.542046308517456, 1.0626624822616577, 1.5893479585647583, 1.6203988790512085, 1.5553034543991089, 1.5530048608779907, 1.643118977546692, 1.568209171295166, 1.790354609489441, 1.6872258186340332, 2.4207370281219482, 1.6566106081008911, 1.4736628532409668, 1.3566330671310425, 0.861210286617279, 1.5992337465286255]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6148737668991089, 1.4014723300933838, 1.6305363178253174, 1.8772536516189575, 1.2866135835647583, 1.4404747486114502, 2.139753580093384, 1.9500601291656494, 2.5687499046325684, 1.6407151222229004, 1.4380751848220825, 1.3609243631362915, 1.5849308967590332, 1.2835111618041992, 1.7912259101867676, 1.7073918581008911]
Running loglikelihood requests:  52%|█████████████████████████▍                       | 261/504 [14:33<13:14,  3.27s/it]Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.7244291305541992, 1.6013522148132324, 1.7057541608810425, 1.565354585647583, 1.7816706895828247, 1.4563401937484741, 1.6750601530075073, 1.5862230062484741, 2.1283278465270996, 1.5819861888885498, 1.8395732641220093, 1.7522836923599243, 2.2467849254608154, 1.7501802444458008, 1.718088984489441, 1.6968449354171753]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.172582983970642, 1.177073359489441, 1.2177283763885498, 0.9780696034431458, 0.4059852063655853, 1.3787710666656494, 1.2957031726837158, 2.124072313308716, 1.0277137756347656, 1.3634164333343506, 1.3336012363433838, 1.1825542449951172, 1.1997145414352417, 1.391826868057251, 1.166941523551941, 0.7792928814888]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.2518630027770996, 2.4411659240722656, 2.3137619495391846, 2.2003605365753174, 2.1809194087982178, 2.338341236114502, 2.3292367458343506, 2.3960938453674316, 2.132512092590332, 2.3743388652801514, 2.53804087638855, 2.8079326152801514, 2.0738582611083984, 2.763401508331299, 2.4522836208343506, 2.389122486114502]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.8116285800933838, 1.577734351158142, 2.097476005554199, 1.363131046295166, 1.8074820041656494, 1.7511417865753174, 1.654807686805725, 1.607932686805725, 1.7248497009277344, 1.917578101158142, 1.7417067289352417, 2.2485578060150146, 1.801682710647583, 1.767427921295166, 1.6223558187484741, 1.8877403736114502]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.3589844703674316, 1.8428785800933838, 1.4922025203704834, 1.682031273841858, 1.6905648708343506, 1.5950120687484741, 1.6206430196762085, 1.5529747009277344, 1.5783804655075073, 1.520943522453308, 1.648978352546692, 1.5524940490722656, 1.4826021194458008, 1.0615992546081543, 1.545673131942749, 1.6608473062515259]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.4840145111083984, 2.4751803874969482, 2.3378005027770996, 2.29194712638855, 2.8510818481445312, 2.4543869495391846, 2.655468702316284, 2.426682710647583, 2.412559986114502, 2.5171875953674316, 2.7249398231506348, 2.3846755027770996, 2.358473539352417, 2.516826868057251, 2.5117788314819336, 2.47554087638855]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7092849016189575, 1.7599458694458008, 1.5847054719924927, 1.775931477546692, 1.5018179416656494, 1.600661039352417, 1.6583232879638672, 1.8798377513885498, 1.6610276699066162, 1.6330829858779907, 1.6208232641220093, 1.562650203704834, 1.7211538553237915, 1.498948335647583, 1.7619290351867676, 2.043118953704834]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.8362380266189575, 1.7947847843170166, 1.732421875, 1.7984074354171753, 1.819651484489441, 1.7832069396972656, 1.8380107879638672, 1.7636268138885498, 1.770849585533142, 1.9029146432876587, 2.230889320373535, 1.723197102546692, 1.7834434509277344, 1.8525240421295166, 1.7809739112854004, 1.7596426010131836]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5787222385406494, 1.668231725692749, 1.6447829008102417, 1.715309500694275, 1.605934500694275, 1.5949368476867676, 1.7479628324508667, 1.681373953819275, 1.6641632318496704, 1.7484976053237915, 1.4976862668991089, 1.6322040557861328, 1.6299203634262085, 1.7964693307876587, 1.6349008083343506, 1.6290414333343506]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1637020111083984, 2.2859675884246826, 2.4870643615722656, 2.201923131942749, 2.2088491916656494, 2.1720852851867676, 2.588641881942749, 2.2561147212982178, 2.1647536754608154, 2.166285991668701, 2.2006008625030518, 2.032572031021118, 2.122220516204834, 2.0885818004608154, 2.3842546939849854, 2.1515023708343506]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.239182472229004, 5.570252418518066, 5.127583980560303, 5.187019348144531, 5.263401508331299, 5.152283668518066, 5.210637092590332, 5.4285454750061035, 5.424038410186768, 5.238431453704834, 5.239182472229004, 5.119290828704834, 5.290234565734863, 5.273257255554199, 5.363521575927734, 5.266345977783203]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.0444111824035645, 3.873197078704834, 3.860456705093384, 3.805889368057251, 3.8344953060150146, 3.8992488384246826, 3.739550828933716, 3.9511117935180664, 3.936448335647583, 4.037049293518066, 3.83984375, 3.6403920650482178, 3.9140625, 3.86234974861145, 3.9282453060150146, 3.9788010120391846]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.909104585647583, 3.0520431995391846, 2.857271671295166, 2.7200119495391846, 2.783653736114502, 2.9620792865753174, 2.8090744018554688, 2.7545673847198486, 2.87602162361145, 2.9027042388916016, 3.074248790740967, 2.3985202312469482, 2.9847354888916016, 2.874398946762085, 3.1637020111083984, 2.960306406021118]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.07128791511058807, 0.0858972817659378, 0.08566178381443024, -0.17628200352191925, -0.1621166467666626, -0.12976685166358948, 0.10661618411540985, -0.134322389960289, 0.08479142934083939, 0.07395626604557037, 0.0806751549243927, 0.07552798092365265, 0.083991639316082, 0.0888361856341362, -0.9134533405303955, 0.0975821390748024]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08016616851091385, 0.0713481605052948, 0.041806478053331375, 0.0439758226275444, 0.0795283243060112, 0.027299972251057625, 0.0568576380610466, 0.07301916182041168, 0.02486310712993145, 0.0625845268368721, -0.16074958443641663, 0.04006307199597359, 0.013386363163590431, -0.013971843756735325, 0.0004645150911528617, 0.03404172137379646]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0730411559343338, 0.05276946350932121, 0.0907491073012352, 0.06295289844274521, 0.1227099671959877, 0.1050628200173378, 0.06598911434412003, -0.06973278522491455, 0.07086393237113953, 0.06635884195566177, 0.0014255766291171312, 0.1033690944314003, -0.11513961106538773, 0.048949409276247025, 0.0013812837423756719, 0.10156552493572235]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11830998957157135, 0.1645725816488266, 0.1111597940325737, 0.1574489027261734, 0.1513703316450119, 0.1337050199508667, 0.0553787462413311, 0.20134952664375305, 0.1803194135427475, -0.3399217426776886, 0.0002037532685790211, 0.09165191650390625, 0.17120754718780518, -0.1372421532869339, -0.07109384983778, -0.014034573920071125]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07606566697359085, 0.10362287610769272, 0.10433632880449295, 0.0789756178855896, 0.04011683911085129, -0.04085831344127655, 0.05865802615880966, 0.07796139270067215, -0.055935513228178024, 0.01749190129339695, -0.0687527135014534, 0.17830340564250946, -0.1128590926527977, -0.06204720214009285, 0.12261109054088593, -0.03089105524122715]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07411245256662369, 0.1555446982383728, 0.0728091299533844, 0.0296198520809412, -0.0774218812584877, 0.0914907306432724, 0.1699286550283432, 0.0416349396109581, 0.132915198802948, -0.0574815534055233, -0.1288226842880249, 0.11717066913843155, -0.07115327566862106, 0.12998495995998383, 0.14513391256332397, 0.13167408108711243]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08109023422002792, 0.06321161985397339, 0.142730712890625, 0.3635137677192688, 0.1721118688583374, 0.10305489599704742, 0.003941959701478481, -0.0256226547062397, 0.5722636580467224, 0.1533547043800354, -0.386445552110672, 0.3449605405330658, 0.2015613317489624, -0.1203962042927742, 0.1229916512966156, 0.1622304767370224]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3037869930267334, 0.27804625034332275, 0.32187554240226746, 0.12670710682868958, -0.7232201099395752, 0.3139665424823761, 0.15986965596675873, 0.056003388017416, 0.2840813398361206, 0.2140217125415802, -0.005063253920525312, 0.23636494576931, 0.4154840409755707, 0.12480539083480835, -0.2984832227230072, -0.0555928535759449]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2129008024930954, 0.36163273453712463, -0.1263369619846344, 0.35659268498420715, 0.4799654483795166, 0.268117755651474, 0.30219683051109314, -0.148397296667099, 0.2335297167301178, 0.6283218264579773, 0.02460733987390995, 0.3719618022441864, 0.3437325656414032, -0.1919681578874588, -0.2142769992351532, 0.26558250188827515]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3978646993637085, 0.3540223240852356, 0.5700063705444336, 0.6752822995185852, 0.462473064661026, 0.3376639187335968, 0.6262461543083191, 0.5561678409576416, 0.3304520845413208, 0.2708372175693512, 0.4825512170791626, 0.6667441725730896, 1.0752156972885132, 0.7344576120376587, 0.7566654086112976, 0.7119295597076416]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9266154170036316, 0.8049781322479248, 0.668354332447052, 0.8568095564842224, 1.0675610303878784, 0.6694743037223816, 0.6155676245689392, 0.6779901385307312, 0.4151969850063324, 1.038407564163208, 0.6900421380996704, 0.8443002700805664, 0.902260422706604, 0.3051970899105072, 0.4665570855140686, 0.4538680911064148]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.1290379762649536, 1.0929245948791504, 0.9903894066810608, 1.4067925214767456, 1.3924851417541504, 0.7950913906097412, 0.9263005256652832, 1.2741659879684448, 0.974018394947052, 1.2810407876968384, 1.0146784782409668, 1.0202209949493408, 0.7837699055671692, 0.7000643014907837, 0.1519499272108078, 0.9204121232032776]
Layer: gate_11 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.3694564700126648, 0.93496173620224, 1.0510525703430176, 1.217850923538208, 0.5559546947479248, 0.7603934407234192, 1.0712319612503052, 1.0990396738052368, 0.8553781509399414, 0.729081392288208, 0.7889316082000732, 0.8281095027923584, 1.0932539701461792, 0.8983290195465088, 0.6875852346420288, 0.959883451461792]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7381572127342224, 0.901452898979187, 0.9580581784248352, 0.5532827377319336, 1.1700303554534912, 0.175276979804039, 1.090975284576416, 2.2051401138305664, 0.9035993218421936, 0.9020337462425232, 1.0808919668197632, 1.3294271230697632, 0.00290655717253685, 1.0815178155899048, 1.1767936944961548, 0.7525702714920044]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5765119194984436, 0.7305482029914856, 0.8179602026939392, 0.850957989692688, 0.835038423538208, 0.462394118309021, 0.6459345817565918, 0.8316993117332458, 0.6312934160232544, 0.8688926100730896, -0.01563517190515995, 0.58449786901474, 0.9827905297279358, 0.9202861189842224, 1.0517345666885376, 0.4497729241847992]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.3429875373840332, 1.2730034589767456, 0.9581783413887024, 0.7800176739692688, 0.6230352520942688, 0.7871016263961792, 0.9885292649269104, 0.949668288230896, 0.8011997938156128, 0.8999255895614624, 0.7020787000656128, -0.2392694354057312, 0.8635408878326416, 0.755781888961792, 1.0227710008621216, 1.3300877809524536]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7561558485031128, 0.5564984679222107, 0.8236083984375, 0.7527475357055664, 0.6934272050857544, 0.5050112009048462, 0.2972235381603241, 1.0035536289215088, -0.759052574634552, 2.3984375, -0.82651287317276, 0.7293197512626648, 0.868241548538208, 0.9378332495689392, 0.8424750566482544, 0.6691405177116394]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.2357158362865448, 0.6654554009437561, 1.2593780755996704, 1.2424006462097168, 1.3888578414916992, 1.294689416885376, 1.2492249011993408, 1.3361467123031616, 1.4984266757965088, 1.6967076063156128, 1.8344106674194336, 1.2411295175552368, 1.2342674732208252, 1.1290013790130615, 0.5848684310913086, 1.4013594388961792]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3498263359069824, 1.2734375, 1.4095613956451416, 1.5049448013305664, 0.926591157913208, 1.3053075075149536, 1.8152902126312256, 1.795665979385376, 1.8920975923538208, 1.365234375, 1.0987403392791748, 0.9356520175933838, 1.2995682954788208, 1.3832014799118042, 1.356290340423584, 1.356212854385376]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.321521520614624, 1.232654333114624, 1.0682199001312256, 1.1272166967391968, 1.3334883451461792, 0.8134492039680481, 1.0155320167541504, 1.0845346450805664, 1.701404333114624, 1.2503100633621216, 1.3050285577774048, 1.2592850923538208, 2.071211576461792, 1.264198899269104, 1.1986297369003296, 1.1775329113006592]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8474528193473816, 1.1377882957458496, 0.9900987148284912, 0.8938719630241394, 0.07333628088235855, 1.1233259439468384, 1.1690073013305664, 1.8105818033218384, 0.7707674503326416, 1.1407334804534912, 1.363692045211792, 1.0332767963409424, 0.9981011152267456, 1.3505859375, 1.1416558027267456, 0.4920785129070282]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0359933376312256, 2.228391647338867, 2.03066086769104, 2.0443637371063232, 1.9434213638305664, 2.114800453186035, 1.9693700075149536, 2.2310268878936768, 1.9726872444152832, 2.059368848800659, 2.3927950859069824, 2.90488600730896, 1.838045597076416, 2.386904716491699, 2.1936073303222656, 2.1650545597076416]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4606274366378784, 1.2890005111694336, 1.7780567407608032, 1.1656745672225952, 1.5279637575149536, 1.5362722873687744, 1.370039701461792, 1.3956162929534912, 1.5960131883621216, 1.584139347076416, 1.4711681604385376, 2.2655630111694336, 1.5265376567840576, 1.5785900354385376, 1.2712208032608032, 1.6526848077774048]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Running loglikelihood requests:  53%|█████████████████████████▊                       | 265/504 [14:46<12:53,  3.23s/it]Layer: gate_23 - Captured router_logits: [2.393275737762451, 1.6859809160232544, 1.2274073362350464, 1.3783172369003296, 1.3930121660232544, 1.4763765335083008, 1.3367280960083008, 1.4428943395614624, 1.384804368019104, 1.3075706958770752, 1.4987598657608032, 1.3823319673538208, 1.3450831174850464, 0.9255613088607788, 1.3720431327819824, 1.4876612424850464]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.271515369415283, 2.357576847076416, 2.269097328186035, 2.1432912349700928, 2.886284828186035, 2.3717758655548096, 2.6793155670166016, 2.23828125, 2.3151662349700928, 2.302269458770752, 2.5677082538604736, 2.177765369415283, 2.2801339626312256, 2.3801462650299072, 2.370659828186035, 2.4011037349700928]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6309213638305664, 1.8512524366378784, 1.4903273582458496, 1.652188777923584, 1.5238715410232544, 1.5502852201461792, 1.5715526342391968, 1.9139385223388672, 1.5792100429534912, 1.5167100429534912, 1.662341833114624, 1.4513888359069824, 1.5552765130996704, 1.546347975730896, 1.668495774269104, 2.05214524269104]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9765315055847168, 1.6674145460128784, 1.6950955390930176, 1.7705233097076416, 1.6561027765274048, 1.7318464517593384, 1.6656087636947632, 1.8054159879684448, 1.7197304964065552, 1.8731708526611328, 2.11826491355896, 1.6067166328430176, 1.6396329402923584, 1.8723493814468384, 1.7274150848388672, 1.821649432182312]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6519562005996704, 1.6981452703475952, 1.6245291233062744, 1.808948278427124, 1.6604818105697632, 1.591641902923584, 1.7507634162902832, 1.746182918548584, 1.63134765625, 1.7799246311187744, 1.5187561511993408, 1.7447683811187744, 1.6563894748687744, 1.7955244779586792, 1.6336960792541504, 1.6368166208267212]
Layer: gate_27 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.042271137237549, 2.0703125, 2.6190476417541504, 2.056175708770752, 2.158327102661133, 2.096633195877075, 2.6305184364318848, 2.165794849395752, 2.021437883377075, 2.0345981121063232, 1.9519779682159424, 1.912481427192688, 1.9949777126312256, 1.8893228769302368, 2.259099006652832, 2.070080041885376]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.484994888305664, 5.781126022338867, 5.591641902923584, 5.618489742279053, 5.455790996551514, 5.288256645202637, 5.618427753448486, 5.903273582458496, 5.742931365966797, 5.539558410644531, 5.5319318771362305, 5.389881134033203, 5.591145992279053, 5.614335536956787, 5.707341194152832, 5.590649604797363]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.197296619415283, 3.982886791229248, 3.937066078186035, 3.804656505584717, 3.9637277126312256, 4.007254600524902, 3.8902530670166016, 3.948040723800659, 3.9771206378936768, 4.017423152923584, 3.9572792053222656, 3.7207088470458984, 4.015191078186035, 3.995225667953491, 4.0415425300598145, 4.091053009033203]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.058903694152832, 3.080543041229248, 2.9332218170166016, 2.768353223800659, 2.949528694152832, 2.984808921813965, 2.9317336082458496, 2.857700824737549, 2.886594772338867, 2.949342727661133, 3.149181604385376, 2.474299430847168, 3.0038442611694336, 2.903211832046509, 3.144841194152832, 2.9760043621063232]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.07076623290777206, 0.08548072725534439, 0.08639523386955261, -0.18089517951011658, -0.16204684972763062, -0.13405534625053406, 0.1080835834145546, -0.15065957605838776, 0.08118633925914764, 0.07343775779008865, 0.08085154742002487, 0.0746365338563919, 0.08215270191431046, 0.08988878130912781, -0.9241241216659546, 0.09927938878536224]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08017414063215256, 0.06919711828231812, 0.04154050350189209, 0.04324619099497795, 0.07920781522989273, 0.027299461886286736, 0.05448578670620918, 0.06862590461969376, 0.02692866139113903, 0.0630292072892189, -0.16188322007656097, 0.04095710441470146, 0.014827805571258068, -0.014468309469521046, -0.001476318808272481, 0.032513223588466644]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0735095888376236, 0.053163185715675354, 0.0877915695309639, 0.06247689574956894, 0.1168062761425972, 0.10571102797985077, 0.06634186208248138, -0.0687764510512352, 0.07149828225374222, 0.062179937958717346, -0.0010573224863037467, 0.10687333345413208, -0.11967207491397858, 0.052217748016119, -0.005936537403613329, 0.10257640480995178]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11653807014226913, 0.1642872840166092, 0.10899613797664642, 0.15177805721759796, 0.14504706859588623, 0.1271306425333023, 0.06292327493429184, 0.2001643031835556, 0.17722375690937042, -0.3198738396167755, 0.0016163771506398916, 0.08776471018791199, 0.16216690838336945, -0.12833169102668762, -0.06938444077968597, -0.013729188591241837]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08127942681312561, 0.09761589765548706, 0.10547482967376709, 0.07131002843379974, 0.04443458467721939, -0.03874501213431358, 0.05784819275140762, 0.07445941865444183, -0.04768867790699005, 0.018993843346834183, -0.06941081583499908, 0.1768605262041092, -0.11903108656406403, -0.06565187126398087, 0.12360859662294388, -0.02848108857870102]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06778685748577118, 0.15731441974639893, 0.06524186581373215, 0.02363220416009426, -0.072905994951725, 0.08935298770666122, 0.16917605698108673, 0.04441746696829796, 0.12445518374443054, -0.05248263478279114, -0.1111554428935051, 0.11076156795024872, -0.07132454216480255, 0.12746664881706238, 0.14147356152534485, 0.1309318244457245]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08348070830106735, 0.06785937398672104, 0.1487467736005783, 0.36778494715690613, 0.17985448241233826, 0.09965651482343674, 0.0019835184793919325, -0.023267297074198723, 0.5675376057624817, 0.15699630975723267, -0.3772483766078949, 0.35550954937934875, 0.19379207491874695, -0.12193335592746735, 0.11168726533651352, 0.15660442411899567]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.29434823989868164, 0.28192636370658875, 0.3234376907348633, 0.13395163416862488, -0.70751953125, 0.3143262267112732, 0.16320180892944336, 0.06513939797878265, 0.2785075008869171, 0.237701416015625, -0.012735506519675255, 0.22615237534046173, 0.4206756353378296, 0.12833404541015625, -0.29868122935295105, -0.058807622641325]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21483084559440613, 0.3704785406589508, -0.12810175120830536, 0.354445219039917, 0.47628387808799744, 0.26950445771217346, 0.3074316084384918, -0.15025614202022552, 0.22298772633075714, 0.6185421943664551, 0.027498571202158928, 0.3750327527523041, 0.34477221965789795, -0.18101240694522858, -0.2207038700580597, 0.2761629819869995]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.40311935544013977, 0.36088624596595764, 0.5749332904815674, 0.6730431318283081, 0.4661758542060852, 0.3327353894710541, 0.6266797184944153, 0.5722775459289551, 0.33896592259407043, 0.29111388325691223, 0.4798772633075714, 0.6805370450019836, 1.0879700183868408, 0.7212703227996826, 0.7578521966934204, 0.7231643795967102]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9334131479263306, 0.8019205927848816, 0.6820852160453796, 0.8653455376625061, 1.0828490257263184, 0.6913994550704956, 0.6346386075019836, 0.6948758363723755, 0.3957598805427551, 1.0371108055114746, 0.7037839889526367, 0.8469456434249878, 0.915324866771698, 0.3209773600101471, 0.4843246340751648, 0.45226654410362244]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.1465479135513306, 1.0960862636566162, 1.0095751285552979, 1.4235979318618774, 1.3883572816848755, 0.8040475249290466, 0.9436054825782776, 1.277385950088501, 0.9913300275802612, 1.2747474908828735, 1.027252435684204, 1.021016001701355, 0.7860405445098877, 0.7120606899261475, 0.16550153493881226, 0.9261782169342041]
Layer: gate_11 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.3482757806777954, 0.9237606525421143, 1.038284420967102, 1.2024580240249634, 0.5563468337059021, 0.7530726194381714, 1.0852011442184448, 1.1054329872131348, 0.8459482789039612, 0.7045109272003174, 0.7859551906585693, 0.836525022983551, 1.081554889678955, 0.9122760891914368, 0.7017990946769714, 0.960064172744751]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7013643980026245, 0.8697956204414368, 0.9410013556480408, 0.5225844979286194, 1.1361788511276245, 0.15173041820526123, 1.0689151287078857, 2.12595272064209, 0.8788824081420898, 0.8657504320144653, 1.059300422668457, 1.3015751838684082, -0.0011939072282984853, 1.0453546047210693, 1.1440489292144775, 0.725784420967102]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.585128664970398, 0.7308955192565918, 0.8203680515289307, 0.8537538051605225, 0.8333690762519836, 0.442964643239975, 0.6399941444396973, 0.8235927820205688, 0.625591516494751, 0.8516399264335632, -0.04075039550662041, 0.5798816084861755, 0.982113242149353, 0.9253604412078857, 1.0406266450881958, 0.4375019967556]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.3360099494457245, 1.2766767740249634, 0.9578728675842285, 0.7803925275802612, 0.6206253170967102, 0.7837747931480408, 0.979736328125, 0.9493775367736816, 0.7990504503250122, 0.9045509696006775, 0.6972487568855286, -0.22827644646167755, 0.8524119257926941, 0.7547319531440735, 1.0308053493499756, 1.3182402849197388]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7739496231079102, 0.5623764395713806, 0.8510245680809021, 0.7599442601203918, 0.7230055928230286, 0.506752073764801, 0.30818262696266174, 1.0248546600341797, -0.7422928810119629, 2.3942136764526367, -0.8391123414039612, 0.7501002550125122, 0.8698233962059021, 0.9441295266151428, 0.8596409559249878, 0.6448887586593628]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.23977313935756683, 0.7001327872276306, 1.2789316177368164, 1.2430747747421265, 1.399517297744751, 1.3099355697631836, 1.2747793197631836, 1.3611613512039185, 1.503753423690796, 1.7293730974197388, 1.8401613235473633, 1.259257435798645, 1.2547478675842285, 1.1435586214065552, 0.59136962890625, 1.416549563407898]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.380652904510498, 1.3116583824157715, 1.4471683502197266, 1.520134687423706, 0.963954508304596, 1.3287442922592163, 1.8122776746749878, 1.810943841934204, 1.879207968711853, 1.3992314338684082, 1.1258530616760254, 0.9713261127471924, 1.3291054964065552, 1.4334439039230347, 1.3889259099960327, 1.3887512683868408]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3324599266052246, 1.2532234191894531, 1.0713446140289307, 1.1428639888763428, 1.353245735168457, 0.8286122679710388, 1.0294398069381714, 1.1009749174118042, 1.7046176195144653, 1.2578125, 1.310276985168457, 1.280741810798645, 2.0991170406341553, 1.2819486856460571, 1.2183053493499756, 1.1923907995224]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8372336030006409, 1.1425225734710693, 0.9979913234710693, 0.8978663682937622, 0.07154114544391632, 1.1237772703170776, 1.1819581985473633, 1.8285855054855347, 0.7866166234016418, 1.1530344486236572, 1.370212435722351, 1.0442509651184082, 1.0035251379013062, 1.3712366819381714, 1.1562631130218506, 0.4950706660747528]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.066469669342041, 2.2480309009552, 2.0529725551605225, 2.064056158065796, 1.9694169759750366, 2.1345908641815186, 1.9906948804855347, 2.2541921138763428, 2.0022547245025635, 2.0772993564605713, 2.4169206619262695, 2.9036457538604736, 1.8621697425842285, 2.3999619483947754, 2.222529172897339, 2.1846418380737305]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4830728769302368, 1.3093876838684082, 1.782329797744751, 1.1938198804855347, 1.545191764831543, 1.558498501777649, 1.4048209190368652, 1.4249554872512817, 1.619537591934204, 1.6053416728973389, 1.483327031135559, 2.2874746322631836, 1.5517657995224, 1.609375, 1.2960492372512817, 1.6748285293579102]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.3970084190368652, 1.6889290809631348, 1.2595075368881226, 1.3900851011276245, 1.4147294759750366, 1.494537591934204, 1.361058235168457, 1.4571900367736816, 1.4144117832183838, 1.326743483543396, 1.5149263143539429, 1.3992472887039185, 1.3743013143539429, 0.958740234375, 1.3960278034210205, 1.510670781135559]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.275787591934204, 2.3690295219421387, 2.292492389678955, 2.168699264526367, 2.9122841358184814, 2.3931021690368652, 2.6978530883789062, 2.2594003677368164, 2.338097095489502, 2.3178353309631348, 2.575965404510498, 2.203252077102661, 2.285632610321045, 2.3964684009552, 2.3891005516052246, 2.4291794300079346]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6349085569381714, 1.865059733390808, 1.4915523529052734, 1.6458967924118042, 1.532901406288147, 1.546366810798645, 1.5773310661315918, 1.9200329780578613, 1.5826663970947266, 1.5185149908065796, 1.6517244577407837, 1.458111047744751, 1.5577679872512817, 1.541888952255249, 1.651740312576294, 2.0605628490448]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.972957968711853, 1.6501246690750122, 1.688944935798645, 1.7626873254776, 1.646484375, 1.7216936349868774, 1.6486676931381226, 1.7865536212921143, 1.714682936668396, 1.8639799356460571, 2.117274761199951, 1.592678189277649, 1.6340827941894531, 1.855198860168457, 1.6996792554855347, 1.8121387958526611]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Running loglikelihood requests:  53%|██████████████████████████▏                      | 269/504 [14:59<12:36,  3.22s/it]Layer: gate_27 - Captured router_logits: [1.642649531364441, 1.6932322978973389, 1.621349811553955, 1.8053827285766602, 1.6605452299118042, 1.5829998254776, 1.737689733505249, 1.7404705286026, 1.6225069761276245, 1.772254467010498, 1.5261051654815674, 1.7358914613723755, 1.6427369117736816, 1.7962284088134766, 1.6290730237960815, 1.6329811811447144]
Layer: gate_27 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.024977684020996, 2.0555131435394287, 2.6047463417053223, 2.045255422592163, 2.1332411766052246, 2.082158327102661, 2.6249840259552, 2.1551225185394287, 2.002413511276245, 2.0232152938842773, 1.9305766820907593, 1.900088906288147, 1.9686229228973389, 1.876175045967102, 2.2472052574157715, 2.0587525367736816]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.479801654815674, 5.771023750305176, 5.606071949005127, 5.618457794189453, 5.448933124542236, 5.282711029052734, 5.597815036773682, 5.911013603210449, 5.717288970947266, 5.534298896789551, 5.523183345794678, 5.404026985168457, 5.579776287078857, 5.621315956115723, 5.696709632873535, 5.579331874847412]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.2002034187316895, 4.002223014831543, 3.954268217086792, 3.832825183868408, 3.9627795219421387, 3.9995553493499756, 3.910029172897339, 3.9706554412841797, 4.006986618041992, 4.0284552574157715, 3.9793572425842285, 3.7579832077026367, 4.046811580657959, 4.004382610321045, 4.060340404510498, 4.101181507110596]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.0595147609710693, 3.0799670219421387, 2.9300684928894043, 2.757685422897339, 2.950076103210449, 2.9928226470947266, 2.9180004596710205, 2.8746824264526367, 2.8887829780578613, 2.9655423164367676, 3.1269054412841797, 2.4994382858276367, 2.9978086948394775, 2.901994466781616, 3.1522483825683594, 2.9746570587158203]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09117951989173889, 0.0972842201590538, 0.10381448268890381, -0.1984757035970688, -0.16586904227733612, -0.09092581272125244, 0.12088162451982498, -0.22396200895309448, 0.11315761506557465, 0.08603768050670624, 0.09077160060405731, 0.09602105617523193, 0.08940649777650833, 0.10821282863616943, -0.9748815298080444, 0.11238235980272293]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.10162316262722015, 0.047875262796878815, 0.034243106842041016, 0.05571465566754341, 0.07628931850194931, 0.05734331160783768, 0.05124439299106598, 0.054813072085380554, -0.009002185426652431, 0.06637711077928543, -0.13582248985767365, 0.04665030911564827, 0.012571366503834724, 0.003487133653834462, 0.0023365647066384554, -0.00737724918872118]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06919035315513611, 0.07938341051340103, 0.1205059140920639, 0.0492565892636776, 0.0965692475438118, 0.08952656388282776, 0.06476305425167084, -0.06568358093500137, 0.07737387716770172, 0.05561012402176857, 0.010198624804615974, 0.09683077782392502, -0.10739810764789581, 0.03918281942605972, -0.031730275601148605, 0.11677441745996475]
Layer: gate_2 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11794668436050415, 0.15094006061553955, 0.13644759356975555, 0.1667519211769104, 0.12640196084976196, 0.13891638815402985, 0.05102714151144028, 0.19058728218078613, 0.18485698103904724, -0.451481431722641, 0.012225041165947914, 0.15616245567798615, 0.10151104629039764, -0.18783269822597504, -0.09292727708816528, 0.006628818344324827]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.11463852971792221, 0.12032371014356613, 0.09226331859827042, 0.02963782101869583, 0.08836965262889862, -0.061856504529714584, 0.08308476209640503, 0.05958307161927223, -0.014260589145123959, 0.0030927814077585936, -0.11661867052316666, 0.1746145784854889, -0.16431714594364166, -0.10941999405622482, 0.15744169056415558, -0.019276103004813194]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.1508779078722, 0.16893327236175537, 0.11863960325717926, 0.061742812395095825, -0.09064070880413055, 0.015229271724820137, 0.10499078780412674, 0.06517203897237778, 0.06548202782869339, -0.005250024143606424, -0.2854143977165222, 0.1257362961769104, -0.12285901606082916, 0.14928311109542847, 0.14647486805915833, 0.11834779381752014]
Layer: gate_5 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.2351122945547104, 0.17429038882255554, 0.1499718874692917, 0.31717169284820557, 0.20359326899051666, 0.20779669284820557, -0.06387653946876526, -0.005348830949515104, 0.22173383831977844, 0.2613072693347931, -0.3619164526462555, 0.31160467863082886, 0.2217717468738556, -0.15199029445648193, 0.07692130655050278, 0.14041562378406525]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.210418701171875, 0.34136638045310974, 0.30054786801338196, 0.1686217486858368, -0.5707952976226807, 0.2666941285133362, 0.29134631156921387, -0.05263231694698334, 0.26307833194732666, 0.1136041209101677, -0.1424671858549118, 0.2610088288784027, 0.24108198285102844, 0.21200712025165558, -0.4724736511707306, -0.3300996422767639]
Layer: gate_7 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3206443786621094, 0.4118572175502777, -0.05208500102162361, 0.43479156494140625, 0.48233532905578613, 0.5251985192298889, 0.327619343996048, -0.09330424666404724, 0.2788274884223938, 0.5684704184532166, 0.23397617042064667, -0.19045370817184448, 0.5754234194755554, -0.44344767928123474, -0.4209064543247223, 0.27686867117881775]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.7909621000289917, 0.3369310796260834, 0.45556339621543884, 0.5795168280601501, 0.5977427959442139, 0.29528114199638367, 0.8378795981407166, 0.6768338680267334, 0.6211032271385193, 0.25113165378570557, 0.22289688885211945, 0.7862368822097778, 0.7223762273788452, 0.24747034907341003, 0.7125244140625, 0.7925525307655334]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9993771314620972, 0.9323570132255554, 0.8166428804397583, 0.8783619403839111, 1.0790175199508667, 0.417824923992157, 0.8681080341339111, 0.8280609846115112, 0.392111599445343, 0.52631014585495, 0.8278528451919556, 0.8250672221183777, 0.34631797671318054, 0.40513136982917786, 0.9987652897834778, 0.6821268796920776]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0766521692276, 1.0132516622543335, 1.0483118295669556, 1.2542774677276611, 1.2824987173080444, 0.6366942524909973, 1.0029187202453613, 1.0185567140579224, 1.0584046840667725, 1.1778143644332886, 0.9762663245201111, 1.352579116821289, 0.7148417234420776, 0.8660728335380554, 0.2969345450401306, 0.9652024507522583]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.48726266622543335, 1.2039459943771362, 1.3783539533615112, 1.2928177118301392, 0.820634663105011, 0.9845671057701111, 1.0345178842544556, 1.103163480758667, 0.8328157067298889, 0.48447680473327637, 1.2473745346069336, 1.1071337461471558, 1.2233206033706665, 1.0759637355804443, 0.9062820076942444, 1.2392898797988892]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.2002112865447998, 1.1579229831695557, 1.602394938468933, 1.0453261137008667, 1.4182249307632446, 0.05185987055301666, 1.3868788480758667, 1.9165599346160889, 1.2518210411071777, 1.5022093057632446, 1.3809714317321777, 1.6477330923080444, 0.7378024458885193, 1.4018034934997559, 1.559474229812622, 1.138889193534851]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.1975587606430054, 1.0164895057678223, 1.1848664283752441, 1.2352715730667114, 1.384717583656311, 1.5283633470535278, 0.6827402710914612, 1.3382563591003418, 1.000156044960022, 1.180459976196289, -0.08589047193527222, 1.1485955715179443, 1.322121500968933, 1.4047051668167114, 1.403416395187378, 1.0707440376281738]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.8166143894195557, 1.2613985538482666, 1.260598063468933, 1.1474609375, 1.0034600496292114, 1.2357838153839111, 1.4931640625, 1.2250896692276, 0.9981589317321777, 1.2812820672988892, 1.1736479997634888, 0.27204468846321106, 1.4584320783615112, 1.287685751914978, 1.074666976928711, 1.449435830116272]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.8112832903862, 0.3746175169944763, 1.0033459663391113, 0.7663784623146057, 0.7429399490356445, 0.7013489603996277, 0.08371972292661667, 0.897993266582489, -0.43861863017082214, 2.035958766937256, -0.7647475004196167, 0.682939887046814, 0.9719358086585999, 0.9123895168304443, 0.9803206920623779, 0.22429420053958893]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.11208906024694443, 1.0305135250091553, 1.5137999057769775, 1.2667956352233887, 1.560642957687378, 1.542392373085022, 1.524854302406311, 1.722256064414978, 1.5794857740402222, 1.4670170545578003, 1.9030802249908447, 1.4576876163482666, 1.432468056678772, 1.2879598140716553, 0.764173150062561, 1.287545919418335]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.717629313468933, 1.570408582687378, 1.7583167552947998, 1.8061603307724, 1.407314658164978, 1.420017957687378, 1.673924207687378, 1.9407018423080444, 2.165471315383911, 1.7725729942321777, 1.4107165336608887, 1.1913222074508667, 1.6562340259552002, 0.9751516580581665, 1.7702035903930664, 1.6832095384597778]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.7174052000045776, 1.608254313468933, 1.6545850038528442, 1.8760886192321777, 1.8494492769241333, 1.397412896156311, 1.6004258394241333, 1.6186763048171997, 1.9406298398971558, 1.7228163480758667, 1.8220734596252441, 1.7134349346160889, 2.0195953845977783, 1.8177189826965332, 1.8589907884597778, 1.7231525182724]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2054682970046997, 1.5211962461471558, 1.3305504322052002, 1.1607486009597778, 0.24033167958259583, 1.5988529920578003, 1.462794542312622, 2.097668170928955, 1.2957884073257446, 1.499159574508667, 1.4749935865402222, 1.3610559701919556, 1.5397188663482666, 1.5870261192321777, 1.2707639932632446, 0.7827455997467041]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.3864946365356445, 2.4773309230804443, 2.2965548038482666, 2.165567398071289, 2.3675718307495117, 2.3386270999908447, 2.262166976928711, 2.3769211769104004, 2.2954020500183105, 2.4530608654022217, 2.5336194038391113, 2.4414703845977783, 1.9946849346160889, 2.6910860538482666, 2.4820055961608887, 2.4693262577056885]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.892225980758667, 1.7573962211608887, 2.021388292312622, 1.4602971076965332, 1.8331838846206665, 1.9420785903930664, 1.704053521156311, 1.7985719442367554, 1.9116930961608887, 1.8747438192367554, 1.736232042312622, 2.0, 1.9722720384597778, 1.8940190076828003, 1.6874279975891113, 2.048379898071289]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.221599578857422, 1.838915228843689, 1.4863440990447998, 1.700595498085022, 1.7475665807724, 1.8071528673171997, 1.797995686531067, 1.8371862173080444, 1.7142754793167114, 1.6428502798080444, 1.7372887134552002, 1.611232042312622, 1.5561763048171997, 1.0096831321716309, 1.5037461519241333, 1.7793608903884888]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.511270523071289, 2.6248719692230225, 2.388831853866577, 2.5881147384643555, 2.6743083000183105, 2.570504665374756, 2.6396005153656006, 2.412397623062134, 2.252401351928711, 2.5807504653930664, 2.5762038230895996, 2.4707350730895996, 2.4366674423217773, 2.6027791500091553, 2.547323226928711, 2.5180583000183105]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.777791976928711, 1.7493916749954224, 1.7482069730758667, 1.8603515625, 1.4237641096115112, 1.6955046653747559, 1.7702996730804443, 1.8087538480758667, 1.8002690076828003, 1.735495686531067, 1.7681224346160889, 1.7047579288482666, 1.7507044076919556, 1.5274717807769775, 1.6177958250045776, 1.9341059923171997]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.7768634557724, 1.799472451210022, 1.8469678163528442, 1.8763127326965332, 1.8500275611877441, 1.9005587100982666, 1.8763487339019775, 1.7964907884597778, 1.8969706296920776, 1.907796859741211, 2.1230368614196777, 1.7509385347366333, 1.8748239278793335, 1.863425374031067, 1.8506180047988892, 1.8772783279418945]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5859614610671997, 1.5871942043304443, 1.623643159866333, 1.669812798500061, 1.549886703491211, 1.6517033576965332, 1.6921266317367554, 1.6649270057678223, 1.6004078388214111, 1.6852507591247559, 1.3826524019241333, 1.6116242408752441, 1.5999855995178223, 1.6159948110580444, 1.5812628269195557, 1.6330926418304443]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.246910333633423, 2.2913758754730225, 2.489457845687866, 2.217341184616089, 2.277111530303955, 2.261094331741333, 2.389669895172119, 2.349945545196533, 2.247154474258423, 2.286757230758667, 2.235607624053955, 2.0787012577056885, 2.1869397163391113, 2.128730058670044, 2.399402141571045, 2.284395694732666]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.304111003875732, 5.425012588500977, 5.156186103820801, 5.107005596160889, 5.18359375, 5.248142719268799, 5.347336292266846, 5.377753734588623, 5.4296875, 5.298027515411377, 5.261014461517334, 5.170209884643555, 5.2519211769104, 5.229251861572266, 5.363089084625244, 5.155033111572266]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.847912311553955, 3.804239273071289, 3.7585809230804443, 3.6119565963745117, 3.671426773071289, 3.722504138946533, 3.542450428009033, 3.719782590866089, 3.735677719116211, 3.877633571624756, 3.7007956504821777, 3.3566434383392334, 3.712606430053711, 3.707671642303467, 3.7913999557495117, 3.818711519241333]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  54%|██████████████████████████▌                      | 273/504 [15:12<12:34,  3.26s/it]Layer: gate_31 - Captured router_logits: [2.735623836517334, 2.8133325576782227, 2.7774078845977783, 2.486680269241333, 2.7644083499908447, 2.792616605758667, 2.654200792312622, 2.5270235538482666, 2.83984375, 2.6516072750091553, 2.680327892303467, 2.1232550144195557, 2.6907339096069336, 2.6990907192230225, 2.8236424922943115, 2.7684426307678223]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.0920376405119896, 0.10046480596065521, 0.10398326814174652, -0.20140688121318817, -0.1570819765329361, -0.09263160824775696, 0.121304452419281, -0.22215896844863892, 0.11341732740402222, 0.08769632130861282, 0.09283798933029175, 0.0942995697259903, 0.09094588458538055, 0.108429454267025, -0.9614577889442444, 0.11507340520620346]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.10108112543821335, 0.049328867346048355, 0.03670601546764374, 0.05702722445130348, 0.07629244774580002, 0.05642875283956528, 0.04999467357993126, 0.05383050814270973, -0.008141439408063889, 0.06637022644281387, -0.13845299184322357, 0.04747221991419792, 0.012891894206404686, -0.00023446317936759442, 3.814697265625e-06, -0.00689334562048316]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06891819834709167, 0.0794106125831604, 0.1202453225851059, 0.05050327628850937, 0.09611038863658905, 0.09305760264396667, 0.06864053755998611, -0.06578239053487778, 0.07666590809822083, 0.05228911712765694, 0.00807265006005764, 0.0975915864109993, -0.105438731610775, 0.035207998007535934, -0.02889389358460903, 0.11290737986564636]
Layer: gate_2 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11551341414451599, 0.1487661898136139, 0.13829465210437775, 0.16709193587303162, 0.12447506934404373, 0.131334587931633, 0.04560614377260208, 0.19173668324947357, 0.183747336268425, -0.4476827383041382, 0.014179792255163193, 0.1533837616443634, 0.09752476960420609, -0.19127316772937775, -0.09678850322961807, 0.006502745673060417]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.11521511524915695, 0.12026889622211456, 0.09478137642145157, 0.026158254593610764, 0.08875062316656113, -0.06905777752399445, 0.0813862532377243, 0.06575562804937363, -0.01784828118979931, -0.0021424840670078993, -0.11471207439899445, 0.17263005673885345, -0.17772743105888367, -0.10715323686599731, 0.15243829786777496, -0.025333091616630554]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.13753797113895416, 0.16757802665233612, 0.12245697528123856, 0.06144701689481735, -0.09857815504074097, 0.019195806235074997, 0.10848705470561981, 0.06855711340904236, 0.05762919411063194, -0.009244324639439583, -0.27730223536491394, 0.12447644770145416, -0.12552130222320557, 0.1464417278766632, 0.1491624116897583, 0.1200554296374321]
Layer: gate_5 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.2319415956735611, 0.1646961122751236, 0.1242295578122139, 0.31578490138053894, 0.1982571929693222, 0.20159511268138885, -0.04296599701046944, 0.006733378395438194, 0.22546818852424622, 0.2677808701992035, -0.35343557596206665, 0.30802643299102783, 0.22929006814956665, -0.15235374867916107, 0.06266309320926666, 0.14688685536384583]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.20644791424274445, 0.3508720397949219, 0.3017442524433136, 0.15701569616794586, -0.5529525279998779, 0.2698216736316681, 0.2878187894821167, -0.04672829061746597, 0.254892498254776, 0.10836353898048401, -0.13725580275058746, 0.26774072647094727, 0.2625829875469208, 0.20977149903774261, -0.47473543882369995, -0.31761494278907776]
Layer: gate_7 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3024856746196747, 0.40327203273773193, -0.06864654272794724, 0.41040781140327454, 0.4852074682712555, 0.5147104859352112, 0.300883948802948, -0.09999541193246841, 0.26585662364959717, 0.5445907115936279, 0.22024017572402954, -0.18152455985546112, 0.551469624042511, -0.42513036727905273, -0.4039911925792694, 0.26559773087501526]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.7797641158103943, 0.3315059542655945, 0.47215595841407776, 0.5593066811561584, 0.5773265361785889, 0.3390963077545166, 0.8377720713615417, 0.6561459302902222, 0.569750189781189, 0.2211388796567917, 0.2361290156841278, 0.7662653923034668, 0.7337031364440918, 0.29647600650787354, 0.7052962779998779, 0.7703236937522888]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0262861251831055, 0.8894283175468445, 0.7833542227745056, 0.8548283576965332, 1.059130072593689, 0.3750900626182556, 0.8296778798103333, 0.7981957793235779, 0.40438443422317505, 0.5703690052032471, 0.8396516442298889, 0.8105508685112, 0.37602460384368896, 0.3696649372577667, 0.9273691773414612, 0.6493720412254333]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0618356466293335, 0.9909027814865112, 1.0217045545578003, 1.1287291049957275, 1.2758508920669556, 0.5972030162811279, 0.966213047504425, 1.0012646913528442, 0.9938011765480042, 1.149558186531067, 0.9760181903839111, 1.2747843265533447, 0.6579740047454834, 0.8001323938369751, 0.24170571565628052, 0.9427580237388611]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.4634849727153778, 1.1682509183883667, 1.308065414428711, 1.2254018783569336, 0.7491735219955444, 0.952832818031311, 0.9593135714530945, 1.0514636039733887, 0.8202874660491943, 0.4803231656551361, 1.1632260084152222, 1.0672286748886108, 1.1815110445022583, 1.0644690990447998, 0.8673716187477112, 1.2108914852142334]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.1565221548080444, 1.0638427734375, 1.5172059535980225, 0.9806368350982666, 1.365390419960022, 0.05803542956709862, 1.338418960571289, 1.8452868461608887, 1.1963491439819336, 1.429831624031067, 1.2554911375045776, 1.5934938192367554, 0.655382513999939, 1.345859169960022, 1.5377177000045776, 1.0776166915893555]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.2029919624328613, 1.0148966312408447, 1.2185418605804443, 1.2632876634597778, 1.4258372783660889, 1.4653668403625488, 0.6839739680290222, 1.3230170011520386, 0.9829021692276001, 1.173732042312622, 0.01819998398423195, 1.1786574125289917, 1.377102255821228, 1.453076958656311, 1.4509397745132446, 1.1100133657455444]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.8799788951873779, 1.2998846769332886, 1.2914799451828003, 1.2056224346160889, 1.0542112588882446, 1.2540183067321777, 1.418627142906189, 1.2424596548080444, 0.9838146567344666, 1.3121317625045776, 1.2003633975982666, 0.26134154200553894, 1.4297674894332886, 1.3064805269241333, 1.0851850509643555, 1.4626184701919556]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8561411499977112, 0.417243093252182, 1.0365650653839111, 0.7854083776473999, 0.7731208205223083, 0.697069525718689, 0.031193967908620834, 0.9584720730781555, -0.4327412545681, 2.0167295932769775, -0.6656609177589417, 0.7545762658119202, 0.9947009682655334, 0.9529649019241333, 1.0179263353347778, 0.26436978578567505]
Running loglikelihood requests:  55%|██████████████████████████▉                      | 277/504 [15:26<12:28,  3.30s/it]Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.16899234056472778, 1.0286329984664917, 1.5777087211608887, 1.3362677097320557, 1.6031954288482666, 1.5779328346252441, 1.5956350564956665, 1.7663774490356445, 1.6142578125, 1.457675576210022, 1.9071944952011108, 1.4987993240356445, 1.4599539041519165, 1.3255114555358887, 0.8586325645446777, 1.3713668584823608]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.7762871980667114, 1.588338851928711, 1.8327916860580444, 1.8655545711517334, 1.4420666694641113, 1.4848552942276, 1.7182697057724, 2.015817165374756, 2.177446126937866, 1.8276447057724, 1.5061554908752441, 1.27045476436615, 1.7081838846206665, 1.0503029823303223, 1.8550524711608887, 1.7140432596206665]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.7897989749908447, 1.6472848653793335, 1.7003554105758667, 1.9744492769241333, 1.9133260250091553, 1.4622142314910889, 1.6847944259643555, 1.6983221769332886, 1.9515560865402222, 1.7954661846160889, 1.880282998085022, 1.7748463153839111, 2.117267370223999, 1.8996541500091553, 1.9179047346115112, 1.7927606105804443]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1938529014587402, 1.5389344692230225, 1.3626008033752441, 1.1725153923034668, 0.2588030695915222, 1.6192166805267334, 1.5042264461517334, 2.0496084690093994, 1.3373702764511108, 1.5456422567367554, 1.5134317874908447, 1.3900166749954224, 1.609326958656311, 1.625928521156311, 1.306932806968689, 0.8092095851898193]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.4482581615448, 2.519723415374756, 2.3418288230895996, 2.2047579288482666, 2.4139983654022217, 2.3827805519104004, 2.317911148071289, 2.406057834625244, 2.3314549922943115, 2.4941086769104004, 2.586385726928711, 2.4809811115264893, 2.0390384197235107, 2.7352075576782227, 2.5306737422943115, 2.505699396133423]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.916463851928711, 1.7686346769332886, 1.9916431903839111, 1.468349814414978, 1.8530353307724, 1.9793801307678223, 1.723488688468933, 1.8303343057632446, 1.9438716173171997, 1.906346082687378, 1.760342001914978, 2.026895523071289, 2.002113103866577, 1.938076376914978, 1.7106974124908447, 2.0671746730804443]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.207191228866577, 1.8426934480667114, 1.47021484375, 1.6956807374954224, 1.744284749031067, 1.8056800365447998, 1.8118596076965332, 1.8427894115447998, 1.7137690782546997, 1.646836519241333, 1.7340548038482666, 1.5981205701828003, 1.5535988807678223, 0.9855766892433167, 1.4988954067230225, 1.7609503269195557]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.4837987422943115, 2.6181480884552, 2.3581583499908447, 2.5859375, 2.6409451961517334, 2.558849811553955, 2.636462688446045, 2.4102203845977783, 2.2457735538482666, 2.5722975730895996, 2.591252565383911, 2.4612576961517334, 2.425525188446045, 2.600217819213867, 2.5385501384735107, 2.509413480758667]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7678343057632446, 1.7316213846206665, 1.7448770999908447, 1.8597592115402222, 1.3813155889511108, 1.6852906942367554, 1.7629674673080444, 1.804111123085022, 1.7955782413482666, 1.734823226928711, 1.7587970495224, 1.6926549673080444, 1.7443327903747559, 1.5183786153793335, 1.5817110538482666, 1.9310963153839111]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.766209363937378, 1.7721147537231445, 1.8273885250091553, 1.8611359596252441, 1.8341144323349, 1.8843573331832886, 1.8651703596115112, 1.7746697664260864, 1.882772445678711, 1.8896324634552002, 2.0912699699401855, 1.7324528694152832, 1.8602315187454224, 1.856349229812622, 1.8332239389419556, 1.8680729866027832]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5975282192230225, 1.5807745456695557, 1.6257283687591553, 1.6673003435134888, 1.5583416223526, 1.6486376523971558, 1.6932553052902222, 1.6701099872589111, 1.6093670129776, 1.681552529335022, 1.3688524961471558, 1.6159467697143555, 1.6080782413482666, 1.6212217807769775, 1.5840483903884888, 1.6381715536117554]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.2203588485717773, 2.263103485107422, 2.4922995567321777, 2.205270290374756, 2.254786729812622, 2.2527456283569336, 2.367607831954956, 2.338578939437866, 2.2364561557769775, 2.2615106105804443, 2.22509765625, 2.0613152980804443, 2.1889889240264893, 2.1136975288391113, 2.3974690437316895, 2.2679483890533447]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.292712688446045, 5.407466888427734, 5.149974346160889, 5.0849127769470215, 5.161244869232178, 5.211257457733154, 5.329917907714844, 5.33933162689209, 5.410284519195557, 5.297835350036621, 5.231621265411377, 5.131467819213867, 5.219006061553955, 5.224064826965332, 5.355916976928711, 5.116483211517334]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.8675076961517334, 3.8246030807495117, 3.7633516788482666, 3.606950521469116, 3.678887128829956, 3.715900421142578, 3.5457863807678223, 3.719102144241333, 3.754162311553955, 3.908379316329956, 3.7219197750091553, 3.354706287384033, 3.7590532302856445, 3.701011896133423, 3.8058722019195557, 3.8261399269104004]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.7847719192504883, 2.8569416999816895, 2.819159746170044, 2.5156891345977783, 2.812051773071289, 2.839939832687378, 2.688908815383911, 2.5542712211608887, 2.8934426307678223, 2.6917264461517334, 2.7075436115264893, 2.114013671875, 2.717853546142578, 2.731365203857422, 2.867635726928711, 2.807281017303467]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.07088489085435867, 0.08126849681138992, 0.09070234000682831, -0.28610411286354065, -0.2607561945915222, -0.08261971175670624, 0.10890322923660278, -0.04098760709166527, 0.07378149777650833, 0.07011688500642776, 0.08378326147794724, 0.05912618339061737, 0.08272577822208405, 0.09518983215093613, -1.1130731105804443, 0.10409258306026459]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07026419043540955, 0.027821088209748268, 0.026119200512766838, 0.03202425688505173, 0.06619162857532501, 0.006372295320034027, 0.03681417554616928, 0.08684489876031876, 0.015245609916746616, 0.06007973104715347, -0.22811689972877502, 0.03916887193918228, 0.021895112469792366, -0.038606736809015274, 0.04054563492536545, 0.014095134101808071]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07915828377008438, 0.07329962402582169, 0.10087548196315765, 0.09227064996957779, 0.11054292321205139, 0.09056466817855835, 0.06768307834863663, -0.10590099543333054, 0.0784451812505722, 0.11157801747322083, 0.04443296790122986, 0.08671582490205765, -0.2275460660457611, -7.229163747979328e-05, -0.06191691383719444, 0.10185591876506805]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13542450964450836, 0.149723619222641, 0.12160704284906387, 0.15050619840621948, 0.13633477687835693, 0.13765978813171387, 0.00820234976708889, 0.18594235181808472, 0.18739481270313263, -0.5514382719993591, 0.04465362802147865, 0.046266525983810425, 0.2894245982170105, -0.3079536259174347, 0.01792970299720764, -0.07603935897350311]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07375361025333405, 0.11219562590122223, 0.0931401476264, 0.2413291335105896, 0.061403244733810425, -0.07090508937835693, 0.019178297370672226, 0.020648518577218056, -0.09422502666711807, 0.0351981483399868, -0.24598844349384308, 0.14631740748882294, 0.02594919688999653, -0.23263677954673767, 0.13105067610740662, -0.08570399135351181]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06699133664369583, 0.12487580627202988, 0.03379953280091286, 0.16194753348827362, -0.11337980628013611, -0.11175504326820374, 0.1127585768699646, -0.037624672055244446, 0.20911173522472382, -0.10115095227956772, 0.04950201138854027, 0.06567071378231049, -0.2717435359954834, 0.14335857331752777, 0.13391238451004028, 0.0642634704709053]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.10492800176143646, -0.07397210597991943, 0.13106699287891388, 0.35159051418304443, 0.1790761500597, 0.16909164190292358, -0.16759978234767914, -0.10797469317913055, 0.3518953025341034, 0.16320300102233887, -0.6362124681472778, 0.2788648009300232, 0.2039114534854889, -0.30559903383255005, 0.33015891909599304, 0.22288794815540314]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5530415773391724, 0.17313428223133087, 0.16807693243026733, 0.12365297228097916, -1.0501208305358887, 0.371056467294693, 0.1989544779062271, -0.4935742914676666, 0.5891468524932861, 0.18375983834266663, -0.3304443359375, 0.26722291111946106, 0.17713527381420135, 0.2219080626964569, -0.8279416561126709, -0.2100594937801361]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2894279658794403, 0.2869735360145569, -0.3422606289386749, 0.28434765338897705, 0.4940635859966278, 0.22852878272533417, 0.240525484085083, -0.31551873683929443, 0.1480797976255417, 0.9064976572990417, 0.03157975152134895, 0.2636201083660126, 0.3992174565792084, -0.6339031457901001, -0.5795338153839111, 0.4201057255268097]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.29691803455352783, 0.10063634067773819, 0.4682484567165375, 1.1663798093795776, 0.2745596468448639, 0.15791745483875275, 0.5101038217544556, 0.49987712502479553, 0.7774513363838196, 0.004560126457363367, 0.15022553503513336, 0.6333888173103333, 0.6727097034454346, 0.14047810435295105, 0.7250015735626221, 0.6130891442298889]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.087437391281128, 0.7911877036094666, 0.5239717960357666, 0.6839099526405334, 0.7770546078681946, 0.561863124370575, 0.595533013343811, 0.6937515735626221, 0.1159825548529625, 0.44752776622772217, 0.7156322002410889, 0.9455966353416443, 0.17854633927345276, 0.1557156890630722, 0.976536214351654, 0.31822431087493896]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.8967165350914001, 1.1279932260513306, 0.8790183067321777, 1.0658769607543945, 1.1915223598480225, 0.2956222891807556, 0.8475121855735779, 0.8479284048080444, 1.025926947593689, 1.4684457778930664, 0.806867778301239, 0.827924907207489, 0.22398851811885834, 0.35537245869636536, -0.24455685913562775, 0.6662717461585999]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.0652613416314125, 0.758396327495575, 1.2464219331741333, 1.1183401346206665, 0.1676170527935028, 0.5755475163459778, 1.0983285903930664, 1.241760492324829, 0.4348084628582001, 0.2035752534866333, 0.7664139270782471, 0.8724705576896667, 0.8169625997543335, 0.8436699509620667, 0.5325347185134888, 0.8536517024040222]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6283659338951111, 0.9030561447143555, 0.8962642550468445, 0.3289870023727417, 1.0175460577011108, -0.3942725956439972, 1.100682020187378, 2.3124358654022217, 1.0074843168258667, 0.9079629778862, 1.2120816707611084, 1.1065093278884888, -0.13251833617687225, 0.6976369619369507, 1.1498943567276, 0.5187798142433167]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6048814058303833, 0.7179015278816223, 0.9051613807678223, 0.8122918605804443, 0.9699987173080444, 0.8359835147857666, 0.16769659519195557, 1.37702214717865, 0.8939309120178223, 0.7882109880447388, -0.44617098569869995, 0.697431743144989, 0.834058403968811, 1.035740613937378, 1.1326844692230225, 0.5438979864120483]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4178564250469208, 1.149758219718933, 0.9038966298103333, 1.0867218971252441, 0.6858610510826111, 0.8764088153839111, 1.3458402156829834, 1.0258709192276, 0.9443519711494446, 1.0153048038482666, 0.7189421057701111, -0.3870856463909149, 0.9801886081695557, 0.9662205576896667, 0.8503217697143555, 1.285169243812561]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.675398051738739, 0.1309134066104889, 0.8128171563148499, 0.8641257286071777, 0.7249365448951721, 0.4561917781829834, 0.2563413977622986, 0.8312668204307556, -1.0951918363571167, 2.5861775875091553, -0.9151791334152222, 0.5297486186027527, 0.760694146156311, 0.7398301362991333, 0.8041672110557556, 0.3146302402019501]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.025101209059357643, 0.5820212364196777, 1.3224257230758667, 1.1134753227233887, 1.3965643644332886, 1.3085616827011108, 1.2343590259552002, 1.2863569259643555, 1.837842583656311, 1.2872815132141113, 2.0353963375091553, 1.3622406721115112, 1.3834028244018555, 1.3821080923080444, 0.4716266691684723, 1.1137688159942627]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3864306211471558, 1.2042136192321777, 1.339003324508667, 1.717149019241333, 0.7967560291290283, 1.1024750471115112, 1.8696209192276, 1.5642930269241333, 2.199923038482666, 1.289894938468933, 1.0445536375045776, 0.6460241079330444, 1.106480360031128, 0.8177870512008667, 1.2731493711471558, 1.3339523077011108]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.2334144115447998, 1.1629579067230225, 1.2651606798171997, 1.145956039428711, 1.4059938192367554, 0.7383056282997131, 1.009365439414978, 1.0543593168258667, 1.7835073471069336, 1.319688081741333, 1.4131019115447998, 1.2813620567321777, 1.5845887660980225, 1.2766072750091553, 1.2594774961471558, 1.2357838153839111]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0485659837722778, 0.9162717461585999, 0.9086753726005554, 0.8585785627365112, -0.09641177952289581, 1.191990613937378, 1.0123591423034668, 1.4528127908706665, 0.8110291361808777, 1.1598200798034668, 1.1122126579284668, 1.1554495096206665, 0.9350185990333557, 1.169345498085022, 0.7370555400848389, 0.36546599864959717]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  56%|███████████████████████████▎                     | 281/504 [15:38<12:01,  3.24s/it]Layer: gate_21 - Captured router_logits: [1.8801229000091553, 2.045210123062134, 1.8880635499954224, 1.7300524711608887, 1.7250256538391113, 1.919825792312622, 1.682088851928711, 1.9746414422988892, 1.7411949634552002, 1.8062564134597778, 2.1221823692321777, 2.496541976928711, 1.4759541749954224, 2.017129898071289, 1.8814677000045776, 1.846823811531067]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.2228803634643555, 1.2617508172988892, 1.7869492769241333, 0.9194756150245667, 1.2804815769195557, 1.4240843057632446, 1.13671875, 1.2375768423080444, 1.3443583250045776, 1.3377785682678223, 1.5051229000091553, 1.9757940769195557, 1.3209208250045776, 1.3864625692367554, 0.9889296293258667, 1.4669249057769775]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.395843982696533, 1.6821528673171997, 0.7479467988014221, 1.2501600980758667, 1.3709015846252441, 1.2275230884552002, 1.1977459192276, 1.1758133172988892, 1.327004313468933, 1.3290694952011108, 1.271388292312622, 1.3721823692321777, 1.0185546875, 0.43374934792518616, 1.0393826961517334, 1.2412269115447998]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.404585123062134, 2.318263292312622, 2.2733733654022217, 2.0366930961608887, 2.5265753269195557, 2.07421875, 2.3436858654022217, 2.0642290115356445, 2.0693519115448, 2.0747950077056885, 2.861039876937866, 2.016073226928711, 2.237640857696533, 2.3737833499908447, 2.231621503829956, 2.2115137577056885]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.5975922346115112, 1.5989049673080444, 1.5031697750091553, 1.6914702653884888, 1.2542104721069336, 1.5576331615447998, 1.6102715730667114, 1.9788358211517334, 1.6359022855758667, 1.5333632230758667, 1.5783971548080444, 1.4178086519241333, 1.5254546403884888, 1.4611936807632446, 1.920370101928711, 1.9295274019241333]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.7255538702011108, 1.733056902885437, 1.6108318567276, 1.650918960571289, 1.5937179327011108, 1.7278112173080444, 1.6140822172164917, 1.6442351341247559, 1.7022324800491333, 1.7767914533615112, 2.240682601928711, 1.577100396156311, 1.5540311336517334, 1.802382230758667, 1.8363738059997559, 1.6686371564865112]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5433789491653442, 1.6352719068527222, 1.5251044034957886, 1.6694276332855225, 1.5296651124954224, 1.5468806028366089, 1.7234057188034058, 1.6338610649108887, 1.5302884578704834, 1.6194367408752441, 1.2736616134643555, 1.6155065298080444, 1.6137855052947998, 1.9054975509643555, 1.5403432846069336, 1.5441874265670776]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.8966124057769775, 1.9570952653884888, 2.2499358654022217, 1.8358094692230225, 2.0171139240264893, 1.8750640153884888, 2.466604709625244, 1.9528849124908447, 1.8827484846115112, 1.8461834192276, 1.807088851928711, 1.6695055961608887, 1.8909531831741333, 1.6703060865402222, 2.1552574634552, 1.8534516096115112]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.8496413230896, 5.284964084625244, 4.870709419250488, 4.7587890625, 4.753777980804443, 4.642417907714844, 4.8496413230896, 5.073930740356445, 5.093621730804443, 4.795466423034668, 4.8027663230896, 4.7248334884643555, 4.913229942321777, 4.872951030731201, 5.010053634643555, 4.83933162689209]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.5629162788391113, 3.383452892303467, 3.297034978866577, 3.204766035079956, 3.518826961517334, 3.5184426307678223, 3.161701202392578, 3.480276584625244, 3.5909643173217773, 3.423924207687378, 3.2676100730895996, 2.9160635471343994, 3.21728515625, 3.498943328857422, 3.455174207687378, 3.532402753829956]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.380955457687378, 2.5020172595977783, 2.4345543384552, 2.486232042312622, 2.3374104499816895, 2.421586751937866, 2.5550718307495117, 2.190797805786133, 2.394083023071289, 2.2343430519104004, 3.0559041500091553, 1.834660530090332, 2.4071145057678223, 2.258612871170044, 2.844198226928711, 2.3528432846069336]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06982825696468353, 0.08383197337388992, 0.10184977203607559, -0.26717281341552734, -0.26122644543647766, -0.09232557564973831, 0.10332173854112625, -0.11197277903556824, 0.06776119023561478, 0.0676303580403328, 0.07770229130983353, 0.035554807633161545, 0.0801967903971672, 0.1021910086274147, -1.0514752864837646, 0.10193262249231339]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07984155416488647, 0.04776139557361603, 0.02798316814005375, 0.040377531200647354, 0.06475438922643661, 0.03179093077778816, 0.0616547130048275, 0.08109759539365768, 0.05998734384775162, 0.06168535724282265, -0.1987617462873459, 0.052602753043174744, 0.014141760766506195, -0.02232758142054081, 0.034719135612249374, 0.03130480647087097]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09440524876117706, 0.04697917774319649, 0.08690353482961655, 0.06883050501346588, 0.1356409192085266, 0.09018032252788544, 0.07215113192796707, -0.08987043797969818, 0.1026938259601593, 0.12665006518363953, 0.036678094416856766, 0.08696173131465912, -0.20154514908790588, 0.017630962654948235, -0.015427266247570515, 0.11358604580163956]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1412046104669571, 0.14510956406593323, 0.12661567330360413, 0.13253484666347504, 0.12947170436382294, 0.12075629085302353, 0.0026733146514743567, 0.1724662482738495, 0.1731320172548294, -0.5433127880096436, 0.025747818872332573, 0.08581063896417618, 0.26308876276016235, -0.3016275465488434, 0.08397554606199265, -0.06994426995515823]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06551165878772736, 0.09354507923126221, 0.08256763964891434, 0.23710745573043823, 0.0709688812494278, -0.03281307965517044, 0.03027009591460228, 0.015325436368584633, -0.1180325374007225, 0.007925364188849926, -0.26361536979675293, 0.10223805159330368, 0.07549808919429779, -0.22315868735313416, 0.13524910807609558, -0.11562057584524155]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.04069153219461441, 0.13732002675533295, 0.059582922607660294, 0.22680816054344177, -0.14193284511566162, -0.12080418318510056, 0.12888455390930176, -0.017278451472520828, 0.1869117170572281, -0.07114315778017044, -0.05226501077413559, 0.10125228017568588, -0.24941587448120117, 0.1792425960302353, 0.12309498339891434, 0.08199404925107956]
Layer: gate_5 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.12485522776842117, -0.02612481266260147, 0.18423840403556824, 0.2963411808013916, 0.17251649498939514, 0.15295031666755676, -0.20055049657821655, -0.08143502473831177, 0.3557502031326294, 0.18664045631885529, -0.5869150757789612, 0.30176329612731934, 0.2515980005264282, -0.335662841796875, 0.24893541634082794, 0.19928854703903198]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.4958898425102234, 0.2013310194015503, 0.21784526109695435, 0.1560666412115097, -0.822743833065033, 0.3094164729118347, 0.21720747649669647, -0.42580747604370117, 0.596906304359436, 0.21042890846729279, -0.21304850280284882, 0.2683660387992859, 0.13670815527439117, 0.26330137252807617, -0.6656817197799683, -0.3054825961589813]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.29092785716056824, 0.2867320775985718, -0.30627214908599854, 0.2841413617134094, 0.4852050244808197, 0.39103761315345764, 0.2682207524776459, -0.30417802929878235, 0.23132804036140442, 0.8763498067855835, 0.1278325915336609, 0.25120556354522705, 0.5092410445213318, -0.5694895386695862, -0.5697162747383118, 0.4384995102882385]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4555596113204956, 0.20484936237335205, 0.5073111057281494, 1.0696123838424683, 0.3831595480442047, 0.1848580241203308, 0.6378456354141235, 0.6244010329246521, 0.8905362486839294, 0.21696257591247559, 0.166900634765625, 0.7059336304664612, 0.6618999242782593, 0.0026149277109652758, 0.7960194945335388, 0.6754745841026306]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0894795656204224, 0.9370924234390259, 0.49151358008384705, 0.6875605583190918, 0.7972543239593506, 0.7286686897277832, 0.6576845645904541, 0.7600319385528564, 0.04284012317657471, 0.49481505155563354, 0.8161641359329224, 0.9855573773384094, 0.3293870687484741, 0.24449674785137177, 1.0616296529769897, 0.45200619101524353]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.1172358989715576, 1.35566246509552, 0.9607760906219482, 1.342787504196167, 1.4516217708587646, 0.22638343274593353, 1.0244140625, 0.8590965867042542, 1.081716537475586, 1.6644418239593506, 0.90537029504776, 1.0000323057174683, 0.2985718846321106, 0.5755025148391724, 0.010603975504636765, 0.7971150875091553]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.1326768398284912, 0.8382134437561035, 1.3529506921768188, 1.2090004682540894, 0.4604416489601135, 0.5735287070274353, 1.2791929244995117, 1.36962890625, 0.3652964234352112, 0.14066433906555176, 0.91464763879776, 0.9302524328231812, 0.8645322322845459, 0.8968233466148376, 0.7335518002510071, 0.98456871509552]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.850377082824707, 1.0824428796768188, 1.2189315557479858, 0.4886282980442047, 1.1564760208129883, -0.22189457714557648, 1.2069666385650635, 2.1659348011016846, 1.2473770380020142, 1.02695631980896, 1.549102544784546, 1.30526864528656, 0.23984496295452118, 0.8789488673210144, 1.1295276880264282, 0.7035825252532959]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8796548247337341, 0.8251226544380188, 1.0689727067947388, 0.8435078859329224, 1.0597398281097412, 1.1568049192428589, 0.46165981888771057, 1.6599887609481812, 0.7813448309898376, 0.8568569421768188, -0.32344067096710205, 0.8262283802032471, 0.9806463122367859, 1.1369527578353882, 1.136404037475586, 0.5158968567848206]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4355821907520294, 1.258070707321167, 1.0346074104309082, 1.0671488046646118, 0.8300397992134094, 0.9546584486961365, 1.7154672145843506, 1.0933141708374023, 1.1039191484451294, 1.067826747894287, 0.7675660252571106, -0.28973037004470825, 1.1288729906082153, 0.9204545617103577, 0.995448112487793, 1.4539552927017212]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6449130773544312, 0.05249704420566559, 0.7936961054801941, 0.7893580794334412, 0.7323895692825317, 0.48804113268852234, 0.6284369826316833, 0.79482501745224, -0.9601924419403076, 2.1930365562438965, -0.949416995048523, 0.5065262317657471, 0.8034930229187012, 0.8877033591270447, 0.8478317856788635, 0.35883527994155884]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.04777589812874794, 0.8261854648590088, 1.502873182296753, 1.326991081237793, 1.4729467630386353, 1.5061984062194824, 1.5087769031524658, 1.4241509437561035, 1.958913803100586, 1.412052869796753, 2.4705095291137695, 1.7304970026016235, 1.4616355895996094, 1.3635233640670776, 0.4849122166633606, 1.315006971359253]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5598528385162354, 1.35508131980896, 1.5778828859329224, 1.9346914291381836, 1.0444175004959106, 1.2156507968902588, 2.2458677291870117, 1.805139422416687, 2.5396597385406494, 1.5291838645935059, 1.1999269723892212, 0.9117300510406494, 1.247283935546875, 0.95378577709198, 1.5673747062683105, 1.5807076692581177]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.4758522510528564, 1.3664449453353882, 1.7011395692825317, 1.330352544784546, 1.6397210359573364, 1.0494576692581177, 1.2730339765548706, 1.2451171875, 2.1182689666748047, 1.4131747484207153, 1.68788743019104, 1.4974173307418823, 1.7512383460998535, 1.5432108640670776, 1.5354790687561035, 1.4992252588272095]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2880617380142212, 1.034236192703247, 1.0104435682296753, 0.9100856184959412, -3.026536614925135e-05, 1.3983970880508423, 1.1623514890670776, 1.7458032369613647, 0.7930887937545776, 1.3089165687561035, 1.24551260471344, 1.104278326034546, 1.11972177028656, 1.31621253490448, 0.9003885984420776, 0.35753145813941956]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.130746364593506, 2.3066890239715576, 2.2093234062194824, 1.9880552291870117, 1.9410511255264282, 2.1530861854553223, 2.0186595916748047, 2.290870428085327, 2.0472946166992188, 2.151794910430908, 2.401794910430908, 2.825025796890259, 1.7453027963638306, 2.466554641723633, 2.2068698406219482, 2.161350727081299]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.369398832321167, 1.4057011604309082, 2.0796101093292236, 1.0067129135131836, 1.4221655130386353, 1.5585614442825317, 1.216094732284546, 1.3701252937316895, 1.4453125, 1.569779872894287, 1.5652117729187012, 2.114088296890259, 1.4864411354064941, 1.5721526145935059, 1.1856598854064941, 1.6322636604309082]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.46875, 1.892303705215454, 1.1822701692581177, 1.492639422416687, 1.714585542678833, 1.4949315786361694, 1.5046164989471436, 1.453125, 1.5292645692825317, 1.5055203437805176, 1.5539772510528564, 1.5707160234451294, 1.23866868019104, 0.7184776067733765, 1.3608276844024658, 1.4871190786361694]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.361053705215454, 2.3527891635894775, 2.2950026988983154, 2.025632858276367, 2.538126230239868, 2.146758794784546, 2.380100727081299, 2.0370285511016846, 2.0380940437316895, 2.116154432296753, 2.784252405166626, 2.076188087463379, 2.2328898906707764, 2.3813273906707764, 2.227208137512207, 2.1459193229675293]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Running loglikelihood requests:  57%|███████████████████████████▋                     | 285/504 [15:51<11:52,  3.25s/it]Layer: gate_25 - Captured router_logits: [1.6954416036605835, 1.681366205215454, 1.5588842630386353, 1.8211195468902588, 1.3562757968902588, 1.71836256980896, 1.70761239528656, 2.0112345218658447, 1.700703740119934, 1.5782217979431152, 1.64414381980896, 1.4732695817947388, 1.6384620666503906, 1.4703963994979858, 2.1357502937316895, 2.0110085010528564]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_26 - Captured router_logits: [1.8105629682540894, 2.003074884414673, 1.7425426244735718, 1.7729532718658447, 1.7999095916748047, 1.9352381229400635, 1.8299843072891235, 1.8065115213394165, 1.8608620166778564, 1.9145629405975342, 2.4844717979431152, 1.7224464416503906, 1.761976957321167, 1.91444993019104, 1.9317898750305176, 1.8482695817947388]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.64126455783844, 1.7143898010253906, 1.61689293384552, 1.702398657798767, 1.5753809213638306, 1.600429892539978, 1.827824354171753, 1.7187459468841553, 1.6080857515335083, 1.687637209892273, 1.3312565088272095, 1.6224093437194824, 1.735036849975586, 2.070300340652466, 1.682875394821167, 1.6114169359207153]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.111053705215454, 2.208742141723633, 2.44256854057312, 2.049199342727661, 2.2044808864593506, 2.124321937561035, 2.6539578437805176, 2.232276678085327, 2.100142002105713, 2.044679641723633, 2.074735164642334, 1.8653473854064941, 2.00897479057312, 1.898179292678833, 2.4295260906219482, 2.080223321914673]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.103370189666748, 5.636169910430908, 5.112668037414551, 4.897921085357666, 4.976013660430908, 4.973786354064941, 5.103370189666748, 5.30136251449585, 5.302234172821045, 5.019821643829346, 5.03357458114624, 4.997998237609863, 5.081982612609863, 5.184206962585449, 5.309981822967529, 5.17423152923584]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.7179105281829834, 3.598334312438965, 3.496222972869873, 3.365678310394287, 3.5870673656463623, 3.587228775024414, 3.2852208614349365, 3.7247869968414307, 3.7454802989959717, 3.5729596614837646, 3.427492141723633, 3.0495283603668213, 3.3639914989471436, 3.5844523906707764, 3.653280019760132, 3.6407217979431152]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.7276601791381836, 2.8448476791381836, 2.9081223011016846, 2.7173941135406494, 2.78983736038208, 2.806753635406494, 2.764979362487793, 2.4855048656463623, 2.8056559562683105, 2.55039381980896, 3.260782480239868, 1.9598116874694824, 2.8091747760772705, 2.683303117752075, 3.309788227081299, 2.747417449951172]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.08762075752019882, 0.09111456573009491, 0.09908414632081985, -0.19518601894378662, -0.1597955822944641, -0.07484126836061478, 0.11626260727643967, -0.20325034856796265, 0.11042685061693192, 0.08141468465328217, 0.0870843380689621, 0.07628700882196426, 0.08242721855640411, 0.10248307138681412, -0.923690915107727, 0.10608755052089691]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.10117913782596588, 0.05265442281961441, 0.02937524951994419, 0.05991120636463165, 0.06929079443216324, 0.05756762623786926, 0.04986751824617386, 0.052512429654598236, -0.0019508551340550184, 0.06798496842384338, -0.13828913867473602, 0.04775300994515419, 0.019127460196614265, -0.0025927803944796324, 0.004200541414320469, -0.003868292085826397]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0665937066078186, 0.08277470618486404, 0.11785466223955154, 0.05059849098324776, 0.09391702711582184, 0.09234165400266647, 0.05861493572592735, -0.058667901903390884, 0.07938318699598312, 0.05710916966199875, 0.011806653812527657, 0.09537651389837265, -0.10136451572179794, 0.032188937067985535, -0.03647417947649956, 0.11373195052146912]
Layer: gate_2 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11835019290447235, 0.1622321456670761, 0.14707203209400177, 0.1754905730485916, 0.13068459928035736, 0.1326388567686081, 0.03481614217162132, 0.18013466894626617, 0.19031694531440735, -0.4575035274028778, 0.026539195328950882, 0.1622355431318283, 0.10454975813627243, -0.2113707959651947, -0.08184029161930084, 0.013812609016895294]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.11951062083244324, 0.11577152460813522, 0.1022985503077507, 0.022937428206205368, 0.09410335123538971, -0.06285561621189117, 0.08075921982526779, 0.06668175756931305, -0.032572850584983826, 0.003070421516895294, -0.13214552402496338, 0.16810986399650574, -0.16885149478912354, -0.11572767049074173, 0.15359951555728912, -0.04503190144896507]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.12173430621623993, 0.16316008567810059, 0.12141822278499603, 0.04869735240936279, -0.09970887005329132, -0.009560325182974339, 0.1016964241862297, 0.0639466866850853, 0.051165621727705, -0.015509487129747868, -0.1854010969400406, 0.11464104801416397, -0.15057681500911713, 0.14328104257583618, 0.15145975351333618, 0.10890494287014008]
Layer: gate_5 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.2488037645816803, 0.1381690949201584, 0.12237346917390823, 0.32649874687194824, 0.18612040579319, 0.21074582636356354, -0.07732082158327103, -0.019791405647993088, 0.19562827050685883, 0.2623124420642853, -0.3937675654888153, 0.30242452025413513, 0.23086723685264587, -0.2024301290512085, 0.0521712489426136, 0.14287106692790985]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.1858065277338028, 0.2869558334350586, 0.28085756301879883, 0.19192719459533691, -0.5765870213508606, 0.27363649010658264, 0.2916547358036041, -0.08605704456567764, 0.254717618227005, 0.1146332323551178, -0.18931390345096588, 0.26230841875076294, 0.23664326965808868, 0.1990543007850647, -0.5697349309921265, -0.3331596553325653]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3138372302055359, 0.3820255994796753, -0.10876565426588058, 0.4312211275100708, 0.48221203684806824, 0.5190712213516235, 0.31158536672592163, -0.09194693714380264, 0.2719389796257019, 0.4967454671859741, 0.22608885169029236, -0.19166652858257294, 0.54936683177948, -0.5272287130355835, -0.5443137884140015, 0.25667741894721985]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.7383934855461121, 0.22097349166870117, 0.48674502968788147, 0.5237910747528076, 0.5456477403640747, 0.1978631168603897, 0.7435706257820129, 0.6355726718902588, 0.45976823568344116, 0.16078375279903412, 0.11861148476600647, 0.7548666596412659, 0.7393001914024353, 0.060519635677337646, 0.6652246713638306, 0.7600117921829224]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_10 - Captured router_logits: [1.007330298423767, 0.8946967124938965, 0.7544187307357788, 0.8423457145690918, 1.0049474239349365, 0.3156415522098541, 0.8499685525894165, 0.7876097559928894, 0.251592218875885, 0.41028085350990295, 0.8191341757774353, 0.7875048518180847, 0.34342390298843384, 0.3055403530597687, 0.87541663646698, 0.5760539770126343]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0424926280975342, 0.9816501140594482, 1.04648756980896, 1.0894644260406494, 1.2732831239700317, 0.44148796796798706, 0.9762386679649353, 0.9090551137924194, 1.0160748958587646, 1.1343297958374023, 0.9586675763130188, 1.2474415302276611, 0.5792922377586365, 0.7457152009010315, 0.09161225706338882, 0.8572801351547241]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.32236248254776, 1.0751529932022095, 1.2884652614593506, 1.21151864528656, 0.6771451830863953, 0.8385685682296753, 0.9529837369918823, 1.0028480291366577, 0.7132447361946106, 0.4211044907569885, 1.1369285583496094, 1.047988772392273, 1.0464048385620117, 1.03867506980896, 0.8461873531341553, 1.1927298307418823]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.161641240119934, 1.1043146848678589, 1.5133167505264282, 0.9139666557312012, 1.3339036703109741, -0.015748079866170883, 1.3220235109329224, 1.764333724975586, 1.1680817604064941, 1.4164191484451294, 1.2680381536483765, 1.5874871015548706, 0.6415269374847412, 1.2733443975448608, 1.4481372833251953, 0.9956871867179871]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.1048190593719482, 0.9846171140670776, 1.1096655130386353, 1.1675490140914917, 1.38437819480896, 1.4030418395996094, 0.4958254098892212, 1.1710799932479858, 0.9321975111961365, 1.115972876548767, -0.16733822226524353, 1.10312819480896, 1.2580254077911377, 1.3804316520690918, 1.3797616958618164, 0.9745811223983765]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.7294074296951294, 1.2944053411483765, 1.237635612487793, 1.1501646041870117, 0.9728297591209412, 1.2060950994491577, 1.27389395236969, 1.1806236505508423, 0.98017817735672, 1.2789578437805176, 1.1354031562805176, 0.17027068138122559, 1.3756214380264282, 1.2896758317947388, 1.0550587177276611, 1.3042824268341064]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.8029724359512329, 0.30398812890052795, 1.0146574974060059, 0.76249760389328, 0.7218531966209412, 0.6696857810020447, 0.012367311865091324, 0.9270806312561035, -0.5829755067825317, 2.024237632751465, -0.8304216265678406, 0.6619318127632141, 0.9541419148445129, 0.9150572419166565, 0.9895685315132141, 0.15227539837360382]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.0009538634913042188, 0.9131787419319153, 1.4897339344024658, 1.2140952348709106, 1.523163080215454, 1.4825348854064941, 1.4662319421768188, 1.644273042678833, 1.5028085708618164, 1.3796709775924683, 1.8150503635406494, 1.4061532020568848, 1.3291512727737427, 1.2432528734207153, 0.6418890953063965, 1.2844018936157227]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.7233988046646118, 1.5593039989471436, 1.7594165802001953, 1.8010717630386353, 1.327616572380066, 1.3272775411605835, 1.5825154781341553, 1.9367252588272095, 2.1538610458374023, 1.794647455215454, 1.367849349975586, 1.1365845203399658, 1.62461256980896, 0.9703994393348694, 1.7520661354064941, 1.661189317703247]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.6285834312438965, 1.5641463994979858, 1.5822087526321411, 1.8804235458374023, 1.811208724975586, 1.3005976676940918, 1.565486192703247, 1.5448734760284424, 1.7996513843536377, 1.6973463296890259, 1.7790870666503906, 1.678460717201233, 2.025463342666626, 1.7916128635406494, 1.8247352838516235, 1.682479977607727]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1825566291809082, 1.4976433515548706, 1.292266607284546, 1.137471318244934, 0.11047161370515823, 1.615880012512207, 1.4223915338516235, 1.920151948928833, 1.3343071937561035, 1.49590003490448, 1.4969170093536377, 1.3610053062438965, 1.5489411354064941, 1.6081805229187012, 1.2293468713760376, 0.7857585549354553]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.333580732345581, 2.4197442531585693, 2.257554292678833, 2.0822248458862305, 2.285446882247925, 2.279183864593506, 2.1749417781829834, 2.295454502105713, 2.2418646812438965, 2.3650567531585693, 2.4834065437316895, 2.3470427989959717, 1.8539353609085083, 2.623579502105713, 2.3654441833496094, 2.3922390937805176]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.778764247894287, 1.6304235458374023, 1.8614734411239624, 1.3196184635162354, 1.6729403734207153, 1.8418775796890259, 1.5219686031341553, 1.67206871509552, 1.7856404781341553, 1.7583613395690918, 1.6200929880142212, 1.861796259880066, 1.8543388843536377, 1.7879648208618164, 1.5369882583618164, 1.9311401844024658]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.0992703437805176, 1.7370867729187012, 1.3212205171585083, 1.5844202041625977, 1.6405926942825317, 1.7051265239715576, 1.7217845916748047, 1.7202672958374023, 1.6118123531341553, 1.545648217201233, 1.6239992380142212, 1.5032122135162354, 1.367050290107727, 0.7947105169296265, 1.3424022197723389, 1.640173077583313]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.364217519760132, 2.5000646114349365, 2.262396812438965, 2.4655861854553223, 2.5206611156463623, 2.3949508666992188, 2.512655019760132, 2.2776987552642822, 2.0758328437805176, 2.4398889541625977, 2.4262008666992188, 2.325671434402466, 2.32147479057312, 2.4804365634918213, 2.40702486038208, 2.3252196311950684]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6898889541625977, 1.6932463645935059, 1.6951833963394165, 1.8292548656463623, 1.3153328895568848, 1.6469202041625977, 1.7283058166503906, 1.7794421911239624, 1.7683367729187012, 1.685078740119934, 1.7239798307418823, 1.6181559562683105, 1.7080966234207153, 1.4598076343536377, 1.5163675546646118, 1.8846849203109741]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.671229362487793, 1.723705768585205, 1.7611054182052612, 1.7991671562194824, 1.7684820890426636, 1.8560305833816528, 1.830830693244934, 1.7155539989471436, 1.8284862041473389, 1.834040880203247, 2.024874210357666, 1.691462755203247, 1.7861570119857788, 1.804945707321167, 1.7899061441421509, 1.824254035949707]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5610650777816772, 1.5609463453292847, 1.5915931463241577, 1.64833664894104, 1.5075219869613647, 1.627023696899414, 1.6727699041366577, 1.639196515083313, 1.5733158588409424, 1.6577833890914917, 1.316462755203247, 1.5776245594024658, 1.58266282081604, 1.5914134979248047, 1.5678871870040894, 1.60751473903656]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.2118091583251953, 2.2571022510528564, 2.464988946914673, 2.1561853885650635, 2.2366347312927246, 2.224754571914673, 2.312209367752075, 2.30975604057312, 2.20819354057312, 2.1983470916748047, 2.2079029083251953, 2.001969337463379, 2.182544469833374, 2.102756977081299, 2.3591814041137695, 2.2538740634918213]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  57%|████████████████████████████                     | 289/504 [16:05<11:46,  3.29s/it]Layer: gate_29 - Captured router_logits: [5.258716583251953, 5.358987808227539, 5.117316722869873, 5.010556697845459, 5.126678943634033, 5.20157527923584, 5.277117729187012, 5.282186031341553, 5.3831353187561035, 5.219944477081299, 5.1677751541137695, 5.113442897796631, 5.197120189666748, 5.191761493682861, 5.314888954162598, 5.097171783447266]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.839198112487793, 3.8138558864593506, 3.7727272510528564, 3.60585618019104, 3.6540627479553223, 3.719193935394287, 3.539034366607666, 3.7353758811950684, 3.732882022857666, 3.8639752864837646, 3.6959340572357178, 3.2943458557128906, 3.6623282432556152, 3.6888880729675293, 3.801685094833374, 3.8216440677642822]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.7428009510040283, 2.847946882247925, 2.85117506980896, 2.5452609062194824, 2.8266398906707764, 2.8439435958862305, 2.709646224975586, 2.5345427989959717, 2.913546085357666, 2.647533655166626, 2.7460615634918213, 2.170300245285034, 2.734375, 2.7370221614837646, 2.880746364593506, 2.782186269760132]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.07456553727388382, 0.09000556915998459, 0.08941685408353806, -0.16906161606311798, -0.15382295846939087, -0.13220541179180145, 0.11177152395248413, -0.1609281599521637, 0.08562511205673218, 0.07753118872642517, 0.08470064401626587, 0.07996954768896103, 0.0863148644566536, 0.09271752834320068, -0.9123173356056213, 0.10247802734375]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08297418802976608, 0.07455495744943619, 0.04582573473453522, 0.04773320257663727, 0.0814439132809639, 0.029549991711974144, 0.058432091027498245, 0.067441925406456, 0.02808121219277382, 0.06452938914299011, -0.15993160009384155, 0.041237134486436844, 0.013835313729941845, -0.014261454343795776, -0.005470436066389084, 0.03228291869163513]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07260224968194962, 0.053672950714826584, 0.08876428753137589, 0.061769139021635056, 0.12076818943023682, 0.10833124816417694, 0.06391298025846481, -0.06756437569856644, 0.0705438181757927, 0.06708969175815582, -0.0006181091885082424, 0.10406651347875595, -0.10901090502738953, 0.054833803325891495, 0.006750347092747688, 0.10315550118684769]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11933135986328125, 0.1650872677564621, 0.11075931787490845, 0.15906082093715668, 0.14773841202259064, 0.13345690071582794, 0.06774646043777466, 0.20571796596050262, 0.1815365105867386, -0.31557226181030273, -0.004574174527078867, 0.09182732552289963, 0.15594129264354706, -0.12055186927318573, -0.07436032593250275, -0.008838813751935959]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07944341003894806, 0.10014926642179489, 0.1109289601445198, 0.060196660459041595, 0.04136547073721886, -0.04063229635357857, 0.058798301964998245, 0.08168145269155502, -0.04741264879703522, 0.016876220703125, -0.058405451476573944, 0.17939072847366333, -0.13248103857040405, -0.05310462415218353, 0.12626737356185913, -0.02077246829867363]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07076109200716019, 0.15595261752605438, 0.06764362007379532, 0.00988487433642149, -0.06789327412843704, 0.09681868553161621, 0.17491400241851807, 0.05667421966791153, 0.10888159275054932, -0.0520193912088871, -0.09609369933605194, 0.11076239496469498, -0.06509091705083847, 0.12082640081644058, 0.1361270546913147, 0.130125030875206]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08736919611692429, 0.05834794417023659, 0.15339326858520508, 0.38105785846710205, 0.1822378933429718, 0.1026466116309166, 0.0004823187773581594, -0.024636661633849144, 0.5601509213447571, 0.15707461535930634, -0.3772762417793274, 0.3531869947910309, 0.19119493663311005, -0.12399061024188995, 0.0833192691206932, 0.14929763972759247]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.26138895750045776, 0.27927905321121216, 0.3219732642173767, 0.14231039583683014, -0.706986129283905, 0.3350568413734436, 0.17573368549346924, 0.0736868754029274, 0.24757885932922363, 0.2253369241952896, -0.013923196122050285, 0.2276311218738556, 0.4338986575603485, 0.12098142504692078, -0.3079167306423187, -0.06403049826622009]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2340744435787201, 0.3731032907962799, -0.12688285112380981, 0.3600488305091858, 0.4824092984199524, 0.26899245381355286, 0.3069266080856323, -0.15441176295280457, 0.23194871842861176, 0.5637747049331665, 0.028252096846699715, 0.3752298653125763, 0.33375871181488037, -0.18336191773414612, -0.22012662887573242, 0.24332024157047272]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.39726758003234863, 0.33709049224853516, 0.6000669002532959, 0.6098735332489014, 0.45364508032798767, 0.29140183329582214, 0.619324266910553, 0.5661436319351196, 0.270149290561676, 0.27662262320518494, 0.46518123149871826, 0.6800732016563416, 1.098122477531433, 0.7134833931922913, 0.7483505010604858, 0.7191151976585388]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9378272294998169, 0.8118229508399963, 0.6754283905029297, 0.8617056012153625, 1.0624507665634155, 0.6329481601715088, 0.6297966241836548, 0.6894859671592712, 0.3711383640766144, 1.017727255821228, 0.6905609965324402, 0.839790403842926, 0.9166588187217712, 0.2919996380805969, 0.4196782410144806, 0.43379533290863037]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.1576040983200073, 1.0750148296356201, 1.0193588733673096, 1.378028154373169, 1.41219961643219, 0.7493578195571899, 0.952041745185852, 1.2668395042419434, 0.9902046322822571, 1.2329388856887817, 1.0299451351165771, 1.0045299530029297, 0.7539370059967041, 0.6919682621955872, 0.12062495946884155, 0.9058725237846375]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.3483332693576813, 0.9386899471282959, 1.0304129123687744, 1.2084755897521973, 0.5490989089012146, 0.7563107013702393, 1.055030107498169, 1.0640223026275635, 0.824699878692627, 0.7169858813285828, 0.7813097238540649, 0.8615907430648804, 1.0800001621246338, 0.9138081073760986, 0.7058166861534119, 0.9795660376548767]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7084386348724365, 0.9117004871368408, 0.9176897406578064, 0.49682334065437317, 1.1646287441253662, 0.138519287109375, 1.1132484674453735, 2.128873348236084, 0.9102342128753662, 0.8774721622467041, 1.0466206073760986, 1.348657488822937, -0.04669291898608208, 1.0440956354141235, 1.1869663000106812, 0.7365436553955078]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.541144847869873, 0.7255100011825562, 0.832457959651947, 0.8410500884056091, 0.8268078565597534, 0.3477608859539032, 0.5721057057380676, 0.690982460975647, 0.6206424236297607, 0.8288676738739014, -0.043438564985990524, 0.5652121901512146, 0.9484493732452393, 0.920890212059021, 1.0432066917419434, 0.4157243072986603]
Running loglikelihood requests:  58%|████████████████████████████▍                    | 293/504 [16:17<11:28,  3.26s/it]Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.33818092942237854, 1.2985327243804932, 0.9661157727241516, 0.7983562350273132, 0.6195570826530457, 0.7773601412773132, 0.9015241265296936, 0.9676831960678101, 0.8032103180885315, 0.9128643870353699, 0.7062680721282959, -0.2734149396419525, 0.8332965970039368, 0.7487854361534119, 1.041787028312683, 1.2774103879928589]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.740418016910553, 0.5343192219734192, 0.827133059501648, 0.7439190745353699, 0.7147945165634155, 0.4936041235923767, 0.279331237077713, 1.0137395858764648, -0.7912412881851196, 2.3638064861297607, -0.8631243109703064, 0.7147655487060547, 0.8804080486297607, 0.9651392102241516, 0.8532161116600037, 0.6504449844360352]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.24835051596164703, 0.6582504510879517, 1.2707622051239014, 1.2486069202423096, 1.3912979364395142, 1.3023568391799927, 1.2538652420043945, 1.3534581661224365, 1.4331095218658447, 1.732208490371704, 1.8129513263702393, 1.2409647703170776, 1.229453206062317, 1.1408497095108032, 0.5953348875045776, 1.4260684251785278]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3621652126312256, 1.2913931608200073, 1.436790108680725, 1.5004760026931763, 0.9243738651275635, 1.3078428506851196, 1.7778360843658447, 1.8061975240707397, 1.825187087059021, 1.3647912740707397, 1.098991870880127, 0.957787036895752, 1.3073381185531616, 1.4421330690383911, 1.3609342575073242, 1.3687959909439087]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3218060731887817, 1.2400538921356201, 1.03408944606781, 1.1274946928024292, 1.3353794813156128, 0.8186178803443909, 1.0188008546829224, 1.083902359008789, 1.6518514156341553, 1.2546284198760986, 1.295135259628296, 1.276244044303894, 2.1045167446136475, 1.2611935138702393, 1.2055212259292603, 1.1825761795043945]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.7976422905921936, 1.130974292755127, 0.9794142246246338, 0.8774726986885071, 0.04190473631024361, 1.1173187494277954, 1.1763557195663452, 1.7777293920516968, 0.7684869766235352, 1.1446051597595215, 1.3590959310531616, 1.025177240371704, 0.9928193688392639, 1.353532075881958, 1.144749402999878, 0.48507723212242126]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0218617916107178, 2.2141544818878174, 2.0069262981414795, 2.039128065109253, 1.9353007078170776, 2.11150860786438, 1.9565389156341553, 2.219143867492676, 1.9584428071975708, 2.041032075881958, 2.386423349380493, 2.890428066253662, 1.8276818990707397, 2.3802521228790283, 2.185530424118042, 2.1490283012390137]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4477251768112183, 1.2559250593185425, 1.724625825881958, 1.1490776538848877, 1.51473867893219, 1.5161502361297607, 1.3554564714431763, 1.3727350234985352, 1.5885963439941406, 1.5644038915634155, 1.4425551891326904, 2.261291980743408, 1.5173976421356201, 1.5677192211151123, 1.2643243074417114, 1.6403295993804932]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.3800387382507324, 1.6569393873214722, 1.2489701509475708, 1.3748687505722046, 1.3758206367492676, 1.4687172174453735, 1.3362985849380493, 1.441603183746338, 1.3883928060531616, 1.2749146223068237, 1.5043330192565918, 1.3884831666946411, 1.352662205696106, 0.9543313384056091, 1.3834567070007324, 1.486114740371704]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.234243631362915, 2.341780424118042, 2.2488181591033936, 2.14213490486145, 2.8969931602478027, 2.38041615486145, 2.695181131362915, 2.23614764213562, 2.3168985843658447, 2.3013393878936768, 2.516084671020508, 2.1725971698760986, 2.2507877349853516, 2.364824056625366, 2.367449998855591, 2.394564151763916]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6048122644424438, 1.850610613822937, 1.4672071933746338, 1.6250656843185425, 1.5306919813156128, 1.5148043632507324, 1.5485819578170776, 1.8775275945663452, 1.5485655069351196, 1.4892988204956055, 1.638556957244873, 1.4252232313156128, 1.534171462059021, 1.5276391506195068, 1.5883009433746338, 2.0512735843658447]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.973641037940979, 1.6253405809402466, 1.673713207244873, 1.7493927478790283, 1.6334772109985352, 1.7026326656341553, 1.6336331367492676, 1.778336763381958, 1.699973702430725, 1.8726530075073242, 2.0831966400146484, 1.5801888704299927, 1.6224559545516968, 1.8355599641799927, 1.6961166858673096, 1.7951722145080566]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6516215801239014, 1.691636085510254, 1.6241506338119507, 1.8157907724380493, 1.6677225828170776, 1.5819000005722046, 1.7350889444351196, 1.7523716688156128, 1.6373752355575562, 1.7886275053024292, 1.53515625, 1.74698007106781, 1.6514081954956055, 1.7516167163848877, 1.6334444284439087, 1.642228364944458]
Layer: gate_27 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0462021827697754, 2.0596768856048584, 2.6224231719970703, 2.05646014213562, 2.138458490371704, 2.1017019748687744, 2.620765447616577, 2.1661264896392822, 2.0183494091033936, 2.037355661392212, 1.946264386177063, 1.9184283018112183, 1.9865086078643799, 1.8904608488082886, 2.2626214027404785, 2.0912225246429443]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.515296936035156, 5.794183254241943, 5.623555660247803, 5.6629462242126465, 5.480895519256592, 5.315454483032227, 5.6477155685424805, 5.951746463775635, 5.761029243469238, 5.566242218017578, 5.560399055480957, 5.41832971572876, 5.626378536224365, 5.648897171020508, 5.7372636795043945, 5.591714859008789]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.243041038513184, 4.042476177215576, 3.988182783126831, 3.8543527126312256, 3.9911699295043945, 4.035845756530762, 3.964449882507324, 3.994550943374634, 4.006335258483887, 4.0792412757873535, 4.012014389038086, 3.7970759868621826, 4.099100589752197, 4.044642925262451, 4.0984110832214355, 4.147846698760986]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.076089859008789, 3.10369610786438, 2.9408481121063232, 2.773831367492676, 2.966452121734619, 3.015165328979492, 2.933823585510254, 2.8996849060058594, 2.9014575481414795, 2.994485378265381, 3.1109507083892822, 2.5297727584838867, 3.015821933746338, 2.919774055480957, 3.133272171020508, 2.9873621463775635]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.07028576731681824, 0.08635786920785904, 0.08358898013830185, -0.1626194417476654, -0.14744842052459717, -0.13805246353149414, 0.10803014039993286, -0.14735399186611176, 0.088956817984581, 0.07409113645553589, 0.07911577820777893, 0.08224425464868546, 0.08443669229745865, 0.08703482896089554, -0.884825587272644, 0.09793847054243088]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08078531175851822, 0.07484201341867447, 0.04425746574997902, 0.04189756140112877, 0.08234366029500961, 0.02897239662706852, 0.05843868479132652, 0.06604336202144623, 0.0226711705327034, 0.058505214750766754, -0.15769782662391663, 0.03747975826263428, 0.01657148450613022, -0.01773301139473915, -0.006428612396121025, 0.034193024039268494]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06964489817619324, 0.05546564981341362, 0.08872874826192856, 0.061726123094558716, 0.11942258477210999, 0.10380280017852783, 0.06718558818101883, -0.06338723003864288, 0.06685716658830643, 0.056455887854099274, -0.002117972122505307, 0.10289584845304489, -0.10338429361581802, 0.053704939782619476, 0.004826994147151709, 0.10392898321151733]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11771529912948608, 0.17001447081565857, 0.1110297292470932, 0.16122853755950928, 0.14604982733726501, 0.1426645964384079, 0.07807596027851105, 0.20408956706523895, 0.18001745641231537, -0.3148459494113922, -0.012583610601723194, 0.09672664105892181, 0.15807509422302246, -0.11437466740608215, -0.08988574147224426, -0.011507605202496052]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0824103057384491, 0.09628784656524658, 0.11158948391675949, 0.052006714046001434, 0.037686534225940704, -0.044127438217401505, 0.06848157942295074, 0.07916311919689178, -0.04264362156391144, 0.020648662000894547, -0.048378318548202515, 0.1865818351507187, -0.13726194202899933, -0.04020247608423233, 0.11723575741052628, -0.016244741156697273]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07096024602651596, 0.16302254796028137, 0.06082414090633392, 0.010935726575553417, -0.0561269111931324, 0.12271509319543839, 0.16931961476802826, 0.06019631400704384, 0.09889984130859375, -0.05332751199603081, -0.0777520090341568, 0.10967261344194412, -0.054444923996925354, 0.1161864846944809, 0.14273326098918915, 0.12645317614078522]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.08031509071588516, 0.08240802586078644, 0.14134281873703003, 0.3808426856994629, 0.17530784010887146, 0.1060587540268898, 0.01773703843355179, -0.016630124300718307, 0.5655781030654907, 0.15023307502269745, -0.37537428736686707, 0.34076762199401855, 0.19401876628398895, -0.11445239186286926, 0.07149316370487213, 0.14570552110671997]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.2525368630886078, 0.3055258095264435, 0.3313572406768799, 0.13296489417552948, -0.698302686214447, 0.3136179447174072, 0.17395776510238647, 0.10112664848566055, 0.21757246553897858, 0.22412501275539398, 0.004590678494423628, 0.23040875792503357, 0.45937198400497437, 0.10589710623025894, -0.2785206437110901, -0.06290481239557266]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.23273800313472748, 0.3764181435108185, -0.12438130378723145, 0.3592031002044678, 0.4741182327270508, 0.27474167943000793, 0.30356472730636597, -0.12494835257530212, 0.21702496707439423, 0.5329619646072388, 0.014497675001621246, 0.37281370162963867, 0.31991395354270935, -0.15866219997406006, -0.19021789729595184, 0.23185233771800995]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.39671358466148376, 0.3500601053237915, 0.6062439680099487, 0.5591811537742615, 0.4509689509868622, 0.322662889957428, 0.6223243474960327, 0.551574170589447, 0.24766658246517181, 0.25885793566703796, 0.5155321359634399, 0.6496561169624329, 1.0890309810638428, 0.7725476622581482, 0.7295589447021484, 0.7129156589508057]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9368113875389099, 0.7881819009780884, 0.6627771258354187, 0.8576472401618958, 1.0486278533935547, 0.6208152174949646, 0.6263396143913269, 0.6685196161270142, 0.40448781847953796, 1.0565742254257202, 0.6794371008872986, 0.820433497428894, 0.9360032081604004, 0.286125510931015, 0.3709847331047058, 0.4464017450809479]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.1489883661270142, 1.0487029552459717, 1.0040063858032227, 1.3706263303756714, 1.3757679462432861, 0.7753707766532898, 0.94140625, 1.2715177536010742, 0.9406717419624329, 1.196656346321106, 1.006760835647583, 1.0029380321502686, 0.8143393993377686, 0.6841923594474792, 0.12844327092170715, 0.9086538553237915]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.3708767294883728, 0.9302300214767456, 0.9898086786270142, 1.1870659589767456, 0.5523775815963745, 0.7338700294494629, 1.0257996320724487, 1.0112847089767456, 0.8364487290382385, 0.7289726138114929, 0.7335549592971802, 0.8342848420143127, 1.0868055820465088, 0.8678969740867615, 0.676115095615387, 0.9542601704597473]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6890274286270142, 0.8433191180229187, 0.8786141276359558, 0.5085261464118958, 1.135433316230774, 0.17104241251945496, 1.056623935699463, 2.0869390964508057, 0.8578392267227173, 0.8448100090026855, 0.9866202473640442, 1.2837539911270142, -0.06317920982837677, 1.0549525022506714, 1.2162235975265503, 0.7090970277786255]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5507770776748657, 0.7346233129501343, 0.800597608089447, 0.8380241990089417, 0.813843846321106, 0.4010593891143799, 0.6454306244850159, 0.6784257292747498, 0.6253484487533569, 0.8481758236885071, 0.010327461175620556, 0.5633826851844788, 0.9517644643783569, 0.8980284929275513, 1.025390625, 0.42788878083229065]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.3176535665988922, 1.254590630531311, 0.9461471438407898, 0.7738840579986572, 0.6006422638893127, 0.7590728402137756, 0.8766088485717773, 0.9418569803237915, 0.7658253312110901, 0.8797743320465088, 0.6873747706413269, -0.271599143743515, 0.7870427966117859, 0.7258780598640442, 1.0221186876296997, 1.283477544784546]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7570863366127014, 0.5684097409248352, 0.8354513049125671, 0.7305553555488586, 0.7130554914474487, 0.5185927748680115, 0.2509404420852661, 0.998902440071106, -0.7665932178497314, 2.333099603652954, -0.8394149541854858, 0.7139862775802612, 0.8674212098121643, 0.9564344882965088, 0.8387169241905212, 0.6720831394195557]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.24623538553714752, 0.6469371914863586, 1.2422375679016113, 1.2507094144821167, 1.363264560699463, 1.2901725769042969, 1.2344375848770142, 1.3399438858032227, 1.3779797554016113, 1.7351679801940918, 1.768245816230774, 1.1994858980178833, 1.2100303173065186, 1.1239937543869019, 0.593185544013977, 1.400098443031311]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3500267267227173, 1.2766426801681519, 1.433366298675537, 1.4579660892486572, 0.9195963740348816, 1.309278130531311, 1.7344751358032227, 1.7909321784973145, 1.777727723121643, 1.3410122394561768, 1.089432716369629, 0.9628934860229492, 1.3001302480697632, 1.4350706338882446, 1.3380241394042969, 1.3550680875778198]
Running loglikelihood requests:  59%|████████████████████████████▉                    | 297/504 [16:30<11:07,  3.22s/it]Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.318209171295166, 1.2332398891448975, 1.0264256000518799, 1.1241319179534912, 1.311832308769226, 0.8078237771987915, 1.0059094429016113, 1.0817557573318481, 1.6248998641967773, 1.2669938802719116, 1.2751067876815796, 1.2684462070465088, 2.09662127494812, 1.253739356994629, 1.1888021230697632, 1.1656817197799683]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.7960507869720459, 1.1240150928497314, 0.975999116897583, 0.8660158514976501, 0.02630823850631714, 1.0911041498184204, 1.1701555252075195, 1.76754891872406, 0.7910656929016113, 1.1274038553237915, 1.366962194442749, 0.9904764294624329, 0.9871460795402527, 1.3618289232254028, 1.1520557403564453, 0.4747351109981537]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.005842685699463, 2.1752803325653076, 1.9774305820465088, 1.9976962804794312, 1.901442289352417, 2.065972328186035, 1.9300881624221802, 2.180288553237915, 1.936765432357788, 2.0186965465545654, 2.3436832427978516, 2.846888303756714, 1.7977097034454346, 2.350093364715576, 2.155515432357788, 2.115718364715576]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4633747339248657, 1.2529380321502686, 1.7117388248443604, 1.1565171480178833, 1.5298477411270142, 1.5219017267227173, 1.3716613054275513, 1.3911925554275513, 1.605635643005371, 1.5855368375778198, 1.436965823173523, 2.234241485595703, 1.5192641019821167, 1.5835002660751343, 1.278028130531311, 1.6682358980178833]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.331263303756714, 1.6653646230697632, 1.2615810632705688, 1.3867855072021484, 1.3849492073059082, 1.478532314300537, 1.3497262001037598, 1.4564803838729858, 1.4061164855957031, 1.2777026891708374, 1.4991652965545654, 1.3535490036010742, 1.3598757982254028, 0.9556832313537598, 1.3863139152526855, 1.4981637001037598]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.197983503341675, 2.3020832538604736, 2.234508514404297, 2.123931646347046, 2.8660523891448975, 2.3499598503112793, 2.669203996658325, 2.19818377494812, 2.2821848392486572, 2.272970199584961, 2.4742255210876465, 2.133880853652954, 2.254206657409668, 2.330061435699463, 2.3429486751556396, 2.369324207305908]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.5861711502075195, 1.8355368375778198, 1.4466813802719116, 1.608707308769226, 1.495926856994629, 1.483473539352417, 1.5292134284973145, 1.8463207483291626, 1.5375100374221802, 1.4737913608551025, 1.622512698173523, 1.4124265909194946, 1.5115852355957031, 1.501268744468689, 1.5513488054275513, 2.026742696762085]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9799178838729858, 1.633130669593811, 1.6759982109069824, 1.7519030570983887, 1.6202507019042969, 1.7035131454467773, 1.638763666152954, 1.780498743057251, 1.6970444917678833, 1.863131046295166, 2.0550546646118164, 1.5883580446243286, 1.623430848121643, 1.862396478652954, 1.6890441179275513, 1.797438383102417]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6353999376296997, 1.6732856035232544, 1.6099154949188232, 1.7975751161575317, 1.6551315784454346, 1.5712556838989258, 1.7234408855438232, 1.7287180423736572, 1.6228173971176147, 1.785748839378357, 1.518462896347046, 1.729325294494629, 1.6298410892486572, 1.7305819988250732, 1.6224209070205688, 1.6172428131103516]
Layer: gate_27 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0233707427978516, 2.0484442710876465, 2.6000516414642334, 2.0472421646118164, 2.1276626586914062, 2.085486888885498, 2.5863046646118164, 2.1526107788085938, 2.010082721710205, 2.0284454822540283, 1.934328317642212, 1.8962674140930176, 1.9935647249221802, 1.8777377605438232, 2.236645221710205, 2.0777077674865723]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.509815692901611, 5.7757744789123535, 5.604366779327393, 5.654447078704834, 5.476295471191406, 5.307291507720947, 5.6268696784973145, 5.935296535491943, 5.746594429016113, 5.561765670776367, 5.533186435699463, 5.406650543212891, 5.613481521606445, 5.6273369789123535, 5.7078657150268555, 5.5793938636779785]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.257345199584961, 4.030248165130615, 3.976696014404297, 3.8486578464508057, 3.9916532039642334, 4.046875, 3.9566640853881836, 4.001736164093018, 4.019831657409668, 4.07899284362793, 4.0179619789123535, 3.7984869480133057, 4.099058628082275, 4.023170471191406, 4.088074207305908, 4.152510643005371]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1034321784973145, 3.1261351108551025, 2.9488515853881836, 2.740852117538452, 2.9623396396636963, 3.017094135284424, 2.9059829711914062, 2.900207042694092, 2.9183359146118164, 3.013488292694092, 3.127136707305908, 2.5513572692871094, 3.0319178104400635, 2.9392361640930176, 3.1489717960357666, 3.030381917953491]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.07451851665973663, 0.08594238758087158, 0.09741419553756714, -0.2743137776851654, -0.24652881920337677, -0.08536992222070694, 0.10774341970682144, -0.062348585575819016, 0.07494204491376877, 0.07450990378856659, 0.08580121397972107, 0.06312880665063858, 0.08611525595188141, 0.09913661330938339, -1.0858696699142456, 0.10153902322053909]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08219167590141296, 0.04397093877196312, 0.03616376966238022, 0.04113694652915001, 0.06990576535463333, 0.020342281088232994, 0.04446166753768921, 0.08757742494344711, 0.022656824439764023, 0.08053230494260788, -0.20322725176811218, 0.045392680913209915, 0.024539001286029816, -0.024279406294226646, 0.03222816064953804, 0.015636011958122253]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0840488001704216, 0.06631571054458618, 0.09573370963335037, 0.07113230228424072, 0.10789215564727783, 0.1043291836977005, 0.07513441145420074, -0.10124246031045914, 0.07318434864282608, 0.09744226932525635, 0.02660076506435871, 0.08907891809940338, -0.1845056265592575, 0.022328564897179604, -0.04139135777950287, 0.0994485393166542]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13093122839927673, 0.1434103101491928, 0.12128350883722305, 0.14535783231258392, 0.13849112391471863, 0.12520632147789001, -0.010193653404712677, 0.18067529797554016, 0.1735353320837021, -0.5401119589805603, 0.053418200463056564, 0.06399822980165482, 0.24884431064128876, -0.24818859994411469, 0.002032809890806675, -0.048238638788461685]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06092827767133713, 0.11239102482795715, 0.07913051545619965, 0.1810394674539566, 0.061837613582611084, -0.044233959168195724, 0.014556558802723885, 0.024844666942954063, -0.07972939312458038, 0.02718333713710308, -0.17902915179729462, 0.14487802982330322, 0.023656079545617104, -0.2071179449558258, 0.13285338878631592, -0.06128581613302231]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06592892110347748, 0.1095096543431282, 0.05516671761870384, 0.1488318145275116, -0.08970002830028534, -0.05873864144086838, 0.10523543506860733, -0.0025840825401246548, 0.20632745325565338, -0.0996738001704216, -0.05197358876466751, 0.07492776215076447, -0.15527409315109253, 0.13192617893218994, 0.1524997353553772, 0.06800776720046997]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.09190736711025238, -0.010218954645097256, 0.15048810839653015, 0.3311944901943207, 0.18126189708709717, 0.11904959380626678, -0.1607937216758728, -0.08192704617977142, 0.4283541142940521, 0.15969952940940857, -0.5433245301246643, 0.28229501843452454, 0.20749397575855255, -0.22719892859458923, 0.319457471370697, 0.19296303391456604]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5622464418411255, 0.22804729640483856, 0.23033441603183746, 0.13832969963550568, -0.9352396130561829, 0.3288019895553589, 0.19570478796958923, -0.4136759340763092, 0.5535721778869629, 0.2118764966726303, -0.1741584688425064, 0.2839442789554596, 0.1481088548898697, 0.23193477094173431, -0.5397219061851501, -0.18269960582256317]
Layer: gate_7 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2721009850502014, 0.31289908289909363, -0.3078300356864929, 0.2851690351963043, 0.4419185221195221, 0.2875423729419708, 0.24932798743247986, -0.2879231870174408, 0.16357657313346863, 0.8492160439491272, 0.06356628984212875, 0.1873445361852646, 0.4799136817455292, -0.4512156844139099, -0.4450099468231201, 0.3899552822113037]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3262263834476471, 0.17840576171875, 0.46589648723602295, 1.1202882528305054, 0.3597845137119293, 0.18262358009815216, 0.5621585845947266, 0.5196327567100525, 0.6931186318397522, 0.10904634743928909, 0.20540259778499603, 0.6632236838340759, 0.6865526437759399, 0.16620029509067535, 0.7604604959487915, 0.6309469938278198]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0843461751937866, 0.8384706974029541, 0.6169329285621643, 0.7245134115219116, 0.9152503609657288, 0.6293767690658569, 0.6665831804275513, 0.7155699133872986, 0.22311519086360931, 0.514229416847229, 0.7612513303756714, 0.8959460258483887, 0.27171599864959717, 0.22817809879779816, 0.9653260111808777, 0.4033025801181793]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9224738478660583, 1.1028664112091064, 0.8765774965286255, 1.055130958557129, 1.1877044439315796, 0.2673673629760742, 0.8270696401596069, 0.9021914005279541, 0.9420740008354187, 1.4282935857772827, 0.8347439169883728, 0.863450288772583, 0.23025773465633392, 0.38769322633743286, -0.14000342786312103, 0.6771417856216431]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.1072235107421875, 0.7521576285362244, 1.1472690105438232, 1.1591212749481201, 0.17143262922763824, 0.6043753623962402, 1.0091062784194946, 1.2836830615997314, 0.46709108352661133, 0.18342134356498718, 0.8317463994026184, 0.8484575152397156, 0.7543277740478516, 0.8554019927978516, 0.5979817509651184, 0.9109908938407898]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.5418312549591064, 0.8944603204727173, 0.9593328237533569, 0.2762691080570221, 0.9291867017745972, -0.34832659363746643, 1.027510643005371, 2.0096821784973145, 0.9362229704856873, 0.8351362347602844, 1.1991175413131714, 1.055371880531311, -0.13331590592861176, 0.6270031929016113, 1.0189939737319946, 0.5080344676971436]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6083556413650513, 0.7108080983161926, 0.9439269304275513, 0.7903061509132385, 0.9642678499221802, 0.85716313123703, 0.11831013113260269, 1.3161954879760742, 0.7431223392486572, 0.6880248188972473, -0.5315327644348145, 0.6189463138580322, 0.8876285552978516, 1.0240885019302368, 1.1353164911270142, 0.39189577102661133]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.33113712072372437, 1.1181306838989258, 0.9681490659713745, 0.96607905626297, 0.6928898692131042, 0.8235176205635071, 1.3845303058624268, 0.9482505321502686, 0.9267828464508057, 1.0332865715026855, 0.5898187160491943, -0.4728586673736572, 1.0293688774108887, 0.8450353741645813, 0.82936030626297, 1.263607144355774]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6094630360603333, 0.06223968788981438, 0.8075430989265442, 0.799758791923523, 0.7926766276359558, 0.3459806442260742, 0.2247249186038971, 0.8214226365089417, -1.186325192451477, 2.2836058139801025, -1.1125216484069824, 0.4397134482860565, 0.8197616338729858, 0.7824519276618958, 0.8192211985588074, 0.07617656886577606]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.20954881608486176, 0.48633021116256714, 1.2945045232772827, 1.0496962070465088, 1.3876535892486572, 1.343916893005371, 1.1468015909194946, 1.2805489301681519, 1.8361377716064453, 1.2033921480178833, 2.0663394927978516, 1.3814269304275513, 1.30465829372406, 1.2738548517227173, 0.2772081196308136, 1.1236945390701294]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4020432233810425, 1.201322078704834, 1.3892436027526855, 1.6607905626296997, 0.7731620669364929, 1.1033987998962402, 1.7332398891448975, 1.6268362998962402, 2.3090946674346924, 1.3294605016708374, 1.0195854902267456, 0.5934404134750366, 1.0759882926940918, 0.7636525630950928, 1.309795618057251, 1.3848490715026855]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3747663497924805, 1.2576789855957031, 1.416182518005371, 1.268796682357788, 1.5220352411270142, 0.8604391813278198, 1.1576355695724487, 1.1739429235458374, 1.8401693105697632, 1.3606938123703003, 1.5447216033935547, 1.3934128284454346, 1.768779993057251, 1.4629740715026855, 1.39700186252594, 1.4055989980697632]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1248507499694824, 1.0548710823059082, 0.9495609402656555, 0.8660106062889099, -0.22976763546466827, 1.266526460647583, 1.1172375679016113, 1.633346676826477, 0.7749320864677429, 1.2670940160751343, 1.217531442642212, 1.1285723447799683, 1.0819227695465088, 1.310697078704834, 0.8662672638893127, 0.3745993673801422]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0008347034454346, 2.1448984146118164, 2.073450803756714, 1.8671875, 1.8599425554275513, 2.0445380210876465, 1.7614516019821167, 2.066239356994629, 1.8420138359069824, 1.9908186197280884, 2.263822078704834, 2.557358503341675, 1.558059573173523, 2.2399840354919434, 2.0436699390411377, 1.9348958730697632]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3001302480697632, 1.3672208786010742, 1.8436498641967773, 0.9325587749481201, 1.3529647588729858, 1.5337873697280884, 1.2555421590805054, 1.3183093070983887, 1.461104393005371, 1.520282506942749, 1.5391960144042969, 2.024906635284424, 1.4067507982254028, 1.4697182178497314, 1.1480118036270142, 1.578291893005371]
Running loglikelihood requests:  60%|█████████████████████████████▎                   | 301/504 [16:43<10:57,  3.24s/it]Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.3418803215026855, 1.8342013359069824, 0.9992613196372986, 1.3525307178497314, 1.561965823173523, 1.4094551801681519, 1.3506276607513428, 1.35546875, 1.488114356994629, 1.4848424196243286, 1.474759578704834, 1.4963942766189575, 1.245559573173523, 0.5793123245239258, 1.2806657552719116, 1.468916893005371]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.2476630210876465, 2.274305582046509, 2.140023946762085, 1.9778980016708374, 2.433159828186035, 2.0797276496887207, 2.324185371398926, 1.9878472089767456, 1.9485176801681519, 1.9807358980178833, 2.680889368057251, 1.995926856994629, 2.196781635284424, 2.318843364715576, 2.142294406890869, 2.166199207305908]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.637453317642212, 1.6508413553237915, 1.5169938802719116, 1.7343082427978516, 1.2154947519302368, 1.5862046480178833, 1.6566840410232544, 1.982572078704834, 1.6452323198318481, 1.540998935699463, 1.6198583841323853, 1.522536039352417, 1.5807291269302368, 1.4478164911270142, 1.916165828704834, 1.9188034534454346]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.7415364980697632, 1.8012596368789673, 1.6719751358032227, 1.7141426801681519, 1.6768579483032227, 1.8120743036270142, 1.6745139360427856, 1.7003204822540283, 1.8061795234680176, 1.8998814821243286, 2.290998935699463, 1.6458250284194946, 1.6595219373703003, 1.821364164352417, 1.8143800497055054, 1.785503625869751]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5751358270645142, 1.6465219259262085, 1.5974856615066528, 1.6663662195205688, 1.537643551826477, 1.5549076795578003, 1.739549994468689, 1.6570898294448853, 1.5313940048217773, 1.6485710144042969, 1.2646567821502686, 1.5821939706802368, 1.6125593185424805, 1.8792046308517456, 1.5542033910751343, 1.5976061820983887]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9459635019302368, 1.9725227355957031, 2.3355202674865723, 1.9063835144042969, 2.045330762863159, 1.973190426826477, 2.5056424140930176, 2.053502321243286, 1.900240421295166, 1.9215410947799683, 1.8956997394561768, 1.7249599695205688, 1.8903495073318481, 1.7586472034454346, 2.1912059783935547, 1.9498196840286255]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.80261754989624, 5.217748165130615, 4.773904800415039, 4.639472961425781, 4.684895992279053, 4.620526313781738, 4.826388835906982, 4.9646100997924805, 5.026909828186035, 4.801382064819336, 4.780114650726318, 4.676682472229004, 4.779230117797852, 4.81944465637207, 4.947849750518799, 4.770298957824707]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.623764753341675, 3.4812700748443604, 3.457932710647583, 3.3004391193389893, 3.5652711391448975, 3.593148946762085, 3.250358819961548, 3.5182292461395264, 3.678786039352417, 3.5203325748443604, 3.4316906929016113, 3.0464744567871094, 3.358715534210205, 3.5584936141967773, 3.584268093109131, 3.603440523147583]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.6481704711914062, 2.7274973392486572, 2.6402244567871094, 2.6306090354919434, 2.55962872505188, 2.707465171813965, 2.719818353652954, 2.411224603652954, 2.6351494789123535, 2.4958267211914062, 3.1360843181610107, 1.998673915863037, 2.6386218070983887, 2.5496127605438232, 3.0317842960357666, 2.61431622505188]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.07263336330652237, 0.08887020498514175, 0.08848562091588974, -0.15888287127017975, -0.14636866748332977, -0.13133180141448975, 0.1106221079826355, -0.16908064484596252, 0.08673974871635437, 0.07643111050128937, 0.08340533822774887, 0.08040203899145126, 0.08693542331457138, 0.09113318473100662, -0.895315408706665, 0.10153225064277649]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.082758329808712, 0.07622156292200089, 0.046255625784397125, 0.044999074190855026, 0.0804223120212555, 0.029608817771077156, 0.0555436834692955, 0.06451767683029175, 0.02453334629535675, 0.06338761001825333, -0.15299105644226074, 0.03863608464598656, 0.014490011148154736, -0.011534716002643108, -0.00322424853220582, 0.03214048221707344]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07038906216621399, 0.05288843810558319, 0.08738993853330612, 0.05879192426800728, 0.11700784415006638, 0.10666822642087936, 0.059811003506183624, -0.060046982020139694, 0.07129218429327011, 0.06493686139583588, 0.0007989966543391347, 0.10557649284601212, -0.10382504761219025, 0.05569474771618843, 0.002964981831610203, 0.10463986545801163]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12344937771558762, 0.164720356464386, 0.10819993913173676, 0.15821532905101776, 0.14487463235855103, 0.13052590191364288, 0.07538611441850662, 0.20433282852172852, 0.18087980151176453, -0.3051503002643585, -0.0032075964845716953, 0.09522459656000137, 0.1508043110370636, -0.10593593120574951, -0.08227837830781937, -0.0011322021018713713]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0837792456150055, 0.09988263994455338, 0.11458461731672287, 0.04812058061361313, 0.04057169333100319, -0.041873831301927567, 0.06530649214982986, 0.08187361806631088, -0.03590220585465431, 0.016016686335206032, -0.05106307193636894, 0.17720994353294373, -0.14391492307186127, -0.04541798308491707, 0.12197384983301163, -0.008630901575088501]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07291589677333832, 0.16225884854793549, 0.076576828956604, 0.00738226855173707, -0.06693725287914276, 0.11288425326347351, 0.18203973770141602, 0.05722470581531525, 0.11179716885089874, -0.04855897277593613, -0.13197526335716248, 0.12155731767416, -0.05563460662961006, 0.12402565777301788, 0.14445114135742188, 0.13652357459068298]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.07630207389593124, 0.08083124458789825, 0.15107686817646027, 0.3773793876171112, 0.18179281055927277, 0.09359695017337799, 0.018653802573680878, -0.007521654013544321, 0.5849224328994751, 0.15160973370075226, -0.36481773853302, 0.370696097612381, 0.191777765750885, -0.10790007561445236, 0.08583971112966537, 0.14503465592861176]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.24867872893810272, 0.28505581617355347, 0.335160493850708, 0.12881788611412048, -0.6990659236907959, 0.3222256898880005, 0.16887499392032623, 0.11201278120279312, 0.22432370483875275, 0.23112845420837402, 0.01402879785746336, 0.2267012894153595, 0.44781070947647095, 0.11394128948450089, -0.25678497552871704, -0.04259616881608963]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20305760204792023, 0.3740260899066925, -0.10852847248315811, 0.36244916915893555, 0.48892346024513245, 0.2729906141757965, 0.3112649619579315, -0.12990829348564148, 0.23298963904380798, 0.552371621131897, 0.02433392032980919, 0.3953319787979126, 0.335077702999115, -0.14296768605709076, -0.17482273280620575, 0.23386164009571075]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.41579777002334595, 0.36364004015922546, 0.5881369113922119, 0.5797119140625, 0.47576266527175903, 0.36211681365966797, 0.6448759436607361, 0.568236231803894, 0.264618456363678, 0.311374306678772, 0.5192543268203735, 0.6819972991943359, 1.1432828903198242, 0.767865777015686, 0.7585088610649109, 0.7345787882804871]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.91429603099823, 0.7959111928939819, 0.6927564740180969, 0.885003387928009, 1.095779538154602, 0.6630282402038574, 0.638344943523407, 0.677657961845398, 0.439792275428772, 1.1183826923370361, 0.692365825176239, 0.8383704423904419, 0.996033251285553, 0.33111080527305603, 0.39113715291023254, 0.4673212468624115]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.1705927848815918, 1.052825689315796, 1.014283299446106, 1.416856288909912, 1.4268172979354858, 0.848843514919281, 0.944369912147522, 1.31538724899292, 0.971002459526062, 1.205613136291504, 1.044913411140442, 1.0320227146148682, 0.839800238609314, 0.7288051247596741, 0.15994608402252197, 0.9325694441795349]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.39567965269088745, 0.9613281488418579, 1.0181385278701782, 1.205978274345398, 0.5916779637336731, 0.756470799446106, 1.0527077913284302, 1.0447732210159302, 0.8883494734764099, 0.79886794090271, 0.784645676612854, 0.831470787525177, 1.126010537147522, 0.9007472991943359, 0.70851731300354, 0.9727411866188049]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.750611424446106, 0.8733101487159729, 0.9467136263847351, 0.5651674866676331, 1.169565200805664, 0.25171536207199097, 1.0717730522155762, 2.124286651611328, 0.8584238886833191, 0.8854619860649109, 1.0229789018630981, 1.363569974899292, -0.001216987962834537, 1.1076277494430542, 1.169256329536438, 0.762346625328064]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5542735457420349, 0.716064453125, 0.8032354116439819, 0.855146050453186, 0.8153999447822571, 0.385012149810791, 0.65689617395401, 0.668254554271698, 0.603592038154602, 0.8764075040817261, 0.028909433633089066, 0.563753604888916, 0.984048068523407, 0.9110351800918579, 1.04122793674469, 0.4248513877391815]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.350707471370697, 1.295754075050354, 0.976672887802124, 0.779347836971283, 0.6229598522186279, 0.794378399848938, 0.904412567615509, 0.9622452259063721, 0.797197699546814, 0.905723512172699, 0.723106324672699, -0.23286770284175873, 0.845252275466919, 0.741338312625885, 1.0492357015609741, 1.338064432144165]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7900857925415039, 0.6075804233551025, 0.85496985912323, 0.7515242695808411, 0.7172427177429199, 0.524946391582489, 0.267453134059906, 1.059791088104248, -0.749416172504425, 2.3947010040283203, -0.832120418548584, 0.7636426687240601, 0.894836962223053, 0.974532961845398, 0.867387056350708, 0.697432816028595]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.297977089881897, 0.698136031627655, 1.277564525604248, 1.2722910642623901, 1.398505449295044, 1.309969425201416, 1.284914255142212, 1.36960768699646, 1.41843581199646, 1.770431399345398, 1.817060112953186, 1.225785493850708, 1.2120633125305176, 1.1428817510604858, 0.6321671009063721, 1.4669795036315918]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3787363767623901, 1.2980468273162842, 1.447819709777832, 1.5091712474822998, 0.9668987989425659, 1.342085599899292, 1.77564537525177, 1.8333560228347778, 1.8347996473312378, 1.379993200302124, 1.127394676208496, 1.010892391204834, 1.3487728834152222, 1.524782419204712, 1.401358723640442, 1.3885869979858398]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3613111972808838, 1.27411687374115, 1.048862099647522, 1.1578974723815918, 1.3584579229354858, 0.86279296875, 1.056183099746704, 1.1202737092971802, 1.6566746234893799, 1.275101900100708, 1.314673900604248, 1.301613450050354, 2.1759002208709717, 1.295822024345398, 1.230247974395752, 1.1962635517120361]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.80101478099823, 1.1613960266113281, 1.0054219961166382, 0.892566442489624, 0.084911048412323, 1.1182829141616821, 1.1991678476333618, 1.803422212600708, 0.786098837852478, 1.1466032266616821, 1.391745924949646, 1.027933955192566, 1.0133236646652222, 1.390013575553894, 1.18437659740448, 0.509026050567627]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0638246536254883, 2.2523436546325684, 2.0447349548339844, 2.0879416465759277, 1.9735393524169922, 2.1430366039276123, 2.00577449798584, 2.2515625953674316, 1.9940897226333618, 2.0880095958709717, 2.4245245456695557, 2.9338314533233643, 1.8780571222305298, 2.427241802215576, 2.238213300704956, 2.201154947280884]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.5052989721298218, 1.294123649597168, 1.7569633722305298, 1.1993886232376099, 1.5701086521148682, 1.554721474647522, 1.419123649597168, 1.4250679016113281, 1.6484375, 1.6202784776687622, 1.4672214984893799, 2.291032552719116, 1.559069275856018, 1.6119225025177002, 1.3197181224822998, 1.691372275352478]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.370601177215576, 1.6816236972808838, 1.2914530038833618, 1.4021399021148682, 1.3959578275680542, 1.5132472515106201, 1.3565047979354858, 1.4779212474822998, 1.4133915901184082, 1.29615318775177, 1.52904212474823, 1.3922384977340698, 1.4005604982376099, 1.0004936456680298, 1.4178200960159302, 1.520618200302124]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.2574727535247803, 2.3656249046325684, 2.27139949798584, 2.177513599395752, 2.9438178539276123, 2.41745924949646, 2.728532552719116, 2.265557050704956, 2.3665761947631836, 2.3421874046325684, 2.507540702819824, 2.21331524848938, 2.2875678539276123, 2.387364149093628, 2.3988451957702637, 2.4360733032226562]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.616745948791504, 1.861684799194336, 1.476460576057434, 1.627275824546814, 1.537839651107788, 1.5131114721298218, 1.5494904518127441, 1.8847825527191162, 1.552870273590088, 1.498403549194336, 1.645176649093628, 1.4311140775680542, 1.534544825553894, 1.531793475151062, 1.570176601409912, 2.0615148544311523]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Running loglikelihood requests:  61%|█████████████████████████████▋                   | 305/504 [16:55<10:36,  3.20s/it]Layer: gate_26 - Captured router_logits: [1.986616849899292, 1.619263768196106, 1.6774625778198242, 1.7585598230361938, 1.6372706890106201, 1.7052819728851318, 1.635633945465088, 1.7893767356872559, 1.6992484331130981, 1.872036337852478, 2.08602237701416, 1.5846339464187622, 1.622656226158142, 1.854738473892212, 1.695932388305664, 1.802636742591858]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.658423900604248, 1.7027003765106201, 1.632511019706726, 1.825239896774292, 1.682205319404602, 1.596034288406372, 1.741457223892212, 1.7550992965698242, 1.6433806419372559, 1.800025463104248, 1.5559782981872559, 1.755867838859558, 1.650781273841858, 1.7586064338684082, 1.641992211341858, 1.6480967998504639]
Layer: gate_27 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0479788780212402, 2.071773052215576, 2.644420862197876, 2.0690557956695557, 2.1464505195617676, 2.10538387298584, 2.6260359287261963, 2.1761465072631836, 2.021976947784424, 2.043919801712036, 1.95040762424469, 1.9282609224319458, 2.00543475151062, 1.898913025856018, 2.268461227416992, 2.09281587600708]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.562907695770264, 5.808559894561768, 5.6735053062438965, 5.718546390533447, 5.534442901611328, 5.3543477058410645, 5.6913042068481445, 6.003125190734863, 5.796603202819824, 5.609442710876465, 5.605434894561768, 5.459443092346191, 5.671467304229736, 5.686548709869385, 5.765896797180176, 5.634103298187256]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.308016300201416, 4.100407600402832, 4.051494598388672, 3.924898147583008, 4.061277389526367, 4.099116802215576, 4.038145542144775, 4.05264949798584, 4.060326099395752, 4.136277198791504, 4.096263408660889, 3.887007474899292, 4.175169944763184, 4.109035491943359, 4.159375190734863, 4.215047359466553]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1118545532226562, 3.1376359462738037, 2.967866897583008, 2.7916440963745117, 3.0005435943603516, 3.037262201309204, 2.963247299194336, 2.9342050552368164, 2.935258150100708, 3.04110050201416, 3.132608652114868, 2.590603828430176, 3.052241802215576, 2.9618546962738037, 3.1768343448638916, 3.036820650100708]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.08411301672458649, 0.09537642449140549, 0.10585512965917587, -0.275641530752182, -0.250149667263031, -0.10112379491329193, 0.11702283471822739, -0.07480946183204651, 0.08262196183204651, 0.08374083042144775, 0.09616536647081375, 0.07430420070886612, 0.0930522084236145, 0.10725495964288712, -1.1050176620483398, 0.11432919651269913]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08318813145160675, 0.04186985269188881, 0.04230451211333275, 0.041347701102495193, 0.07759957015514374, 0.020319664850831032, 0.043772026896476746, 0.08444306999444962, 0.02293216809630394, 0.07697281241416931, -0.20824770629405975, 0.043088365346193314, 0.03266258165240288, -0.02880275622010231, 0.02994394674897194, 0.01115327887237072]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07898559421300888, 0.07207648456096649, 0.098300039768219, 0.07299526035785675, 0.10538183897733688, 0.10154902935028076, 0.07805444300174713, -0.10670245438814163, 0.06651359051465988, 0.09342051297426224, 0.030186578631401062, 0.089222252368927, -0.19679763913154602, 0.023869257420301437, -0.04227719455957413, 0.1028401255607605]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13071514666080475, 0.14162306487560272, 0.12495163828134537, 0.1430925875902176, 0.141590416431427, 0.12115903198719025, 0.014387578703463078, 0.1886623203754425, 0.17615163326263428, -0.5495858788490295, 0.046028003096580505, 0.05981976166367531, 0.246955007314682, -0.2589777410030365, -0.010347449220716953, -0.058047618716955185]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06362596899271011, 0.11391647905111313, 0.08301200717687607, 0.175754576921463, 0.063270702958107, -0.050474051386117935, 0.018809840083122253, 0.02492181584239006, -0.06872343271970749, 0.032676100730895996, -0.183466374874115, 0.143329918384552, 0.007810907904058695, -0.21303372085094452, 0.1294441819190979, -0.05523204058408737]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07352394610643387, 0.10974983870983124, 0.06323719769716263, 0.14327657222747803, -0.08137445896863937, -0.06978434324264526, 0.11728528887033463, 0.0005250350222922862, 0.19562841951847076, -0.10033675283193588, -0.04393098130822182, 0.07463418692350388, -0.16625432670116425, 0.12958653271198273, 0.15441496670246124, 0.06894060224294662]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.09059952199459076, -0.00324945873580873, 0.15552686154842377, 0.32727158069610596, 0.18429651856422424, 0.12358424812555313, -0.17202945053577423, -0.07964291423559189, 0.4185430109500885, 0.162381112575531, -0.5618105530738831, 0.2915330231189728, 0.20828592777252197, -0.231512188911438, 0.31567078828811646, 0.1924629509449005]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5638778209686279, 0.22649960219860077, 0.2340105175971985, 0.13879235088825226, -0.9588092565536499, 0.33982375264167786, 0.20516039431095123, -0.4272827208042145, 0.5627205967903137, 0.22855636477470398, -0.19030283391475677, 0.2759988605976105, 0.143970787525177, 0.235266774892807, -0.5737496018409729, -0.18168307840824127]
Layer: gate_7 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.28688541054725647, 0.309414803981781, -0.32373046875, 0.284900963306427, 0.455744206905365, 0.28146177530288696, 0.24327883124351501, -0.25712358951568604, 0.1610811948776245, 0.81675124168396, 0.07341255247592926, 0.18151219189167023, 0.484397292137146, -0.49182236194610596, -0.46625977754592896, 0.3848477602005005]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.33492061495780945, 0.150540292263031, 0.46696245670318604, 1.094017505645752, 0.34498026967048645, 0.13909035921096802, 0.573207676410675, 0.5284426212310791, 0.7131461501121521, 0.09842024743556976, 0.1872691959142685, 0.686646580696106, 0.718064546585083, 0.08347486704587936, 0.7665655016899109, 0.6499150991439819]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0688799619674683, 0.832663893699646, 0.5714525580406189, 0.7066236138343811, 0.8967603445053101, 0.575892448425293, 0.6312839388847351, 0.693036675453186, 0.1171208918094635, 0.46399402618408203, 0.752318263053894, 0.8903883099555969, 0.24562139809131622, 0.17726121842861176, 0.917834997177124, 0.365050733089447]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.877169668674469, 1.061790943145752, 0.867026150226593, 1.047652006149292, 1.2045303583145142, 0.21535061299800873, 0.7969387173652649, 0.8554559946060181, 0.9178328514099121, 1.369004726409912, 0.821093738079071, 0.844819962978363, 0.20052437484264374, 0.370586633682251, -0.195379376411438, 0.6381135582923889]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.0467890202999115, 0.745367705821991, 1.2092561721801758, 1.154160976409912, 0.11802076548337936, 0.553778886795044, 1.0069776773452759, 1.208243489265442, 0.39942386746406555, 0.15215586125850677, 0.840236485004425, 0.829738438129425, 0.7102518081665039, 0.8921704888343811, 0.599414050579071, 0.911820650100708]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.530035138130188, 0.8809995055198669, 0.9657948613166809, 0.29526448249816895, 0.8953040242195129, -0.36792415380477905, 1.0276155471801758, 1.931793451309204, 0.895214855670929, 0.803872287273407, 1.143864631652832, 1.074150800704956, -0.12799867987632751, 0.5766090154647827, 0.9496665596961975, 0.473089337348938]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6231371164321899, 0.723389744758606, 0.978553831577301, 0.792730987071991, 0.9725543260574341, 0.957263708114624, 0.015297002159059048, 1.1875594854354858, 0.7701660394668579, 0.708296537399292, -0.5365133285522461, 0.5769484639167786, 0.8516750335693359, 1.0438178777694702, 1.1881283521652222, 0.38033050298690796]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.30377277731895447, 1.1663552522659302, 0.945533275604248, 0.9671705365180969, 0.6787703633308411, 0.8338994383811951, 1.3396409749984741, 0.9292798638343811, 0.950849175453186, 1.047418475151062, 0.598925769329071, -0.5019294023513794, 1.068357229232788, 0.8160156011581421, 0.8168308138847351, 1.314786434173584]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.5867404937744141, 0.028190281242132187, 0.7995923757553101, 0.7946798801422119, 0.7728855013847351, 0.3104725778102875, 0.22294656932353973, 0.8056994676589966, -1.2708920240402222, 2.284553289413452, -1.131486177444458, 0.4105168879032135, 0.809043824672699, 0.8283882737159729, 0.7910177707672119, 0.09806597977876663]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.18967550992965698, 0.523311197757721, 1.344836950302124, 1.080001711845398, 1.4213824272155762, 1.367153525352478, 1.1705502271652222, 1.2795686721801758, 1.839266300201416, 1.1964949369430542, 2.1295855045318604, 1.4083983898162842, 1.313386082649231, 1.2686821222305298, 0.2517445385456085, 1.183771014213562]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.425679326057434, 1.2015115022659302, 1.479645013809204, 1.703158974647522, 0.7592476010322571, 1.10767662525177, 1.7715692520141602, 1.6481318473815918, 2.36531925201416, 1.367764949798584, 1.0181810855865479, 0.609873354434967, 1.0819717645645142, 0.8400018811225891, 1.343104600906372, 1.4221128225326538]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.389504075050354, 1.28804349899292, 1.4138246774673462, 1.2737092971801758, 1.527989149093628, 0.844915509223938, 1.1720702648162842, 1.1727581024169922, 1.836837649345398, 1.381759524345398, 1.559239149093628, 1.4239979982376099, 1.800628423690796, 1.4722486734390259, 1.413519024848938, 1.4054347276687622]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1570408344268799, 1.082421898841858, 0.9880944490432739, 0.90869140625, -0.280656635761261, 1.3024965524673462, 1.1585087776184082, 1.66309654712677, 0.8156992793083191, 1.3245244026184082, 1.235971450805664, 1.146705150604248, 1.104101538658142, 1.3345787525177002, 0.870677649974823, 0.380032479763031]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0605978965759277, 2.219972848892212, 2.10597825050354, 1.9281929731369019, 1.9188518524169922, 2.12737774848938, 1.833831548690796, 2.1343750953674316, 1.9221467971801758, 2.0443954467773438, 2.3262908458709717, 2.6434104442596436, 1.5635148286819458, 2.320516347885132, 2.1026833057403564, 1.994972825050354]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3578804731369019, 1.4259850978851318, 1.8913723230361938, 0.949547529220581, 1.4221467971801758, 1.6011549234390259, 1.3138841390609741, 1.3904551267623901, 1.5359715223312378, 1.5772079229354858, 1.5946331024169922, 2.0906929969787598, 1.4813178777694702, 1.538077473640442, 1.1928074359893799, 1.674694299697876]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4229958057403564, 1.8752038478851318, 1.0220873355865479, 1.411718726158142, 1.621569275856018, 1.4511888027191162, 1.3897079229354858, 1.401902198791504, 1.539215326309204, 1.519599199295044, 1.517968773841858, 1.5490659475326538, 1.2699048519134521, 0.534739077091217, 1.2831776142120361, 1.50492525100708]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.398369550704956, 2.429483652114868, 2.2984375953674316, 2.134918451309204, 2.590217351913452, 2.193070650100708, 2.4291439056396484, 2.122894048690796, 2.0932743549346924, 2.109069347381592, 2.842459201812744, 2.1035666465759277, 2.368342399597168, 2.4749999046325684, 2.2916440963745117, 2.3053667545318604]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.674150824546814, 1.6893682479858398, 1.566270351409912, 1.7624660730361938, 1.2375679016113281, 1.6252717971801758, 1.696908950805664, 2.0403192043304443, 1.6883831024169922, 1.562466025352478, 1.657082200050354, 1.53974187374115, 1.6090013980865479, 1.503260850906372, 1.967629075050354, 1.9663722515106201]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.7878227233886719, 1.838769555091858, 1.70091712474823, 1.752853274345398, 1.710903525352478, 1.8499119281768799, 1.70247220993042, 1.73212468624115, 1.840888261795044, 1.9148691892623901, 2.3082964420318604, 1.679874300956726, 1.6797723770141602, 1.842917799949646, 1.871736764907837, 1.8370956182479858]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6024552583694458, 1.664607048034668, 1.6047660112380981, 1.7026876211166382, 1.550789713859558, 1.5723919868469238, 1.754562258720398, 1.690460443496704, 1.5496289730072021, 1.6664656400680542, 1.2525814771652222, 1.611871600151062, 1.6465703248977661, 1.910880208015442, 1.585636019706726, 1.620061993598938]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9977072477340698, 2.0140624046325684, 2.389673948287964, 1.9451426267623901, 2.090064525604248, 2.0174083709716797, 2.558525800704956, 2.1128652095794678, 1.9460598230361938, 1.9578803777694702, 1.9360394477844238, 1.756182074546814, 1.941372275352478, 1.7877037525177002, 2.2450406551361084, 1.9958219528198242]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.84171199798584, 5.264911651611328, 4.846942901611328, 4.691287517547607, 4.739418983459473, 4.672622203826904, 4.891576290130615, 5.000373840332031, 5.053464889526367, 4.835699558258057, 4.815454959869385, 4.717662811279297, 4.842391490936279, 4.875, 4.995312690734863, 4.8058762550354]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  61%|██████████████████████████████                   | 309/504 [17:08<10:26,  3.21s/it]Layer: gate_30 - Captured router_logits: [3.6002378463745117, 3.4881794452667236, 3.4511208534240723, 3.300755739212036, 3.529789447784424, 3.5558083057403564, 3.2279891967773438, 3.513009548187256, 3.6791610717773438, 3.519667148590088, 3.394803047180176, 3.0241339206695557, 3.32379412651062, 3.5635530948638916, 3.56742525100708, 3.5743377208709717]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.628260850906372, 2.706317901611328, 2.658491849899292, 2.6355979442596436, 2.5641982555389404, 2.6945652961730957, 2.7125000953674316, 2.3779211044311523, 2.6090352535247803, 2.469055652618408, 3.105163097381592, 2.022887706756592, 2.608288049697876, 2.5213654041290283, 3.006929397583008, 2.582982301712036]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.08080656826496124, 0.09222859889268875, 0.10040399432182312, -0.25257608294487, -0.22830121219158173, -0.09382974356412888, 0.11272788792848587, -0.06502579152584076, 0.07689931988716125, 0.07966550439596176, 0.09070633351802826, 0.05444642901420593, 0.08092578500509262, 0.10091210901737213, -1.0276356935501099, 0.11186470091342926]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08598752319812775, 0.04307914525270462, 0.03893551975488663, 0.0463750921189785, 0.073982834815979, 0.023389335721731186, 0.044451307505369186, 0.09048435091972351, 0.026338377967476845, 0.07624419033527374, -0.21284796297550201, 0.046660978347063065, 0.03451206535100937, -0.03222828730940819, 0.03912270441651344, 0.009912773035466671]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08633953332901001, 0.062256887555122375, 0.09573788940906525, 0.09304875880479813, 0.10583323240280151, 0.10164446383714676, 0.06968019157648087, -0.09509675204753876, 0.07997974008321762, 0.09759946167469025, 0.029980866238474846, 0.08541790395975113, -0.20660533010959625, 0.022935552522540092, -0.04566279053688049, 0.10894457250833511]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13253253698349, 0.14388029277324677, 0.12021129578351974, 0.155439555644989, 0.129543274641037, 0.1210666373372078, 0.009388533420860767, 0.19446836411952972, 0.19713054597377777, -0.5399191379547119, 0.0237530916929245, 0.07145757228136063, 0.261741578578949, -0.261691153049469, -0.00801683496683836, -0.07334568351507187]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07071559876203537, 0.11143374443054199, 0.1040303111076355, 0.15733656287193298, 0.048657093197107315, -0.05696314945816994, 0.024815501645207405, 0.03176906332373619, -0.047445811331272125, 0.035337895154953, -0.19950959086418152, 0.14144326746463776, 0.0003293244808446616, -0.22738602757453918, 0.12017835676670074, -0.05737026035785675]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07142271101474762, 0.10552991926670074, 0.04299130663275719, 0.12270467728376389, -0.08211245387792587, -0.07380185276269913, 0.11932585388422012, 0.020328223705291748, 0.176903635263443, -0.09420696645975113, 0.023021133616566658, 0.05260966718196869, -0.19090457260608673, 0.12064971774816513, 0.15457965433597565, 0.049076180905103683]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.09410931169986725, -0.016727613285183907, 0.108623206615448, 0.33759552240371704, 0.1746300756931305, 0.11500933766365051, -0.14292417466640472, -0.07343723624944687, 0.358919233083725, 0.15090198814868927, -0.5313540101051331, 0.29780086874961853, 0.20142875611782074, -0.23592661321163177, 0.2704077661037445, 0.19446569681167603]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.500881016254425, 0.24159705638885498, 0.23323440551757812, 0.16674913465976715, -0.8961775898933411, 0.35105571150779724, 0.173198401927948, -0.36407947540283203, 0.5311177372932434, 0.183455690741539, -0.17903946340084076, 0.2861054837703705, 0.19967253506183624, 0.2034730315208435, -0.59898841381073, -0.1643984615802765]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.336824893951416, 0.28585657477378845, -0.30027279257774353, 0.29782554507255554, 0.41864120960235596, 0.29382747411727905, 0.23388777673244476, -0.25414666533470154, 0.1286008059978485, 0.7300592660903931, 0.059932809323072433, 0.21704897284507751, 0.42093029618263245, -0.4638480842113495, -0.48463717103004456, 0.3602661192417145]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.2761004865169525, 0.15147386491298676, 0.455239474773407, 0.997726321220398, 0.27655771374702454, 0.11403277516365051, 0.5300736427307129, 0.474349707365036, 0.5625671148300171, 0.07964079827070236, 0.24055494368076324, 0.6788255572319031, 0.7265826463699341, 0.12062404304742813, 0.7515508532524109, 0.6026366949081421]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0662870407104492, 0.833479106426239, 0.6277784109115601, 0.726711094379425, 0.8440302014350891, 0.5069757699966431, 0.6234438419342041, 0.6791185736656189, 0.25998324155807495, 0.508702278137207, 0.713200569152832, 0.8791525363922119, 0.4508693516254425, 0.2051127552986145, 0.7490741014480591, 0.406410276889801]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9086998701095581, 1.067981481552124, 0.8724439740180969, 0.9796110987663269, 1.2342349290847778, 0.2645263671875, 0.807012140750885, 0.83104407787323, 0.8573624491691589, 1.331419825553894, 0.8687998652458191, 0.865505039691925, 0.3334653079509735, 0.4460000693798065, -0.04350585862994194, 0.680613100528717]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.156183123588562, 0.7380174994468689, 1.0803837776184082, 1.0818274021148682, 0.22327828407287598, 0.606228768825531, 0.905177891254425, 1.110060691833496, 0.504256010055542, 0.32387590408325195, 0.7665373682975769, 0.7912194132804871, 0.7129033803939819, 0.844650149345398, 0.5840438008308411, 0.8848845362663269]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6996878981590271, 0.8894616365432739, 0.930408239364624, 0.40805134177207947, 0.9220151305198669, -0.13764144480228424, 1.0489130020141602, 1.794769048690796, 0.902921199798584, 0.8558593988418579, 1.127352237701416, 1.130672574043274, 0.011885933578014374, 0.66109299659729, 1.0433608293533325, 0.597477912902832]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6040293574333191, 0.722779393196106, 0.9128821492195129, 0.7611328363418579, 0.9577106237411499, 0.8102931976318359, 0.199724018573761, 1.1323214769363403, 0.739457368850708, 0.726273775100708, -0.3179212510585785, 0.6810595989227295, 0.884627640247345, 1.006827473640442, 1.165540099143982, 0.4271208345890045]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.41052988171577454, 1.1498640775680542, 0.980163037776947, 0.9454314112663269, 0.775946855545044, 0.8547893762588501, 1.1568412780761719, 0.9898437261581421, 0.8877037763595581, 0.9949048757553101, 0.641385018825531, -0.39692065119743347, 0.9894096255302429, 0.8313858509063721, 0.788790762424469, 1.259548544883728]
Running loglikelihood requests:  62%|██████████████████████████████▍                  | 313/504 [17:22<10:20,  3.25s/it]Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7442212700843811, 0.11856529861688614, 0.812400221824646, 0.826721727848053, 0.815323531627655, 0.42337435483932495, 0.230035662651062, 0.8507770299911499, -1.0273973941802979, 2.2476391792297363, -0.7675938010215759, 0.520808219909668, 0.8564028739929199, 0.8576978445053101, 0.805859386920929, 0.28309890627861023]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.13093474507331848, 0.633995532989502, 1.383355975151062, 1.2828401327133179, 1.4054347276687622, 1.4031250476837158, 1.2406589984893799, 1.362228274345398, 1.8173233270645142, 1.284914255142212, 2.123318672180176, 1.4126697778701782, 1.3326829671859741, 1.2853409051895142, 0.527621865272522, 1.314508318901062]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3763247728347778, 1.1732337474822998, 1.4246220588684082, 1.7083560228347778, 0.8534625768661499, 1.156326413154602, 1.8021060228347778, 1.64741849899292, 2.31742525100708, 1.3350883722305298, 1.1747877597808838, 0.754849374294281, 1.152267336845398, 0.982150137424469, 1.3501359224319458, 1.451494574546814]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.395584225654602, 1.265013575553894, 1.372944951057434, 1.2867017984390259, 1.4790760278701782, 0.922070324420929, 1.203855276107788, 1.2020677328109741, 1.7788892984390259, 1.378617525100708, 1.523114800453186, 1.413773775100708, 1.839741826057434, 1.45771062374115, 1.3562160730361938, 1.32540762424469]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1331590414047241, 1.011752724647522, 1.01320481300354, 0.994040846824646, -0.0720076858997345, 1.2384086847305298, 1.190794825553894, 1.569356083869934, 0.750612735748291, 1.224898099899292, 1.2489639520645142, 1.097469449043274, 1.0813689231872559, 1.284578800201416, 0.914474368095398, 0.488767147064209]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0887229442596436, 2.2641303539276123, 2.20550274848938, 2.05808424949646, 1.981691598892212, 2.133763551712036, 1.966474175453186, 2.1845788955688477, 1.9427310228347778, 2.1575746536254883, 2.35699725151062, 2.655163049697876, 1.719170331954956, 2.310529947280884, 2.1713993549346924, 2.0605978965759277]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.399949073791504, 1.363145351409912, 1.851494550704956, 1.0293828248977661, 1.4382473230361938, 1.558525800704956, 1.3522758483886719, 1.3992187976837158, 1.4579483270645142, 1.611650824546814, 1.5336617231369019, 2.0181386470794678, 1.49660325050354, 1.504959225654602, 1.2907778024673462, 1.6577445268630981]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.365896701812744, 1.815964698791504, 1.1060206890106201, 1.347622275352478, 1.5851901769638062, 1.435869574546814, 1.3664401769638062, 1.4159647226333618, 1.4718071222305298, 1.444701075553894, 1.5442255735397339, 1.481300950050354, 1.2855638265609741, 0.6428520083427429, 1.317374348640442, 1.463688850402832]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3697011470794678, 2.4055027961730957, 2.3097825050354004, 2.1210598945617676, 2.6131114959716797, 2.238586902618408, 2.426426649093628, 2.1453804969787598, 2.165285348892212, 2.203396797180176, 2.7641983032226562, 2.1947689056396484, 2.2838315963745117, 2.4504754543304443, 2.332472801208496, 2.359442949295044]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.705230951309204, 1.690862774848938, 1.6224524974822998, 1.7955502271652222, 1.2957031726837158, 1.654687523841858, 1.6912363767623901, 2.044259548187256, 1.6985054016113281, 1.6157269477844238, 1.648148775100708, 1.558661699295044, 1.6340693235397339, 1.5666440725326538, 1.92255437374115, 1.95822012424469]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.784935474395752, 1.8073256015777588, 1.7214164733886719, 1.7543308734893799, 1.7581521272659302, 1.7909986972808838, 1.6938320398330688, 1.7356572151184082, 1.8166694641113281, 1.908933401107788, 2.2509171962738037, 1.653218388557434, 1.6368036270141602, 1.877089023590088, 1.8318678140640259, 1.7538690567016602]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5419625043869019, 1.6645539999008179, 1.596124529838562, 1.7124724388122559, 1.5815386772155762, 1.570812463760376, 1.74942147731781, 1.65381920337677, 1.5626565217971802, 1.6825577020645142, 1.2943274974822998, 1.6188349723815918, 1.595558762550354, 1.8979215621948242, 1.580086588859558, 1.659555435180664]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.036379098892212, 2.0972485542297363, 2.4056639671325684, 1.9902852773666382, 2.0987770557403564, 2.0556554794311523, 2.5363450050354004, 2.13090181350708, 2.0056045055389404, 2.0103261470794678, 2.015692949295044, 1.822995901107788, 2.0324814319610596, 1.9009510278701782, 2.2842390537261963, 2.0072689056396484]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.93444299697876, 5.3108696937561035, 4.910122394561768, 4.825390815734863, 4.908814430236816, 4.767798900604248, 4.940149307250977, 5.109341144561768, 5.192934989929199, 4.947927951812744, 4.91202449798584, 4.819021701812744, 4.970890045166016, 4.953532695770264, 5.062635898590088, 4.892119407653809]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.7155230045318604, 3.6007473468780518, 3.5611073970794678, 3.445596933364868, 3.6242527961730957, 3.704008102416992, 3.399592399597168, 3.6031250953674316, 3.7664742469787598, 3.670584201812744, 3.50594425201416, 3.1764180660247803, 3.4964334964752197, 3.628702402114868, 3.6673574447631836, 3.675475597381592]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.6591711044311523, 2.771127700805664, 2.6513586044311523, 2.573777198791504, 2.5394022464752197, 2.698777198791504, 2.6960597038269043, 2.440115451812744, 2.6334238052368164, 2.537262201309204, 3.110461950302124, 2.1210405826568604, 2.6757473945617676, 2.55706524848938, 3.0239131450653076, 2.6363790035247803]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.05873887985944748, 0.06860126554965973, 0.09026336669921875, -0.2851223349571228, -0.2564740777015686, -0.08467238396406174, 0.0908050537109375, -0.04233768954873085, 0.07607923448085785, 0.05687195807695389, 0.06029563769698143, 0.03501517325639725, 0.06709302961826324, 0.09622246772050858, -1.0270298719406128, 0.09764807671308517]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.07991000264883041, 0.04007291793823242, 0.02847749926149845, 0.03514964133501053, 0.06671196967363358, 0.021821429952979088, 0.051193851977586746, 0.09807634353637695, 0.06789524108171463, 0.06536401808261871, -0.20238998532295227, 0.05268212780356407, 0.010547978803515434, -0.02730846405029297, 0.029445035383105278, 0.035086002200841904]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08577550947666168, 0.0411054752767086, 0.06664453446865082, 0.07082720845937729, 0.14278970658779144, 0.07389218360185623, 0.06898168474435806, -0.10077769309282303, 0.08697496354579926, 0.13390660285949707, 0.03423295542597771, 0.07551847398281097, -0.21733339130878448, 0.005689791403710842, -0.01316887978464365, 0.10884924978017807]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1298719197511673, 0.13631603121757507, 0.12119865417480469, 0.13922882080078125, 0.14740991592407227, 0.12148039788007736, -0.014951569959521294, 0.17408207058906555, 0.1668044477701187, -0.5285208821296692, 0.023583276197314262, 0.08394176512956619, 0.31336846947669983, -0.32750755548477173, 0.08877045661211014, -0.0868387222290039]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.04433795437216759, 0.0913330465555191, 0.07300935685634613, 0.26749950647354126, 0.06475912034511566, -0.05010250583291054, 0.0222287867218256, 0.013154438696801662, -0.11301831156015396, 0.0011155264219269156, -0.2805047631263733, 0.12155751138925552, 0.1213596910238266, -0.24155695736408234, 0.1449245661497116, -0.11470740288496017]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.05639662221074104, 0.14916297793388367, 0.06224605068564415, 0.21419143676757812, -0.1547592729330063, -0.13833509385585785, 0.1306234747171402, -0.022125380113720894, 0.2224317342042923, -0.08986200392246246, -0.07901688665151596, 0.11822427809238434, -0.2674015462398529, 0.19714872539043427, 0.13353361189365387, 0.09513739496469498]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.15321479737758636, -0.0160707738250494, 0.13840211927890778, 0.3071703314781189, 0.17472662031650543, 0.15818364918231964, -0.22968332469463348, -0.09984779357910156, 0.34453582763671875, 0.1749136745929718, -0.6274290680885315, 0.3248436748981476, 0.2770429253578186, -0.35635867714881897, 0.32865121960639954, 0.23604297637939453]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.518281102180481, 0.1826455295085907, 0.20774759352207184, 0.17666353285312653, -0.9262041449546814, 0.3221959173679352, 0.22621753811836243, -0.4966144561767578, 0.6098204851150513, 0.13014766573905945, -0.26025110483169556, 0.2698516845703125, 0.09781183302402496, 0.28595271706581116, -0.7437596917152405, -0.2974044382572174]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2939284145832062, 0.28133392333984375, -0.28376007080078125, 0.3206765353679657, 0.5220206379890442, 0.35352420806884766, 0.27689334750175476, -0.2675395607948303, 0.2355109602212906, 0.7853589653968811, 0.13307783007621765, 0.25377655029296875, 0.5462559461593628, -0.5598591566085815, -0.5890589952468872, 0.4093295633792877]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4394749104976654, 0.2274823933839798, 0.505095362663269, 1.1967315673828125, 0.4070521891117096, 0.20323726534843445, 0.6254447102546692, 0.6353018879890442, 1.0001667737960815, 0.1294730007648468, 0.18297985196113586, 0.7238682508468628, 0.6658654808998108, -0.0392717644572258, 0.8296595811843872, 0.6894879937171936]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1599966287612915, 0.9242335557937622, 0.5006561279296875, 0.6957877278327942, 0.861541748046875, 0.6318340301513672, 0.6453726887702942, 0.7254464030265808, 0.031650543212890625, 0.39074817299842834, 0.7568621039390564, 1.0065699815750122, 0.23959459364414215, 0.1390487104654312, 1.087269902229309, 0.45058169960975647]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0347813367843628, 1.3559722900390625, 0.9768241047859192, 1.2525895833969116, 1.3917170763015747, 0.31155723333358765, 1.03070068359375, 0.9319632649421692, 1.0878993272781372, 1.6205967664718628, 0.9182521104812622, 0.9616371989250183, 0.34332275390625, 0.497832715511322, -0.00451605673879385, 0.718842625617981]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.0294832494109869, 0.8142961859703064, 1.4063459634780884, 1.1921952962875366, 0.3856658935546875, 0.5708182454109192, 1.1723741292953491, 1.4474923610687256, 0.4660399258136749, 0.2134966105222702, 0.9165239334106445, 0.8730992078781128, 0.8877825140953064, 0.9444580078125, 0.7411760687828064, 0.9612688422203064]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.9682268500328064, 1.1308943033218384, 1.3142350912094116, 0.49990954995155334, 1.2826625108718872, -0.022011348977684975, 1.2352992296218872, 2.1067593097686768, 1.2452218532562256, 1.0684640407562256, 1.65972900390625, 1.3655134439468384, 0.26129966974258423, 0.9841025471687317, 1.1436244249343872, 0.7455248236656189]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8202601671218872, 0.8089032769203186, 1.0924246311187744, 0.8678675889968872, 1.1189488172531128, 1.3764845132827759, 0.3629695475101471, 1.542338490486145, 0.7849295735359192, 0.7552533745765686, -0.34714046120643616, 0.7222359776496887, 0.9692121148109436, 1.1070992946624756, 1.2000819444656372, 0.4387487769126892]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.3692275583744049, 1.2875453233718872, 1.0649763345718384, 1.0136631727218628, 0.9056113362312317, 0.9734410047531128, 1.704801321029663, 1.1032191514968872, 1.0641915798187256, 1.0744279623031616, 0.7389286756515503, -0.340698778629303, 1.1526838541030884, 0.9139578938484192, 1.0100620985031128, 1.4557691812515259]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6251961588859558, 0.10621697455644608, 0.6881037950515747, 0.8024248480796814, 0.7223358154296875, 0.347188264131546, 0.5055966973304749, 0.7284384965896606, -0.9780781865119934, 2.281276226043701, -0.9548187255859375, 0.5338330864906311, 0.7997611165046692, 0.7973981499671936, 0.7493155598640442, 0.24784360826015472]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.09555026143789291, 0.8629962205886841, 1.52325439453125, 1.4002206325531006, 1.5795027017593384, 1.5380510091781616, 1.5068446397781372, 1.4061802625656128, 2.0875015258789062, 1.4048200845718384, 2.4994595050811768, 1.7450125217437744, 1.5022027492523193, 1.3980799913406372, 0.4885602593421936, 1.348726511001587]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5465959310531616, 1.4158412218093872, 1.5707658529281616, 1.9568568468093872, 0.9915591478347778, 1.2349853515625, 2.0931918621063232, 1.8094308376312256, 2.6424038410186768, 1.5697195529937744, 1.1939173936843872, 0.8359241485595703, 1.2317941188812256, 0.97038733959198, 1.5605119466781616, 1.5431779623031616]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.5064697265625, 1.3300431966781616, 1.6667306423187256, 1.3604038953781128, 1.65966796875, 0.9655674695968628, 1.301025390625, 1.2276524305343628, 2.083400249481201, 1.4532819986343384, 1.7191511392593384, 1.4718366861343384, 1.7940847873687744, 1.5527867078781128, 1.5420271158218384, 1.5330810546875]
Running loglikelihood requests:  63%|██████████████████████████████▊                  | 317/504 [17:35<10:10,  3.26s/it]Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2272502183914185, 0.96258544921875, 0.8687046766281128, 0.7926243543624878, -0.21704809367656708, 1.2955845594406128, 1.0929478406906128, 1.5635267496109009, 0.6059319376945496, 1.2237898111343384, 1.1703054904937744, 1.0623037815093994, 0.9905133843421936, 1.2300153970718384, 0.723602294921875, 0.26655933260917664]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.158970355987549, 2.3173129558563232, 2.2865512371063232, 2.0174038410186768, 1.9661341905593872, 2.192592144012451, 2.008754253387451, 2.2918527126312256, 2.0482351779937744, 2.146763324737549, 2.4139926433563232, 2.7621371746063232, 1.7038224935531616, 2.4534738063812256, 2.1929757595062256, 2.1871511936187744]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3932408094406128, 1.4559152126312256, 2.1702358722686768, 1.0197099447250366, 1.4226423501968384, 1.62841796875, 1.23492431640625, 1.4561244249343872, 1.5596052408218384, 1.6236048936843872, 1.6431013345718384, 2.1410434246063232, 1.4880021810531616, 1.6038644313812256, 1.1738717555999756, 1.7084611654281616]
Layer: gate_22 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4348843097686768, 1.8774064779281616, 1.00653076171875, 1.4619837999343872, 1.6865234375, 1.4103306531906128, 1.4617396593093872, 1.370361328125, 1.4483468532562256, 1.4786550998687744, 1.4684709310531616, 1.5113176107406616, 1.1392298936843872, 0.5049979090690613, 1.220703125, 1.4371163845062256]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.426478862762451, 2.4497768878936768, 2.423828125, 2.0530831813812256, 2.5418527126312256, 2.201381206512451, 2.394321918487549, 2.10107421875, 2.0719168186187744, 2.1531808376312256, 2.91455078125, 2.0809152126312256, 2.3662807941436768, 2.5015347003936768, 2.2293527126312256, 2.231654644012451]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6565638780593872, 1.5901576280593872, 1.5308663845062256, 1.80126953125, 1.2178431749343872, 1.6874302625656128, 1.6737234592437744, 2.0545828342437744, 1.6781179904937744, 1.5482003688812256, 1.6248605251312256, 1.4372209310531616, 1.6259765625, 1.453857421875, 2.1556918621063232, 1.9747488498687744]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_26 - Captured router_logits: [1.75732421875, 1.9670366048812866, 1.7308523654937744, 1.7320033311843872, 1.7567312717437744, 1.9203317165374756, 1.8211854696273804, 1.746826171875, 1.8296095132827759, 1.8876081705093384, 2.5252859592437744, 1.6917375326156616, 1.7254115343093872, 1.8700822591781616, 1.9035208225250244, 1.8518327474594116]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.664337158203125, 1.7071598768234253, 1.6365705728530884, 1.6807774305343628, 1.5625697374343872, 1.565889596939087, 1.8575657606124878, 1.7293875217437744, 1.5897325277328491, 1.6738978624343872, 1.2770298719406128, 1.6214163303375244, 1.7492719888687134, 2.130161762237549, 1.6286752223968506, 1.6343907117843628]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.028529644012451, 2.114467144012451, 2.3804757595062256, 1.9956752061843872, 2.1463797092437744, 2.0822055339813232, 2.6704800128936768, 2.1907784938812256, 2.0415737628936768, 2.0142300128936768, 2.015415668487549, 1.8130580186843872, 1.9246476888656616, 1.8497140407562256, 2.37939453125, 1.9666224718093872]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.816336631774902, 5.3846259117126465, 4.847865581512451, 4.6317138671875, 4.685791015625, 4.764718055725098, 4.815848350524902, 4.932442665100098, 5.035086631774902, 4.7757744789123535, 4.748848915100098, 4.727818012237549, 4.797031879425049, 5.0064873695373535, 5.033482074737549, 4.786586284637451]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.4946987628936768, 3.3484933376312256, 3.2634975910186768, 3.1346347332000732, 3.359619140625, 3.3628628253936768, 2.975742816925049, 3.480433940887451, 3.538051128387451, 3.320521831512451, 3.171142578125, 2.722224712371826, 3.035365581512451, 3.36572265625, 3.3968331813812256, 3.423130512237549]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.597377300262451, 2.7132742404937744, 2.7422573566436768, 2.6302316188812256, 2.633579730987549, 2.632045269012451, 2.6578543186187744, 2.28173828125, 2.658203125, 2.325892925262451, 3.236537456512451, 1.7619279623031616, 2.6392648220062256, 2.536098003387451, 3.1815707683563232, 2.5716378688812256]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06543367356061935, 0.07723559439182281, 0.08766772598028183, -0.26452869176864624, -0.23926496505737305, -0.07681136578321457, 0.1035216748714447, -0.03735021501779556, 0.06787210702896118, 0.06351031363010406, 0.08059582114219666, 0.05231757462024689, 0.08274676650762558, 0.09102537482976913, -1.0656496286392212, 0.10061673074960709]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07329033315181732, 0.033318426460027695, 0.03220590949058533, 0.03436226025223732, 0.061721526086330414, 0.006544594187289476, 0.03161196783185005, 0.0868714451789856, 0.021915195509791374, 0.06341113150119781, -0.22176697850227356, 0.04247105121612549, 0.020502708852291107, -0.035920631140470505, 0.03853156790137291, 0.014713532291352749]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07676415145397186, 0.05932455509901047, 0.09685413539409637, 0.0904448926448822, 0.11155645549297333, 0.09180551767349243, 0.061022303998470306, -0.09961239993572235, 0.08244241029024124, 0.12249958515167236, 0.0430021733045578, 0.09003359079360962, -0.22524721920490265, 0.0066800848580896854, -0.05368028208613396, 0.10292087495326996]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13631954789161682, 0.1386944204568863, 0.11715894192457199, 0.14625412225723267, 0.1264422982931137, 0.12485579401254654, -0.0013383744517341256, 0.19370345771312714, 0.18990270793437958, -0.5569243431091309, 0.053115569055080414, 0.05118076130747795, 0.2880946695804596, -0.2829364538192749, 0.031225530430674553, -0.06134252995252609]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06381747871637344, 0.10649065673351288, 0.09929390996694565, 0.2303464114665985, 0.05724066495895386, -0.06223633885383606, 0.023624075576663017, 0.025324812158942223, -0.09078264981508255, 0.03331151604652405, -0.24230681359767914, 0.1354328840970993, 0.02224583737552166, -0.2136462777853012, 0.13400089740753174, -0.05876145884394646]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06648838520050049, 0.11098019778728485, 0.043371181935071945, 0.13981862366199493, -0.1078348234295845, -0.10663583874702454, 0.15372143685817719, -0.029649339616298676, 0.22333836555480957, -0.10205312073230743, 0.06905330717563629, 0.06403783708810806, -0.2528161406517029, 0.1308240443468094, 0.1368662565946579, 0.060154613107442856]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.09692327678203583, -0.08004183322191238, 0.1329602748155594, 0.3702447712421417, 0.1796671599149704, 0.1584859937429428, -0.18132224678993225, -0.09285563975572586, 0.3803199529647827, 0.15701188147068024, -0.6261855363845825, 0.3063986897468567, 0.1984555423259735, -0.2955586314201355, 0.3195858597755432, 0.2246958464384079]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5167401432991028, 0.16312511265277863, 0.1603841632604599, 0.14292949438095093, -1.0259801149368286, 0.3792364299297333, 0.193374365568161, -0.47838035225868225, 0.573957085609436, 0.17133694887161255, -0.30735763907432556, 0.28396907448768616, 0.19604849815368652, 0.2350546419620514, -0.7936561107635498, -0.1918840855360031]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2771815359592438, 0.26984795928001404, -0.34859392046928406, 0.2934795618057251, 0.5011745095252991, 0.2315146028995514, 0.24303044378757477, -0.30400675535202026, 0.17185905575752258, 0.8722226023674011, 0.03261133283376694, 0.3001992106437683, 0.4119729995727539, -0.6620752811431885, -0.5816743969917297, 0.3889748454093933]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.2714436948299408, 0.06632541865110397, 0.47931239008903503, 1.0900548696517944, 0.2562745213508606, 0.09453795850276947, 0.42526382207870483, 0.4862995445728302, 0.6117256879806519, -0.0028565553948283195, 0.09573584049940109, 0.6205219030380249, 0.7373093366622925, 0.04622217267751694, 0.6989548206329346, 0.610228419303894]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0967742204666138, 0.8139736652374268, 0.520133912563324, 0.6976747512817383, 0.7369132041931152, 0.5410270094871521, 0.5860936641693115, 0.7085532546043396, 0.06279582530260086, 0.39067450165748596, 0.6886613368988037, 0.911018431186676, 0.2628701627254486, 0.1847638636827469, 0.8187443017959595, 0.25296926498413086]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.8463541865348816, 1.0704004764556885, 0.841920018196106, 0.9829409718513489, 1.207761526107788, 0.11577709019184113, 0.797675609588623, 0.8272804021835327, 0.9516733288764954, 1.4283766746520996, 0.7809651494026184, 0.7122043967247009, 0.09780911356210709, 0.2503543198108673, -0.31574422121047974, 0.5531841516494751]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [-0.03080790676176548, 0.6570484042167664, 1.121164083480835, 1.0516961812973022, 0.04753786325454712, 0.5518369674682617, 1.0063036680221558, 1.1240960359573364, 0.3066609799861908, 0.19774290919303894, 0.740993082523346, 0.843521237373352, 0.7179966568946838, 0.8060951828956604, 0.4734392464160919, 0.8139252662658691]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.5264958739280701, 0.8670027256011963, 0.8155220746994019, 0.1332985758781433, 0.9215758442878723, -0.43822142481803894, 1.0772804021835327, 2.2099521160125732, 0.9336993098258972, 0.8410138487815857, 1.112320065498352, 1.1156214475631714, -0.25522372126579285, 0.5491224527359009, 1.0451594591140747, 0.44877997040748596]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5203104019165039, 0.6250923871994019, 0.8725805878639221, 0.7740797400474548, 0.9136313796043396, 0.6301946043968201, 0.016112731769680977, 1.2288521528244019, 0.8841673731803894, 0.7064989805221558, -0.5730722546577454, 0.6018022298812866, 0.790412962436676, 1.0068535804748535, 1.1591356992721558, 0.44539663195610046]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.31160151958465576, 1.1939136981964111, 0.8484656810760498, 1.0355784893035889, 0.6019957661628723, 0.7991800308227539, 1.1457079648971558, 0.9767032861709595, 0.8718855381011963, 0.9870847463607788, 0.5994890332221985, -0.48365578055381775, 0.9575288891792297, 0.8509818315505981, 0.7598360180854797, 1.0632896423339844]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6603198647499084, 0.08690636605024338, 0.7738355994224548, 0.8567928075790405, 0.7515814304351807, 0.38343384861946106, 0.2115618735551834, 0.8139076828956604, -1.216688871383667, 2.476316213607788, -1.0511034727096558, 0.39429810643196106, 0.7398736476898193, 0.7397856712341309, 0.8144311308860779, 0.20829154551029205]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.15031859278678894, 0.46610185503959656, 1.2307854890823364, 0.9959046244621277, 1.3075731992721558, 1.2430320978164673, 1.1166419982910156, 1.1387686729431152, 1.714386224746704, 1.1974679231643677, 1.9454532861709595, 1.283097505569458, 1.3503693342208862, 1.3080570697784424, 0.3148380219936371, 1.0824037790298462]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3394566774368286, 1.1468539237976074, 1.3159135580062866, 1.6552294492721558, 0.6961422562599182, 1.021026849746704, 1.839667797088623, 1.5318483114242554, 2.1613528728485107, 1.2584108114242554, 0.9422948360443115, 0.5502973794937134, 1.0178310871124268, 0.7844304442405701, 1.2156003713607788, 1.2623170614242554]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.154472827911377, 1.0849169492721558, 1.2133833169937134, 1.1038851737976074, 1.3519495725631714, 0.6480014324188232, 0.921822190284729, 0.973892331123352, 1.6800570487976074, 1.308241844177246, 1.3838330507278442, 1.2402520179748535, 1.6714879274368286, 1.232193112373352, 1.2087204456329346, 1.177435278892517]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0310959815979004, 0.9177752137184143, 0.8660613894462585, 0.7756611704826355, -0.18457773327827454, 1.2029402256011963, 0.980644702911377, 1.4093072414398193, 0.770072340965271, 1.0962309837341309, 1.125365138053894, 1.0877938270568848, 0.8986222743988037, 1.168215036392212, 0.6890451312065125, 0.3151632845401764]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.8731348514556885, 2.055180072784424, 1.898191213607788, 1.7261754274368286, 1.6985852718353271, 1.9427083730697632, 1.656003713607788, 1.9800816774368286, 1.759994387626648, 1.7981418371200562, 2.145551919937134, 2.4906389713287354, 1.456802487373352, 2.105644702911377, 1.8719383478164673, 1.8654983043670654]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.195594072341919, 1.2630208730697632, 1.7687569856643677, 0.8874555826187134, 1.2365920543670654, 1.400689721107483, 1.1008938550949097, 1.2456010580062866, 1.3466356992721558, 1.3427822589874268, 1.4819819927215576, 1.9685388803482056, 1.3052153587341309, 1.3677153587341309, 0.9749525189399719, 1.470579981803894]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.3616623878479004, 1.7006263732910156, 0.7454426884651184, 1.2566863298416138, 1.3795045614242554, 1.232720971107483, 1.1886965036392212, 1.148895025253296, 1.3092975616455078, 1.3153153657913208, 1.284698724746704, 1.3876336812973022, 0.9948620200157166, 0.39614567160606384, 1.0256633758544922, 1.201295018196106]
Running loglikelihood requests:  64%|███████████████████████████████▏                 | 321/504 [17:47<09:45,  3.20s/it]Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3113033771514893, 2.237260580062866, 2.199676275253296, 1.9783220291137695, 2.4712133407592773, 2.018897771835327, 2.316793441772461, 2.008199691772461, 1.9958826303482056, 2.015836238861084, 2.721635580062866, 1.9500281810760498, 2.198690891265869, 2.293144702911377, 2.1545608043670654, 2.1276745796203613]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.533150315284729, 1.539344072341919, 1.4464386701583862, 1.6158150434494019, 1.193711280822754, 1.5010205507278442, 1.5345579385757446, 1.9258164167404175, 1.5538077354431152, 1.449500322341919, 1.5257776975631714, 1.3406531810760498, 1.4451717138290405, 1.3915399312973022, 1.8245354890823364, 1.8496973514556885]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.6827491521835327, 1.7074968814849854, 1.5657376050949097, 1.6129645109176636, 1.5815385580062866, 1.699967622756958, 1.5796982049942017, 1.613853096961975, 1.6671944856643677, 1.7489794492721558, 2.1901392936706543, 1.542832374572754, 1.5152907371520996, 1.8248170614242554, 1.8062655925750732, 1.6576895713806152]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5145230293273926, 1.5962090492248535, 1.4993776082992554, 1.6352208852767944, 1.4969031810760498, 1.5098475217819214, 1.688650369644165, 1.5847134590148926, 1.4950517416000366, 1.5835621356964111, 1.2349733114242554, 1.595892310142517, 1.5887000560760498, 1.8556512594223022, 1.5000616312026978, 1.5191353559494019]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9057573080062866, 1.966849684715271, 2.315192222595215, 1.845755934715271, 2.0391857624053955, 1.8904138803482056, 2.4506616592407227, 1.9778997898101807, 1.8940385580062866, 1.8520903587341309, 1.8220016956329346, 1.6768369674682617, 1.9310423135757446, 1.675605297088623, 2.1550183296203613, 1.864882469177246]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.847057819366455, 5.240322589874268, 4.859726905822754, 4.736389636993408, 4.777695655822754, 4.644460678100586, 4.838893413543701, 5.069327354431152, 5.0839667320251465, 4.794095039367676, 4.8261895179748535, 4.7437357902526855, 4.8925251960754395, 4.851633071899414, 5.004152774810791, 4.838647365570068]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.551309108734131, 3.378448724746704, 3.2862472534179688, 3.180694818496704, 3.5092201232910156, 3.5226633548736572, 3.120601177215576, 3.4628379344940186, 3.4946157932281494, 3.4053702354431152, 3.258481025695801, 2.892578125, 3.2090988159179688, 3.476597785949707, 3.444714307785034, 3.520991802215576]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.3418495655059814, 2.4621341228485107, 2.399352550506592, 2.43764066696167, 2.3227758407592773, 2.356489419937134, 2.546311855316162, 2.1498100757598877, 2.368102550506592, 2.1781039237976074, 3.0015485286712646, 1.8339574337005615, 2.3799972534179688, 2.2368383407592773, 2.7840652465820312, 2.3104941844940186]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.061018455773591995, 0.07459039241075516, 0.08341340720653534, -0.2584097981452942, -0.23534990847110748, -0.06469094008207321, 0.10233622789382935, -0.03033694624900818, 0.06857547163963318, 0.06216856837272644, 0.07324108481407166, 0.05296976864337921, 0.08031360805034637, 0.08698630332946777, -1.0267982482910156, 0.09327463805675507]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.06681885570287704, 0.0313776358962059, 0.020866634324193, 0.02873171493411064, 0.05429194122552872, 0.00434270640835166, 0.0331820584833622, 0.07801557332277298, 0.02313344180583954, 0.0590774267911911, -0.21397936344146729, 0.03940458223223686, 0.02077140472829342, -0.0437256395816803, 0.03922807797789574, 0.01776442676782608]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06960833072662354, 0.05530015379190445, 0.08868559449911118, 0.08879494667053223, 0.11142270267009735, 0.0818074569106102, 0.06424252688884735, -0.09359879046678543, 0.07161761075258255, 0.11330348253250122, 0.04796656221151352, 0.08511970937252045, -0.21254201233386993, 0.009261328727006912, -0.04895651713013649, 0.10118848830461502]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.135286346077919, 0.1439511477947235, 0.1157975047826767, 0.1473478078842163, 0.1269747018814087, 0.14184817671775818, 0.0028285635635256767, 0.17917543649673462, 0.18963097035884857, -0.5281119346618652, 0.04929681867361069, 0.06152673810720444, 0.30469268560409546, -0.3025652766227722, 0.03608249872922897, -0.05305659770965576]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0677827000617981, 0.0999520793557167, 0.10984870791435242, 0.24433554708957672, 0.053374283015728, -0.05269169062376022, 0.027768384665250778, 0.026727968826889992, -0.09934213757514954, 0.025331513956189156, -0.23470601439476013, 0.1378418505191803, 0.02068926766514778, -0.21550065279006958, 0.12404082715511322, -0.0717264786362648]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06368386000394821, 0.11724495887756348, 0.030686112120747566, 0.15488839149475098, -0.11161130666732788, -0.09996019303798676, 0.14966727793216705, -0.018837319687008858, 0.20024225115776062, -0.09372546523809433, 0.07975329458713531, 0.06448143720626831, -0.25280430912971497, 0.1347348988056183, 0.13825026154518127, 0.056294914335012436]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.09923367947340012, -0.06575630605220795, 0.11709842085838318, 0.3735215365886688, 0.17415432631969452, 0.16442705690860748, -0.16469869017601013, -0.11033643782138824, 0.3704586625099182, 0.15659642219543457, -0.6028338074684143, 0.2844669818878174, 0.20279067754745483, -0.28122636675834656, 0.2954067289829254, 0.23240964114665985]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5328786969184875, 0.18044109642505646, 0.17963361740112305, 0.1255655437707901, -0.9781944751739502, 0.3714061975479126, 0.19555313885211945, -0.4593604803085327, 0.5727835893630981, 0.19078490138053894, -0.29626134037971497, 0.2830060124397278, 0.21765659749507904, 0.212247833609581, -0.7569420337677002, -0.19706107676029205]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.27619343996047974, 0.26337191462516785, -0.31794092059135437, 0.28626176714897156, 0.4967997670173645, 0.23102740943431854, 0.2436583936214447, -0.3264283835887909, 0.151576966047287, 0.8803523778915405, 0.022498328238725662, 0.2899491488933563, 0.3905089795589447, -0.5986866354942322, -0.5471279621124268, 0.4012005925178528]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.2755616307258606, 0.09130694717168808, 0.4900347590446472, 1.076295018196106, 0.2644405961036682, 0.12016475200653076, 0.44533008337020874, 0.4643964469432831, 0.6193000674247742, 0.01758994720876217, 0.14174315333366394, 0.602662205696106, 0.7168362140655518, 0.14674624800682068, 0.6999313831329346, 0.5747730135917664]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.119072437286377, 0.7785710692405701, 0.5185568928718567, 0.6666886806488037, 0.7348698973655701, 0.5232588052749634, 0.5612924695014954, 0.6815438270568848, 0.05516533553600311, 0.42788395285606384, 0.6890484094619751, 0.9069428443908691, 0.25242382287979126, 0.18444466590881348, 0.8373422622680664, 0.2815183401107788]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.8834261298179626, 1.108224630355835, 0.8837978839874268, 1.007135033607483, 1.2348676919937134, 0.14750602841377258, 0.8369580507278442, 0.828125, 1.0086263418197632, 1.4390748739242554, 0.7996419072151184, 0.747008740901947, 0.1466551125049591, 0.30250728130340576, -0.3102906346321106, 0.63427734375]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.02600730024278164, 0.7064541578292847, 1.2002569437026978, 1.0986239910125732, 0.08516328781843185, 0.5734203457832336, 1.0718092918395996, 1.1586111783981323, 0.34342557191848755, 0.2028300017118454, 0.7177778482437134, 0.8756334185600281, 0.7709394693374634, 0.822036862373352, 0.5094423294067383, 0.8436444401741028]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6127912998199463, 0.9451629519462585, 0.8797035813331604, 0.24036607146263123, 1.0054371356964111, -0.4011417329311371, 1.1627076864242554, 2.347691535949707, 1.0300204753875732, 0.9306904673576355, 1.1657758951187134, 1.1705025434494019, -0.19625772535800934, 0.6552338600158691, 1.2007759809494019, 0.4974794089794159]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5344876050949097, 0.6692884564399719, 0.8394126296043396, 0.7651762962341309, 0.8989126086235046, 0.5624920129776001, 0.03717755898833275, 1.2189388275146484, 0.8462837934494019, 0.6880850791931152, -0.5986515283584595, 0.5623528957366943, 0.803764820098877, 0.976448118686676, 1.1043602228164673, 0.4289149343967438]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.3073972463607788, 1.1401323080062866, 0.8711993098258972, 1.0652977228164673, 0.6082049012184143, 0.8081186413764954, 1.2089557647705078, 0.9829321503639221, 0.8411106467247009, 0.957946240901947, 0.5986745953559875, -0.487552672624588, 0.9226580262184143, 0.9171593189239502, 0.7833702564239502, 1.141555666923523]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6613989472389221, 0.11189860850572586, 0.8049519658088684, 0.8636771440505981, 0.7562465071678162, 0.41716182231903076, 0.12667255103588104, 0.8334850668907166, -1.1960657835006714, 2.590301275253296, -1.0649007558822632, 0.42815062403678894, 0.7620882391929626, 0.792915940284729, 0.8423115611076355, 0.20457197725772858]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.09599510580301285, 0.5150591731071472, 1.2783994674682617, 1.0640989542007446, 1.3641786575317383, 1.2962063550949097, 1.1801625490188599, 1.2402783632278442, 1.7767982482910156, 1.3145191669464111, 1.9971319437026978, 1.2881211042404175, 1.3391625881195068, 1.3443834781646729, 0.3495609760284424, 1.1428816318511963]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.347198724746704, 1.1394284963607788, 1.3370636701583862, 1.65625, 0.7345113754272461, 1.074517846107483, 1.8431869745254517, 1.5481771230697632, 2.191195011138916, 1.3025407791137695, 0.9624727368354797, 0.5791323781013489, 1.0669957399368286, 0.810973584651947, 1.2798247337341309, 1.312605619430542]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.1525460481643677, 1.103339672088623, 1.1997994184494019, 1.1085655689239502, 1.3298141956329346, 0.6350133419036865, 0.926331102848053, 0.9695110321044922, 1.6752445697784424, 1.289467215538025, 1.361732840538025, 1.2164801359176636, 1.6268035173416138, 1.2256827354431152, 1.2002393007278442, 1.1791244745254517]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.9940944314002991, 0.8943201303482056, 0.8580729365348816, 0.7875140905380249, -0.195463165640831, 1.1600550413131714, 0.9702280163764954, 1.3714104890823364, 0.7721199989318848, 1.1047472953796387, 1.143255591392517, 1.0950433015823364, 0.8888126611709595, 1.177910327911377, 0.6852433681488037, 0.30855128169059753]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.8750351667404175, 2.062077760696411, 1.8933699131011963, 1.7333896160125732, 1.7132248878479004, 1.926062822341919, 1.6768369674682617, 1.9421452283859253, 1.7586570978164673, 1.8121832609176636, 2.136120557785034, 2.4984514713287354, 1.4355952739715576, 2.0718610286712646, 1.8759149312973022, 1.8565596342086792]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.2220579385757446, 1.2602055072784424, 1.7661176919937134, 0.8871762156486511, 1.2562289237976074, 1.4206081628799438, 1.113633155822754, 1.261648416519165, 1.375105619430542, 1.3658853769302368, 1.5027449131011963, 1.9960585832595825, 1.3285472393035889, 1.3922438621520996, 0.9948708415031433, 1.4896537065505981]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.346952438354492, 1.6791596412658691, 0.7343618273735046, 1.2262282371520996, 1.327825903892517, 1.2024915218353271, 1.1611416339874268, 1.1548775434494019, 1.288490653038025, 1.2973676919937134, 1.2507389783859253, 1.3628941774368286, 0.9615005850791931, 0.3489008843898773, 1.0175957679748535, 1.1911951303482056]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.389921188354492, 2.3115146160125732, 2.257460594177246, 2.0398366451263428, 2.4973957538604736, 2.0732686519622803, 2.38295316696167, 2.0712978839874268, 2.067110061645508, 2.0834038257598877, 2.819890260696411, 2.0063695907592773, 2.2169201374053955, 2.3769004344940186, 2.2288851737976074, 2.2255771160125732]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.5793215036392212, 1.5792863368988037, 1.4793074131011963, 1.6707136631011963, 1.1782622337341309, 1.5346283912658691, 1.574447512626648, 1.9748733043670654, 1.5960373878479004, 1.4953194856643677, 1.566511869430542, 1.3915752172470093, 1.493735909461975, 1.443060278892517, 1.8978040218353271, 1.9051238298416138]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.7316828966140747, 1.7335788011550903, 1.630401849746704, 1.652854084968567, 1.614152193069458, 1.7223504781723022, 1.608132243156433, 1.648947834968567, 1.6823796033859253, 1.7795277833938599, 2.216904640197754, 1.5524275302886963, 1.5414378643035889, 1.8424478769302368, 1.7931270599365234, 1.6849640607833862]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Running loglikelihood requests:  64%|███████████████████████████████▌                 | 325/504 [17:59<09:21,  3.13s/it]Layer: gate_27 - Captured router_logits: [1.524642825126648, 1.6078200340270996, 1.5013185739517212, 1.639091968536377, 1.5066930055618286, 1.5191705226898193, 1.7033230066299438, 1.603392481803894, 1.5121424198150635, 1.6090054512023926, 1.2256475687026978, 1.5908070802688599, 1.5892894268035889, 1.874267578125, 1.518035650253296, 1.5241764783859253]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.875769853591919, 1.941353440284729, 2.269566535949707, 1.8197846412658691, 1.9984339475631714, 1.8674118518829346, 2.4317638874053955, 1.9497642517089844, 1.8510786294937134, 1.826418161392212, 1.7858777046203613, 1.6491764783859253, 1.8917511701583862, 1.6526604890823364, 2.138108730316162, 1.8348729610443115]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.932432651519775, 5.3531813621521, 4.921171188354492, 4.833474159240723, 4.829110145568848, 4.714949131011963, 4.919129848480225, 5.15477180480957, 5.17201566696167, 4.889287948608398, 4.894425868988037, 4.805250644683838, 4.977371692657471, 4.923986434936523, 5.0800251960754395, 4.904138565063477]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.660050630569458, 3.4710023403167725, 3.3808417320251465, 3.263187885284424, 3.582488775253296, 3.5881898403167725, 3.2326505184173584, 3.5645763874053955, 3.632179021835327, 3.5078125, 3.3488528728485107, 2.9845070838928223, 3.324535369873047, 3.569749355316162, 3.543215036392212, 3.6034276485443115]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.4119510650634766, 2.5491273403167725, 2.4735360145568848, 2.5137951374053955, 2.375563144683838, 2.4514358043670654, 2.5841779708862305, 2.222093105316162, 2.422578811645508, 2.2746341228485107, 3.125563144683838, 1.8645844459533691, 2.4600930213928223, 2.3212978839874268, 2.879926919937134, 2.3855574131011963]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06550633162260056, 0.08038400858640671, 0.08712324500083923, -0.27528390288352966, -0.27007001638412476, -0.06654177606105804, 0.10031585395336151, -0.11789772659540176, 0.048889245837926865, 0.06996452808380127, 0.07764115929603577, 0.04967339336872101, 0.0757855474948883, 0.09469646215438843, -1.0760475397109985, 0.09954085201025009]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0792752355337143, 0.03885175660252571, 0.029015280306339264, 0.061268217861652374, 0.07166019827127457, 0.029326386749744415, 0.04124452546238899, 0.07636267691850662, 0.0170576274394989, 0.07099595665931702, -0.1839807629585266, 0.05794506147503853, 0.02528020739555359, -0.011568658985197544, 0.04072459787130356, 0.023984042927622795]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0672164186835289, 0.06260854750871658, 0.08190876245498657, 0.07532435655593872, 0.11533251404762268, 0.08721039444208145, 0.0445941761136055, -0.08869407325983047, 0.09077107161283493, 0.086590975522995, 0.034678615629673004, 0.0757237896323204, -0.19652099907398224, 0.03706664964556694, -0.017714032903313637, 0.10334888845682144]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12507851421833038, 0.15406548976898193, 0.14823968708515167, 0.12222636491060257, 0.11447346955537796, 0.12007363140583038, 0.012416770681738853, 0.16361278295516968, 0.18976564705371857, -0.5363414287567139, 0.09710311889648438, 0.07309674471616745, 0.23000259697437286, -0.2906150221824646, 0.03661637753248215, -0.03843633458018303]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.09517822414636612, 0.08572769165039062, 0.08114492148160934, 0.20656821131706238, 0.1330270916223526, -0.07563545554876328, 0.01977532170712948, 0.020211514085531235, -0.07908928394317627, 0.026624921709299088, -0.20867809653282166, 0.11065885424613953, -0.006602061912417412, -0.19431333243846893, 0.14251819252967834, -0.07458551228046417]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.05224141106009483, 0.13410800695419312, 0.08840470761060715, 0.17367012798786163, -0.12143748998641968, -0.08354159444570541, 0.14039264619350433, -0.02030639722943306, 0.17001280188560486, -0.06609705090522766, -0.08697786927223206, 0.11635929346084595, -0.20955921709537506, 0.20355334877967834, 0.12183365970849991, 0.10451555997133255]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11793649941682816, -0.01361208874732256, 0.172584667801857, 0.321273535490036, 0.18839222192764282, 0.1422596275806427, -0.19388344883918762, -0.07021345943212509, 0.3546100854873657, 0.25175225734710693, -0.5497336387634277, 0.2974531650543213, 0.2683660387992859, -0.31168684363365173, 0.24227850139141083, 0.17936734855175018]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.4280168116092682, 0.1593835949897766, 0.2008705884218216, 0.13227303326129913, -0.8900168538093567, 0.30622565746307373, 0.2719193994998932, -0.4445478916168213, 0.5107810497283936, 0.20353615283966064, -0.17999212443828583, 0.24550780653953552, 0.101384736597538, 0.32360896468162537, -0.5835859775543213, -0.2198220044374466]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.25361329317092896, 0.29272961616516113, -0.33728691935539246, 0.30014702677726746, 0.5262972712516785, 0.4102361500263214, 0.27790194749832153, -0.3300337493419647, 0.17580761015415192, 0.7587369084358215, 0.1921532303094864, 0.21707569062709808, 0.6228560209274292, -0.5032853484153748, -0.5095381140708923, 0.3837524354457855]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.48358044028282166, 0.20242586731910706, 0.38543978333473206, 1.0766112804412842, 0.42161449790000916, 0.22207225859165192, 0.5953469276428223, 0.6671031713485718, 0.7866488099098206, 0.2061670422554016, 0.2201060950756073, 0.6729314923286438, 0.6973005533218384, 0.07012606412172318, 0.7594016194343567, 0.7162819504737854]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9848998785018921, 0.8582608103752136, 0.587292492389679, 0.6984197497367859, 0.902301013469696, 0.5775650143623352, 0.6948708295822144, 0.7924005389213562, 0.04933305084705353, 0.37207695841789246, 0.8267666697502136, 0.8735928535461426, 0.34442776441574097, 0.21672411262989044, 1.115433692932129, 0.3985988199710846]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [0.9097833633422852, 1.0992542505264282, 0.9499289989471436, 1.197222352027893, 1.2812234163284302, 0.2990700602531433, 0.8875799179077148, 0.8352283835411072, 0.9762251377105713, 1.5507724285125732, 0.8359885215759277, 0.9010519981384277, 0.27742865681648254, 0.5318905711174011, -0.04905298352241516, 0.7342923283576965]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.05511918663978577, 0.7754127979278564, 1.2899502515792847, 1.0461292266845703, 0.31508374214172363, 0.5498202443122864, 1.0775113105773926, 1.1888716220855713, 0.45801281929016113, 0.26816073060035706, 1.1269330978393555, 0.9240589737892151, 0.7554953694343567, 0.8512251377105713, 0.7778142690658569, 1.0149680376052856]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7871603965759277, 0.9827769994735718, 1.3527432680130005, 0.46149957180023193, 1.1219992637634277, -0.1980685144662857, 1.1108310222625732, 1.926171898841858, 1.0481889247894287, 1.0809303522109985, 1.3925070762634277, 1.289595127105713, 0.22522208094596863, 0.8195270895957947, 1.0156782865524292, 0.6843525171279907]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8444158434867859, 0.8680841326713562, 0.9947975873947144, 0.8264737129211426, 1.1644176244735718, 1.1787619590759277, 0.1807117760181427, 1.4480024576187134, 0.7588601112365723, 0.8635919690132141, -0.4904951751232147, 0.7846313714981079, 0.9920454621315002, 1.2973366975784302, 1.1124112606048584, 0.4867151379585266]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.29255205392837524, 1.0676047801971436, 0.9858487248420715, 0.9803799986839294, 0.7150834798812866, 0.9479225873947144, 1.378679633140564, 0.947265625, 0.9729048013687134, 1.126953125, 0.5954872965812683, -0.3086525797843933, 1.2600563764572144, 0.922656238079071, 0.9037464261054993, 1.276782751083374]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6949618458747864, 0.08521728217601776, 1.0216641426086426, 0.8602805137634277, 0.6580233573913574, 0.47470176219940186, 0.3941997289657593, 0.7798145413398743, -0.9993563294410706, 2.3798828125, -1.002178430557251, 0.5640899538993835, 0.8412109613418579, 0.8999201059341431, 0.9321688413619995, 0.12934459745883942]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.009205210953950882, 0.832775890827179, 1.546306848526001, 1.2365906238555908, 1.4826171398162842, 1.5057705640792847, 1.3237438201904297, 1.3670454025268555, 1.7907581329345703, 1.3054065704345703, 2.254687547683716, 1.443889856338501, 1.3159911632537842, 1.2006570100784302, 0.40005770325660706, 1.2955111265182495]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5916903018951416, 1.2409090995788574, 1.4979952573776245, 1.8830965757369995, 0.9813753962516785, 1.113299012184143, 1.8440518379211426, 1.7213068008422852, 2.428959608078003, 1.4432528018951416, 1.1591219902038574, 0.7773326635360718, 1.1550337076187134, 0.9121039509773254, 1.4405362606048584, 1.4938920736312866]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.356019139289856, 1.2194602489471436, 1.4325106143951416, 1.2521483898162842, 1.5813565254211426, 0.9358465075492859, 1.2351207733154297, 1.1810812950134277, 1.8169033527374268, 1.336345911026001, 1.5985084772109985, 1.460493564605713, 1.8562322854995728, 1.531835913658142, 1.4658735990524292, 1.4696377515792847]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.165607213973999, 1.0878373384475708, 1.023455262184143, 0.9136741161346436, -0.077602319419384, 1.4867453575134277, 1.163529872894287, 1.6013838052749634, 0.7417946457862854, 1.3732954263687134, 1.220170497894287, 1.1300203800201416, 1.2359552383422852, 1.3813210725784302, 0.918261706829071, 0.48401182889938354]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.070561170578003, 2.2255682945251465, 2.1634232997894287, 1.9225142002105713, 1.8893465995788574, 2.14453125, 1.9025923013687134, 2.231605052947998, 1.9339133501052856, 2.1186790466308594, 2.3357954025268555, 2.550497055053711, 1.7044034004211426, 2.3773436546325684, 2.1482598781585693, 2.0794034004211426]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4077060222625732, 1.4654474258422852, 2.021733045578003, 1.0892045497894287, 1.4856534004211426, 1.686079502105713, 1.3596057891845703, 1.4431108236312866, 1.5604047775268555, 1.683061122894287, 1.6118252277374268, 2.1192116737365723, 1.5167258977890015, 1.6232954263687134, 1.2964400053024292, 1.7032670974731445]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.3896660804748535, 1.8738635778427124, 1.1404829025268555, 1.4308593273162842, 1.7045810222625732, 1.5064630508422852, 1.496661901473999, 1.397052526473999, 1.6277166604995728, 1.4699573516845703, 1.5709161758422852, 1.5744140148162842, 1.2872692346572876, 0.7504050731658936, 1.4002485275268555, 1.5698508024215698]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.319176197052002, 2.482670545578003, 2.2947442531585693, 2.102627754211426, 2.568110704421997, 2.1803977489471436, 2.503124952316284, 2.1186790466308594, 2.083203077316284, 2.169105052947998, 2.7537641525268555, 2.1784801483154297, 2.2990057468414307, 2.4976563453674316, 2.3198153972625732, 2.302414655685425]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7084872722625732, 1.7034446001052856, 1.6337357759475708, 1.881392002105713, 1.3509232997894287, 1.6330965757369995, 1.731036901473999, 2.0342328548431396, 1.7546164989471436, 1.6326349973678589, 1.704527735710144, 1.5763493776321411, 1.7079545259475708, 1.5309303998947144, 1.9704545736312866, 1.9528053998947144]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.7323508262634277, 1.8096390962600708, 1.7020596265792847, 1.7306108474731445, 1.700133204460144, 1.8709428310394287, 1.7006125450134277, 1.6838067770004272, 1.8204267024993896, 1.8849343061447144, 2.3261630535125732, 1.6423295736312866, 1.6837003231048584, 1.7951881885528564, 1.7901544570922852, 1.7543556690216064]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6207607984542847, 1.652936577796936, 1.6051092147827148, 1.690611720085144, 1.5362749099731445, 1.565247654914856, 1.7527399063110352, 1.701440453529358, 1.565247654914856, 1.6485795974731445, 1.3188388347625732, 1.578955054283142, 1.6878639459609985, 1.9186056852340698, 1.5827414989471436, 1.6112349033355713]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9046519994735718, 1.9641690254211426, 2.264204502105713, 1.8700639009475708, 1.9815696477890015, 1.9513493776321411, 2.382244348526001, 2.046271324157715, 1.874573826789856, 1.8692116737365723, 1.9088423252105713, 1.7096946239471436, 1.8487216234207153, 1.7345880270004272, 2.2093393802642822, 1.9043323993682861]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.813991546630859, 5.210866451263428, 4.7389912605285645, 4.567897796630859, 4.609694480895996, 4.563068389892578, 4.708593845367432, 4.92503547668457, 4.931534290313721, 4.6828837394714355, 4.669353485107422, 4.607315540313721, 4.736754417419434, 4.7861504554748535, 4.937073707580566, 4.655681610107422]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.4505326747894287, 3.377201795578003, 3.260298252105713, 3.130042552947998, 3.298792600631714, 3.3845880031585693, 3.076171875, 3.4063210487365723, 3.484410524368286, 3.2948508262634277, 3.1740057468414307, 2.9047763347625732, 3.175053358078003, 3.3566761016845703, 3.4761364459991455, 3.5107598304748535]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Running loglikelihood requests:  65%|███████████████████████████████▉                 | 329/504 [18:12<09:11,  3.15s/it]Layer: gate_31 - Captured router_logits: [2.5029120445251465, 2.6295454502105713, 2.6488635540008545, 2.488849401473999, 2.5285511016845703, 2.6583807468414307, 2.5348012447357178, 2.3202414512634277, 2.501633405685425, 2.358274221420288, 2.973224401473999, 1.825623631477356, 2.5313210487365723, 2.4538352489471436, 3.011150598526001, 2.515340805053711]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.08174531906843185, 0.09456536173820496, 0.10240397602319717, -0.2948285639286041, -0.26516470313072205, -0.07608816027641296, 0.11511874198913574, -0.12382045388221741, 0.07094244658946991, 0.08240865916013718, 0.09618153423070908, 0.0658542737364769, 0.08695998042821884, 0.10680004209280014, -1.1327340602874756, 0.11382773518562317]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0855981633067131, 0.04828972741961479, 0.03831363096833229, 0.06391054391860962, 0.07768333703279495, 0.030761465430259705, 0.05729297176003456, 0.08569125831127167, 0.013332996517419815, 0.07329244166612625, -0.21692608296871185, 0.060587190091609955, 0.015368260443210602, -0.017164282500743866, 0.04555942118167877, 0.01794024184346199]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08046330511569977, 0.07047659903764725, 0.08999297767877579, 0.06882679462432861, 0.11436798423528671, 0.09577626734972, 0.046215303242206573, -0.08024667203426361, 0.08658004552125931, 0.11342587321996689, 0.03354700654745102, 0.086468406021595, -0.20260941982269287, 0.029645832255482674, -0.04426116123795509, 0.11442926526069641]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.11850255727767944, 0.15400430560112, 0.1480981707572937, 0.13039377331733704, 0.13305817544460297, 0.12272994220256805, -0.013994689099490643, 0.17571517825126648, 0.19083677232265472, -0.5291109681129456, 0.050110701471567154, 0.08493349701166153, 0.2297355979681015, -0.28890514373779297, 0.03144850581884384, -0.0705208033323288]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06715071201324463, 0.08767832815647125, 0.07822803407907486, 0.20821891725063324, 0.06922110915184021, -0.06467381864786148, 0.022651830688118935, 0.02069847658276558, -0.09910569339990616, 0.0417085699737072, -0.21551640331745148, 0.11627155542373657, 0.007598317228257656, -0.20672985911369324, 0.12635397911071777, -0.054006874561309814]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.08181259036064148, 0.13787701725959778, 0.0691554918885231, 0.18246978521347046, -0.12141614407300949, -0.08519189804792404, 0.12227329611778259, 0.014788076281547546, 0.1876506209373474, -0.0785738080739975, -0.0854940116405487, 0.10809116065502167, -0.2188020795583725, 0.1825074404478073, 0.12602835893630981, 0.08800170570611954]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.10433329641819, 0.00685105612501502, 0.16686619818210602, 0.31279847025871277, 0.19346883893013, 0.13553276658058167, -0.1477210372686386, -0.062023479491472244, 0.3681063950061798, 0.16427332162857056, -0.5573428273200989, 0.3204146921634674, 0.2572726905345917, -0.3216037452220917, 0.2785080373287201, 0.18331678211688995]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.47878482937812805, 0.20253592729568481, 0.21587631106376648, 0.14486750960350037, -0.8858944773674011, 0.3304026126861572, 0.21627457439899445, -0.34198278188705444, 0.5697903633117676, 0.1755470484495163, -0.18819350004196167, 0.23366595804691315, 0.15422071516513824, 0.29392009973526, -0.6273965835571289, -0.17540821433067322]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2550155222415924, 0.3230372369289398, -0.32038864493370056, 0.3413543701171875, 0.5075101256370544, 0.3361676335334778, 0.2308800369501114, -0.26870307326316833, 0.17103955149650574, 0.7572816610336304, 0.10715995728969574, 0.22209447622299194, 0.543797492980957, -0.5016518831253052, -0.49883976578712463, 0.3678423762321472]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3788471817970276, 0.28401297330856323, 0.3766070604324341, 1.0915840864181519, 0.3643082082271576, 0.22353342175483704, 0.6186176538467407, 0.5813816785812378, 0.8071602582931519, 0.16869865357875824, 0.26517200469970703, 0.6741775274276733, 0.7113686800003052, 0.1718183159828186, 0.7637614607810974, 0.6749300956726074]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0454851388931274, 0.8993558287620544, 0.590576171875, 0.7277540564537048, 0.9409269094467163, 0.6200964450836182, 0.6745851635932922, 0.7611632943153381, 0.21181327104568481, 0.469220370054245, 0.7562983632087708, 0.9161723852157593, 0.3091139495372772, 0.2016332745552063, 1.064680576324463, 0.46593791246414185]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [0.9057214260101318, 1.16001296043396, 0.8939399123191833, 1.2573578357696533, 1.2974215745925903, 0.42959707975387573, 0.9002159237861633, 0.8356093764305115, 0.9813377857208252, 1.5653132200241089, 0.8551193475723267, 0.9124408960342407, 0.33739492297172546, 0.45609891414642334, -0.004270781297236681, 0.7088432908058167]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.12580744922161102, 0.8142201900482178, 1.2427787780761719, 1.0957568883895874, 0.3645993769168854, 0.6969677209854126, 1.1339917182922363, 1.2991775274276733, 0.5880164504051208, 0.37594074010849, 1.0152509212493896, 0.8367796540260315, 0.844162106513977, 0.8240216374397278, 0.6737654209136963, 0.985916018486023]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8458979725837708, 0.9811944365501404, 1.2218364477157593, 0.5428093075752258, 1.1311728954315186, -0.07069159299135208, 1.084396481513977, 2.068699836730957, 1.0172914266586304, 0.9698609709739685, 1.4422842264175415, 1.2386395931243896, 0.173968106508255, 0.8595049381256104, 1.0410515069961548, 0.7075158953666687]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7730724215507507, 0.8268706798553467, 1.01548171043396, 0.8410801291465759, 1.1495842933654785, 1.1966025829315186, 0.4332629442214966, 1.5607526302337646, 0.7481678128242493, 0.7977261543273926, -0.30727729201316833, 0.8511525988578796, 1.0256470441818237, 1.1904475688934326, 1.1670997142791748, 0.5220420956611633]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4339871108531952, 1.1673415899276733, 1.1041427850723267, 1.0504229068756104, 0.9260746836662292, 0.9958249926567078, 1.5376665592193604, 1.0440438985824585, 1.0495628118515015, 1.0994480848312378, 0.6933638453483582, -0.3136288821697235, 1.2289009094238281, 0.9789994359016418, 0.9506880640983582, 1.472834587097168]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6740745306015015, 0.10357022285461426, 0.8257384896278381, 0.8651895523071289, 0.6688103675842285, 0.4402101933956146, 0.3838176131248474, 0.7787839770317078, -0.969085156917572, 2.3365466594696045, -0.86722332239151, 0.6239853501319885, 0.8488657474517822, 0.8854913115501404, 0.8185161352157593, 0.21851958334445953]
Running loglikelihood requests:  66%|████████████████████████████████▍                | 333/504 [18:25<09:03,  3.18s/it]Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.0822305977344513, 0.7598428726196289, 1.488576889038086, 1.3799231052398682, 1.4784618616104126, 1.45688796043396, 1.371480107307434, 1.44050133228302, 1.917923927307129, 1.3382244110107422, 2.3342530727386475, 1.5432913303375244, 1.4341682195663452, 1.293013572692871, 0.5133493542671204, 1.2994149923324585]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.534152865409851, 1.3116040229797363, 1.5670244693756104, 1.9244911670684814, 0.9047560095787048, 1.1718839406967163, 1.8973623514175415, 1.831421971321106, 2.4752724170684814, 1.4609016180038452, 1.2367939949035645, 0.7858965396881104, 1.126832127571106, 0.9064751267433167, 1.4391485452651978, 1.5287772417068481]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.5817086696624756, 1.3698036670684814, 1.596706509590149, 1.3711475133895874, 1.7298953533172607, 1.0790858268737793, 1.3961617946624756, 1.3721240758895874, 2.0540244579315186, 1.486740231513977, 1.7393742799758911, 1.6205382347106934, 1.956027865409851, 1.6324361562728882, 1.5723193883895874, 1.5507453680038452]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.147796869277954, 1.0269584655761719, 0.9679078459739685, 0.8915903568267822, -0.07936446368694305, 1.4164366722106934, 1.1425422430038452, 1.67900550365448, 0.7728798985481262, 1.3177143335342407, 1.2286947965621948, 1.140477180480957, 1.1100021600723267, 1.3439291715621948, 0.9447072744369507, 0.4870443046092987]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0985522270202637, 2.251648426055908, 2.235450029373169, 1.9707927703857422, 1.9712944030761719, 2.1506593227386475, 1.9381450414657593, 2.2009031772613525, 1.959934115409851, 2.196674346923828, 2.3413846492767334, 2.6429903507232666, 1.7753872871398926, 2.3760035037994385, 2.17592453956604, 2.1382596492767334]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.361668586730957, 1.384281873703003, 2.0108227729797363, 1.0404534339904785, 1.4504371881484985, 1.5907397270202637, 1.3110486268997192, 1.430440068244934, 1.4890697002410889, 1.6258063316345215, 1.583034634590149, 2.0690224170684814, 1.4590381383895874, 1.5692015886306763, 1.2219932079315186, 1.671050786972046]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4017703533172607, 1.8143634796142578, 1.0867483615875244, 1.4151555299758911, 1.7291069030761719, 1.4460291862487793, 1.3833858966827393, 1.362671971321106, 1.5231866836547852, 1.4596831798553467, 1.4621201753616333, 1.4943019151687622, 1.2228354215621948, 0.6899391412734985, 1.2873064279556274, 1.5808128118515015]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3807339668273926, 2.4931910037994385, 2.3456852436065674, 2.0938217639923096, 2.596330165863037, 2.2477781772613525, 2.5083141326904297, 2.149082660675049, 2.152451276779175, 2.1990036964416504, 2.8641772270202637, 2.1675028800964355, 2.3334288597106934, 2.514263153076172, 2.2932913303375244, 2.3807339668273926]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7296086549758911, 1.7296086549758911, 1.5885536670684814, 1.9179149866104126, 1.3774727582931519, 1.6853139400482178, 1.7307195663452148, 2.072175979614258, 1.749659538269043, 1.6990036964416504, 1.7117975950241089, 1.6134604215621948, 1.7465596199035645, 1.5792717933654785, 2.0344035625457764, 1.9884963035583496]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.7691729068756104, 1.8467826843261719, 1.7928612232208252, 1.7643169164657593, 1.6934982538223267, 1.8571799993515015, 1.7361489534378052, 1.7433006763458252, 1.8238334655761719, 1.9320595264434814, 2.371331214904785, 1.6842119693756104, 1.7232296466827393, 1.8497885465621948, 1.8239141702651978, 1.779123306274414]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.674168586730957, 1.6992008686065674, 1.6674939393997192, 1.7619248628616333, 1.6000999212265015, 1.6346759796142578, 1.841044306755066, 1.7573555707931519, 1.6503727436065674, 1.7387560606002808, 1.3762900829315186, 1.6535621881484985, 1.7544080018997192, 2.007185459136963, 1.6268634796142578, 1.6618585586547852]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0211215019226074, 2.094054698944092, 2.3957765102386475, 1.983765721321106, 2.1104724407196045, 2.0531532764434814, 2.540433168411255, 2.156247854232788, 2.0182948112487793, 1.9935672283172607, 1.9910765886306763, 1.8448251485824585, 1.9168219566345215, 1.8346115350723267, 2.3140499591827393, 2.0160460472106934]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.887041091918945, 5.360951900482178, 4.868405818939209, 4.785837173461914, 4.756307125091553, 4.748280048370361, 4.879587173461914, 5.086296081542969, 5.079701900482178, 4.842459678649902, 4.832568645477295, 4.729501247406006, 4.901232719421387, 4.927895545959473, 5.092030048370361, 4.876433372497559]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.662199020385742, 3.5648653507232666, 3.4445958137512207, 3.3919150829315186, 3.5420727729797363, 3.592961549758911, 3.3249714374542236, 3.6690080165863037, 3.707998752593994, 3.5486669540405273, 3.454630136489868, 3.1500144004821777, 3.404243230819702, 3.5710291862487793, 3.6204845905303955, 3.7053468227386475]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.5569093227386475, 2.684417963027954, 2.6805474758148193, 2.553612470626831, 2.5408544540405273, 2.647899866104126, 2.589951276779175, 2.340237855911255, 2.5472333431243896, 2.4343104362487793, 3.100630760192871, 1.9132202863693237, 2.594538450241089, 2.513474702835083, 3.043649673461914, 2.537557363510132]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.08623892813920975, 0.09760382771492004, 0.10158489644527435, -0.279075562953949, -0.24450795352458954, -0.11966530233621597, 0.12030274420976639, -0.09312228858470917, 0.07295814901590347, 0.08637573570013046, 0.09827864170074463, 0.06041437387466431, 0.08698755502700806, 0.10597354918718338, -1.1028492450714111, 0.11568065732717514]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09067892283201218, 0.047028183937072754, 0.036336030811071396, 0.04535011947154999, 0.0728922188282013, 0.025348208844661713, 0.04970379173755646, 0.08074866980314255, 0.01990327052772045, 0.07155013829469681, -0.22217482328414917, 0.03657608851790428, 0.03697057440876961, -0.025478117167949677, 0.027649065479636192, 0.0063164387829601765]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07958340644836426, 0.06435394287109375, 0.08587135374546051, 0.08186662197113037, 0.09993498772382736, 0.10340797156095505, 0.08360514789819717, -0.09608422964811325, 0.08161688596010208, 0.08686280995607376, 0.024734217673540115, 0.08121189475059509, -0.18840292096138, 0.023452339693903923, -0.034859754145145416, 0.1143023669719696]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13127045333385468, 0.155551940202713, 0.1271265745162964, 0.14500507712364197, 0.12922681868076324, 0.1265745908021927, 0.013547005131840706, 0.20556920766830444, 0.18821589648723602, -0.5247119665145874, 0.016616471111774445, 0.09036710113286972, 0.23857970535755157, -0.22869929671287537, -0.032680828124284744, -0.07769859582185745]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06985543668270111, 0.10907885432243347, 0.1047290489077568, 0.15060169994831085, 0.04408754035830498, -0.04036838561296463, 0.04771279916167259, 0.03669140487909317, -0.027126487344503403, 0.039480507373809814, -0.19074298441410065, 0.14888997375965118, -0.015599521808326244, -0.22982710599899292, 0.12255572527647018, -0.04622349143028259]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06828322261571884, 0.1143159419298172, 0.05026742070913315, 0.10347306728363037, -0.06571204215288162, -0.06958036124706268, 0.12145604193210602, 0.04925915226340294, 0.16550850868225098, -0.07997516542673111, 0.05247287452220917, 0.06592566519975662, -0.17977988719940186, 0.12227840721607208, 0.15121740102767944, 0.057706572115421295]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.09847035259008408, 0.010919413529336452, 0.11705703288316727, 0.359148770570755, 0.20094691216945648, 0.12528838217258453, -0.15453961491584778, -0.041574180126190186, 0.38271841406822205, 0.15162280201911926, -0.5265184044837952, 0.3000321686267853, 0.22478632628917694, -0.22707206010818481, 0.27288663387298584, 0.20015457272529602]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.48399198055267334, 0.2652358412742615, 0.25612032413482666, 0.1547582745552063, -0.8503507375717163, 0.34796008467674255, 0.20273597538471222, -0.3585798740386963, 0.512982964515686, 0.19056785106658936, -0.09909449517726898, 0.2690874934196472, 0.24266010522842407, 0.19952797889709473, -0.5189354419708252, -0.12731429934501648]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.32539576292037964, 0.3294932544231415, -0.24568861722946167, 0.3095826208591461, 0.4250859320163727, 0.29140886664390564, 0.23967190086841583, -0.30317142605781555, 0.14076219499111176, 0.7132195830345154, 0.06745168566703796, 0.22253082692623138, 0.42792925238609314, -0.4345826208591461, -0.4449585974216461, 0.3442276418209076]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3439943194389343, 0.1821143478155136, 0.46525588631629944, 1.0006786584854126, 0.3912045657634735, 0.15097731351852417, 0.5382567048072815, 0.50334632396698, 0.5742562413215637, 0.16776379942893982, 0.2945878505706787, 0.6718347072601318, 0.7937120795249939, 0.13070370256900787, 0.7205541729927063, 0.59866863489151]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0653159618377686, 0.8621479272842407, 0.6780793070793152, 0.7325204014778137, 0.8424822688102722, 0.5700543522834778, 0.643722653388977, 0.7045226693153381, 0.2261638045310974, 0.5366255640983582, 0.721043586730957, 0.9031859040260315, 0.4327639043331146, 0.2602328956127167, 0.7806292772293091, 0.4479140639305115]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.8916284441947937, 1.0948295593261719, 0.8575159311294556, 1.0449442863464355, 1.2323143482208252, 0.30321258306503296, 0.80610191822052, 0.8836994767189026, 0.8624390959739685, 1.3344950675964355, 0.8215488791465759, 0.8270286321640015, 0.30508169531822205, 0.4674201011657715, -0.033097852021455765, 0.7483013868331909]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.2711862027645111, 0.8164107203483582, 1.093705177307129, 1.1688647270202637, 0.30893197655677795, 0.6037351489067078, 0.996083676815033, 1.1800537109375, 0.5543403029441833, 0.34772050380706787, 0.8013546466827393, 0.849914014339447, 0.7791624665260315, 0.8545907139778137, 0.7266789674758911, 0.9364249110221863]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7014137506484985, 0.9285765290260315, 1.0067553520202637, 0.5398656129837036, 0.9479286074638367, -0.16502785682678223, 1.1004873514175415, 1.909403681755066, 0.9251944422721863, 0.8810385465621948, 1.1669048070907593, 1.1233335733413696, 0.07920543104410172, 0.7344420552253723, 0.9941109418869019, 0.6917489171028137]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5984289646148682, 0.771560549736023, 0.9178791642189026, 0.742527961730957, 0.9553110599517822, 0.6377283334732056, 0.20331314206123352, 1.0715539455413818, 0.6843135952949524, 0.73967444896698, -0.4086936414241791, 0.6858699917793274, 0.8632230162620544, 1.0204272270202637, 1.1714986562728882, 0.4988453686237335]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.3821013569831848, 1.1362797021865845, 0.9428755640983582, 0.8995305299758911, 0.7073425650596619, 0.8412772417068481, 1.1490103006362915, 0.9388976693153381, 0.8991184234619141, 0.9774584174156189, 0.6136261820793152, -0.38438260555267334, 0.9712048172950745, 0.8967710733413696, 0.8615968823432922, 1.2660841941833496]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.644419252872467, 0.04092085361480713, 0.75373375415802, 0.7505207657814026, 0.7515074014663696, 0.42934080958366394, 0.17453955113887787, 0.7709938287734985, -1.0500309467315674, 2.2266879081726074, -0.8740010261535645, 0.5236915946006775, 0.8298093676567078, 0.7946231365203857, 0.7468284368515015, 0.1751367449760437]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.11521981656551361, 0.6478809118270874, 1.3706995248794556, 1.2665477991104126, 1.3630304336547852, 1.371846318244934, 1.233443260192871, 1.4034905433654785, 1.7480647563934326, 1.3446190357208252, 2.103461980819702, 1.3949075937271118, 1.373530626296997, 1.2477960586547852, 0.47430139780044556, 1.3488926887512207]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4162126779556274, 1.2762328386306763, 1.4406267404556274, 1.7263116836547852, 0.9135047793388367, 1.208715558052063, 1.8107081651687622, 1.6830562353134155, 2.3271214962005615, 1.3637830018997192, 1.2252365350723267, 0.8237164616584778, 1.1769235134124756, 1.0555644035339355, 1.4115897417068481, 1.457783818244934]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.456027865409851, 1.2899584770202637, 1.3803218603134155, 1.306300163269043, 1.55483078956604, 0.9321832060813904, 1.234330177307129, 1.2551919221878052, 1.823287010192871, 1.4026663303375244, 1.5419294834136963, 1.4712406396865845, 1.8840174674987793, 1.49623703956604, 1.3955705165863037, 1.4869911670684814]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Running loglikelihood requests:  67%|████████████████████████████████▊                | 337/504 [18:38<08:54,  3.20s/it]Layer: gate_20 - Captured router_logits: [1.0701030492782593, 1.0058056116104126, 0.9417645931243896, 0.8536612391471863, -0.13252076506614685, 1.2508422136306763, 1.1143383979797363, 1.567575454711914, 0.7351902723312378, 1.225953221321106, 1.2107404470443726, 1.0496076345443726, 1.0598838329315186, 1.3487671613693237, 0.9418519735336304, 0.5660831332206726]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0580203533172607, 2.2090022563934326, 2.1328842639923096, 1.9724770784378052, 1.9637328386306763, 2.092531442642212, 1.88274085521698, 2.1272218227386475, 1.91420578956604, 2.137148857116699, 2.316012144088745, 2.611525297164917, 1.7263563871383667, 2.277451276779175, 2.115431547164917, 2.043219566345215]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3703770637512207, 1.3502724170684814, 1.8113532066345215, 1.0053128004074097, 1.446315884590149, 1.5482009649276733, 1.3165675401687622, 1.3597333431243896, 1.4368549585342407, 1.6129586696624756, 1.4960578680038452, 2.0391342639923096, 1.4596115350723267, 1.5080991983413696, 1.2552143335342407, 1.6653884649276733]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.38295578956604, 1.7856221199035645, 1.1470845937728882, 1.3435708284378052, 1.5340094566345215, 1.4362099170684814, 1.3814865350723267, 1.4175745248794556, 1.4867044687271118, 1.4398294687271118, 1.4896788597106934, 1.4679436683654785, 1.2738317251205444, 0.7537878155708313, 1.3483012914657593, 1.5072749853134155]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.264119863510132, 2.322606086730957, 2.1680045127868652, 2.042431116104126, 2.494767665863037, 2.1748852729797363, 2.373351573944092, 2.0832855701446533, 2.0549025535583496, 2.094395160675049, 2.6115968227386475, 2.0986955165863037, 2.1745986938476562, 2.3698394298553467, 2.272505760192871, 2.233944892883301]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6638474464416504, 1.6652092933654785, 1.5389549732208252, 1.7791355848312378, 1.336546778678894, 1.5861166715621948, 1.6596903800964355, 1.997563123703003, 1.6735235452651978, 1.6075831651687622, 1.6273114681243896, 1.5903455018997192, 1.658686876296997, 1.51555335521698, 1.8562572002410889, 1.9705418348312378]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8021072149276733, 1.788838505744934, 1.704253911972046, 1.77336585521698, 1.702632188796997, 1.8008991479873657, 1.7079943418502808, 1.7340704202651978, 1.8075320720672607, 1.9117599725723267, 2.274324417114258, 1.6612492799758911, 1.7003655433654785, 1.863621711730957, 1.8244158029556274, 1.7653260231018066]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5649727582931519, 1.6594966650009155, 1.6327178478240967, 1.7042471170425415, 1.5665227174758911, 1.5717034339904785, 1.7374547719955444, 1.6757392883300781, 1.5663611888885498, 1.6904386281967163, 1.3530676364898682, 1.5875680446624756, 1.617671251296997, 1.8824182748794556, 1.585005760192871, 1.6067230701446533]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9791786670684814, 2.0273079872131348, 2.355316400527954, 1.944380760192871, 2.082129716873169, 2.000967502593994, 2.4789278507232666, 2.0730719566345215, 1.9447391033172607, 1.965847134590149, 1.95695960521698, 1.8185564279556274, 1.932464838027954, 1.829056739807129, 2.253368616104126, 2.045262336730957]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.980361461639404, 5.32088565826416, 4.941226959228516, 4.8469038009643555, 4.867151737213135, 4.837155818939209, 4.9484663009643555, 5.197749614715576, 5.177035331726074, 4.96903657913208, 4.9633026123046875, 4.809203147888184, 5.004909515380859, 4.951548099517822, 5.105648040771484, 4.942015647888184]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.717818260192871, 3.5859375, 3.5556910037994385, 3.4211180210113525, 3.6174025535583496, 3.6573967933654785, 3.404494047164917, 3.6014907360076904, 3.7788846492767334, 3.6800458431243896, 3.5260536670684814, 3.240583896636963, 3.5424671173095703, 3.633744239807129, 3.705705165863037, 3.727924346923828]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.8052608966827393, 2.891592502593994, 2.754730463027954, 2.6724483966827393, 2.6545655727386475, 2.9135427474975586, 2.786482334136963, 2.616891860961914, 2.7378153800964355, 2.732475519180298, 3.1762471199035645, 2.265916109085083, 2.806981086730957, 2.7411482334136963, 3.178039073944092, 2.7813217639923096]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.06443186104297638, 0.07583583146333694, 0.08438583463430405, -0.2548411190509796, -0.22937746345996857, -0.053054243326187134, 0.10515382885932922, -0.05115537345409393, 0.07021543383598328, 0.06086384877562523, 0.07356232404708862, 0.06346667557954788, 0.08327130973339081, 0.08692935854196548, -1.0315574407577515, 0.09617388993501663]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.06732036173343658, 0.0323810912668705, 0.029107235372066498, 0.027729175984859467, 0.06359270215034485, 0.0048172385431826115, 0.03913430869579315, 0.08500827103853226, 0.009649700485169888, 0.06650853902101517, -0.22353164851665497, 0.03512560948729515, 0.02609238773584366, -0.04403725266456604, 0.037364501506090164, 0.01176530309021473]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07430168241262436, 0.054070331156253815, 0.09126656502485275, 0.08996016532182693, 0.10670994222164154, 0.0914018452167511, 0.05568154528737068, -0.09390640258789062, 0.08158111572265625, 0.1288079172372818, 0.0362650565803051, 0.08862919360399246, -0.2264200896024704, 0.013835341669619083, -0.054040130227804184, 0.10584725439548492]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1362016499042511, 0.13923899829387665, 0.1200820803642273, 0.14574404060840607, 0.13135062158107758, 0.12203922867774963, 0.001967677380889654, 0.19394400715827942, 0.18636886775493622, -0.5286895036697388, 0.0595584437251091, 0.056035105139017105, 0.28501012921333313, -0.28244414925575256, 0.0445404052734375, -0.045255377888679504]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.059054408222436905, 0.11139290779829025, 0.10994183272123337, 0.22689564526081085, 0.060309093445539474, -0.06791121512651443, 0.009228670969605446, 0.027713987976312637, -0.08079662919044495, 0.011413008905947208, -0.21269084513187408, 0.13474541902542114, 0.014628940261900425, -0.20698493719100952, 0.13316571712493896, -0.05104799568653107]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07310330122709274, 0.11801571398973465, 0.049944277852773666, 0.15308859944343567, -0.11734065413475037, -0.09064427018165588, 0.17226918041706085, -0.016017772257328033, 0.2024529129266739, -0.09568193554878235, 0.0496470145881176, 0.07450151443481445, -0.21644790470600128, 0.13643674552440643, 0.13112477958202362, 0.0707324892282486]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11818073689937592, -0.06621070951223373, 0.15042544901371002, 0.3692355751991272, 0.18376639485359192, 0.17745378613471985, -0.16183188557624817, -0.0932345911860466, 0.3704158663749695, 0.159454345703125, -0.6195836663246155, 0.2973262667655945, 0.2000189870595932, -0.31304338574409485, 0.2817540466785431, 0.21445634961128235]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.47090771794319153, 0.18156856298446655, 0.16859549283981323, 0.13613355159759521, -1.0299569368362427, 0.3949466347694397, 0.20724564790725708, -0.45904427766799927, 0.5649611949920654, 0.19346562027931213, -0.3335706889629364, 0.2800779342651367, 0.18044507503509521, 0.23212461173534393, -0.7691458463668823, -0.2190958708524704]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.26378390192985535, 0.26070544123649597, -0.3564961850643158, 0.3058255612850189, 0.5599365234375, 0.23693649470806122, 0.2652679681777954, -0.29880326986312866, 0.1795196533203125, 0.8338689208030701, 0.0323130302131176, 0.29111847281455994, 0.4024590253829956, -0.6295182704925537, -0.5809891223907471, 0.36363375186920166]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.27843645215034485, 0.04876906797289848, 0.47993525862693787, 1.0216776132583618, 0.241302490234375, 0.12013188004493713, 0.43039053678512573, 0.4589996337890625, 0.5585889220237732, -0.0389251708984375, 0.0974578857421875, 0.6093207597732544, 0.76116943359375, 0.06839215010404587, 0.7243788242340088, 0.6112828850746155]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1008334159851074, 0.7658736705780029, 0.49709293246269226, 0.6825945973396301, 0.7445791959762573, 0.47720733284950256, 0.5645412802696228, 0.6721733808517456, 0.023999249562621117, 0.3808198869228363, 0.6931875944137573, 0.8843270540237427, 0.23954015970230103, 0.16286128759384155, 0.7737607359886169, 0.2225714772939682]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.8494647145271301, 1.0222371816635132, 0.8761031627655029, 0.9568911194801331, 1.2367169857025146, 0.17061813175678253, 0.8195540904998779, 0.8298611044883728, 0.9842845797538757, 1.3514630794525146, 0.7909703850746155, 0.7261782288551331, 0.10223106294870377, 0.25016361474990845, -0.31793835759162903, 0.5687323808670044]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.0376196950674057, 0.7214909195899963, 1.216154932975769, 1.0524450540542603, 0.07201413810253143, 0.5702040195465088, 1.003846287727356, 1.0648170709609985, 0.35903817415237427, 0.26412031054496765, 0.7424206137657166, 0.8679289817810059, 0.7478207945823669, 0.8214699029922485, 0.5089608430862427, 0.8414532542228699]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.5675364136695862, 0.9003589749336243, 0.8324448466300964, 0.16712895035743713, 0.9591832756996155, -0.3063371479511261, 1.0840204954147339, 2.181640625, 0.9290330410003662, 0.8518699407577515, 1.1143120527267456, 1.1591435670852661, -0.23642823100090027, 0.6201375126838684, 1.1259493827819824, 0.48006749153137207]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5577686429023743, 0.6508698463439941, 0.8613461852073669, 0.7799298167228699, 0.9098668694496155, 0.6192740201950073, 0.040925201028585434, 1.2067458629608154, 0.8929884433746338, 0.7481237053871155, -0.46280020475387573, 0.5832931995391846, 0.8576207756996155, 1.0111852884292603, 1.1257596015930176, 0.45102155208587646]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.33650490641593933, 1.1187065839767456, 0.8688693642616272, 1.0553205013275146, 0.5562382340431213, 0.8117404580116272, 1.1137973070144653, 0.9732349514961243, 0.8397713899612427, 0.9431785345077515, 0.5933837890625, -0.5215035676956177, 0.9134532809257507, 0.8583260774612427, 0.7868019342422485, 1.1669198274612427]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.697021484375, 0.13748480379581451, 0.8377289772033691, 0.8973072171211243, 0.7667869329452515, 0.4237297773361206, 0.14138652384281158, 0.914577066898346, -1.2192631959915161, 2.635354518890381, -0.9849876165390015, 0.4356652796268463, 0.8138924837112427, 0.8388717174530029, 0.8663578629493713, 0.29418888688087463]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.06613384932279587, 0.5008293390274048, 1.3189923763275146, 1.1120718717575073, 1.4259439706802368, 1.295283555984497, 1.2342665195465088, 1.2363642454147339, 1.7669090032577515, 1.2926794290542603, 1.9839318990707397, 1.3288122415542603, 1.3489384651184082, 1.388581395149231, 0.45031964778900146, 1.191071629524231]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3237847089767456, 1.113407850265503, 1.3203576803207397, 1.6733940839767456, 0.6800593733787537, 1.0296766757965088, 1.8206018209457397, 1.5363860130310059, 2.1388165950775146, 1.2819372415542603, 0.9623119235038757, 0.6123510003089905, 1.03806734085083, 0.8989178538322449, 1.2631293535232544, 1.271846055984497]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.1889467239379883, 1.1239510774612427, 1.1896882057189941, 1.1302083730697632, 1.326533555984497, 0.6919872164726257, 0.9602321982383728, 1.0322265625, 1.6915147304534912, 1.3125, 1.3845486640930176, 1.2685004472732544, 1.683503270149231, 1.2528935670852661, 1.2111544609069824, 1.1800130605697632]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.9720301032066345, 0.9600694179534912, 0.8906973600387573, 0.8021669983863831, -0.1553802490234375, 1.1769295930862427, 1.022858738899231, 1.4018102884292603, 0.7760365605354309, 1.1130461692810059, 1.1794975996017456, 1.1111201047897339, 0.9099030494689941, 1.1796332597732544, 0.7426814436912537, 0.34949296712875366]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.925383448600769, 2.1150174140930176, 1.9456018209457397, 1.8025535345077515, 1.7726056575775146, 2.0096933841705322, 1.7360749244689941, 1.9967447519302368, 1.8258463144302368, 1.8635705709457397, 2.1801939010620117, 2.5293691158294678, 1.5312771797180176, 2.110243082046509, 1.9675202369689941, 1.9098669290542603]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.2353154420852661, 1.2774884700775146, 1.7345919609069824, 0.8832962512969971, 1.2749565839767456, 1.4081670045852661, 1.1576606035232544, 1.2684099674224854, 1.3807146549224854, 1.4016927480697632, 1.4951533079147339, 1.9856771230697632, 1.3370225429534912, 1.3899016380310059, 1.0200376510620117, 1.537109375]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.408818006515503, 1.7002314329147339, 0.7904323935508728, 1.2528935670852661, 1.3511284589767456, 1.2122033834457397, 1.1652560234069824, 1.1962890625, 1.3068214654922485, 1.2950665950775146, 1.2841435670852661, 1.3663556575775146, 1.029694676399231, 0.3723890483379364, 1.0876374244689941, 1.2235604524612427]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  68%|█████████████████████████████████▏               | 341/504 [18:50<08:31,  3.14s/it]Layer: gate_24 - Captured router_logits: [2.3939526081085205, 2.325448513031006, 2.2565104961395264, 2.074942111968994, 2.5576534271240234, 2.116753578186035, 2.44140625, 2.116464138031006, 2.112847328186035, 2.124276638031006, 2.7821903228759766, 2.046079397201538, 2.2787904739379883, 2.3779659271240234, 2.2677950859069824, 2.2581019401550293]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.5900607109069824, 1.6059027910232544, 1.4844834804534912, 1.6919126510620117, 1.1959997415542603, 1.546513319015503, 1.5771845579147339, 1.9984809160232544, 1.5853949785232544, 1.4979745149612427, 1.5856119394302368, 1.3923249244689941, 1.5045572519302368, 1.4631075859069824, 1.8317780494689941, 1.8979310989379883]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.740143895149231, 1.7037444114685059, 1.6110206842422485, 1.6354166269302368, 1.6076931953430176, 1.7170568704605103, 1.5971871614456177, 1.6436994075775146, 1.6711199283599854, 1.775544285774231, 2.190312147140503, 1.5333658456802368, 1.5432400703430176, 1.9017107486724854, 1.8143401145935059, 1.691269040107727]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5303548574447632, 1.6028023958206177, 1.495845079421997, 1.6392663717269897, 1.5062482357025146, 1.5162285566329956, 1.684666395187378, 1.5997495651245117, 1.5130419731140137, 1.6118615865707397, 1.2414278984069824, 1.6005452871322632, 1.6013364791870117, 1.8371220827102661, 1.5077853202819824, 1.5207948684692383]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9272822141647339, 1.989655613899231, 2.3355305194854736, 1.8581814765930176, 2.0547146797180176, 1.9092520475387573, 2.4350404739379883, 2.0047833919525146, 1.9114583730697632, 1.8666449785232544, 1.8300057649612427, 1.6982421875, 1.989836573600769, 1.689055323600769, 2.1736834049224854, 1.8899919986724854]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.875650882720947, 5.245876789093018, 4.896194934844971, 4.764919757843018, 4.773980140686035, 4.666449546813965, 4.882884979248047, 5.113787651062012, 5.141854763031006, 4.824544429779053, 4.835394859313965, 4.756365776062012, 4.913375377655029, 4.858000755310059, 5.043547630310059, 4.860857963562012]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.6425418853759766, 3.4713542461395264, 3.3772425651550293, 3.2581357955932617, 3.5770761966705322, 3.5635128021240234, 3.211733102798462, 3.563657522201538, 3.559931993484497, 3.4960215091705322, 3.3406033515930176, 3.0017857551574707, 3.3159542083740234, 3.5606915950775146, 3.5071613788604736, 3.6009294986724854]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.3763744831085205, 2.522894859313965, 2.4819517135620117, 2.515263319015503, 2.3985097408294678, 2.4238462448120117, 2.6072049140930176, 2.206796169281006, 2.3936994075775146, 2.2466542720794678, 3.0857205390930176, 1.8891240358352661, 2.4159793853759766, 2.2783384323120117, 2.8200955390930176, 2.3782551288604736]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06280674785375595, 0.07249372452497482, 0.08020251244306564, -0.27742865681648254, -0.26557251811027527, -0.029714779928326607, 0.09193620085716248, -0.07246491312980652, 0.07961497455835342, 0.059087734669446945, 0.06936385482549667, 0.05614425241947174, 0.06900437921285629, 0.09325002878904343, -1.0650646686553955, 0.09251675009727478]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07772441953420639, 0.04269993677735329, 0.020056644454598427, 0.06164887547492981, 0.07227175682783127, 0.03552199900150299, 0.052808813750743866, 0.07848893105983734, 0.002228745725005865, 0.06866554915904999, -0.1906748265028, 0.06710615754127502, 0.024536453187465668, -0.01550251990556717, 0.05136668309569359, 0.027898414060473442]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07565393298864365, 0.08042679727077484, 0.07886077463626862, 0.07720661908388138, 0.10624966025352478, 0.07611283659934998, 0.05226662755012512, -0.08202383667230606, 0.09484656155109406, 0.08676937222480774, 0.030619576573371887, 0.06927818059921265, -0.18429023027420044, 0.026331963017582893, -0.05355178937315941, 0.11553327739238739]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.10815543681383133, 0.15494252741336823, 0.14673762023448944, 0.12295789271593094, 0.1314164698123932, 0.13476818799972534, 0.033950164914131165, 0.16428495943546295, 0.1810302734375, -0.5452370047569275, 0.10456185042858124, 0.07500679045915604, 0.22547613084316254, -0.3037058115005493, 0.03842783346772194, -0.02164323814213276]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07802718877792358, 0.086397685110569, 0.07066930085420609, 0.19596506655216217, 0.132080078125, -0.08014900237321854, 0.01887398213148117, 0.019114164635539055, -0.10435713827610016, 0.020668672397732735, -0.20016735792160034, 0.12146651744842529, -0.002735743997618556, -0.17279887199401855, 0.12746764719486237, -0.09196372330188751]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.03413134440779686, 0.14185084402561188, 0.08036676049232483, 0.18525025248527527, -0.14929084479808807, -0.05318443849682808, 0.15586011111736298, -0.00301696197129786, 0.15639539062976837, -0.07376355677843094, -0.053961921483278275, 0.1255302131175995, -0.2119220495223999, 0.19781379401683807, 0.12083501368761063, 0.10088220238685608]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.14614596962928772, 0.03994123637676239, 0.1835733950138092, 0.3435104191303253, 0.17180748283863068, 0.15311495959758759, -0.16565939784049988, -0.08391663432121277, 0.27685776352882385, 0.21741506457328796, -0.5312619805335999, 0.3239227831363678, 0.24979008734226227, -0.3279384672641754, 0.24242764711380005, 0.1789238452911377]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3935258090496063, 0.21860647201538086, 0.22336670756340027, 0.14884506165981293, -0.8282071352005005, 0.32015493512153625, 0.2245677411556244, -0.3586300313472748, 0.5530843138694763, 0.19564563035964966, -0.21859726309776306, 0.2575415372848511, 0.1218358725309372, 0.3144029378890991, -0.6519558429718018, -0.20605988800525665]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2578860819339752, 0.31076592206954956, -0.31423407793045044, 0.326242595911026, 0.5213634371757507, 0.433034747838974, 0.2487390786409378, -0.27991265058517456, 0.18234309554100037, 0.7781702876091003, 0.17646817862987518, 0.18349164724349976, 0.61016446352005, -0.4819276034832001, -0.5768398642539978, 0.3810066878795624]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.42426279187202454, 0.2148836851119995, 0.4337739944458008, 1.0659362077713013, 0.4481070041656494, 0.16341108083724976, 0.6129549741744995, 0.6390780210494995, 0.8124030232429504, 0.15082870423793793, 0.19098955392837524, 0.6690639853477478, 0.6563971638679504, 0.03126369044184685, 0.7552478909492493, 0.710983157157898]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.024505853652954, 0.876629114151001, 0.6019138693809509, 0.6898182034492493, 0.9400668740272522, 0.6201524138450623, 0.7500388026237488, 0.7392030358314514, 0.09246540814638138, 0.27409347891807556, 0.7316643595695496, 0.8413816094398499, 0.3154701888561249, 0.1971546709537506, 1.1147209405899048, 0.4502580463886261]
Layer: gate_10 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_11 - Captured router_logits: [0.9543297290802002, 1.2153092622756958, 0.9603533744812012, 1.289379596710205, 1.3417741060256958, 0.42184391617774963, 0.9832432866096497, 0.8471938967704773, 1.0178337097167969, 1.6027854681015015, 0.8642304539680481, 1.095210313796997, 0.5034139752388, 0.573646068572998, 0.1319243460893631, 0.793377161026001]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.14231687784194946, 0.8115599155426025, 1.3614376783370972, 1.091869831085205, 0.4361238479614258, 0.6597774624824524, 1.1550657749176025, 1.2940821647644043, 0.5997982025146484, 0.36742478609085083, 1.0696736574172974, 0.8760404586791992, 0.8979195356369019, 0.7915449738502502, 0.7705442905426025, 1.0904278755187988]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7312125563621521, 0.9083948135375977, 1.2027919292449951, 0.44974297285079956, 1.0740909576416016, -0.20742569863796234, 1.0392632484436035, 1.9120911359786987, 1.0158257484436035, 0.9417165517807007, 1.4191917181015015, 1.2100430727005005, 0.21624727547168732, 0.8371695876121521, 1.236163854598999, 0.7005478143692017]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8907117247581482, 0.8921582698822021, 1.0964332818984985, 0.9265844225883484, 1.2194801568984985, 1.4086047410964966, 0.4941166639328003, 1.7693088054656982, 0.7344434261322021, 0.9749653339385986, -0.28714701533317566, 1.0079699754714966, 1.1562180519104004, 1.2997407913208008, 1.198077917098999, 0.6046443581581116]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.5028327107429504, 1.1637157201766968, 1.1556111574172974, 1.0129326581954956, 0.8889342546463013, 1.024897813796997, 1.6182005405426025, 1.085408091545105, 1.0231271982192993, 1.0949182510375977, 0.79131680727005, -0.18505145609378815, 1.3240180015563965, 0.9603899121284485, 1.0023912191390991, 1.5297417640686035]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.8086485266685486, 0.18646839261054993, 0.9924624562263489, 0.9416709542274475, 0.7529296875, 0.5440627932548523, 0.5132320523262024, 0.8639246821403503, -0.7738083004951477, 2.604665517807007, -0.7694165706634521, 0.7559825778007507, 0.9483882188796997, 1.012535572052002, 0.9241886138916016, 0.32104262709617615]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.06801655143499374, 0.865062415599823, 1.5649094581604004, 1.4086594581604004, 1.569217324256897, 1.586959719657898, 1.4427752494812012, 1.5510458946228027, 2.0180320739746094, 1.3436535596847534, 2.489413022994995, 1.5984411239624023, 1.4760174751281738, 1.289920449256897, 0.5464916825294495, 1.3913094997406006]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5474591255187988, 1.3052716255187988, 1.5203161239624023, 1.9287748336791992, 0.9921523928642273, 1.2704803943634033, 1.947356939315796, 1.7759929895401, 2.610360622406006, 1.4709038734436035, 1.3130019903182983, 0.8845055103302002, 1.2264392375946045, 0.937911868095398, 1.5176876783370972, 1.6158732175827026]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.5355943441390991, 1.3689032793045044, 1.6506462097167969, 1.3819180727005005, 1.6586229801177979, 1.1047455072402954, 1.3800288438796997, 1.4304815530776978, 2.1141390800476074, 1.5241676568984985, 1.7723788022994995, 1.5655848979949951, 1.9563239812850952, 1.6677497625350952, 1.569983959197998, 1.5544319152832031]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.273754596710205, 1.1059342622756958, 1.0495765209197998, 1.0191935300827026, 0.009042900055646896, 1.5394366979599, 1.292202115058899, 1.7581145763397217, 0.7679409384727478, 1.4110324382781982, 1.290157675743103, 1.1916755437850952, 1.2115398645401, 1.3980358839035034, 1.0110981464385986, 0.5705823302268982]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.127190351486206, 2.2863609790802, 2.2282419204711914, 1.9741895198822021, 1.9926255941390991, 2.118647813796997, 1.9497298002243042, 2.210280418395996, 2.025956392288208, 2.2486493587493896, 2.3718605041503906, 2.6496787071228027, 1.808922290802002, 2.4085135459899902, 2.192063331604004, 2.1355140209198]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4683849811553955, 1.5097838640213013, 2.133396625518799, 1.1293123960494995, 1.5472766160964966, 1.6800525188446045, 1.4200222492218018, 1.4522852897644043, 1.5270516872406006, 1.710444688796997, 1.6616530418395996, 2.205242395401001, 1.5563668012619019, 1.6558119058609009, 1.353100299835205, 1.7167421579360962]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.3972327709198, 1.8568559885025024, 1.1678614616394043, 1.4462982416152954, 1.7448159456253052, 1.4515917301177979, 1.4581995010375977, 1.3711302280426025, 1.5505256652832031, 1.4842289686203003, 1.5052205324172974, 1.5120837688446045, 1.2524276971817017, 0.7769473195075989, 1.2712197303771973, 1.5236564874649048]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3212616443634033, 2.4453125, 2.327467918395996, 2.096524477005005, 2.5836009979248047, 2.289719581604004, 2.400408983230591, 2.129307746887207, 2.056658983230591, 2.1780080795288086, 2.867260456085205, 2.1666910648345947, 2.314544439315796, 2.477292537689209, 2.2624855041503906, 2.316223621368408]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6919903755187988, 1.709477186203003, 1.5472400188446045, 1.879234790802002, 1.3617981672286987, 1.633506178855896, 1.6975029706954956, 2.032710313796997, 1.7421510219573975, 1.616530418395996, 1.6329220533370972, 1.5863391160964966, 1.7191150188446045, 1.4852876663208008, 2.0675745010375977, 1.972327709197998]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_26 - Captured router_logits: [1.8198379278182983, 1.915618658065796, 1.776686668395996, 1.8191807270050049, 1.7803373336791992, 1.9302784204483032, 1.8086826801300049, 1.7633615732192993, 1.8535133600234985, 1.9779497385025024, 2.4116714000701904, 1.7254124879837036, 1.7816880941390991, 1.9219297170639038, 1.8829585313796997, 1.804161548614502]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.643235206604004, 1.712890625, 1.6664172410964966, 1.7150262594223022, 1.558666706085205, 1.6201446056365967, 1.8342581987380981, 1.744907259941101, 1.6074765920639038, 1.7250382900238037, 1.3729373216629028, 1.638671875, 1.7341924905776978, 1.980541706085205, 1.6144859790802002, 1.6494232416152954]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  68%|█████████████████████████████████▌               | 345/504 [19:03<08:23,  3.17s/it]Layer: gate_28 - Captured router_logits: [2.0085060596466064, 2.090756416320801, 2.3563449382781982, 1.962981939315796, 2.1086082458496094, 2.0510733127593994, 2.530592918395996, 2.1422312259674072, 1.9824765920639038, 1.979775071144104, 1.975905418395996, 1.8352073431015015, 1.8867005109786987, 1.8647780418395996, 2.288806915283203, 2.042384624481201]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.774532794952393, 5.355724334716797, 4.763580799102783, 4.5373101234436035, 4.626898288726807, 4.689398288726807, 4.747225284576416, 4.934871673583984, 4.962032794952393, 4.723423004150391, 4.683557033538818, 4.712616920471191, 4.78694486618042, 4.89033317565918, 5.024386882781982, 4.71320104598999]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.4109959602355957, 3.3482038974761963, 3.201080560684204, 3.097473621368408, 3.317318916320801, 3.3703269958496094, 3.01171875, 3.4357476234436035, 3.431731939315796, 3.25766658782959, 3.1358790397644043, 2.8000471591949463, 3.081209897994995, 3.288478374481201, 3.366384267807007, 3.4623613357543945]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.4541471004486084, 2.6176254749298096, 2.673335313796997, 2.4554615020751953, 2.5109519958496094, 2.592289686203003, 2.4733498096466064, 2.2663185596466064, 2.5203709602355957, 2.3068413734436035, 3.0904643535614014, 1.7644294500350952, 2.5423481464385986, 2.389821767807007, 2.9840829372406006, 2.4476490020751953]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09527573734521866, 0.10441453754901886, 0.10650549083948135, -0.19304265081882477, -0.13693322241306305, -0.14653196930885315, 0.12538018822669983, -0.19158592820167542, 0.07963676005601883, 0.08711763471364975, 0.10198589414358139, 0.058819565922021866, 0.08933565020561218, 0.10482834279537201, -0.8516674637794495, 0.12867337465286255]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.095550537109375, 0.06571035087108612, 0.04141634702682495, 0.059456318616867065, 0.09053167700767517, 0.06345164030790329, 0.04853738844394684, 0.0633702501654625, 0.004262639209628105, 0.08064127713441849, -0.15303096175193787, 0.04566356539726257, 0.021586908027529716, -0.005279059521853924, 0.00958950724452734, 0.009765268303453922]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07673680782318115, 0.09439926594495773, 0.11634185165166855, 0.07112421095371246, 0.10527937114238739, 0.12179601192474365, 0.0661003589630127, -0.044445574283599854, 0.08077489584684372, 0.04119595140218735, -0.00074696762021631, 0.1092352494597435, -0.11247125267982483, 0.05154889449477196, -0.006256673950701952, 0.12667761743068695]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13731427490711212, 0.18501660227775574, 0.16995838284492493, 0.18040622770786285, 0.12484341859817505, 0.14782850444316864, 0.06170254945755005, 0.22706076502799988, 0.20347638428211212, -0.4416073262691498, -0.02937972918152809, 0.12438037991523743, 0.11494816839694977, -0.15093937516212463, -0.13279670476913452, -0.042491164058446884]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.13602226972579956, 0.14346177875995636, 0.10196839272975922, -0.00659051351249218, 0.03728506341576576, -0.07343555986881256, 0.0844757929444313, 0.0992346778512001, 0.002943377709016204, 0.03941359370946884, -0.0684705376625061, 0.18428966403007507, -0.16029314696788788, -0.16978789865970612, 0.12711569666862488, -0.012974819168448448]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09704232960939407, 0.1580529808998108, 0.10126074403524399, 0.03841777890920639, -0.09697017818689346, 0.009903952479362488, 0.10058194398880005, 0.05960225686430931, 0.06699249893426895, 0.00694973673671484, -0.21901296079158783, 0.11490795016288757, -0.06893949210643768, 0.12837882339954376, 0.19779057800769806, 0.09771443158388138]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.16692936420440674, 0.12008153647184372, 0.0861164852976799, 0.37192657589912415, 0.20518849790096283, 0.16563571989536285, -0.07456684857606888, 0.026160694658756256, 0.18931494653224945, 0.2678471505641937, -0.2113550454378128, 0.35009878873825073, 0.24786661565303802, -0.08275126665830612, 0.0929357260465622, 0.15679889917373657]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.2108122855424881, 0.3171875774860382, 0.35257357358932495, 0.21654973924160004, -0.4931047260761261, 0.28756943345069885, 0.20767496526241302, -0.17270439863204956, 0.18147262930870056, 0.059828855097293854, -0.06596474349498749, 0.2921867072582245, 0.30897194147109985, 0.1495475471019745, -0.30946534872055054, -0.2049320936203003]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.39207273721694946, 0.3790169060230255, -0.12846103310585022, 0.37061116099357605, 0.4412073791027069, 0.518577516078949, 0.30100172758102417, -0.033486124128103256, 0.20288456976413727, 0.3434288501739502, 0.2588500380516052, -0.06930713355541229, 0.5170487761497498, -0.21911393105983734, -0.40097931027412415, 0.21648499369621277]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.6412593126296997, 0.42948171496391296, 0.43187278509140015, 0.585304319858551, 0.5778244137763977, 0.4236464500427246, 0.8381878137588501, 0.6436459422111511, 0.6546602249145508, 0.48825615644454956, 0.4955107867717743, 0.8435674905776978, 0.8451083302497864, 0.22713491320610046, 0.7777932286262512, 0.9055198431015015]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_10 - Captured router_logits: [1.0832736492156982, 0.9511581659317017, 0.8217955827713013, 0.8949693441390991, 1.0580096244812012, 0.4079715311527252, 0.8477041721343994, 0.7943012714385986, 0.5039991140365601, 0.5420982837677002, 0.7921655774116516, 0.8441059589385986, 0.6954357028007507, 0.35881713032722473, 0.7496834397315979, 0.62359219789505]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0253998041152954, 1.121658444404602, 1.0068998336791992, 1.2092496156692505, 1.3976479768753052, 0.641403079032898, 1.0189288854599, 1.085612416267395, 0.9720938205718994, 1.2056713104248047, 1.045414686203003, 1.3566826581954956, 0.6785991191864014, 0.9050161838531494, 0.4583723247051239, 1.0302711725234985]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.5704334378242493, 1.1169685125350952, 1.254923939704895, 1.3265655040740967, 0.8745550513267517, 0.9485160112380981, 0.9687739610671997, 1.1385852098464966, 0.8307717442512512, 0.5643224716186523, 1.0878814458847046, 1.0342800617218018, 0.9645973443984985, 1.0962141752243042, 0.8546473383903503, 1.1969553232192993]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.1755805015563965, 1.1431440114974976, 1.4699729681015015, 1.076122760772705, 1.327796459197998, 0.12339483201503754, 1.3659827709197998, 1.767742395401001, 1.1978405714035034, 1.3029351234436035, 1.4174120426177979, 1.5922532081604004, 0.7818889021873474, 1.197913646697998, 1.440384030342102, 1.3063668012619019]
Running loglikelihood requests:  69%|█████████████████████████████████▉               | 349/504 [19:15<08:11,  3.17s/it]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.0028088092803955, 0.9790997505187988, 1.1262503862380981, 1.1543333530426025, 1.1678227186203003, 1.2630603313446045, 0.7627683281898499, 1.081504464149475, 0.9520891308784485, 1.0206903219223022, -0.12403598427772522, 1.0561431646347046, 1.0839319229125977, 1.2243173122406006, 1.3645224571228027, 0.921161949634552]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.8475010991096497, 1.3007994890213013, 1.2183301448822021, 1.141391634941101, 0.9659526348114014, 1.141391634941101, 1.378940463066101, 1.3729190826416016, 1.0516209602355957, 1.205242395401001, 1.0469845533370972, 0.21573247015476227, 1.2972400188446045, 1.1195056438446045, 1.0955753326416016, 1.3029351234436035]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9147287607192993, 0.390254944562912, 0.9582553505897522, 0.8524181246757507, 0.8596305251121521, 0.7331885099411011, 0.22966445982456207, 0.9676274061203003, -0.3294871747493744, 1.8747650384902954, -0.6186876893043518, 0.7625733613967896, 1.0001825094223022, 0.9521484375, 0.9467636346817017, 0.3433889150619507]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.4498439431190491, 1.0927506685256958, 1.538368821144104, 1.4918133020401, 1.6293625831604004, 1.543625831604004, 1.5401942729949951, 1.7708820104599, 1.645717740058899, 1.5588310956954956, 1.9976270198822021, 1.5509090423583984, 1.5907447338104248, 1.4245034456253052, 0.8768299221992493, 1.4652652740478516]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.7007155418395996, 1.63671875, 1.7908604145050049, 1.7929322719573975, 1.4621416330337524, 1.6845428943634033, 1.6968457698822021, 2.0046730041503906, 2.1983790397644043, 1.730140209197998, 1.503901720046997, 1.2967432737350464, 1.6281760931015015, 1.2056245803833008, 1.7263799905776978, 1.9131498336791992]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.788040280342102, 1.686806321144104, 1.6927570104599, 1.8176109790802002, 1.849079966545105, 1.5179580450057983, 1.6883761882781982, 1.788204550743103, 1.9315310716629028, 1.8665486574172974, 1.8483864068984985, 1.7795342206954956, 2.2125437259674072, 1.8414865732192993, 1.825642466545105, 1.7590172290802002]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.303802251815796, 1.4340318441390991, 1.339195728302002, 1.3412903547286987, 0.4404325485229492, 1.4469279050827026, 1.6146320104599, 2.0490801334381104, 1.1550201177597046, 1.4538094997406006, 1.5238025188446045, 1.3189070224761963, 1.4476672410964966, 1.6469042301177979, 1.3403332233428955, 0.8710064888000488]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.3795998096466064, 2.5345356464385986, 2.382082462310791, 2.447429895401001, 2.4454219341278076, 2.3957359790802, 2.275408983230591, 2.438084125518799, 2.350905418395996, 2.5805344581604004, 2.5819218158721924, 2.5179615020751953, 2.136390209197998, 2.689471483230591, 2.6163113117218018, 2.4753942489624023]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.8812061548233032, 1.6849080324172974, 2.0435893535614014, 1.5523669719696045, 1.860032081604004, 1.8981454372406006, 1.800708293914795, 1.7630330324172974, 1.9554249048233032, 1.954147219657898, 1.7252482175827026, 2.0105140209198, 1.951007604598999, 1.920086145401001, 1.8067684173583984, 2.071042537689209]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.2713565826416016, 1.9062864780426025, 1.6818596124649048, 1.703818678855896, 1.7398327589035034, 1.8268837928771973, 1.6985799074172974, 1.8449182510375977, 1.747633934020996, 1.6780081987380981, 1.926109790802002, 1.702518105506897, 1.7033075094223022, 1.1917005777359009, 1.6681512594223022, 1.785922884941101]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.668297290802002, 2.7131643295288086, 2.557461977005005, 2.6565420627593994, 2.878504753112793, 2.7978971004486084, 2.8055636882781982, 2.579512357711792, 2.539792537689209, 2.6914427280426025, 2.7071406841278076, 2.6588056087493896, 2.587470769882202, 2.7156105041503906, 2.7867259979248047, 2.8015480041503906]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.882045865058899, 1.7924941778182983, 1.8282345533370972, 1.9159060716629028, 1.5574254989624023, 1.667439341545105, 1.8243283033370972, 1.9121640920639038, 1.831218957901001, 1.8053081035614014, 1.8253687620162964, 1.783367395401001, 1.744304895401001, 1.716705560684204, 1.673663854598999, 2.0294246673583984]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.8490434885025024, 1.8050389289855957, 1.8348240852355957, 1.8867552280426025, 1.9132137298583984, 1.8428829908370972, 1.7720410823822021, 1.7856330871582031, 1.9155113697052002, 1.961185097694397, 2.1156563758850098, 1.7694423198699951, 1.8593202829360962, 1.915522813796997, 1.8209490776062012, 1.8620730638504028]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6078919172286987, 1.6352139711380005, 1.6684387922286987, 1.697750449180603, 1.6129481792449951, 1.6194783449172974, 1.721344232559204, 1.703083872795105, 1.6256685256958008, 1.7662912607192993, 1.4578709602355957, 1.7111382484436035, 1.5931658744812012, 1.682647943496704, 1.6294492483139038, 1.7571394443511963]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.3179943561553955, 2.3649377822875977, 2.5892961025238037, 2.2982804775238037, 2.3276596069335938, 2.3059287071228027, 2.44350528717041, 2.3939836025238037, 2.321608543395996, 2.330653190612793, 2.2859046459198, 2.1632957458496094, 2.3166983127593994, 2.252601146697998, 2.495358943939209, 2.351224899291992]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.614412784576416, 5.713274002075195, 5.434871673583984, 5.439087867736816, 5.642012119293213, 5.45881986618042, 5.647342205047607, 5.736821174621582, 5.790230751037598, 5.650481700897217, 5.518655300140381, 5.453927993774414, 5.503778457641602, 5.534024715423584, 5.629380702972412, 5.455461502075195]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.06023645401001, 4.006899833679199, 4.043443202972412, 3.8783085346221924, 3.916508436203003, 3.9597327709198, 3.8850412368774414, 3.943852186203003, 3.967453956604004, 4.017687797546387, 3.993647813796997, 3.6768171787261963, 4.00740385055542, 3.8656907081604004, 4.018655300140381, 4.005319595336914]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.936587333679199, 3.056074857711792, 2.876387357711792, 2.6387267112731934, 2.8854410648345947, 2.963310480117798, 2.787602186203003, 2.7308337688446045, 2.887120246887207, 2.87123966217041, 2.8568925857543945, 2.3844096660614014, 2.8851125240325928, 2.838858127593994, 3.0075204372406006, 2.975175142288208]
Layer: gate_31 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.05694223940372467, 0.0700596496462822, 0.08873043209314346, -0.26024338603019714, -0.2476150244474411, -0.08210891485214233, 0.08890821039676666, -0.016269899904727936, 0.07729440182447433, 0.05565056577324867, 0.06472029536962509, 0.04414583742618561, 0.07214874029159546, 0.08463672548532486, -0.9869108200073242, 0.09192527830600739]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.08471333980560303, 0.03763245791196823, 0.032685909420251846, 0.035943515598773956, 0.07164058834314346, 0.030979948118329048, 0.058370769023895264, 0.10059832036495209, 0.05367894470691681, 0.060555942356586456, -0.18833577632904053, 0.05980617552995682, 0.002654237672686577, -0.02397198975086212, 0.032806649804115295, 0.0459781140089035]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0931999608874321, 0.03789478912949562, 0.09280949831008911, 0.07510773837566376, 0.1469702124595642, 0.08336250483989716, 0.07213535159826279, -0.08782541751861572, 0.07082266360521317, 0.1368112415075302, 0.03329474851489067, 0.08774494379758835, -0.1903473436832428, 0.00939552765339613, -0.017809022217988968, 0.10032135248184204]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1419096142053604, 0.13986393809318542, 0.11859681457281113, 0.15182682871818542, 0.14894650876522064, 0.1422850340604782, -0.014028945006430149, 0.17475344240665436, 0.1776105761528015, -0.5556179881095886, 0.03313658386468887, 0.07708301395177841, 0.3214862644672394, -0.3246506154537201, 0.06880907714366913, -0.06921717524528503]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.04628809541463852, 0.11321718990802765, 0.0807449072599411, 0.2580534815788269, 0.052583009004592896, -0.05535712465643883, 0.01766233704984188, 0.019117174670100212, -0.11778705567121506, 0.011653684079647064, -0.2514427602291107, 0.1373036950826645, 0.09737525880336761, -0.2137928307056427, 0.1428673267364502, -0.11084920167922974]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.05788666382431984, 0.13706596195697784, 0.051514822989702225, 0.1756126880645752, -0.14847895503044128, -0.08732634037733078, 0.12731918692588806, -0.006623897701501846, 0.24657483398914337, -0.08496813476085663, -0.07890715450048447, 0.10280422121286392, -0.2227197289466858, 0.16712404787540436, 0.12971597909927368, 0.0840136706829071]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11081422120332718, -0.04633662477135658, 0.12386739253997803, 0.3191974461078644, 0.1670989990234375, 0.1681547313928604, -0.19114916026592255, -0.07808181643486023, 0.2989559471607208, 0.15896548330783844, -0.5719587206840515, 0.29092758893966675, 0.2502026855945587, -0.32481643557548523, 0.3236101269721985, 0.22157086431980133]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_7 - Captured router_logits: [0.5829479098320007, 0.20543642342090607, 0.19920234382152557, 0.1831757128238678, -0.9046538472175598, 0.3268953859806061, 0.1931975781917572, -0.447662353515625, 0.5999860763549805, 0.16636686027050018, -0.23646286129951477, 0.2898162305355072, 0.1320985108613968, 0.24467986822128296, -0.7076162695884705, -0.3518063426017761]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.297916054725647, 0.25795644521713257, -0.33129191398620605, 0.3308013379573822, 0.4844439625740051, 0.32489070296287537, 0.2640722095966339, -0.29652953147888184, 0.1561141163110733, 0.7951441407203674, 0.10706926882266998, 0.24875353276729584, 0.49814820289611816, -0.4914848804473877, -0.5664960741996765, 0.39249852299690247]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.38543614745140076, 0.22274377942085266, 0.5303563475608826, 1.1007932424545288, 0.3828274607658386, 0.1370164453983307, 0.5706280469894409, 0.5887059569358826, 0.7817664742469788, 0.12177060544490814, 0.1690400391817093, 0.6940595507621765, 0.6938336491584778, 0.17545074224472046, 0.8009286522865295, 0.6546285152435303]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1567647457122803, 0.9157438278198242, 0.6005398631095886, 0.7317124605178833, 0.8152408003807068, 0.7034479975700378, 0.7043641209602356, 0.7874041795730591, 0.2356705516576767, 0.4921310842037201, 0.7896521091461182, 1.0166430473327637, 0.39240726828575134, 0.21243372559547424, 0.985680341720581, 0.45561015605926514]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.1077120304107666, 1.32183837890625, 0.9393425583839417, 1.2152036428451538, 1.3500447273254395, 0.1340470165014267, 0.9987931251525879, 0.9755766987800598, 1.0533424615859985, 1.6121848821640015, 0.909298300743103, 0.9306088089942932, 0.29986801743507385, 0.4361315965652466, 0.04650360718369484, 0.7651228904724121]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.05848175659775734, 0.7326061129570007, 1.2152398824691772, 1.1722619533538818, 0.24679623544216156, 0.633049726486206, 1.1089121103286743, 1.3088608980178833, 0.3489934206008911, 0.08174219727516174, 0.8156599998474121, 0.8329893946647644, 0.8265035152435303, 0.9084979295730591, 0.6544442772865295, 0.8562426567077637]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.5840212106704712, 0.9400427341461182, 0.9164854884147644, 0.1659499853849411, 1.0406839847564697, -0.3995908200740814, 1.0285966396331787, 1.9478920698165894, 1.1076982021331787, 0.8071381449699402, 1.3198978900909424, 1.081312656402588, -0.05739608034491539, 0.7009369730949402, 1.342902421951294, 0.5472492575645447]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7064036130905151, 0.7335228323936462, 1.055111289024353, 0.8199439644813538, 1.0035561323165894, 1.052239179611206, 0.33387842774391174, 1.468692421913147, 0.7230523824691772, 0.6726338863372803, -0.36568018794059753, 0.6955612301826477, 0.8419189453125, 1.0147037506103516, 1.115464687347412, 0.4293549656867981]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.3713168799877167, 1.2059533596038818, 1.0079230070114136, 1.0363171100616455, 0.8109453320503235, 0.8481352925300598, 1.5046374797821045, 1.0764482021331787, 0.987433671951294, 1.007554531097412, 0.6121342778205872, -0.4624179005622864, 1.0318914651870728, 0.8713701367378235, 0.8927992582321167, 1.359598994255066]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.5794332027435303, 0.11730986088514328, 0.711418867111206, 0.7783548831939697, 0.740349531173706, 0.3827986717224121, 0.37948206067085266, 0.7838215231895447, -1.011996865272522, 2.153191328048706, -0.8581519722938538, 0.40544071793556213, 0.7602953910827637, 0.7932416796684265, 0.7406949996948242, 0.2855806052684784]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.028492476791143417, 0.6941770315170288, 1.452406406402588, 1.3314093351364136, 1.5447375774383545, 1.4970334768295288, 1.4933459758758545, 1.3464585542678833, 2.014556407928467, 1.343455195426941, 2.316295623779297, 1.6749106645584106, 1.4634623527526855, 1.39013671875, 0.543399453163147, 1.2575962543487549]
Running loglikelihood requests:  70%|██████████████████████████████████▎              | 353/504 [19:28<07:58,  3.17s/it]Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4884287118911743, 1.2414872646331787, 1.5057857036590576, 1.8054614067077637, 0.8623921871185303, 1.209279179573059, 2.03515625, 1.7441037893295288, 2.404554843902588, 1.4167526960372925, 1.1795861721038818, 0.7308015823364258, 1.139828085899353, 0.8642313480377197, 1.4440596103668213, 1.489976406097412]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.5464880466461182, 1.3503096103668213, 1.7237433195114136, 1.3959684371948242, 1.6792083978652954, 1.0223687887191772, 1.3326761722564697, 1.3766306638717651, 2.076061248779297, 1.5141509771347046, 1.7611474990844727, 1.5547611713409424, 1.8331736326217651, 1.5846108198165894, 1.5735554695129395, 1.5885355472564697]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2767126560211182, 1.0342626571655273, 0.9891840815544128, 0.8882043361663818, -0.13611185550689697, 1.3187094926834106, 1.1182193756103516, 1.604851484298706, 0.7666729688644409, 1.2562463283538818, 1.174546718597412, 1.1285884380340576, 1.0872641801834106, 1.2276312112808228, 0.7818902730941772, 0.3457457423210144]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.1134285926818848, 2.2656986713409424, 2.2473466396331787, 1.9889814853668213, 1.9958726167678833, 2.1759285926818848, 1.9311246871948242, 2.2089474201202393, 1.9938089847564697, 2.0879275798797607, 2.3562793731689453, 2.672833204269409, 1.6678212881088257, 2.3301150798797607, 2.1289432048797607, 2.094634532928467]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3900353908538818, 1.4407060146331787, 2.0622050762176514, 0.9918650388717651, 1.4551517963409424, 1.5724499225616455, 1.2646669149398804, 1.3722729682922363, 1.4959094524383545, 1.5589438676834106, 1.6049528121948242, 2.095592498779297, 1.4579524993896484, 1.545216679573059, 1.1828566789627075, 1.6744177341461182]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.3557636737823486, 1.8987691402435303, 1.0192087888717651, 1.4791420698165894, 1.6201725006103516, 1.4169000387191772, 1.3725677728652954, 1.3326576948165894, 1.4735959768295288, 1.4557783603668213, 1.4738354682922363, 1.5334057807922363, 1.1613723039627075, 0.5547128319740295, 1.2092238664627075, 1.4387898445129395]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.349498748779297, 2.3735995292663574, 2.369767189025879, 2.0564563274383545, 2.5344929695129395, 2.1691479682922363, 2.318985939025879, 2.074587345123291, 2.002542734146118, 2.081662654876709, 2.8263561725616455, 2.0184993743896484, 2.284271717071533, 2.409198045730591, 2.2246463298797607, 2.2253096103668213]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6781028509140015, 1.729068398475647, 1.6082326173782349, 1.8530365228652954, 1.2888413667678833, 1.655697226524353, 1.7384655475616455, 2.1008622646331787, 1.749447226524353, 1.610480546951294, 1.7029407024383545, 1.5827683210372925, 1.6974867582321167, 1.532355546951294, 2.184957265853882, 2.014740467071533]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_26 - Captured router_logits: [1.819759726524353, 1.9983404874801636, 1.7844929695129395, 1.743035078048706, 1.7799971103668213, 1.929206132888794, 1.7833493947982788, 1.7459832429885864, 1.8518941402435303, 1.9454046487808228, 2.477041482925415, 1.7080538272857666, 1.7448039054870605, 1.8956921100616455, 1.8883355855941772, 1.8606048822402954]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6483154296875, 1.7172021865844727, 1.6445070505142212, 1.7029430866241455, 1.5708467960357666, 1.5950158834457397, 1.8580610752105713, 1.7318161725997925, 1.5995800495147705, 1.6868274211883545, 1.329230546951294, 1.6514776945114136, 1.7248258590698242, 2.0592432022094727, 1.6305415630340576, 1.6901187896728516]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0729658603668213, 2.1609668731689453, 2.426389217376709, 1.993293046951294, 2.1350605487823486, 2.0811100006103516, 2.619251251220703, 2.179060935974121, 2.027970314025879, 2.0149617195129395, 2.0083651542663574, 1.820607304573059, 2.0247089862823486, 1.873599648475647, 2.3030292987823486, 1.969597578048706]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.913030624389648, 5.419663906097412, 4.911704063415527, 4.740326404571533, 4.856205940246582, 4.781655311584473, 4.884876251220703, 5.020268440246582, 5.159934997558594, 4.8932414054870605, 4.861106872558594, 4.810988903045654, 4.9004645347595215, 4.972877502441406, 5.0858635902404785, 4.875847816467285]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.529388904571533, 3.383328437805176, 3.257923126220703, 3.1269161701202393, 3.402454376220703, 3.417379140853882, 3.0774524211883545, 3.516841173171997, 3.5587780475616455, 3.3932414054870605, 3.236438751220703, 2.856809139251709, 3.1338627338409424, 3.4233858585357666, 3.419921875, 3.4292960166931152]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.4323408603668213, 2.5878169536590576, 2.6215360164642334, 2.5504863262176514, 2.4893131256103516, 2.4356205463409424, 2.5936763286590576, 2.144899845123291, 2.5062646865844727, 2.2444355487823486, 3.1621463298797607, 1.7479835748672485, 2.487175703048706, 2.380601406097412, 3.018794298171997, 2.462264060974121]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09495925903320312, 0.10383065789937973, 0.10574714839458466, -0.1913398951292038, -0.13528844714164734, -0.143049418926239, 0.12484424561262131, -0.18970590829849243, 0.07716499269008636, 0.08653619140386581, 0.10120104253292084, 0.05736347287893295, 0.08946011960506439, 0.1039648950099945, -0.8426352739334106, 0.12796150147914886]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.09581339359283447, 0.06651443243026733, 0.04106960818171501, 0.059802018105983734, 0.08905936032533646, 0.06427023559808731, 0.049001745879650116, 0.06397146731615067, 0.004208582919090986, 0.08101107180118561, -0.15078750252723694, 0.04601014405488968, 0.022319361567497253, -0.004827031400054693, 0.010711310431361198, 0.010600360110402107]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07694438844919205, 0.09562783688306808, 0.11657743901014328, 0.07224734872579575, 0.1051376610994339, 0.12284310907125473, 0.06385209411382675, -0.043868012726306915, 0.0813254565000534, 0.041487425565719604, -0.000984407844953239, 0.11073792725801468, -0.11364976316690445, 0.0532982312142849, -0.007883035577833652, 0.12760522961616516]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13902750611305237, 0.1853022277355194, 0.17155161499977112, 0.18127606809139252, 0.12354645878076553, 0.14370539784431458, 0.060929350554943085, 0.22612200677394867, 0.20390866696834564, -0.4415517747402191, -0.026106636971235275, 0.1249096691608429, 0.11459069699048996, -0.15643310546875, -0.1327514946460724, -0.042951978743076324]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.13595983386039734, 0.14389526844024658, 0.10137337446212769, -0.008518506772816181, 0.040724556893110275, -0.0730498656630516, 0.08256027102470398, 0.0998925268650055, -0.0003857882402371615, 0.03602902218699455, -0.07015523314476013, 0.1840042918920517, -0.15615902841091156, -0.17102396488189697, 0.12638193368911743, -0.018985964357852936]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.08992594480514526, 0.15830987691879272, 0.10093480348587036, 0.040912482887506485, -0.10493160039186478, 0.005906015168875456, 0.10218270868062973, 0.05736131593585014, 0.06309248507022858, 0.006597266998142004, -0.2140456885099411, 0.11512640863656998, -0.07098878175020218, 0.12820225954055786, 0.19782666862010956, 0.09904170781373978]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.16458158195018768, 0.11392398923635483, 0.08943233639001846, 0.37033137679100037, 0.20043571293354034, 0.1634809374809265, -0.08089691400527954, 0.02600644715130329, 0.20353691279888153, 0.264400839805603, -0.21153345704078674, 0.3461930751800537, 0.2486952245235443, -0.0918758288025856, 0.09307688474655151, 0.1564660668373108]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.20950548350811005, 0.31184473633766174, 0.3480500876903534, 0.21642008423805237, -0.4969436228275299, 0.29064738750457764, 0.20175689458847046, -0.188184916973114, 0.17871424555778503, 0.060484111309051514, -0.07331272214651108, 0.2877133786678314, 0.3042798340320587, 0.1506676971912384, -0.31216374039649963, -0.21217893064022064]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3908461034297943, 0.37087610363960266, -0.14007453620433807, 0.372036337852478, 0.4465453028678894, 0.5203880667686462, 0.29808247089385986, -0.04045882448554039, 0.20543253421783447, 0.33791717886924744, 0.2640026807785034, -0.06916435062885284, 0.5186825394630432, -0.2236512452363968, -0.41371357440948486, 0.21313901245594025]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.6538385152816772, 0.4288923144340515, 0.4368562400341034, 0.5781946778297424, 0.5823439359664917, 0.42949193716049194, 0.8439273238182068, 0.6386212110519409, 0.6479515433311462, 0.49061208963394165, 0.4904572069644928, 0.8464954495429993, 0.8589392900466919, 0.1945527344942093, 0.7693884372711182, 0.905632734298706]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_10 - Captured router_logits: [1.0793031454086304, 0.9525952339172363, 0.8411012887954712, 0.8943654298782349, 1.071346640586853, 0.40180420875549316, 0.8515855073928833, 0.7865013480186462, 0.5097635984420776, 0.5411270260810852, 0.8027436137199402, 0.8427043557167053, 0.7068121433258057, 0.3542163670063019, 0.7515742182731628, 0.6321284174919128]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.041195273399353, 1.1382365226745605, 1.0061357021331787, 1.2274250984191895, 1.4060012102127075, 0.6358487010002136, 1.033267855644226, 1.0912994146347046, 0.9798169136047363, 1.1940687894821167, 1.0601322650909424, 1.373793125152588, 0.6914004683494568, 0.930674135684967, 0.46726441383361816, 1.0414302349090576]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.5955580472946167, 1.1258337497711182, 1.2645009756088257, 1.3326761722564697, 0.9186055660247803, 0.965087890625, 0.9840399026870728, 1.1445680856704712, 0.8279423117637634, 0.5764476656913757, 1.0854307413101196, 1.0293751955032349, 0.9698832035064697, 1.0972877740859985, 0.8642117381095886, 1.2223246097564697]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.190982460975647, 1.1620540618896484, 1.482421875, 1.0744585990905762, 1.344984531402588, 0.14175933599472046, 1.3734153509140015, 1.7466280460357666, 1.2052807807922363, 1.3168485164642334, 1.4309496879577637, 1.6113649606704712, 0.820807695388794, 1.2019941806793213, 1.3973687887191772, 1.3300873041152954]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.0065860748291016, 0.9742961525917053, 1.1395747661590576, 1.1656471490859985, 1.1731648445129395, 1.2800408601760864, 0.786197304725647, 1.0782355070114136, 0.9661335349082947, 1.0293751955032349, -0.14276814460754395, 1.0807621479034424, 1.104628086090088, 1.2300541400909424, 1.3799933195114136, 0.943023681640625]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.8511341214179993, 1.3173643350601196, 1.2300817966461182, 1.1459684371948242, 0.9666494727134705, 1.1427439451217651, 1.3920530080795288, 1.378058671951294, 1.0774247646331787, 1.2165205478668213, 1.048957109451294, 0.24573791027069092, 1.303029179573059, 1.124152421951294, 1.097398281097412, 1.295172929763794]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9315922856330872, 0.3890637159347534, 0.9632050395011902, 0.857292890548706, 0.8614755272865295, 0.7349738478660583, 0.2618604004383087, 0.9888063669204712, -0.30587366223335266, 1.9125032424926758, -0.6436580419540405, 0.7647221684455872, 1.0024690628051758, 0.9600070118904114, 0.9539357423782349, 0.35170602798461914]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.4511212110519409, 1.1182631254196167, 1.5536187887191772, 1.5054585933685303, 1.6394641399383545, 1.5577093362808228, 1.5529831647872925, 1.7906471490859985, 1.658332109451294, 1.5832104682922363, 2.0298495292663574, 1.575913906097412, 1.5775771141052246, 1.4340912103652954, 0.8883908987045288, 1.4925670623779297]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.7090948820114136, 1.6487691402435303, 1.7993624210357666, 1.8095518350601196, 1.4847768545150757, 1.6869103908538818, 1.7010982036590576, 2.017688751220703, 2.2242777347564697, 1.7333431243896484, 1.507941484451294, 1.309529423713684, 1.6372346878051758, 1.2441412210464478, 1.7249410152435303, 1.9078714847564697]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.7560068368911743, 1.6775869131088257, 1.678581953048706, 1.794627070426941, 1.827830195426941, 1.5017181634902954, 1.6614644527435303, 1.7605763673782349, 1.9094929695129395, 1.8424233198165894, 1.8209389448165894, 1.7539430856704712, 2.2296764850616455, 1.8227447271347046, 1.8004127740859985, 1.7352962493896484]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.3036441802978516, 1.4139150381088257, 1.3285764455795288, 1.3307253122329712, 0.4838636815547943, 1.4321473836898804, 1.6084905862808228, 2.027203321456909, 1.1300256252288818, 1.4370393753051758, 1.5088443756103516, 1.3050744533538818, 1.433851718902588, 1.6302697658538818, 1.3248636722564697, 0.872005820274353]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.3959131240844727, 2.555572032928467, 2.410893201828003, 2.4644384384155273, 2.4573261737823486, 2.406139373779297, 2.3034346103668213, 2.4590213298797607, 2.361586093902588, 2.606942892074585, 2.601046562194824, 2.5299971103668213, 2.1583504676818848, 2.7085790634155273, 2.636497735977173, 2.500221014022827]
Running loglikelihood requests:  71%|██████████████████████████████████▋              | 357/504 [19:41<07:46,  3.18s/it]Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.8852447271347046, 1.6800191402435303, 2.059109687805176, 1.5604408979415894, 1.8508254289627075, 1.8984006643295288, 1.8005969524383545, 1.763413906097412, 1.9544148445129395, 1.9566627740859985, 1.7238354682922363, 2.014666795730591, 1.952019453048706, 1.9214327335357666, 1.8098835945129395, 2.074329376220703]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.2920475006103516, 1.9173606634140015, 1.7004716396331787, 1.7063863277435303, 1.757259726524353, 1.846808671951294, 1.7053176164627075, 1.8622493743896484, 1.7569224834442139, 1.6950913667678833, 1.9380896091461182, 1.7125728130340576, 1.7182340621948242, 1.1905287504196167, 1.6820091009140015, 1.7979437112808228]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.6797611713409424, 2.727447032928467, 2.571786642074585, 2.670548439025879, 2.9048495292663574, 2.809699296951294, 2.8175854682922363, 2.597435235977173, 2.561394453048706, 2.718528985977173, 2.722066640853882, 2.6793925762176514, 2.600604295730591, 2.7290682792663574, 2.7958431243896484, 2.8183224201202393]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.890882968902588, 1.8036924600601196, 1.8361586332321167, 1.9256154298782349, 1.566701054573059, 1.676463007926941, 1.8276827335357666, 1.918263554573059, 1.8331736326217651, 1.8137528896331787, 1.8344173431396484, 1.7881044149398804, 1.7527638673782349, 1.7177181243896484, 1.6850677728652954, 2.034161329269409]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.855095624923706, 1.8181337118148804, 1.8487894535064697, 1.888607382774353, 1.9229990243911743, 1.852301836013794, 1.7833666801452637, 1.799607753753662, 1.9213175773620605, 1.9724812507629395, 2.136939525604248, 1.7727142572402954, 1.8695828914642334, 1.9318064451217651, 1.8320035934448242, 1.875908613204956]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6205594539642334, 1.6502248048782349, 1.6776002645492554, 1.7106075286865234, 1.6216695308685303, 1.6339963674545288, 1.7353285551071167, 1.7147424221038818, 1.6348508596420288, 1.7777398824691772, 1.4643278121948242, 1.7179484367370605, 1.605828046798706, 1.7016013860702515, 1.643490195274353, 1.7651660442352295]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.311164140701294, 2.3655383586883545, 2.5897793769836426, 2.2935307025909424, 2.3256006240844727, 2.303241014480591, 2.4419429302215576, 2.393218517303467, 2.3188672065734863, 2.322173595428467, 2.277564764022827, 2.1480321884155273, 2.307359218597412, 2.2380969524383545, 2.4961812496185303, 2.347140312194824]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.584463596343994, 5.708505153656006, 5.412883281707764, 5.408166408538818, 5.603552341461182, 5.419885158538818, 5.630159378051758, 5.726783752441406, 5.767394065856934, 5.618550777435303, 5.489202499389648, 5.431382656097412, 5.4784417152404785, 5.515256404876709, 5.622714996337891, 5.420621871948242]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.093602657318115, 4.048496246337891, 4.074440002441406, 3.8958487510681152, 3.9440042972564697, 3.997678279876709, 3.9035298824310303, 3.977999687194824, 4.007112503051758, 4.048128128051758, 4.020931720733643, 3.6968371868133545, 4.031084060668945, 3.899782657623291, 4.048864841461182, 4.033576965332031]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.9337410926818848, 3.061615467071533, 2.8934993743896484, 2.639888048171997, 2.9078714847564697, 2.9624483585357666, 2.784271717071533, 2.728257656097412, 2.889740467071533, 2.8578271865844727, 2.8735995292663574, 2.3781139850616455, 2.8911776542663574, 2.832620859146118, 3.0288915634155273, 2.976783514022827]
Layer: gate_31 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.095169797539711, 0.10697738826274872, 0.11042697727680206, -0.2000238001346588, -0.14622379839420319, -0.14184856414794922, 0.12949272990226746, -0.18679751455783844, 0.0876070186495781, 0.09150886535644531, 0.10202451795339584, 0.06396455317735672, 0.09643840789794922, 0.10787362605333328, -0.8943997621536255, 0.13111671805381775]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.09297268092632294, 0.06288059055805206, 0.039465609937906265, 0.0536763034760952, 0.08825801312923431, 0.055603981018066406, 0.047481097280979156, 0.06160970777273178, 0.006882153917104006, 0.07684106379747391, -0.16818061470985413, 0.0446973592042923, 0.02897614613175392, -0.011278299614787102, 0.011231422424316406, 0.0060427007265388966]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07760635018348694, 0.08603136241436005, 0.12279649823904037, 0.07817099988460541, 0.10167987644672394, 0.1179928407073021, 0.06317310780286789, -0.05616232007741928, 0.08654359728097916, 0.037338659167289734, 0.005195177625864744, 0.10755685716867447, -0.1320260912179947, 0.047707851976156235, -0.012383387424051762, 0.119166299700737]
Layer: gate_2 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13565826416015625, 0.18873360753059387, 0.16024692356586456, 0.1836477369070053, 0.1339450180530548, 0.15085498988628387, 0.05081998556852341, 0.223856121301651, 0.2004729062318802, -0.47137847542762756, -0.03375067934393883, 0.13317225873470306, 0.13178373873233795, -0.18978530168533325, -0.13431960344314575, -0.05502583459019661]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.12307706475257874, 0.14622189104557037, 0.11426246911287308, 0.001728204544633627, 0.03741014748811722, -0.07037998735904694, 0.0869356319308281, 0.09747446328401566, -0.008955735713243484, 0.04707072302699089, -0.12212195992469788, 0.1861167997121811, -0.15319016575813293, -0.17822100222110748, 0.12652148306369781, -0.03327751159667969]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09864484518766403, 0.14790226519107819, 0.08553057163953781, 0.02414175122976303, -0.08112598955631256, -0.0028463511262089014, 0.1115400642156601, 0.06095152720808983, 0.06611703336238861, -0.01982351392507553, -0.09323002398014069, 0.09675157815217972, -0.0909077599644661, 0.11588309705257416, 0.1955242156982422, 0.0836479514837265]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.1661001294851303, 0.11368619650602341, 0.045963581651449203, 0.3750469386577606, 0.19277073442935944, 0.15842613577842712, -0.10195079445838928, -0.005752710159868002, 0.19412650167942047, 0.24827633798122406, -0.29459792375564575, 0.3589259684085846, 0.23518019914627075, -0.10483089089393616, 0.07859978079795837, 0.16706246137619019]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.21976500749588013, 0.3383025527000427, 0.33144620060920715, 0.22411933541297913, -0.552892804145813, 0.31221655011177063, 0.20041774213314056, -0.23907352983951569, 0.1795077621936798, 0.012223463505506516, -0.0874827429652214, 0.3023317754268646, 0.31112200021743774, 0.12166140973567963, -0.37083786725997925, -0.16805854439735413]
Layer: gate_7 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3898579478263855, 0.35850584506988525, -0.1530705988407135, 0.33971258997917175, 0.4456552267074585, 0.49002310633659363, 0.2809894382953644, -0.018604572862386703, 0.1835602968931198, 0.31225350499153137, 0.22209373116493225, -0.02972412109375, 0.48064011335372925, -0.280312180519104, -0.3995954692363739, 0.19122593104839325]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.5968978404998779, 0.3753330409526825, 0.46850043535232544, 0.5355036854743958, 0.5216956734657288, 0.3715013265609741, 0.780930757522583, 0.6003136038780212, 0.513771653175354, 0.31844857335090637, 0.4222464859485626, 0.7944430112838745, 0.9004786610603333, 0.18920840322971344, 0.754441499710083, 0.8495436310768127]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1397705078125, 0.936659574508667, 0.7914851307868958, 0.8892634510993958, 1.009610652923584, 0.3580976724624634, 0.8206669688224792, 0.7689208984375, 0.5331520438194275, 0.5231564044952393, 0.7983445525169373, 0.853877604007721, 0.6976711750030518, 0.31951287388801575, 0.6386831998825073, 0.5988370180130005]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0632065534591675, 1.1201547384262085, 1.022310733795166, 1.107421875, 1.436870813369751, 0.5988088846206665, 0.9794763326644897, 1.1126497983932495, 0.9833914041519165, 1.1893874406814575, 1.054443359375, 1.3424025774002075, 0.7177605032920837, 0.8378806710243225, 0.4212740361690521, 0.964599609375]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.5292757749557495, 1.094698429107666, 1.2215951681137085, 1.279559850692749, 0.798001229763031, 0.9733651876449585, 0.8649415373802185, 1.0812472105026245, 0.8546838164329529, 0.6437659859657288, 1.0201510190963745, 1.0307852029800415, 0.9325326681137085, 1.0814114809036255, 0.8225755095481873, 1.1720956563949585]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.125413179397583, 1.053119421005249, 1.312514066696167, 0.9200627207756042, 1.248685359954834, 0.21246807277202606, 1.293964147567749, 1.599534273147583, 1.130465030670166, 1.214918851852417, 1.2795974016189575, 1.5253530740737915, 0.6806595325469971, 1.092496395111084, 1.438645601272583, 1.1859506368637085]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.9425858855247498, 0.9293118715286255, 1.043316125869751, 1.034517765045166, 1.1065768003463745, 1.066644549369812, 0.6454849243164062, 0.9059524536132812, 0.9751939177513123, 0.9437960386276245, -0.058284465223550797, 1.0269458293914795, 1.033022403717041, 1.1687105894088745, 1.273831844329834, 0.8555749654769897]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.8046640157699585, 1.278545618057251, 1.188082218170166, 1.098482608795166, 0.9319082498550415, 1.0839279890060425, 1.1656506061553955, 1.2579251527786255, 0.969313383102417, 1.1509164571762085, 0.9678579568862915, 0.12490610033273697, 1.158597469329834, 1.062237024307251, 1.0024038553237915, 1.22944176197052]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.8511164784431458, 0.32122480869293213, 0.9305161833763123, 0.7690805196762085, 0.8318058848381042, 0.684401273727417, 0.1587858945131302, 0.9449509978294373, -0.3751760721206665, 1.8199474811553955, -0.5182641744613647, 0.7190572023391724, 0.954514741897583, 0.9202880859375, 0.9095083475112915, 0.34403520822525024]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.506646990776062, 1.0737459659576416, 1.5237380266189575, 1.5241347551345825, 1.5646597146987915, 1.5250149965286255, 1.49908447265625, 1.691969633102417, 1.5888108015060425, 1.503765344619751, 1.921367883682251, 1.525709867477417, 1.5087878704071045, 1.387498140335083, 0.9568387269973755, 1.517480731010437]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6453200578689575, 1.5697866678237915, 1.7503005266189575, 1.73291015625, 1.3644174337387085, 1.580763578414917, 1.6138070821762085, 1.9409555196762085, 2.157921314239502, 1.6822415590286255, 1.522432804107666, 1.2581082582473755, 1.5602463483810425, 1.2437509298324585, 1.631009578704834, 1.8048752546310425]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.70654296875, 1.646409273147583, 1.6456955671310425, 1.7440279722213745, 1.777606725692749, 1.465576171875, 1.666165828704834, 1.7577749490737915, 1.7985464334487915, 1.8197866678237915, 1.7833908796310425, 1.727952241897583, 2.215369701385498, 1.803072452545166, 1.7703200578689575, 1.707594633102417]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2635536193847656, 1.362886905670166, 1.2735595703125, 1.3299349546432495, 0.43768781423568726, 1.3738356828689575, 1.478252649307251, 1.8329867124557495, 1.182222843170166, 1.3694692850112915, 1.4917367696762085, 1.262545108795166, 1.3792067766189575, 1.5646408796310425, 1.3050665855407715, 0.8622013926506042]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.405311107635498, 2.567457914352417, 2.403019905090332, 2.423715353012085, 2.455829381942749, 2.408841609954834, 2.309157133102417, 2.434645414352417, 2.357572078704834, 2.587890625, 2.587440013885498, 2.545748233795166, 2.102313756942749, 2.683293342590332, 2.594275951385498, 2.477238655090332]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.8774038553237915, 1.67236328125, 1.986478328704834, 1.518160343170166, 1.8373647928237915, 1.897536039352417, 1.8115609884262085, 1.7684420347213745, 1.914700984954834, 1.962665319442749, 1.722806453704834, 1.990647554397583, 1.9313400983810425, 1.8902493715286255, 1.810697078704834, 2.069749116897583]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.252065896987915, 1.917705774307251, 1.640305757522583, 1.701340913772583, 1.753399133682251, 1.830716609954834, 1.692965030670166, 1.837252140045166, 1.7501455545425415, 1.6654897928237915, 1.9044846296310425, 1.6746450662612915, 1.7086838483810425, 1.1296292543411255, 1.673189640045166, 1.8056640625]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.606595516204834, 2.652418851852417, 2.554086446762085, 2.622596263885498, 2.843524694442749, 2.714993953704834, 2.761418342590332, 2.557316780090332, 2.505033016204834, 2.652794361114502, 2.6552734375, 2.626126766204834, 2.537635326385498, 2.666841983795166, 2.712139368057251, 2.776141881942749]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  72%|███████████████████████████████████              | 361/504 [19:54<07:35,  3.19s/it]Layer: gate_25 - Captured router_logits: [1.924954891204834, 1.8553560972213745, 1.865309476852417, 1.970365047454834, 1.5591195821762085, 1.74951171875, 1.865140438079834, 1.972581148147583, 1.878981351852417, 1.850848913192749, 1.871018648147583, 1.850360631942749, 1.823091983795166, 1.7742263078689575, 1.7193509340286255, 2.067270040512085]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.8438814878463745, 1.820975661277771, 1.880155086517334, 1.8719576597213745, 1.9159643650054932, 1.8362826108932495, 1.8075960874557495, 1.8107147216796875, 1.9032827615737915, 1.9490708112716675, 2.0917491912841797, 1.7638967037200928, 1.8782864809036255, 1.9442514181137085, 1.817673921585083, 1.8481316566467285]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.563809871673584, 1.610445499420166, 1.6511136293411255, 1.669921875, 1.5880502462387085, 1.6072341203689575, 1.6910775899887085, 1.6686307191848755, 1.615689754486084, 1.746262788772583, 1.4306265115737915, 1.667494535446167, 1.5563976764678955, 1.6466064453125, 1.600689172744751, 1.7316988706588745]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.233060359954834, 2.291522741317749, 2.522799015045166, 2.233452320098877, 2.249098539352417, 2.246873140335083, 2.352609395980835, 2.323016881942749, 2.260789155960083, 2.268190860748291, 2.218433141708374, 2.087759256362915, 2.2723388671875, 2.203350305557251, 2.4205322265625, 2.2705113887786865]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.452073097229004, 5.573768138885498, 5.320237159729004, 5.342125415802002, 5.491098403930664, 5.324331283569336, 5.488581657409668, 5.57204008102417, 5.673903465270996, 5.504244327545166, 5.374812126159668, 5.337252140045166, 5.374953269958496, 5.42893648147583, 5.49290132522583, 5.34446382522583]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.092228889465332, 4.011794090270996, 4.063664436340332, 3.9163923263549805, 3.947716236114502, 3.999061107635498, 3.892056941986084, 3.972956657409668, 4.008610725402832, 4.037221908569336, 4.01198148727417, 3.661278247833252, 4.021395206451416, 3.911264181137085, 4.01881742477417, 4.010521411895752]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.941556453704834, 3.022836446762085, 2.844501256942749, 2.594876766204834, 2.832932710647583, 2.913649320602417, 2.751502513885498, 2.695875883102417, 2.873422384262085, 2.844520092010498, 2.814190149307251, 2.3185319900512695, 2.866135835647583, 2.825345516204834, 2.984675407409668, 2.962177038192749]
Layer: gate_31 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.07471048086881638, 0.08841587603092194, 0.09561721980571747, -0.26014840602874756, -0.24447689950466156, -0.06989112496376038, 0.11256173998117447, -0.04555188864469528, 0.08218324929475784, 0.07131708413362503, 0.08623863756656647, 0.06191488355398178, 0.08968544006347656, 0.1015688106417656, -1.1044546365737915, 0.10737081617116928]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07576903700828552, 0.04095268249511719, 0.03494108468294144, 0.040444888174533844, 0.06664268672466278, 0.013296375051140785, 0.04178117960691452, 0.08429233729839325, 0.024534298107028008, 0.06846420466899872, -0.22107352316379547, 0.03909974917769432, 0.01996348425745964, -0.03525352478027344, 0.03978395462036133, 0.013287397101521492]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07851703464984894, 0.058193281292915344, 0.09184639155864716, 0.09313590824604034, 0.10975324362516403, 0.0987202599644661, 0.07104198634624481, -0.11447202414274216, 0.08455203473567963, 0.10283180326223373, 0.040005169808864594, 0.07872772216796875, -0.21303851902484894, 0.0077588739804923534, -0.04181993752717972, 0.10413507372140884]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13210061192512512, 0.14651371538639069, 0.12810149788856506, 0.161906898021698, 0.1407891809940338, 0.13486525416374207, -0.009666149504482746, 0.1980356127023697, 0.19398337602615356, -0.5585949420928955, 0.027538299560546875, 0.06718327105045319, 0.2710767388343811, -0.2832782566547394, -0.002223968505859375, -0.07987184077501297]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06218455359339714, 0.11529504507780075, 0.1096455529332161, 0.21716690063476562, 0.05209130421280861, -0.07146688550710678, 0.02085120789706707, 0.028010807931423187, -0.09427760541439056, 0.05066005885601044, -0.238840252161026, 0.1363096982240677, 0.014033537358045578, -0.23199492692947388, 0.13091938197612762, -0.050623416900634766]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06766700744628906, 0.11636418849229813, 0.038809336721897125, 0.13364528119564056, -0.10127140581607819, -0.10063112527132034, 0.18752171099185944, -0.0055295503698289394, 0.18436959385871887, -0.0966298058629036, 0.10171743482351303, 0.07316862046718597, -0.2727024257183075, 0.133922278881073, 0.13830412924289703, 0.0522606186568737]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.1077117919921875, -0.07544935494661331, 0.10915161669254303, 0.3818429708480835, 0.16959527134895325, 0.19454333186149597, -0.17407050728797913, -0.09626491367816925, 0.35942304134368896, 0.14397312700748444, -0.6405616402626038, 0.31005623936653137, 0.19048133492469788, -0.2951962351799011, 0.2678211033344269, 0.24015426635742188]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5010998249053955, 0.16834956407546997, 0.1886846125125885, 0.13907299935817719, -0.9845956563949585, 0.36874130368232727, 0.19214878976345062, -0.37905237078666687, 0.5760228037834167, 0.17878106236457825, -0.33002060651779175, 0.25619977712631226, 0.23370742797851562, 0.1921234130859375, -0.8323434591293335, -0.16265633702278137]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.25522202253341675, 0.2845808267593384, -0.31064531207084656, 0.2912154495716095, 0.5481467843055725, 0.23032893240451813, 0.24564236402511597, -0.3808382451534271, 0.18012839555740356, 0.8571707010269165, 0.030961256474256516, 0.29960134625434875, 0.3662484884262085, -0.6918622255325317, -0.6378074288368225, 0.39116522669792175]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.29759863018989563, 0.07516317814588547, 0.484555184841156, 1.115581750869751, 0.2855600118637085, 0.09749779105186462, 0.4429462254047394, 0.4833608865737915, 0.6412268280982971, 0.01446269080042839, 0.11416567116975784, 0.6582219004631042, 0.7547557353973389, 0.05449441820383072, 0.6715182065963745, 0.6027127504348755]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.110512375831604, 0.8298011422157288, 0.5405836701393127, 0.685532808303833, 0.795424222946167, 0.5466817617416382, 0.5590092539787292, 0.682692289352417, 0.0893971398472786, 0.38117510080337524, 0.7150033712387085, 0.895646333694458, 0.2967846095561981, 0.17734336853027344, 0.7776750326156616, 0.2995282709598541]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.8975196480751038, 1.1061636209487915, 0.8816481232643127, 0.993577241897583, 1.26153564453125, 0.24013108015060425, 0.865919828414917, 0.878173828125, 1.0277005434036255, 1.442655086517334, 0.8097909688949585, 0.781935453414917, 0.2181924730539322, 0.3421836197376251, -0.24504588544368744, 0.607468843460083]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.01885722205042839, 0.725844144821167, 1.216843843460083, 1.1195913553237915, 0.17799904942512512, 0.6355355978012085, 1.0681997537612915, 1.1862581968307495, 0.47162511944770813, 0.27944475412368774, 0.694982647895813, 0.9207481741905212, 0.8536471128463745, 0.841552734375, 0.5362548828125, 0.87841796875]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6001316905021667, 0.8909254670143127, 0.8716477751731873, 0.3109201192855835, 0.9795297384262085, -0.287403404712677, 1.121675968170166, 2.277719259262085, 1.018704891204834, 0.9151517152786255, 1.141693115234375, 1.212909460067749, -0.13006064295768738, 0.745000958442688, 1.132615327835083, 0.5711787343025208]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6105282306671143, 0.6964111328125, 0.8797607421875, 0.8032602071762085, 0.9322791695594788, 0.5465880036354065, 0.13462477922439575, 1.226731538772583, 0.9147526621818542, 0.7804189920425415, -0.410788893699646, 0.665366530418396, 0.8389317393302917, 1.0258694887161255, 1.1732083559036255, 0.5356463193893433]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.37099340558052063, 1.1647385358810425, 0.8910006284713745, 1.0037935972213745, 0.669647216796875, 0.8192796111106873, 1.1521313190460205, 0.989332914352417, 0.903245210647583, 0.9642428159713745, 0.6753270030021667, -0.3753415644168854, 0.8731208443641663, 0.8824557065963745, 0.808762788772583, 1.1695512533187866]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.6974534392356873, 0.1579812914133072, 0.8273186087608337, 0.8760023713111877, 0.7451887726783752, 0.46622994542121887, 0.1934743970632553, 0.858168363571167, -1.0929219722747803, 2.6567816734313965, -0.9063650369644165, 0.5425228476524353, 0.769606351852417, 0.7809574604034424, 0.848036527633667, 0.3126056492328644]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.11497145146131516, 0.6462554931640625, 1.4130859375, 1.2020474672317505, 1.4160720109939575, 1.3979867696762085, 1.2534555196762085, 1.343703031539917, 1.915865421295166, 1.380892276763916, 2.117863655090332, 1.4465895891189575, 1.4421950578689575, 1.4282790422439575, 0.5772575736045837, 1.245389461517334]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.45361328125, 1.255051851272583, 1.408311128616333, 1.7409104108810425, 0.917067289352417, 1.160475492477417, 1.9818960428237915, 1.6389724016189575, 2.295635461807251, 1.3910006284713745, 1.1357938051223755, 0.776009202003479, 1.242651104927063, 1.01191246509552, 1.3695913553237915, 1.39404296875]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.26416015625, 1.178635835647583, 1.295015811920166, 1.181734561920166, 1.4099684953689575, 0.7815868854522705, 1.028170108795166, 1.102830171585083, 1.822998046875, 1.372802734375, 1.43115234375, 1.3040114641189575, 1.780001163482666, 1.322829008102417, 1.271146297454834, 1.255108118057251]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.088301420211792, 0.9511155486106873, 0.9161189198493958, 0.877793550491333, -0.024107713252305984, 1.2092941999435425, 1.071514368057251, 1.58624267578125, 0.7959641814231873, 1.158916711807251, 1.248046875, 1.1405874490737915, 0.9856520295143127, 1.2266939878463745, 0.8378835916519165, 0.40452104806900024]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.9996243715286255, 2.185096263885498, 2.0048828125, 1.8765774965286255, 1.842059850692749, 2.022536039352417, 1.824782133102417, 2.098520040512085, 1.887432336807251, 1.9628530740737915, 2.255859375, 2.646183967590332, 1.640667200088501, 2.193359375, 2.035118579864502, 2.010066032409668]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.290865421295166, 1.29541015625, 1.845778226852417, 0.9867835640907288, 1.313401460647583, 1.46484375, 1.191819429397583, 1.3091195821762085, 1.394305944442749, 1.4309645891189575, 1.551908016204834, 2.055739164352417, 1.3944936990737915, 1.4525240659713745, 1.058743953704834, 1.544095516204834]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.489595890045166, 1.738356351852417, 0.8399564027786255, 1.3201247453689575, 1.4256685972213745, 1.289963960647583, 1.2145432233810425, 1.221604585647583, 1.3546799421310425, 1.371732234954834, 1.3353365659713745, 1.3886154890060425, 1.0559645891189575, 0.4658760726451874, 1.117356538772583, 1.2486478090286255]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.494741678237915, 2.3720703125, 2.340519905090332, 2.129131555557251, 2.670748233795166, 2.160456657409668, 2.444035530090332, 2.174429178237915, 2.159630298614502, 2.194411039352417, 2.873572826385498, 2.107797384262085, 2.291616678237915, 2.446063756942749, 2.334209680557251, 2.290715217590332]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.5892428159713745, 1.6322115659713745, 1.501126766204834, 1.67822265625, 1.245286226272583, 1.5683969259262085, 1.6121920347213745, 1.994966983795166, 1.614107608795166, 1.5109299421310425, 1.568622350692749, 1.4127103090286255, 1.5059720277786255, 1.4663461446762085, 1.9166165590286255, 1.9332932233810425]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.757493257522583, 1.758217692375183, 1.6345778703689575, 1.669940710067749, 1.632296085357666, 1.735445499420166, 1.632556676864624, 1.66943359375, 1.7145737409591675, 1.816819429397583, 2.222074031829834, 1.569580078125, 1.5586313009262085, 1.877910852432251, 1.867872953414917, 1.684162974357605]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.505934476852417, 1.631669282913208, 1.4996209144592285, 1.663576602935791, 1.503201961517334, 1.525968074798584, 1.704693078994751, 1.613810658454895, 1.5104012489318848, 1.594970703125, 1.2566481828689575, 1.6114455461502075, 1.574444055557251, 1.860884428024292, 1.525240421295166, 1.519773006439209]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.997633695602417, 2.070988655090332, 2.371769905090332, 1.914400577545166, 2.101412296295166, 1.972092866897583, 2.519869327545166, 2.051194429397583, 1.9797927141189575, 1.9224008321762085, 1.909517765045166, 1.7470703125, 1.9848726987838745, 1.75146484375, 2.210712194442749, 1.9398287534713745]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  72%|███████████████████████████████████▍             | 365/504 [20:06<07:13,  3.12s/it]Layer: gate_29 - Captured router_logits: [5.025090217590332, 5.465294361114502, 5.075871467590332, 4.924654483795166, 4.968449592590332, 4.834885597229004, 5.009840965270996, 5.25954008102417, 5.28786039352417, 4.979191780090332, 4.986741065979004, 4.910606861114502, 5.088904857635498, 5.025240421295166, 5.166841983795166, 5.048828125]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.782414436340332, 3.566556453704834, 3.496845006942749, 3.443673849105835, 3.725135326385498, 3.690805196762085, 3.359675407409668, 3.650728702545166, 3.727238655090332, 3.669996976852417, 3.467698335647583, 3.107384204864502, 3.435884952545166, 3.66015625, 3.638221263885498, 3.770207405090332]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.429161548614502, 2.614558219909668, 2.488206148147583, 2.462815523147583, 2.396409273147583, 2.508826732635498, 2.516901969909668, 2.258188009262085, 2.463341236114502, 2.305213451385498, 3.139197826385498, 1.956834077835083, 2.510892391204834, 2.340219259262085, 2.860651969909668, 2.417518138885498]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09189482778310776, 0.10280238091945648, 0.10450821369886398, -0.20747190713882446, -0.14802443981170654, -0.1558147370815277, 0.127044677734375, -0.18024282157421112, 0.07996175438165665, 0.08653798699378967, 0.10237676650285721, 0.060669947415590286, 0.09215275943279266, 0.10276316851377487, -0.8960897922515869, 0.126190647482872]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09310496598482132, 0.06369912624359131, 0.04365400969982147, 0.051076117902994156, 0.08553522080183029, 0.05231183022260666, 0.044125624001026154, 0.05954410880804062, 0.011543504893779755, 0.07703392207622528, -0.16839784383773804, 0.03900516405701637, 0.031782131642103195, -0.019246727228164673, 0.004695622716099024, 0.008124688640236855]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07378680258989334, 0.08707410842180252, 0.11132689565420151, 0.07389769703149796, 0.10176672041416168, 0.11984260380268097, 0.0657791718840599, -0.055291686207056046, 0.07653546333312988, 0.03617188334465027, -0.0002987408952321857, 0.10820230841636658, -0.11336016654968262, 0.049773089587688446, 0.005838413257151842, 0.12749812006950378]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.1349407136440277, 0.18628977239131927, 0.16425608098506927, 0.17780697345733643, 0.12517139315605164, 0.15598459541797638, 0.06867996603250504, 0.2227761596441269, 0.20403282344341278, -0.445035457611084, -0.028300276026129723, 0.11365331709384918, 0.13271431624889374, -0.1395522654056549, -0.13479726016521454, -0.043844204396009445]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.1234876811504364, 0.13964635133743286, 0.10843519866466522, -0.004091127775609493, 0.026141002774238586, -0.08629939705133438, 0.08696106821298599, 0.09445406496524811, 0.016136709600687027, 0.057512689381837845, -0.08195318281650543, 0.18580211699008942, -0.16174377501010895, -0.16151605546474457, 0.12407029420137405, 0.008858420886099339]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.11186379939317703, 0.15496255457401276, 0.09373509138822556, 0.010167131200432777, -0.06893458217382431, 0.025804923847317696, 0.13673019409179688, 0.08425872772932053, 0.08712413907051086, -0.011693781241774559, -0.1325060874223709, 0.10723213851451874, -0.08040549606084824, 0.12463086098432541, 0.20974422991275787, 0.0904475525021553]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.1638026386499405, 0.15826192498207092, 0.05248592048883438, 0.3959442973136902, 0.20268912613391876, 0.1722128540277481, -0.05987425521016121, 0.03698252514004707, 0.22888429462909698, 0.24875648319721222, -0.2444072663784027, 0.3981616199016571, 0.24276086688041687, -0.06927459686994553, 0.1047770157456398, 0.18412087857723236]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.232696071267128, 0.3205139935016632, 0.35885804891586304, 0.20744061470031738, -0.5405717492103577, 0.31039705872535706, 0.1927422434091568, -0.11409289389848709, 0.2032063752412796, 0.036565762013196945, -0.03397192060947418, 0.28140103816986084, 0.35935187339782715, 0.13476808369159698, -0.33045297861099243, -0.08517918735742569]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.38180941343307495, 0.37697654962539673, -0.05248592048883438, 0.3611043393611908, 0.46630367636680603, 0.48158589005470276, 0.30124595761299133, -0.05199339985847473, 0.1845596730709076, 0.3281428813934326, 0.21889010071754456, 0.016810407862067223, 0.48490026593208313, -0.2509765625, -0.3374134302139282, 0.19764848053455353]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.5966081619262695, 0.3777829706668854, 0.43703731894493103, 0.5708575248718262, 0.5289319157600403, 0.4099709987640381, 0.7898911237716675, 0.6119791865348816, 0.5121244192123413, 0.43189093470573425, 0.5674894452095032, 0.8302951455116272, 0.984978437423706, 0.29190927743911743, 0.7707495093345642, 0.8434146046638489]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0893782377243042, 0.9749694466590881, 0.8286576867103577, 0.8974905014038086, 1.0197186470031738, 0.4159078896045685, 0.8203001618385315, 0.771484375, 0.548150897026062, 0.6370713710784912, 0.7909367084503174, 0.8670938014984131, 0.8044260740280151, 0.3568950593471527, 0.5918424725532532, 0.6939783692359924]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.027738332748413, 1.1192001104354858, 0.9581483602523804, 1.1610661745071411, 1.4101908206939697, 0.6398407816886902, 0.9645379781723022, 1.0926156044006348, 0.9230932593345642, 1.1413352489471436, 0.9981849789619446, 1.29241144657135, 0.7057377696037292, 0.8603130578994751, 0.39235371351242065, 1.0240983963012695]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.6025480031967163, 1.0859423875808716, 1.1505188941955566, 1.3186750411987305, 0.8416575193405151, 0.9239168763160706, 0.9126642346382141, 1.1038658618927002, 0.881557285785675, 0.6796770095825195, 1.0188802480697632, 1.0082416534423828, 1.016650915145874, 1.0549044609069824, 0.8407710194587708, 1.1651968955993652]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.0986623764038086, 1.06340754032135, 1.2681305408477783, 0.9971750974655151, 1.2259114980697632, 0.13298697769641876, 1.2863596677780151, 1.6772806644439697, 1.1005958318710327, 1.21784245967865, 1.2919822931289673, 1.5258837938308716, 0.5764665603637695, 1.1376361846923828, 1.4255445003509521, 1.188427209854126]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.9004510641098022, 0.9319168329238892, 1.0378689765930176, 1.0621646642684937, 1.0703125, 0.8714094161987305, 0.622682511806488, 0.9008702635765076, 0.8815991878509521, 1.0048285722732544, -0.11638356000185013, 0.9702838659286499, 1.0327271223068237, 1.1555989980697632, 1.3134765625, 0.8848075270652771]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Running loglikelihood requests:  73%|███████████████████████████████████▉             | 369/504 [20:18<07:03,  3.14s/it]Layer: gate_15 - Captured router_logits: [0.7700170874595642, 1.2694917917251587, 1.191919207572937, 1.0965514183044434, 0.9099762439727783, 1.1002012491226196, 1.1500269174575806, 1.3145911693572998, 1.0125868320465088, 1.1502525806427002, 1.0291193723678589, 0.04569992050528526, 1.1597222089767456, 1.0621646642684937, 1.039693832397461, 1.2264515161514282]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.941347062587738, 0.35901185870170593, 0.9227221012115479, 0.8309313654899597, 0.8570470213890076, 0.7508384585380554, 0.2133004516363144, 0.9898792505264282, -0.4738670885562897, 1.9272855520248413, -0.5527849197387695, 0.7802697420120239, 0.9985795617103577, 0.9359402060508728, 0.931675136089325, 0.3889406621456146]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.5121614336967468, 0.9672820568084717, 1.5228850841522217, 1.5719006061553955, 1.5633482933044434, 1.5356297492980957, 1.503561019897461, 1.7323824167251587, 1.5831360816955566, 1.5679845809936523, 1.960473895072937, 1.5302635431289673, 1.5429452657699585, 1.4102845191955566, 0.8767521381378174, 1.551706075668335]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6472537517547607, 1.597537875175476, 1.7295267581939697, 1.7609690427780151, 1.368852138519287, 1.6060211658477783, 1.659761667251587, 1.9747474193572998, 2.1454782485961914, 1.6588541269302368, 1.4865796566009521, 1.3028329610824585, 1.608862042427063, 1.2897924184799194, 1.7153172492980957, 1.8578755855560303]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.755445122718811, 1.6359690427780151, 1.6241319179534912, 1.7550899982452393, 1.7814078330993652, 1.4075422286987305, 1.6495423316955566, 1.7247868776321411, 1.806226372718811, 1.777738332748413, 1.7705966234207153, 1.708688497543335, 2.340573787689209, 1.7966777086257935, 1.7352430820465088, 1.6913076639175415]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2523945569992065, 1.3703638315200806, 1.2883917093276978, 1.3011054992675781, 0.3854191303253174, 1.3933672904968262, 1.5228654146194458, 1.8801811933517456, 1.0878486633300781, 1.3805437088012695, 1.4917534589767456, 1.2564709186553955, 1.3930516242980957, 1.6408025026321411, 1.3677719831466675, 0.9056313037872314]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.395596504211426, 2.561079502105713, 2.418797254562378, 2.505129337310791, 2.4840593338012695, 2.4232559204101562, 2.340711832046509, 2.4639363288879395, 2.3562183380126953, 2.6249210834503174, 2.6193971633911133, 2.59824800491333, 2.1781880855560303, 2.6976799964904785, 2.6285512447357178, 2.488715171813965]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.901317834854126, 1.663431167602539, 1.9956597089767456, 1.5236150026321411, 1.86328125, 1.8840357065200806, 1.7926530838012695, 1.7520517110824585, 1.9079467058181763, 2.0025646686553955, 1.713817834854126, 2.0482559204101562, 1.950363039970398, 1.897017002105713, 1.8255603313446045, 2.0792691707611084]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.3083176612854004, 1.9112807512283325, 1.6975023746490479, 1.7010732889175415, 1.6969499588012695, 1.8099353313446045, 1.6717764139175415, 1.8330966234207153, 1.7254533767700195, 1.6418876647949219, 1.9314236640930176, 1.6599342823028564, 1.6855665445327759, 1.1915295124053955, 1.700875997543335, 1.780105710029602]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.6285512447357178, 2.6943655014038086, 2.5514519214630127, 2.629892587661743, 2.903803586959839, 2.7591540813446045, 2.833491086959839, 2.5979323387145996, 2.55800199508667, 2.7042298316955566, 2.678267002105713, 2.672269582748413, 2.554924249649048, 2.709280252456665, 2.782512664794922, 2.8006629943847656]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.874487042427063, 1.8293876647949219, 1.8323469161987305, 1.9308712482452393, 1.5750473737716675, 1.684738039970398, 1.8141769170761108, 1.9317392110824585, 1.830591082572937, 1.8092645406723022, 1.8254221677780151, 1.801412582397461, 1.76862370967865, 1.7360717058181763, 1.6700993776321411, 2.075007915496826]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.8410471677780151, 1.7900760173797607, 1.8241201639175415, 1.8773576021194458, 1.9022451639175415, 1.7972966432571411, 1.7590405941009521, 1.7874596118927002, 1.8750489950180054, 1.9622642993927002, 2.091851234436035, 1.7414205074310303, 1.8324159383773804, 1.9181067943572998, 1.7943497896194458, 1.8251607418060303]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.594331979751587, 1.64226496219635, 1.6704802513122559, 1.715425729751587, 1.6190370321273804, 1.6362650394439697, 1.7143357992172241, 1.708175539970398, 1.641939401626587, 1.790847897529602, 1.4793639183044434, 1.7135614156723022, 1.5838955640792847, 1.690328598022461, 1.6415424346923828, 1.7540026903152466]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.277107000350952, 2.3176393508911133, 2.593494415283203, 2.2619752883911133, 2.2926433086395264, 2.27410888671875, 2.4152660369873047, 2.3508028984069824, 2.2922732830047607, 2.2807469367980957, 2.255612850189209, 2.1401121616363525, 2.2974472045898438, 2.2248263359069824, 2.4799513816833496, 2.319330930709839]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.5539774894714355, 5.678898334503174, 5.404671669006348, 5.404750823974609, 5.580650329589844, 5.385416507720947, 5.576704502105713, 5.709990501403809, 5.7418718338012695, 5.568931579589844, 5.5198073387146, 5.411458492279053, 5.4686713218688965, 5.4867424964904785, 5.594381332397461, 5.408222675323486]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.112649917602539, 4.041193008422852, 4.049755573272705, 3.9182448387145996, 3.9561238288879395, 3.9994475841522217, 3.938279390335083, 3.980034828186035, 4.008739948272705, 4.0659918785095215, 4.029947757720947, 3.7703254222869873, 4.073227405548096, 3.923334836959839, 4.068497657775879, 4.046529293060303]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.907236337661743, 3.027975082397461, 2.850142002105713, 2.618213415145874, 2.8539299964904785, 2.9555318355560303, 2.7681503295898438, 2.7529988288879395, 2.8716065883636475, 2.875433921813965, 2.8543245792388916, 2.401538610458374, 2.8818655014038086, 2.8348326683044434, 3.0040245056152344, 2.938328504562378]
Layer: gate_31 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09237416833639145, 0.10581338405609131, 0.10763826966285706, -0.1626116782426834, -0.12253547459840775, -0.15857365727424622, 0.1296611726284027, -0.19222067296504974, 0.09051066637039185, 0.0927971750497818, 0.09850103408098221, 0.07962898910045624, 0.08962411433458328, 0.10152488946914673, -0.8631678223609924, 0.1284099519252777]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09519942849874496, 0.0612223856151104, 0.03578856587409973, 0.04997195675969124, 0.09521669149398804, 0.0695277526974678, 0.06117048114538193, 0.06691452860832214, 0.015292234718799591, 0.07637231796979904, -0.14060449600219727, 0.04801255092024803, 0.030521893873810768, -0.012198747135698795, 0.00954444520175457, 0.013246709480881691]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08749574422836304, 0.08030758798122406, 0.12286099791526794, 0.07761329412460327, 0.1114552840590477, 0.12602989375591278, 0.0793059766292572, -0.048361729830503464, 0.08340739458799362, 0.041588444262742996, 0.0060485973954200745, 0.11461292952299118, -0.1071099191904068, 0.05152053013443947, -0.013546105474233627, 0.1194712296128273]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.14156827330589294, 0.18351298570632935, 0.1625988930463791, 0.18480828404426575, 0.13258855044841766, 0.15357708930969238, 0.046489983797073364, 0.22862783074378967, 0.19400301575660706, -0.4079512655735016, -0.05710547789931297, 0.12995217740535736, 0.13042165338993073, -0.15812668204307556, -0.14423061907291412, -0.05447511002421379]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.1271272897720337, 0.15217845141887665, 0.11623390018939972, -0.0034173522144556046, 0.028047695755958557, -0.06128484383225441, 0.08201298862695694, 0.11066313832998276, -0.005741928704082966, 0.03524903580546379, -0.09954217821359634, 0.20060715079307556, -0.1667129099369049, -0.1629001796245575, 0.13000641763210297, -0.0229051373898983]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.12062358111143112, 0.15276791155338287, 0.07371444255113602, 0.016710147261619568, -0.06026790291070938, 0.021113231778144836, 0.1033373773097992, 0.0661059319972992, 0.06171109154820442, -0.022870920598506927, -0.10432141274213791, 0.09443926811218262, -0.08195079118013382, 0.11709671467542648, 0.1933487355709076, 0.08165077865123749]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.14200907945632935, 0.11465053260326385, 0.0580715611577034, 0.3843019902706146, 0.1866665780544281, 0.15349909663200378, -0.07857875525951385, -0.0031448518857359886, 0.2233988493680954, 0.23564086854457855, -0.24320536851882935, 0.3652564287185669, 0.23178933560848236, -0.05704043433070183, 0.08268783986568451, 0.16547369956970215]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.23978616297245026, 0.36837705969810486, 0.3382457494735718, 0.20456306636333466, -0.5227016806602478, 0.31491395831108093, 0.20929387211799622, -0.19119909405708313, 0.1867501586675644, 0.041845303028821945, -0.07603562623262405, 0.2969967722892761, 0.3456565737724304, 0.1095241978764534, -0.3574354350566864, -0.14395102858543396]
Layer: gate_7 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3791220188140869, 0.3815588057041168, -0.1275443583726883, 0.34428390860557556, 0.4520360827445984, 0.48234790563583374, 0.2784627377986908, -0.06788527220487595, 0.18392512202262878, 0.3153020739555359, 0.22178445756435394, 0.01421440951526165, 0.4722740054130554, -0.24230340123176575, -0.33895474672317505, 0.21815167367458344]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.5763112306594849, 0.3756946623325348, 0.4540601074695587, 0.5568724274635315, 0.5346889495849609, 0.36734718084335327, 0.7963374257087708, 0.6063244938850403, 0.533151388168335, 0.3045826852321625, 0.4229939877986908, 0.7910353541374207, 0.9093708992004395, 0.2962973117828369, 0.7468903064727783, 0.8468177914619446]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1064181327819824, 0.923744261264801, 0.8063299059867859, 0.9015348553657532, 1.0395162105560303, 0.39382225275039673, 0.8245664834976196, 0.7609789371490479, 0.5727751851081848, 0.5925687551498413, 0.8058909177780151, 0.8591924905776978, 0.7651814222335815, 0.3512449860572815, 0.649392306804657, 0.5952370166778564]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0857599973678589, 1.108219027519226, 0.9809619784355164, 1.1600021123886108, 1.4413471221923828, 0.6341540217399597, 0.9407009482383728, 1.1927502155303955, 0.9965154528617859, 1.2064393758773804, 1.082198977470398, 1.3624427318572998, 0.7488896250724792, 0.8590508699417114, 0.4555892050266266, 0.9538648128509521]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.555305540561676, 1.1492857933044434, 1.2648457288742065, 1.3003472089767456, 0.8206858038902283, 1.01799738407135, 0.8920984864234924, 1.080660104751587, 0.9319044947624207, 0.6805210113525391, 0.9996251463890076, 1.0922703742980957, 1.006717562675476, 1.0352548360824585, 0.8354936242103577, 1.2258127927780151]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.0706281661987305, 1.0125670433044434, 1.287084698677063, 0.9591520428657532, 1.2236623764038086, 0.1719021201133728, 1.2555930614471436, 1.584398627281189, 1.0533607006072998, 1.1922348737716675, 1.2321456670761108, 1.4993884563446045, 0.6483672261238098, 1.0942062139511108, 1.4672703742980957, 1.155391812324524]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.0192674398422241, 0.9822147488594055, 1.0884923934936523, 1.1086647510528564, 1.1448370218276978, 1.2243849039077759, 0.7831538319587708, 1.0070295333862305, 0.970619261264801, 1.0880459547042847, 0.08181546628475189, 1.0890251398086548, 1.1420257091522217, 1.2239681482315063, 1.3225764036178589, 0.9755662083625793]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.8921551704406738, 1.316524624824524, 1.2208806276321411, 1.071239709854126, 0.9671223759651184, 1.139599084854126, 1.228738784790039, 1.2655460834503174, 1.0263967514038086, 1.1517518758773804, 1.041597604751587, 0.20504236221313477, 1.21282160282135, 1.0770201683044434, 1.0551215410232544, 1.3839000463485718]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.9666094779968262, 0.47757020592689514, 0.9796820878982544, 0.8622602820396423, 0.7935606241226196, 0.7455018758773804, 0.25202465057373047, 0.9980764389038086, -0.32215219736099243, 2.013726234436035, -0.38765832781791687, 0.8459423184394836, 1.025094747543335, 0.973169207572937, 0.9692259430885315, 0.4610552489757538]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.6502833366394043, 1.128292202949524, 1.5999250411987305, 1.607711672782898, 1.5729166269302368, 1.6049163341522217, 1.575757622718811, 1.7703993320465088, 1.6946417093276978, 1.6102232933044434, 2.0267043113708496, 1.6288273334503174, 1.5299208164215088, 1.417337417602539, 1.0752938985824585, 1.6231578588485718]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.684619665145874, 1.6395399570465088, 1.757398247718811, 1.8246922492980957, 1.4427576065063477, 1.6853101253509521, 1.6880524158477783, 1.9895044565200806, 2.2641208171844482, 1.7324810028076172, 1.6398457288742065, 1.3826349973678589, 1.6956478357315063, 1.3729580640792847, 1.7029671669006348, 1.8898752927780151]
Running loglikelihood requests:  74%|████████████████████████████████████▎            | 373/504 [20:31<06:54,  3.17s/it]Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.8145517110824585, 1.7424637079238892, 1.732027292251587, 1.8053582906723022, 1.8866792917251587, 1.563671350479126, 1.7817234992980957, 1.8795572519302368, 1.9034978151321411, 1.8549163341522217, 1.8961095809936523, 1.8379497528076172, 2.4398279190063477, 1.9035274982452393, 1.836055874824524, 1.7934027910232544]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2900439500808716, 1.3751972913742065, 1.3096787929534912, 1.3543885946273804, 0.4946967363357544, 1.3970564603805542, 1.5471117496490479, 1.95476496219635, 1.1256855726242065, 1.420454502105713, 1.5626380443572998, 1.2685842514038086, 1.4470881223678589, 1.6024305820465088, 1.368045687675476, 0.9579307436943054]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.462989330291748, 2.6211330890655518, 2.4817707538604736, 2.5396149158477783, 2.559382915496826, 2.44981050491333, 2.3950440883636475, 2.505760669708252, 2.421480417251587, 2.6742424964904785, 2.663273334503174, 2.6575915813446045, 2.1875789165496826, 2.7723326683044434, 2.673532247543335, 2.559422254562378]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.9681975841522217, 1.7027698755264282, 2.0817551612854004, 1.6340948343276978, 1.906881332397461, 1.948705792427063, 1.9255445003509521, 1.8380681276321411, 1.932449460029602, 2.0490846633911133, 1.800899624824524, 2.0930397510528564, 2.0073390007019043, 1.9481533765792847, 1.9115372896194458, 2.120896577835083]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.291903495788574, 1.9214015007019043, 1.6630563735961914, 1.7106612920761108, 1.7541429996490479, 1.841382622718811, 1.6841856241226196, 1.8616634607315063, 1.7529197931289673, 1.6400331258773804, 1.9399462938308716, 1.6852706670761108, 1.755563497543335, 1.1639046669006348, 1.6899068355560303, 1.781644582748413]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.6860005855560303, 2.709280252456665, 2.6025094985961914, 2.675584077835083, 2.9407355785369873, 2.8010573387145996, 2.857086420059204, 2.6491477489471436, 2.5965120792388916, 2.7562341690063477, 2.7357165813446045, 2.687263250350952, 2.5884628295898438, 2.7464487552642822, 2.7562341690063477, 2.8693182468414307]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.9304766654968262, 1.8632417917251587, 1.8742108345031738, 1.959793210029602, 1.5762704610824585, 1.747119665145874, 1.8552320003509521, 1.9687105417251587, 1.8626893758773804, 1.8549952507019043, 1.831912875175476, 1.8536142110824585, 1.8259154558181763, 1.814985752105713, 1.7296795845031738, 2.077730417251587]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.876144289970398, 1.821337103843689, 1.9078677892684937, 1.8962279558181763, 1.943241000175476, 1.8097798824310303, 1.8094655275344849, 1.82331120967865, 1.9026778936386108, 1.9793639183044434, 2.112018585205078, 1.7685743570327759, 1.8686869144439697, 1.9245778322219849, 1.8426920175552368, 1.8100597858428955]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5591763257980347, 1.6365978717803955, 1.6719834804534912, 1.7111446857452393, 1.6369949579238892, 1.6251356601715088, 1.7106661796569824, 1.6933248043060303, 1.6428099870681763, 1.7564117908477783, 1.467211127281189, 1.708589792251587, 1.5526480674743652, 1.6440775394439697, 1.6235893964767456, 1.781782627105713]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_28 - Captured router_logits: [2.2515783309936523, 2.302241086959839, 2.542179584503174, 2.2645201683044434, 2.2537484169006348, 2.2458176612854004, 2.3977668285369873, 2.312697172164917, 2.279829502105713, 2.303819417953491, 2.2478692531585693, 2.1169114112854004, 2.257087469100952, 2.256589412689209, 2.446732997894287, 2.3000710010528564]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.6448073387146, 5.7421088218688965, 5.46875, 5.558396339416504, 5.699731826782227, 5.482796669006348, 5.583885669708252, 5.7542219161987305, 5.823232173919678, 5.707820415496826, 5.5562262535095215, 5.548768997192383, 5.582189083099365, 5.58262300491333, 5.618528842926025, 5.529040336608887]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.2445549964904785, 4.107362747192383, 4.134903907775879, 4.072460651397705, 4.102667331695557, 4.113478660583496, 4.104067802429199, 4.099274158477783, 4.152521133422852, 4.193142414093018, 4.1178975105285645, 3.8320705890655518, 4.194385051727295, 4.074652671813965, 4.150134086608887, 4.167475700378418]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1316683292388916, 3.21875, 2.9207701683044434, 2.6820549964904785, 2.893702745437622, 3.0782432556152344, 2.8549559116363525, 2.870896577835083, 3.0120737552642822, 3.012941837310791, 2.9661457538604736, 2.5747268199920654, 3.0500316619873047, 2.9809815883636475, 3.1162405014038086, 3.1296558380126953]
Layer: gate_31 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06303055584430695, 0.07825925201177597, 0.09674602001905441, -0.25865063071250916, -0.24293237924575806, -0.08423381298780441, 0.10206288844347, -0.07228928804397583, 0.07922635972499847, 0.06570170074701309, 0.07487799227237701, 0.045981425791978836, 0.07616191357374191, 0.09769626706838608, -1.02787184715271, 0.10152076929807663]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08469811081886292, 0.04100530967116356, 0.03090200573205948, 0.042362600564956665, 0.06736724078655243, 0.03221854567527771, 0.06091370806097984, 0.08908812701702118, 0.057470593601465225, 0.05773567780852318, -0.1882694810628891, 0.05304570123553276, 0.011717504821717739, -0.023120490834116936, 0.037968847900629044, 0.03966740146279335]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0884936973452568, 0.03862785920500755, 0.0894428938627243, 0.07385752350091934, 0.14554035663604736, 0.09128477424383163, 0.0625200867652893, -0.08692722022533417, 0.08588206768035889, 0.1317775547504425, 0.03456147015094757, 0.08589094877243042, -0.19294583797454834, 0.010033121332526207, -0.01823456399142742, 0.1050572320818901]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1428835391998291, 0.14346438646316528, 0.11783568561077118, 0.14980576932430267, 0.14183387160301208, 0.1269816905260086, -0.021411817520856857, 0.18280278146266937, 0.1791451871395111, -0.5205439329147339, 0.012599225156009197, 0.0837162584066391, 0.2797424793243408, -0.3232920169830322, 0.05327466130256653, -0.08094507455825806]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06066785380244255, 0.1011241227388382, 0.08821238577365875, 0.21797320246696472, 0.051019005477428436, -0.03967456519603729, 0.030685678124427795, 0.030117755755782127, -0.11113552004098892, 0.0161612369120121, -0.24369314312934875, 0.12975598871707916, 0.05530610308051109, -0.218826562166214, 0.14455273747444153, -0.11545609682798386]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.05753155052661896, 0.1594468653202057, 0.06639141589403152, 0.13750535249710083, -0.11346498131752014, -0.10714317113161087, 0.13688348233699799, 0.0045844875276088715, 0.19326922297477722, -0.051756568253040314, -0.0732487291097641, 0.10664764791727066, -0.22839729487895966, 0.17787015438079834, 0.13292196393013, 0.0943869799375534]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11530046910047531, -0.0027827047742903233, 0.11793985217809677, 0.3241097629070282, 0.17040953040122986, 0.16869120299816132, -0.19835366308689117, -0.07235094904899597, 0.36382558941841125, 0.16669884324073792, -0.5485995411872864, 0.3327104151248932, 0.24134795367717743, -0.29596275091171265, 0.2616368532180786, 0.22008296847343445]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.49961572885513306, 0.19112232327461243, 0.2211233675479889, 0.1572932004928589, -0.8221211433410645, 0.3367930054664612, 0.19030699133872986, -0.3975232243537903, 0.5553591847419739, 0.16551800072193146, -0.22835992276668549, 0.27566808462142944, 0.1939697265625, 0.2411765307188034, -0.7092708945274353, -0.27311334013938904]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2727748453617096, 0.2777697443962097, -0.27170732617378235, 0.33867910504341125, 0.4987033009529114, 0.3514242470264435, 0.2718318998813629, -0.2687543034553528, 0.1804582178592682, 0.7273989319801331, 0.1322012096643448, 0.2855735421180725, 0.48030930757522583, -0.5444478988647461, -0.5849011540412903, 0.37214192748069763]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4286237359046936, 0.22012515366077423, 0.4820232689380646, 0.9771105647087097, 0.4189646244049072, 0.2020665407180786, 0.5745824575424194, 0.564851701259613, 0.7553275227546692, 0.20178674161434174, 0.18644309043884277, 0.7166872620582581, 0.7958452701568604, 0.04083874821662903, 0.7851263284683228, 0.6508290767669678]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0794391632080078, 0.9133450388908386, 0.57659912109375, 0.7322574257850647, 0.8406260013580322, 0.6771246194839478, 0.6554702520370483, 0.7382314205169678, 0.14308680593967438, 0.5175849795341492, 0.7726303339004517, 0.986661970615387, 0.5489346385002136, 0.18353271484375, 0.8708570599555969, 0.47201475501060486]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0599191188812256, 1.2900938987731934, 0.9167131781578064, 1.2436797618865967, 1.4423292875289917, 0.2965858578681946, 1.0459283590316772, 0.9754165410995483, 1.0529735088348389, 1.5215342044830322, 0.9091921448707581, 0.9540318250656128, 0.35690903663635254, 0.5366936326026917, 0.04432398080825806, 0.7927843928337097]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.07192432880401611, 0.8210000991821289, 1.2986985445022583, 1.1988600492477417, 0.41340014338493347, 0.5708506107330322, 1.1635518074035645, 1.2636120319366455, 0.43559545278549194, 0.2418050915002823, 0.8803598880767822, 0.8303771018981934, 0.9225227236747742, 0.929707407951355, 0.7812898755073547, 0.9362244606018066]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8631815910339355, 1.0440648794174194, 1.14554762840271, 0.5055859684944153, 1.1108896732330322, -0.12365909665822983, 1.1501115560531616, 2.0084104537963867, 1.1479392051696777, 0.9852519035339355, 1.43701171875, 1.2969746589660645, 0.21889199316501617, 0.9323727488517761, 1.0863759517669678, 0.730436384677887]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8018425107002258, 0.7804627418518066, 1.0688974857330322, 0.9186463356018066, 1.080098032951355, 1.02876877784729, 0.40110310912132263, 1.3360319137573242, 0.7096320986747742, 0.8406097888946533, -0.2471456676721573, 0.7235082387924194, 0.9336756467819214, 1.0994699001312256, 1.2049684524536133, 0.46629053354263306]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.40036165714263916, 1.2934470176696777, 1.0828484296798706, 1.0132732391357422, 0.8537846803665161, 0.9795718789100647, 1.4909169673919678, 1.1133809089660645, 1.14058518409729, 1.0945870876312256, 0.7624732851982117, -0.30017587542533875, 1.1331812143325806, 0.9627710580825806, 0.987882673740387, 1.4594377279281616]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6959178447723389, 0.16580496728420258, 0.7655826210975647, 0.8071886897087097, 0.7026666402816772, 0.40562188625335693, 0.5029611587524414, 0.8669682741165161, -0.9359068870544434, 2.317781448364258, -0.9030512571334839, 0.5326550602912903, 0.7867406606674194, 0.8664600849151611, 0.7758489847183228, 0.36558470129966736]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.11145953834056854, 0.8174799680709839, 1.5445033311843872, 1.4268823862075806, 1.5634167194366455, 1.5189133882522583, 1.53153395652771, 1.4179787635803223, 1.9950623512268066, 1.5311304330825806, 2.435865640640259, 1.6928819417953491, 1.4588804244995117, 1.4133650064468384, 0.5043073296546936, 1.442115306854248]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.548030972480774, 1.3259327411651611, 1.5935207605361938, 1.9093191623687744, 1.0191326141357422, 1.260562777519226, 2.1011240482330322, 1.8199138641357422, 2.51179838180542, 1.5326051712036133, 1.2339465618133545, 0.8415103554725647, 1.2916234731674194, 1.108597755432129, 1.6043527126312256, 1.5807956457138062]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.5940688848495483, 1.397441029548645, 1.745296597480774, 1.4632095098495483, 1.7297911643981934, 1.1099729537963867, 1.4495177268981934, 1.3824338912963867, 2.0688178539276123, 1.4910913705825806, 1.804667592048645, 1.6200374364852905, 2.042769432067871, 1.6702805757522583, 1.6099729537963867, 1.6144969463348389]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.285380482673645, 1.0770986080169678, 0.987703263759613, 0.9213368892669678, -0.07725276052951813, 1.3916614055633545, 1.1950334310531616, 1.615153431892395, 0.7274007797241211, 1.3040298223495483, 1.2942841053009033, 1.1402711868286133, 1.1585220098495483, 1.3774313926696777, 0.8577208518981934, 0.4716653525829315]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.21085786819458, 2.3997926712036133, 2.3833706378936768, 2.1533801555633545, 2.133171319961548, 2.2589285373687744, 2.0659677982330322, 2.3755581378936768, 2.099928140640259, 2.2291932106018066, 2.482541561126709, 2.810427188873291, 1.803531527519226, 2.4872448444366455, 2.2800540924072266, 2.2680165767669678]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4966119527816772, 1.4652423858642578, 2.1302614212036133, 1.0777313709259033, 1.5056202411651611, 1.6563297510147095, 1.3249362707138062, 1.4544801712036133, 1.5690369606018066, 1.6987005472183228, 1.6407843828201294, 2.1666135787963867, 1.5553650856018066, 1.631855845451355, 1.255679965019226, 1.7230548858642578]
Running loglikelihood requests:  75%|████████████████████████████████████▋            | 377/504 [20:44<06:42,  3.17s/it]Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.468989133834839, 1.89847731590271, 1.032027244567871, 1.4762834310531616, 1.6029576063156128, 1.4638073444366455, 1.4162946939468384, 1.3771125078201294, 1.4723174571990967, 1.4418845176696777, 1.528140902519226, 1.5240354537963867, 1.1860650777816772, 0.588826060295105, 1.258968472480774, 1.4592235088348389]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.4507334232330322, 2.461176633834839, 2.4192442893981934, 2.1882972717285156, 2.6688456535339355, 2.2841198444366455, 2.4598214626312256, 2.2097415924072266, 2.120854616165161, 2.2625956535339355, 2.8525989055633545, 2.1783323287963867, 2.3871970176696777, 2.5261478424072266, 2.3099489212036133, 2.328443765640259]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7221380472183228, 1.7422672510147095, 1.6323740482330322, 1.8782286643981934, 1.3415577411651611, 1.6869419813156128, 1.756058692932129, 2.1315767765045166, 1.7510364055633545, 1.674625277519226, 1.7097417116165161, 1.5835459232330322, 1.7266422510147095, 1.5746970176696777, 2.1198580265045166, 2.0922751426696777]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8377312421798706, 2.0044867992401123, 1.8196747303009033, 1.764469027519226, 1.7957987785339355, 1.9341940879821777, 1.8034206628799438, 1.7803930044174194, 1.8688217401504517, 2.0316684246063232, 2.4747886657714844, 1.7176040410995483, 1.7668606042861938, 1.9221141338348389, 1.8977299928665161, 1.8582663536071777]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6845154762268066, 1.7457325458526611, 1.6783223152160645, 1.7507623434066772, 1.6137795448303223, 1.6596405506134033, 1.8668524026870728, 1.7529196739196777, 1.6400470733642578, 1.7128407955169678, 1.3473373651504517, 1.6758111715316772, 1.7635971307754517, 2.0874969959259033, 1.6622687578201294, 1.7299057245254517]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.158601760864258, 2.2495217323303223, 2.5209662914276123, 2.1120853424072266, 2.228914260864258, 2.1766183376312256, 2.6962292194366455, 2.2742347717285156, 2.1549744606018066, 2.1238839626312256, 2.1353635787963867, 1.9161351919174194, 2.097337484359741, 1.9831792116165161, 2.4309630393981934, 2.0892059803009033]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.106106281280518, 5.543686389923096, 5.068558692932129, 4.872428894042969, 5.035355567932129, 5.003507614135742, 5.05875301361084, 5.2120137214660645, 5.317920684814453, 5.078603267669678, 4.996133804321289, 4.973931789398193, 5.127331733703613, 5.1337690353393555, 5.2568559646606445, 5.0242743492126465]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.6340084075927734, 3.4732940196990967, 3.36324143409729, 3.2434730529785156, 3.5064175128936768, 3.543207883834839, 3.2407724857330322, 3.6216518878936768, 3.63480544090271, 3.532924175262451, 3.347257614135742, 2.964891195297241, 3.2661631107330322, 3.51961088180542, 3.527224063873291, 3.5335419178009033]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.552535057067871, 2.742027997970581, 2.7020089626312256, 2.6201372146606445, 2.581712484359741, 2.5966997146606445, 2.6650989055633545, 2.2669005393981934, 2.65625, 2.339963436126709, 3.159996747970581, 1.9105149507522583, 2.630500555038452, 2.499800682067871, 3.105707883834839, 2.583785057067871]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.05209916830062866, 0.06352792680263519, 0.08352000266313553, -0.2518913149833679, -0.23902876675128937, -0.07242175191640854, 0.08408583700656891, -0.013378654606640339, 0.07150457054376602, 0.04797080159187317, 0.057127803564071655, 0.04630814492702484, 0.06427615135908127, 0.08149562031030655, -0.977861225605011, 0.08827130496501923]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.0781765952706337, 0.037547603249549866, 0.026630587875843048, 0.037380099296569824, 0.0669071152806282, 0.028200248256325722, 0.0514443963766098, 0.09172891825437546, 0.05218910798430443, 0.06186778098344803, -0.1839568167924881, 0.05291067808866501, 0.010414988733828068, -0.028852030634880066, 0.030908683314919472, 0.04267332702875137]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08502433449029922, 0.0372505784034729, 0.0917334035038948, 0.07385890930891037, 0.14005260169506073, 0.08283713459968567, 0.07136464864015579, -0.08313019573688507, 0.0759282037615776, 0.12299378216266632, 0.03189627826213837, 0.08065646141767502, -0.18058769404888153, 0.006760508753359318, -0.012976577505469322, 0.1019449532032013]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13345903158187866, 0.15106311440467834, 0.11893722414970398, 0.1447458118200302, 0.14983564615249634, 0.13650661706924438, -0.028189433738589287, 0.17333008348941803, 0.16815060377120972, -0.5098247528076172, 0.03125503286719322, 0.07569900900125504, 0.3069944977760315, -0.3308684229850769, 0.0643831267952919, -0.053929202258586884]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.05082985758781433, 0.104760080575943, 0.08400765806436539, 0.24432231485843658, 0.06054089590907097, -0.041385140269994736, 0.017671644687652588, 0.02840714529156685, -0.11291173845529556, 0.014658190310001373, -0.2431366890668869, 0.12921111285686493, 0.07342057675123215, -0.19757890701293945, 0.14370067417621613, -0.12430454790592194]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.03909793496131897, 0.14371418952941895, 0.0497494712471962, 0.1789003312587738, -0.13811689615249634, -0.07917329668998718, 0.13210272789001465, 0.0073561519384384155, 0.21149924397468567, -0.07554296404123306, -0.02056916244328022, 0.10173600912094116, -0.22115837037563324, 0.16773027181625366, 0.12828095257282257, 0.08891697227954865]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.13036373257637024, -0.046739716082811356, 0.12940746545791626, 0.3302895426750183, 0.1543462723493576, 0.14946502447128296, -0.19301439821720123, -0.0874413549900055, 0.3339157998561859, 0.16327470541000366, -0.5608086585998535, 0.3189432919025421, 0.23671573400497437, -0.3168976902961731, 0.28997787833213806, 0.2128785103559494]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.5277816653251648, 0.19827301800251007, 0.19970372319221497, 0.17413078248500824, -0.8460655808448792, 0.313839316368103, 0.20420821011066437, -0.4335799217224121, 0.5713790655136108, 0.1835726648569107, -0.24446463584899902, 0.26842600107192993, 0.1529204398393631, 0.23976749181747437, -0.6825945377349854, -0.32021668553352356]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2942536473274231, 0.24517445266246796, -0.3181605339050293, 0.3301234245300293, 0.508005678653717, 0.3178585171699524, 0.2502257227897644, -0.30231529474258423, 0.2011099010705948, 0.7897285223007202, 0.12042621523141861, 0.2710225284099579, 0.4832172095775604, -0.49383607506752014, -0.5711519122123718, 0.39977994561195374]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3941996395587921, 0.27067330479621887, 0.5191335678100586, 1.024575114250183, 0.3885359764099121, 0.16086727380752563, 0.5512544512748718, 0.5990757942199707, 0.7917952537536621, 0.16809459030628204, 0.18733640015125275, 0.6656733155250549, 0.7474997639656067, 0.19264960289001465, 0.7743536829948425, 0.6451554298400879]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1786102056503296, 0.8794750571250916, 0.5466333627700806, 0.7101472020149231, 0.7888032793998718, 0.66957688331604, 0.6482235789299011, 0.7537854313850403, 0.1719970703125, 0.49417051672935486, 0.7623429298400879, 0.9564725160598755, 0.3798019587993622, 0.17967869341373444, 0.9182761311531067, 0.43686574697494507]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0692654848098755, 1.3030905723571777, 0.9634544253349304, 1.2703070640563965, 1.3884050846099854, 0.23664148151874542, 1.0396968126296997, 0.9393172264099121, 1.0273840427398682, 1.5956528186798096, 0.8946722149848938, 0.9442956447601318, 0.38943198323249817, 0.4971533715724945, 0.016094405204057693, 0.7468689680099487]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.06646728515625, 0.7585877180099487, 1.2385430335998535, 1.1988563537597656, 0.3503669798374176, 0.619245707988739, 1.193514108657837, 1.2981863021850586, 0.35621753334999084, 0.17401406168937683, 0.8085371255874634, 0.8467904329299927, 0.8854812383651733, 0.890947163105011, 0.6664686799049377, 0.8755235075950623]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7012423276901245, 0.9767436981201172, 1.0546270608901978, 0.18458856642246246, 1.0505396127700806, -0.3407275080680847, 1.0856958627700806, 2.0390625, 1.0884342193603516, 0.8770537972450256, 1.3477871417999268, 1.1363362073898315, 0.04843800514936447, 0.7555637955665588, 1.3402262926101685, 0.49793219566345215]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.734775185585022, 0.7439392805099487, 1.0381160974502563, 0.7885389924049377, 1.0344111919403076, 0.956258237361908, 0.3770487606525421, 1.4137277603149414, 0.7006080746650696, 0.6835786700248718, -0.4056534767150879, 0.6332630515098572, 0.8855005502700806, 1.0270416736602783, 1.1240234375, 0.3802588880062103]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.30327174067497253, 1.2464762926101685, 0.9814956784248352, 0.9809419512748718, 0.7591238021850586, 0.8520054817199707, 1.5101696252822876, 1.027867317199707, 0.9868113994598389, 1.0017719268798828, 0.6330339908599854, -0.42545655369758606, 1.0453320741653442, 0.801767885684967, 0.8741341829299927, 1.2548928260803223]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6134650111198425, 0.03394514322280884, 0.713168740272522, 0.8229124546051025, 0.7452279329299927, 0.3360668122768402, 0.3487485945224762, 0.8177131414413452, -1.0968002080917358, 2.174522876739502, -1.0536599159240723, 0.40442606806755066, 0.7726019024848938, 0.8318147659301758, 0.7841092348098755, 0.10610678791999817]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.10804717242717743, 0.7296180129051208, 1.421703815460205, 1.251444697380066, 1.4308151006698608, 1.4968388080596924, 1.3774967193603516, 1.3040331602096558, 1.9400194883346558, 1.3249813318252563, 2.254429817199707, 1.5854114294052124, 1.3854905366897583, 1.334648847579956, 0.37380698323249817, 1.268356442451477]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.397148847579956, 1.2485904693603516, 1.4574742317199707, 1.7763772010803223, 0.7878770232200623, 1.0745609998703003, 1.8951352834701538, 1.666800856590271, 2.3921754360198975, 1.4144651889801025, 1.0347334146499634, 0.5884380340576172, 1.0714199542999268, 0.8004238605499268, 1.3616703748703003, 1.402988076210022]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.395316481590271, 1.278672695159912, 1.5220481157302856, 1.2757128477096558, 1.5635470151901245, 0.8888656497001648, 1.2110382318496704, 1.2107059955596924, 1.8573917150497437, 1.3930211067199707, 1.6299532651901245, 1.4584407806396484, 1.809781789779663, 1.462729573249817, 1.4739046096801758, 1.4712870121002197]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.113110065460205, 0.9633839130401611, 0.9042364954948425, 0.7947607636451721, -0.2410784810781479, 1.240858554840088, 1.0634665489196777, 1.499457597732544, 0.6900118589401245, 1.205219030380249, 1.1773921251296997, 1.0942282676696777, 0.9629711508750916, 1.2316768169403076, 0.7101723551750183, 0.24679313600063324]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0792524814605713, 2.2566850185394287, 2.1615657806396484, 1.9232845306396484, 1.8862757682800293, 2.119523286819458, 1.9143847227096558, 2.150773286819458, 1.9946037530899048, 2.035921335220337, 2.3490657806396484, 2.661163091659546, 1.6138852834701538, 2.3404479026794434, 2.0926225185394287, 2.0750644207000732]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3889739513397217, 1.4292848110198975, 2.023759603500366, 0.9851199984550476, 1.4501450061798096, 1.5942332744598389, 1.2273075580596924, 1.4257409572601318, 1.4820393323898315, 1.590991497039795, 1.6187983751296997, 2.07973575592041, 1.4650853872299194, 1.5485663414001465, 1.1637907028198242, 1.666680097579956]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.327641725540161, 1.8426626920700073, 0.9718005061149597, 1.3894973993301392, 1.587749719619751, 1.3649725914001465, 1.4066929817199707, 1.3021504878997803, 1.445815920829773, 1.3910679817199707, 1.3928802013397217, 1.443500280380249, 1.0905888080596924, 0.47962746024131775, 1.1170063018798828, 1.3954172134399414]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.302029609680176, 2.3022713661193848, 2.25757098197937, 1.9807506799697876, 2.4300901889801025, 2.067171335220337, 2.2880959510803223, 1.9889658689498901, 1.953004240989685, 2.0323774814605713, 2.73598575592041, 1.9383859634399414, 2.2139174938201904, 2.3505959510803223, 2.1415109634399414, 2.1399002075195312]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6204091310501099, 1.6332151889801025, 1.5129671096801758, 1.774605393409729, 1.2328648567199707, 1.642598271369934, 1.6559277772903442, 2.046834707260132, 1.672680377960205, 1.5341092348098755, 1.6118717193603516, 1.455581545829773, 1.637242317199707, 1.4521182775497437, 2.034592390060425, 1.9650450944900513]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  76%|█████████████████████████████████████            | 381/504 [20:57<06:28,  3.16s/it]Layer: gate_26 - Captured router_logits: [1.7661887407302856, 1.9196664094924927, 1.7040512561798096, 1.6982120275497437, 1.7315762042999268, 1.8820900917053223, 1.791763186454773, 1.7021785974502563, 1.8090568780899048, 1.882953405380249, 2.439876079559326, 1.684882402420044, 1.684399127960205, 1.892417073249817, 1.877212405204773, 1.8021994829177856]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6297167539596558, 1.6819803714752197, 1.6272400617599487, 1.670098066329956, 1.5371496677398682, 1.5626240968704224, 1.8316285610198975, 1.7123923301696777, 1.5861866474151611, 1.6563204526901245, 1.287089228630066, 1.618637204170227, 1.7016501426696777, 2.0455007553100586, 1.6107391119003296, 1.611992597579956]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0188467502593994, 2.088836908340454, 2.4191365242004395, 1.996858835220337, 2.117670774459839, 2.066688060760498, 2.5808634757995605, 2.1751770973205566, 2.009665012359619, 1.9879993200302124, 1.9902948141098022, 1.7912774085998535, 1.969102382659912, 1.8414143323898315, 2.3352530002593994, 1.9739046096801758]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.8686370849609375, 5.354059219360352, 4.908907890319824, 4.673264503479004, 4.740334987640381, 4.8017072677612305, 4.886919975280762, 4.993919372558594, 5.112838268280029, 4.815077304840088, 4.777424335479736, 4.793734073638916, 4.819164752960205, 4.982120037078857, 5.054606914520264, 4.841414451599121]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.4901740550994873, 3.341414213180542, 3.2097294330596924, 3.0708260536193848, 3.3579251766204834, 3.392719030380249, 3.001288652420044, 3.4563868045806885, 3.5082151889801025, 3.323896646499634, 3.2014739513397217, 2.7567150592803955, 3.0809037685394287, 3.3574018478393555, 3.369241237640381, 3.369563579559326]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.5274243354797363, 2.64682674407959, 2.701594829559326, 2.5918169021606445, 2.5779638290405273, 2.550579786300659, 2.640625, 2.2330057621002197, 2.5703930854797363, 2.302532911300659, 3.1430814266204834, 1.7544348239898682, 2.593427896499634, 2.5024967193603516, 3.1105830669403076, 2.5167524814605713]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.08807047456502914, 0.09905000776052475, 0.10804113000631332, -0.23920106887817383, -0.22637629508972168, -0.08921623229980469, 0.12426980584859848, -0.11506366729736328, 0.08168681710958481, 0.08611228317022324, 0.10117129236459732, 0.07116010040044785, 0.09617948532104492, 0.10957685858011246, -1.0446103811264038, 0.11817105859518051]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08915630728006363, 0.05409685894846916, 0.042182207107543945, 0.05147051811218262, 0.07697073370218277, 0.03557540848851204, 0.05090057849884033, 0.07011870294809341, 0.020560264587402344, 0.08168760687112808, -0.199868842959404, 0.04881604388356209, 0.03534984588623047, -0.01700989343225956, 0.035208702087402344, 0.012494444847106934]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.086781345307827, 0.0692429319024086, 0.09876338392496109, 0.08404549211263657, 0.10602998733520508, 0.12043508142232895, 0.07241598516702652, -0.08124621957540512, 0.0925157442688942, 0.07984733581542969, 0.03710607811808586, 0.0916045531630516, -0.20087242126464844, 0.04376626014709473, -0.04575236514210701, 0.1269078254699707]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13615266978740692, 0.14109516143798828, 0.13368351757526398, 0.15719254314899445, 0.14844290912151337, 0.11063822358846664, 0.015419642440974712, 0.20121829211711884, 0.19591665267944336, -0.5007527470588684, 0.0334140844643116, 0.09312693029642105, 0.19573481380939484, -0.23022206127643585, -0.022624650970101357, -0.05149618908762932]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08113765716552734, 0.09113168716430664, 0.10637608915567398, 0.13009579479694366, 0.0616401843726635, -0.029986143112182617, 0.03331724926829338, 0.05949580669403076, -0.06813939660787582, 0.030737241730093956, -0.19289033114910126, 0.131487175822258, -0.04364204406738281, -0.1938740760087967, 0.13533823192119598, -0.05650520324707031]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_5 - Captured router_logits: [0.03891865536570549, 0.14604265987873077, 0.08011087030172348, 0.1037842258810997, -0.09287738800048828, -0.08233451843261719, 0.14871597290039062, 0.007030805107206106, 0.10494252294301987, -0.040469009429216385, 0.020692506805062294, 0.08516937494277954, -0.23819606006145477, 0.1419283151626587, 0.1374496966600418, 0.07699660211801529]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.16657190024852753, -0.022225379943847656, 0.1301136016845703, 0.3792050778865814, 0.1983439177274704, 0.1456807404756546, -0.15300114452838898, -0.10444959253072739, 0.3815635144710541, 0.16828949749469757, -0.5453770756721497, 0.3158297538757324, 0.18032582104206085, -0.32252058386802673, 0.16609318554401398, 0.21358244121074677]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3627047538757324, 0.2132633477449417, 0.26874080300331116, 0.11792818456888199, -0.7140731811523438, 0.35898423194885254, 0.18371374905109406, -0.30832576751708984, 0.45481300354003906, 0.2446940690279007, -0.23658592998981476, 0.21696121990680695, 0.27250781655311584, 0.2050231248140335, -0.6674105525016785, -0.10733985900878906]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.24954938888549805, 0.33618292212486267, -0.26052984595298767, 0.3049143850803375, 0.4972381591796875, 0.2847200930118561, 0.23661081492900848, -0.3072592318058014, 0.22541093826293945, 0.6779190897941589, 0.13163597881793976, 0.2753365933895111, 0.42135491967201233, -0.5898920893669128, -0.6050176620483398, 0.32955145835876465]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4178721010684967, 0.1831188201904297, 0.47869110107421875, 0.9684829711914062, 0.3932221829891205, 0.20144490897655487, 0.4890540540218353, 0.5489787459373474, 0.5435559153556824, 0.25041070580482483, 0.20737330615520477, 0.6988728642463684, 0.9169381260871887, 0.0198949184268713, 0.6692759394645691, 0.617095947265625]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0657488107681274, 0.8689422607421875, 0.6513569951057434, 0.7596537470817566, 0.9427058100700378, 0.5442025065422058, 0.5831858515739441, 0.709136962890625, 0.11963780969381332, 0.4238497316837311, 0.7294921875, 0.8505604863166809, 0.5159791111946106, 0.29133495688438416, 0.7146231532096863, 0.4141489565372467]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9065755009651184, 1.0590108633041382, 0.9255473017692566, 1.0426229238510132, 1.309277892112732, 0.3612556457519531, 0.9931869506835938, 0.8330866694450378, 0.9566141963005066, 1.265960693359375, 0.8337910771369934, 0.7781829833984375, 0.31587091088294983, 0.5458129048347473, -0.16861343383789062, 0.7142130732536316]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.1511872559785843, 0.8040516972541809, 1.1795247793197632, 1.1560262441635132, 0.42620721459388733, 0.5484161376953125, 0.9484062194824219, 1.1673812866210938, 0.5448163151741028, 0.389923095703125, 0.8223876953125, 0.9009806513786316, 0.8804753422737122, 0.9163716435432434, 0.8706156611442566, 0.9214884638786316]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6275126338005066, 0.8088531494140625, 1.0118112564086914, 0.4046255648136139, 0.9136860966682434, -0.26963362097740173, 1.02587890625, 1.7072957754135132, 0.8869120478630066, 0.8487955927848816, 1.1032193899154663, 1.166748046875, 0.001529693603515625, 0.7737579345703125, 0.9331502914428711, 0.45944151282310486]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6465994715690613, 0.7538655400276184, 0.8089802861213684, 0.7213058471679688, 0.9353840947151184, 0.5651194453239441, 0.1481831818819046, 0.9688059687614441, 0.6218058466911316, 0.8220875859260559, -0.43669891357421875, 0.5273405909538269, 0.8657296299934387, 0.9843342900276184, 1.12042236328125, 0.3683217465877533]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.17690658569335938, 1.147705078125, 0.8966267704963684, 0.86175537109375, 0.6286061406135559, 0.8188069462776184, 1.0643672943115234, 0.8913167119026184, 0.8938395380973816, 0.9144694209098816, 0.63702392578125, -0.3310629427433014, 0.8641033172607422, 0.7907918095588684, 0.8772786259651184, 1.174844741821289]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7075373530387878, 0.119232177734375, 0.8314208984375, 0.8539683222770691, 0.7810618281364441, 0.44656500220298767, 0.15289370715618134, 0.7879638671875, -1.0079091787338257, 2.5081584453582764, -1.0421873331069946, 0.5343494415283203, 0.8468068242073059, 0.8780050277709961, 0.8532231450080872, 0.12835121154785156]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.0327199287712574, 0.8324362635612488, 1.3461507558822632, 1.2123667001724243, 1.355712890625, 1.4390054941177368, 1.2164306640625, 1.3686929941177368, 1.7418212890625, 1.442840576171875, 2.0964152812957764, 1.3580526113510132, 1.3777275085449219, 1.1620374917984009, 0.432769775390625, 1.3328577280044556]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4830728769302368, 1.3535970449447632, 1.5250650644302368, 1.8349609375, 1.1128946542739868, 1.2021280527114868, 1.8151448965072632, 1.7837728261947632, 2.3583576679229736, 1.58154296875, 1.18731689453125, 0.8308102488517761, 1.3254801034927368, 1.2266674041748047, 1.5029296875, 1.5335286855697632]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.5027669668197632, 1.3605550527572632, 1.4029947519302368, 1.3335164785385132, 1.6123861074447632, 1.039642333984375, 1.3252156972885132, 1.2771072387695312, 1.8459676504135132, 1.4290771484375, 1.5865478515625, 1.49334716796875, 2.0869140625, 1.5400797128677368, 1.4878743886947632, 1.4519857168197632]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0426737070083618, 1.07574462890625, 1.0128377676010132, 0.8345832824707031, 0.10631688684225082, 1.2716470956802368, 1.1559854745864868, 1.5665639638900757, 0.8201446533203125, 1.2426961660385132, 1.3272908926010132, 1.09954833984375, 1.0811563730239868, 1.4348958730697632, 0.9742991328239441, 0.5912094116210938]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0633544921875, 2.242431640625, 2.1529133319854736, 1.9810384511947632, 1.9054769277572632, 2.0522053241729736, 1.91455078125, 2.1840007305145264, 2.112548828125, 2.0767009258270264, 2.3518879413604736, 2.6358234882354736, 1.7328695058822632, 2.3599445819854736, 2.145751953125, 2.1465656757354736]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4961751699447632, 1.4317626953125, 1.8950601816177368, 1.2050577402114868, 1.5270181894302368, 1.6424967050552368, 1.3494466543197632, 1.444091796875, 1.5572509765625, 1.6986898183822632, 1.6219075918197632, 2.1097819805145264, 1.5564779043197632, 1.6446939706802368, 1.3016763925552368, 1.6689859628677368]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4604899883270264, 1.777587890625, 1.061907410621643, 1.3770345449447632, 1.5712076425552368, 1.491455078125, 1.4772542715072632, 1.3870035409927368, 1.4618936777114868, 1.3851318359375, 1.4878743886947632, 1.4333902597427368, 1.1876220703125, 0.6728858947753906, 1.30615234375, 1.4412027597427368]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.409912109375, 2.4898273944854736, 2.349609375, 2.1954753398895264, 2.6568195819854736, 2.269287109375, 2.5284016132354736, 2.18505859375, 2.1885578632354736, 2.2699382305145264, 2.826171875, 2.2098796367645264, 2.3502604961395264, 2.5384113788604736, 2.4016926288604736, 2.3349609375]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6597900390625, 1.6810302734375, 1.5162760019302368, 1.7880452871322632, 1.3540445566177368, 1.5833333730697632, 1.6588134765625, 1.9662679433822632, 1.6846517324447632, 1.6417642831802368, 1.6288655996322632, 1.4920247793197632, 1.6754964590072632, 1.4847818613052368, 1.8077799081802368, 1.9806314706802368]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.7841796875, 1.7529067993164062, 1.6671346426010132, 1.73748779296875, 1.6378885507583618, 1.7683334350585938, 1.7551078796386719, 1.6996561288833618, 1.7942123413085938, 1.9322611093521118, 2.221099853515625, 1.6492513418197632, 1.656982421875, 1.82977294921875, 1.8369649648666382, 1.7565969228744507]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5855509042739868, 1.6779403686523438, 1.6235250234603882, 1.7454172372817993, 1.5890909433364868, 1.6187692880630493, 1.7575734853744507, 1.696673035621643, 1.6099456548690796, 1.6951700448989868, 1.381103515625, 1.6241811513900757, 1.6450296640396118, 1.8896230459213257, 1.607696533203125, 1.6378530263900757]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1249186992645264, 2.135986328125, 2.4853618144989014, 2.08251953125, 2.186187744140625, 2.156982421875, 2.5966796875, 2.2371928691864014, 2.0871989727020264, 2.0987956523895264, 2.097900390625, 1.9431966543197632, 2.062042236328125, 1.963623046875, 2.4258220195770264, 2.0854289531707764]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.146321773529053, 5.569417476654053, 5.130289554595947, 5.036946773529053, 5.070475101470947, 5.18603515625, 5.151529788970947, 5.4208984375, 5.41650390625, 5.126790523529053, 5.123860836029053, 4.982991695404053, 5.222086429595947, 5.164388179779053, 5.324137210845947, 5.104085445404053]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  76%|█████████████████████████████████████▍           | 385/504 [21:09<06:17,  3.17s/it]Layer: gate_30 - Captured router_logits: [3.9812824726104736, 3.868408203125, 3.8260905742645264, 3.750218629837036, 3.89208984375, 3.9457194805145264, 3.6921794414520264, 3.830810546875, 3.990234375, 3.9435222148895264, 3.8121337890625, 3.5133869647979736, 3.7923176288604736, 3.864013671875, 3.9514973163604736, 3.97845458984375]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.7511394023895264, 2.892333984375, 2.7848308086395264, 2.591552734375, 2.6932780742645264, 2.8798015117645264, 2.692626953125, 2.6083984375, 2.742431640625, 2.6481525897979736, 3.1726887226104736, 2.2801284790039062, 2.8140461444854736, 2.6771647930145264, 3.1048176288604736, 2.7596843242645264]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.070953868329525, 0.08447273820638657, 0.0836583748459816, -0.2600337564945221, -0.23937733471393585, -0.07158184051513672, 0.10680699348449707, -0.07120513916015625, 0.06702995300292969, 0.06960654258728027, 0.07830015569925308, 0.05632384493947029, 0.07543548196554184, 0.10058339685201645, -1.0237770080566406, 0.10518566519021988]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.090129055082798, 0.05272785946726799, 0.04172674939036369, 0.04750571772456169, 0.07596174627542496, 0.026326537132263184, 0.05618484690785408, 0.08174315840005875, 0.026620546355843544, 0.07140350341796875, -0.22169668972492218, 0.046229999512434006, 0.04143989086151123, -0.028305530548095703, 0.03663631156086922, 0.0053655304946005344]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07398029416799545, 0.06606334447860718, 0.084783636033535, 0.10086552053689957, 0.10002485662698746, 0.10336271673440933, 0.06967195123434067, -0.09128475189208984, 0.10906251519918442, 0.0703677162528038, 0.023485183715820312, 0.07294289022684097, -0.20134608447551727, 0.0390826053917408, -0.02736123465001583, 0.11154397577047348]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.1253262311220169, 0.1522207260131836, 0.1415049284696579, 0.14729690551757812, 0.1305980682373047, 0.13212299346923828, 0.015020211227238178, 0.1956452578306198, 0.18310897052288055, -0.5249341130256653, 0.04207710549235344, 0.08746560662984848, 0.22681236267089844, -0.23002879321575165, -0.01692231558263302, -0.06474844366312027]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06807772070169449, 0.09610692411661148, 0.10722652822732925, 0.12567822635173798, 0.07620318979024887, -0.03773140907287598, 0.030803918838500977, 0.02750433422625065, -0.0756794810295105, 0.030808448791503906, -0.1812152862548828, 0.12438551336526871, -0.011555989272892475, -0.19213049113750458, 0.11858264356851578, -0.06445249170064926]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.02832229994237423, 0.1334056854248047, 0.059406477957963943, 0.11588096618652344, -0.08438650518655777, -0.05647977069020271, 0.14771921932697296, 0.025903701782226562, 0.11954331398010254, -0.09058284759521484, 0.128314808011055, 0.06564585119485855, -0.22161991894245148, 0.12552650272846222, 0.1462288647890091, 0.06373222917318344]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.15064390003681183, -0.0018599828472360969, 0.1465597152709961, 0.3531646728515625, 0.1828470230102539, 0.10908905416727066, -0.13098065555095673, -0.09423605352640152, 0.3848257064819336, 0.17064888775348663, -0.5659688115119934, 0.3208962380886078, 0.17508506774902344, -0.30364036560058594, 0.19829940795898438, 0.1783415526151657]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3815871775150299, 0.22702574729919434, 0.25228023529052734, 0.1117301806807518, -0.8185908198356628, 0.36506399512290955, 0.19247429072856903, -0.3169670104980469, 0.5030671954154968, 0.26089605689048767, -0.22752396762371063, 0.22712834179401398, 0.24538357555866241, 0.22521638870239258, -0.6474812626838684, -0.11135896295309067]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.3011995851993561, 0.2857259213924408, -0.2693087160587311, 0.2766885757446289, 0.47355780005455017, 0.29693856835365295, 0.22552108764648438, -0.2775897979736328, 0.20200221240520477, 0.7274195551872253, 0.07379277795553207, 0.3205769956111908, 0.4207763671875, -0.5337371826171875, -0.53106689453125, 0.36074718832969666]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.35513558983802795, 0.2027333527803421, 0.5064951777458191, 0.9421284794807434, 0.2923460006713867, 0.1564701348543167, 0.4568878710269928, 0.5076472163200378, 0.5325743556022644, 0.10725530236959457, 0.21462886035442352, 0.6536152958869934, 0.8622970581054688, 0.04387156292796135, 0.6658986210823059, 0.60028076171875]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.096666932106018, 0.8710988163948059, 0.55230712890625, 0.7381184697151184, 0.8941853642463684, 0.5995759963989258, 0.631439208984375, 0.7081502079963684, 0.15756161510944366, 0.49416032433509827, 0.7202962040901184, 0.880889892578125, 0.4481894075870514, 0.25022825598716736, 0.8010067939758301, 0.4003772735595703]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9044697880744934, 1.099029541015625, 0.9459940791130066, 1.0983963012695312, 1.33929443359375, 0.43792691826820374, 0.8747660517692566, 0.8518320918083191, 0.887939453125, 1.3145040273666382, 0.853515625, 0.864105224609375, 0.4048391878604889, 0.5185448527336121, -0.039167243987321854, 0.6896608471870422]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.15862782299518585, 0.7802174687385559, 1.1492208242416382, 1.1144815683364868, 0.3657245635986328, 0.560211181640625, 1.0264447927474976, 1.1855214834213257, 0.5712363123893738, 0.4116802215576172, 0.7945709228515625, 0.9169514775276184, 0.8343709111213684, 0.889190673828125, 0.700775146484375, 0.9463908076286316]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6107556223869324, 0.8327738642692566, 0.9574229121208191, 0.3866141736507416, 0.8859049677848816, -0.22544033825397491, 1.04620361328125, 1.7817789316177368, 0.8954060673713684, 0.8159586787223816, 1.1105657815933228, 1.1068115234375, -0.026212850585579872, 0.7376890182495117, 1.0016645193099976, 0.44938406348228455]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6448872685432434, 0.7453715205192566, 0.8836720585823059, 0.7173665165901184, 0.9967244267463684, 0.6193408966064453, 0.13028208911418915, 1.088563323020935, 0.694976806640625, 0.7519270777702332, -0.4361666142940521, 0.5859050750732422, 0.8863296508789062, 1.0263875722885132, 1.149688720703125, 0.3515421450138092]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.2376047819852829, 1.1787109375, 0.9642537236213684, 0.9248453974723816, 0.7752100825309753, 0.8457844853401184, 1.1348413228988647, 0.9044596552848816, 0.9444172978401184, 0.99072265625, 0.6332346796989441, -0.36819806694984436, 0.9420433044433594, 0.7991536259651184, 0.85650634765625, 1.207929253578186]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Running loglikelihood requests:  77%|█████████████████████████████████████▊           | 389/504 [21:22<06:04,  3.17s/it]Layer: gate_16 - Captured router_logits: [0.687957763671875, 0.09197044372558594, 0.8741671442985535, 0.8276970982551575, 0.8448689579963684, 0.38334783911705017, 0.20354461669921875, 0.7765172123908997, -1.1313642263412476, 2.43988037109375, -1.0195573568344116, 0.5364478230476379, 0.8777058720588684, 0.9186045527458191, 0.8472671508789062, 0.1082356795668602]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.03911082074046135, 0.7223231196403503, 1.3971353769302368, 1.25579833984375, 1.37847900390625, 1.473876953125, 1.216796875, 1.3402913808822632, 1.83837890625, 1.3636575937271118, 2.1878254413604736, 1.39703369140625, 1.3560453653335571, 1.2509359121322632, 0.39476776123046875, 1.3544210195541382]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.444580078125, 1.2944742441177368, 1.4692281484603882, 1.7156982421875, 0.9442341923713684, 1.0957437753677368, 1.8169759511947632, 1.7159017324447632, 2.3387451171875, 1.4933267831802368, 1.1484781503677368, 0.7939567565917969, 1.1862283945083618, 1.060415267944336, 1.3826904296875, 1.4667562246322632]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.4070638418197632, 1.2753499746322632, 1.3352762460708618, 1.2492269277572632, 1.5196939706802368, 0.8844477534294128, 1.20220947265625, 1.1724395751953125, 1.7753499746322632, 1.3622232675552368, 1.5259805917739868, 1.4282633066177368, 1.9229329824447632, 1.474853515625, 1.3985189199447632, 1.3440145254135132]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0422292947769165, 1.03863525390625, 0.9068400263786316, 0.8111826777458191, -0.15217287838459015, 1.225860595703125, 1.1253255605697632, 1.4975858926773071, 0.7085660099983215, 1.19879150390625, 1.2703653573989868, 1.0042215585708618, 1.0145263671875, 1.3380533456802368, 0.9180590510368347, 0.4844907224178314]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0202229022979736, 2.1487629413604736, 2.0734050273895264, 1.894287109375, 1.835693359375, 2.0025227069854736, 1.8426920175552368, 2.0956218242645264, 1.9115396738052368, 2.0304362773895264, 2.26708984375, 2.5594890117645264, 1.615966796875, 2.2752277851104736, 2.0475261211395264, 2.0034992694854736]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.380859375, 1.4102376699447632, 1.8301187753677368, 1.043853759765625, 1.423583984375, 1.5584310293197632, 1.2713826894760132, 1.3850911855697632, 1.443115234375, 1.6587320566177368, 1.51123046875, 2.031982421875, 1.4586181640625, 1.5179849863052368, 1.2164510488510132, 1.6258951425552368]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.3329265117645264, 1.7631429433822632, 1.0327199697494507, 1.2987060546875, 1.5120035409927368, 1.3599447011947632, 1.3889974355697632, 1.3070882558822632, 1.4215494394302368, 1.3290201425552368, 1.41943359375, 1.4405924081802368, 1.1058553457260132, 0.5828704833984375, 1.2252197265625, 1.4190673828125]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.204833984375, 2.2730305194854736, 2.1571452617645264, 1.9694010019302368, 2.488037109375, 2.071533203125, 2.3465168476104736, 2.0271809101104736, 1.9796549081802368, 2.0467936992645264, 2.5795085430145264, 2.0096027851104736, 2.130126953125, 2.3319499492645264, 2.17578125, 2.1653645038604736]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.5679931640625, 1.6028646230697632, 1.4537353515625, 1.7246907949447632, 1.2305094003677368, 1.5443929433822632, 1.5712890625, 1.9381917715072632, 1.5971883535385132, 1.5096842050552368, 1.5372720956802368, 1.4448648691177368, 1.5712484121322632, 1.4107259511947632, 1.7475179433822632, 1.9031982421875]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.7512410879135132, 1.735120177268982, 1.644775390625, 1.7391153573989868, 1.6404622793197632, 1.7585328817367554, 1.7128206491470337, 1.667755126953125, 1.7631021738052368, 1.8906351327896118, 2.2241718769073486, 1.5942941904067993, 1.6094156503677368, 1.8810220956802368, 1.807098388671875, 1.7105827331542969]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5059713125228882, 1.6159133911132812, 1.5572103261947632, 1.6520971059799194, 1.5019327402114868, 1.51385498046875, 1.675427794456482, 1.6260108947753906, 1.5231170654296875, 1.6450399160385132, 1.2714437246322632, 1.529541015625, 1.5707906484603882, 1.8340708017349243, 1.5360819101333618, 1.5494537353515625]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9727579355239868, 2.01171875, 2.359710693359375, 1.9302572011947632, 2.0393269062042236, 2.01287841796875, 2.455322265625, 2.1011760234832764, 1.9355875253677368, 1.9541422128677368, 1.9377034902572632, 1.7593587636947632, 1.8933614492416382, 1.8097738027572632, 2.2801513671875, 1.9820150136947632]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.946207523345947, 5.392822265625, 4.921712398529053, 4.807698726654053, 4.796590328216553, 4.862142086029053, 4.92724609375, 5.186279296875, 5.166341304779053, 4.926432132720947, 4.861653804779053, 4.805013179779053, 4.995442867279053, 5.005289554595947, 5.115966796875, 4.876302242279053]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.8116862773895264, 3.6901042461395264, 3.6456706523895264, 3.544530153274536, 3.6483561992645264, 3.7816569805145264, 3.4784953594207764, 3.6615803241729736, 3.87158203125, 3.7207844257354736, 3.5922443866729736, 3.26373291015625, 3.5863850116729736, 3.6591796875, 3.7542316913604736, 3.8010456562042236]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.7229816913604736, 2.8374836444854736, 2.74072265625, 2.5909831523895264, 2.6385905742645264, 2.8553059101104736, 2.6455891132354736, 2.56005859375, 2.6903483867645264, 2.6274006366729736, 3.2198894023895264, 2.157740354537964, 2.7471516132354736, 2.6295979022979736, 3.1181640625, 2.7277019023895264]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06011023372411728, 0.07196824252605438, 0.0803510993719101, -0.2525893449783325, -0.22223800420761108, -0.06244562938809395, 0.10200424492359161, -0.056083597242832184, 0.0645662397146225, 0.05751882866024971, 0.0693817213177681, 0.05751933157444, 0.08039815723896027, 0.08393056690692902, -1.0189144611358643, 0.09321802854537964]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07021283358335495, 0.03447888046503067, 0.029320204630494118, 0.027752967551350594, 0.06389758735895157, 0.006175673566758633, 0.03828456252813339, 0.08090482652187347, 0.008997786790132523, 0.06715112179517746, -0.2274985909461975, 0.033462002873420715, 0.027740158140659332, -0.04172652214765549, 0.03443262353539467, 0.011061578057706356]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07184159010648727, 0.05195252224802971, 0.08953455835580826, 0.09323497861623764, 0.1041722372174263, 0.09101726859807968, 0.0582122802734375, -0.08893143385648727, 0.07893741130828857, 0.1191282570362091, 0.0353381521999836, 0.08916660398244858, -0.20992527902126312, 0.015324401669204235, -0.047962307929992676, 0.1058984026312828]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13056448101997375, 0.14359548687934875, 0.12067260593175888, 0.14773720502853394, 0.12871672213077545, 0.12275390326976776, 0.004101883620023727, 0.20393982529640198, 0.18855662643909454, -0.5034986138343811, 0.051659513264894485, 0.0683474913239479, 0.2758770287036896, -0.2647653818130493, 0.030929725617170334, -0.04054481163620949]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06201509013772011, 0.11473123729228973, 0.11519654840230942, 0.20208676159381866, 0.0537114180624485, -0.053517069667577744, 0.018823564052581787, 0.03168134018778801, -0.07897774875164032, 0.023229338228702545, -0.20160843431949615, 0.1394467055797577, -0.0019148575374856591, -0.1952795386314392, 0.13608799874782562, -0.04282611981034279]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.0773758739233017, 0.12087562680244446, 0.04761457070708275, 0.12290167808532715, -0.0968136414885521, -0.07135459780693054, 0.17925897240638733, -0.0027228104881942272, 0.19518673419952393, -0.07966051250696182, 0.08640650659799576, 0.06915584206581116, -0.20594048500061035, 0.13270890712738037, 0.14324180781841278, 0.07259529829025269]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.10569092631340027, -0.0676751360297203, 0.12950792908668518, 0.3831157386302948, 0.1768679916858673, 0.1770128756761551, -0.15715396404266357, -0.08902491629123688, 0.3791923224925995, 0.1548275649547577, -0.58821702003479, 0.3085536062717438, 0.1939324587583542, -0.28334447741508484, 0.25098878145217896, 0.20902998745441437]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.4502826929092407, 0.18065088987350464, 0.18542303144931793, 0.14334651827812195, -0.9874691367149353, 0.40083199739456177, 0.19749948382377625, -0.398884654045105, 0.5261260867118835, 0.18444053828716278, -0.3003687858581543, 0.27601638436317444, 0.241666778922081, 0.21067248284816742, -0.7195460200309753, -0.18470586836338043]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.26499536633491516, 0.2666870057582855, -0.34850263595581055, 0.3046618103981018, 0.5477166175842285, 0.22684775292873383, 0.26573020219802856, -0.27972733974456787, 0.15228527784347534, 0.772372305393219, 0.020683208480477333, 0.32277220487594604, 0.37224891781806946, -0.605288565158844, -0.55708909034729, 0.3381143808364868]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.28467342257499695, 0.041885536164045334, 0.5055040717124939, 0.9444207549095154, 0.2592991888523102, 0.10362990200519562, 0.42085668444633484, 0.4556557536125183, 0.4652266800403595, -0.0020482114050537348, 0.11711297184228897, 0.6144016981124878, 0.8423243761062622, 0.10387123376131058, 0.7130036950111389, 0.624552845954895]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.153480887413025, 0.8065738081932068, 0.5243061184883118, 0.696525514125824, 0.752555787563324, 0.47045931220054626, 0.5659796595573425, 0.6872944235801697, 0.04415507987141609, 0.43429622054100037, 0.7008377909660339, 0.9026135802268982, 0.33944493532180786, 0.18044401705265045, 0.6546903848648071, 0.2499890774488449]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.882845938205719, 1.0218801498413086, 0.9159950613975525, 0.9055715203285217, 1.285115122795105, 0.16059425473213196, 0.8403577208518982, 0.841838002204895, 0.9498457908630371, 1.293904185295105, 0.8057000637054443, 0.7147306799888611, 0.13514339923858643, 0.26372599601745605, -0.3463277816772461, 0.5941097736358643]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.0449228398501873, 0.710049569606781, 1.1549547910690308, 1.0490953922271729, 0.10419022291898727, 0.5371196269989014, 0.9448704719543457, 0.9829345941543579, 0.3513511121273041, 0.26907381415367126, 0.6887592673301697, 0.8490542769432068, 0.7553967833518982, 0.8681948781013489, 0.5149054527282715, 0.8328741788864136]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.5423789620399475, 0.858676016330719, 0.7510986328125, 0.15942510962486267, 0.9019531011581421, -0.3297825753688812, 1.0706002712249756, 2.1476974487304688, 0.8876747488975525, 0.8320518136024475, 0.9592259526252747, 1.1589432954788208, -0.31529349088668823, 0.6426578164100647, 1.1594469547271729, 0.42439350485801697]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.45723554491996765, 0.612345814704895, 0.7931948900222778, 0.7131578922271729, 0.8506784439086914, 0.2680201530456543, -0.04187140241265297, 0.9176545739173889, 0.8636615872383118, 0.6747982501983643, -0.5235287547111511, 0.4737664461135864, 0.7388254404067993, 0.93074631690979, 1.0809621810913086, 0.32363247871398926]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.2521677017211914, 1.1264597177505493, 0.8133634924888611, 1.0485197305679321, 0.5200220942497253, 0.755859375, 0.9015865921974182, 0.9205592274665833, 0.7913034558296204, 0.9114720225334167, 0.5443050861358643, -0.6096358299255371, 0.7907689213752747, 0.8174547553062439, 0.7428351044654846, 1.0746067762374878]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6429198980331421, 0.09889446198940277, 0.7900596261024475, 0.8713353276252747, 0.7992907166481018, 0.39492958784103394, -0.0014063784619793296, 0.8750231266021729, -1.2956160306930542, 2.5454564094543457, -1.1013376712799072, 0.3620467483997345, 0.770744264125824, 0.8256655931472778, 0.8253160715103149, 0.18295449018478394]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.0654587596654892, 0.44110429286956787, 1.2780016660690308, 1.0742392539978027, 1.3599506616592407, 1.2908717393875122, 1.1663033962249756, 1.1874794960021973, 1.6536389589309692, 1.314576506614685, 1.8697985410690308, 1.2389596700668335, 1.2908459901809692, 1.3876028060913086, 0.37423160672187805, 1.1654084920883179]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3146792650222778, 1.0890830755233765, 1.3334060907363892, 1.6082236766815186, 0.6688810586929321, 0.9965665936470032, 1.6800986528396606, 1.5082236528396606, 2.045600414276123, 1.2463815212249756, 0.9092516303062439, 0.5795519351959229, 1.0420653820037842, 0.941886842250824, 1.2176809310913086, 1.2738897800445557]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.162582278251648, 1.0868009328842163, 1.0941611528396606, 1.08390212059021, 1.2753289937973022, 0.5978721380233765, 0.9146175980567932, 0.9590768814086914, 1.5302631855010986, 1.311985969543457, 1.3400905132293701, 1.2163857221603394, 1.707154631614685, 1.2122533321380615, 1.1815378665924072, 1.149917721748352]
Running loglikelihood requests:  78%|██████████████████████████████████████▏          | 393/504 [21:34<05:45,  3.11s/it]Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8754857182502747, 0.9305509924888611, 0.8637232780456543, 0.7446545958518982, -0.2496909648180008, 1.1379112005233765, 0.9946134686470032, 1.2732267379760742, 0.7930150032043457, 1.0926192998886108, 1.1763569116592407, 1.0524671077728271, 0.8915296196937561, 1.1879522800445557, 0.69179368019104, 0.3120265007019043]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.8799753189086914, 2.036759853363037, 1.8628289699554443, 1.7360197305679321, 1.6949013471603394, 1.949342131614685, 1.6606496572494507, 1.9202302694320679, 1.7574013471603394, 1.7803454399108887, 2.1084704399108887, 2.4517269134521484, 1.4147204160690308, 2.041776418685913, 1.8663650751113892, 1.8250821828842163]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.2173930406570435, 1.2309621572494507, 1.6578947305679321, 0.8261024951934814, 1.2421875, 1.39453125, 1.1049342155456543, 1.2512747049331665, 1.3793174028396606, 1.3687705993652344, 1.4450247287750244, 1.9253289699554443, 1.328083872795105, 1.3823602199554443, 0.9995374083518982, 1.5362253189086914]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.3013980388641357, 1.656373381614685, 0.7788462042808533, 1.2145148515701294, 1.3176398277282715, 1.1858141422271729, 1.1643913984298706, 1.182113528251648, 1.2936882972717285, 1.235855221748352, 1.2715460062026978, 1.3173314332962036, 0.972697377204895, 0.3237189054489136, 1.0192639827728271, 1.188939094543457]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.28133225440979, 2.231661081314087, 2.1756579875946045, 2.0033717155456543, 2.4722039699554443, 2.0409538745880127, 2.338322401046753, 2.0352795124053955, 2.012376546859741, 2.055879831314087, 2.676562547683716, 1.9479440450668335, 2.182236909866333, 2.2928454875946045, 2.2116775512695312, 2.176891565322876]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.52750825881958, 1.5692845582962036, 1.4475740194320679, 1.6424342393875122, 1.142228603363037, 1.5082647800445557, 1.532647967338562, 1.9020559787750244, 1.5476562976837158, 1.4487664699554443, 1.544490098953247, 1.3560855388641357, 1.4591693878173828, 1.4188733100891113, 1.71875, 1.8756579160690308]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.719099521636963, 1.6695232391357422, 1.5826069116592407, 1.6063321828842163, 1.5754625797271729, 1.6770353317260742, 1.5881521701812744, 1.6306126117706299, 1.6519402265548706, 1.775822401046753, 2.1084394454956055, 1.5313990116119385, 1.5026932954788208, 1.84944486618042, 1.766478180885315, 1.6542506217956543]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.505753993988037, 1.5853900909423828, 1.4918649196624756, 1.6312525272369385, 1.4890214204788208, 1.5188168287277222, 1.6664447784423828, 1.5882350206375122, 1.5049715042114258, 1.594767689704895, 1.208634853363037, 1.5969418287277222, 1.5652549266815186, 1.7796181440353394, 1.5009405612945557, 1.51131272315979]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9401110410690308, 1.9908305406570435, 2.3602283000946045, 1.8626644611358643, 2.0442535877227783, 1.9182976484298706, 2.4194490909576416, 2.0138981342315674, 1.9247944355010986, 1.8641446828842163, 1.8440788984298706, 1.6918174028396606, 2.0041427612304688, 1.7086759805679321, 2.16755747795105, 1.9232113361358643]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.985937595367432, 5.330592155456543, 4.981414318084717, 4.8505964279174805, 4.863918781280518, 4.77327299118042, 4.981661319732666, 5.187211990356445, 5.215131759643555, 4.90982723236084, 4.924630165100098, 4.862993240356445, 5.016478061676025, 4.956003189086914, 5.114966869354248, 4.954769611358643]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.803762435913086, 3.6239309310913086, 3.5293996334075928, 3.4073293209075928, 3.7168173789978027, 3.724013090133667, 3.405972480773926, 3.6977384090423584, 3.690624952316284, 3.661677598953247, 3.4851150512695312, 3.135032892227173, 3.4938323497772217, 3.693708896636963, 3.6739721298217773, 3.755232334136963]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.4236841201782227, 2.5634868144989014, 2.508470296859741, 2.47672700881958, 2.4388158321380615, 2.48828125, 2.5929276943206787, 2.244572401046753, 2.4239721298217773, 2.28914475440979, 3.0431742668151855, 2.0059571266174316, 2.4731085300445557, 2.3189144134521484, 2.809786081314087, 2.4209704399108887]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.07142475992441177, 0.08271901309490204, 0.10286549478769302, -0.25373461842536926, -0.2269972860813141, -0.08338053524494171, 0.10628243535757065, -0.05506468191742897, 0.0802694633603096, 0.06998160481452942, 0.0768863782286644, 0.04454314708709717, 0.07901600748300552, 0.10569437593221664, -0.9866573214530945, 0.10571357607841492]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0818886086344719, 0.040380243211984634, 0.026768501847982407, 0.03971374034881592, 0.06434442847967148, 0.03429717198014259, 0.058874066919088364, 0.08741811662912369, 0.06713356822729111, 0.063587486743927, -0.18702971935272217, 0.048683080822229385, 0.01962040178477764, -0.02779242768883705, 0.031046107411384583, 0.034943655133247375]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.09491781145334244, 0.04477839544415474, 0.08318337798118591, 0.0833687111735344, 0.13717599213123322, 0.09514214843511581, 0.07711327075958252, -0.09515140950679779, 0.09514157474040985, 0.14149518311023712, 0.0367581881582737, 0.0849057286977768, -0.2069866806268692, 0.016868162900209427, -0.021925680339336395, 0.1169598177075386]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1508406698703766, 0.14872689545154572, 0.1169809028506279, 0.14664116501808167, 0.1507750153541565, 0.12127240002155304, -0.014173871837556362, 0.181892991065979, 0.17360113561153412, -0.518025279045105, 0.018452204763889313, 0.0896688774228096, 0.28649798035621643, -0.32792818546295166, 0.05747385695576668, -0.09084748476743698]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.055050626397132874, 0.10898512601852417, 0.0954853892326355, 0.22945378720760345, 0.05664336681365967, -0.05306612327694893, 0.03450252115726471, 0.022989680990576744, -0.1136879250407219, 0.008090673014521599, -0.264853835105896, 0.13328209519386292, 0.07225465029478073, -0.255960613489151, 0.14047935605049133, -0.1097833663225174]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.05344673618674278, 0.1512536108493805, 0.04144449904561043, 0.16584967076778412, -0.1125851720571518, -0.12058112025260925, 0.16656185686588287, 0.025979116559028625, 0.1785675287246704, -0.08258330821990967, 0.02673305571079254, 0.09218357503414154, -0.23862002789974213, 0.15952737629413605, 0.1420505791902542, 0.05973635986447334]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11852564662694931, -0.027650639414787292, 0.12107797712087631, 0.3516111969947815, 0.17642006278038025, 0.17276515066623688, -0.22557394206523895, -0.0959877297282219, 0.3005892336368561, 0.15103252232074738, -0.5894679427146912, 0.33679988980293274, 0.23808923363685608, -0.32821518182754517, 0.25749242305755615, 0.22377562522888184]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.4920380115509033, 0.19476181268692017, 0.22576835751533508, 0.1717693954706192, -0.8564727306365967, 0.32871097326278687, 0.20703467726707458, -0.47530612349510193, 0.5651664137840271, 0.15665529668331146, -0.24516244232654572, 0.26316097378730774, 0.18006879091262817, 0.21986544132232666, -0.7085701823234558, -0.24548716843128204]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.27836763858795166, 0.2593953013420105, -0.341552734375, 0.3260820508003235, 0.5289896130561829, 0.32361799478530884, 0.2550610899925232, -0.3127482533454895, 0.1772783249616623, 0.7051652073860168, 0.11881136149168015, 0.32859888672828674, 0.46341732144355774, -0.583740234375, -0.6431843638420105, 0.3656844198703766]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.40259474515914917, 0.2189255654811859, 0.515389084815979, 1.005681037902832, 0.38734281063079834, 0.1864425092935562, 0.5442442893981934, 0.5714481472969055, 0.7842510342597961, 0.15910013020038605, 0.2017499953508377, 0.720703125, 0.8114672303199768, -0.01048707403242588, 0.7693776488304138, 0.6597393155097961]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.164928674697876, 0.8768159747123718, 0.5404834747314453, 0.6944182515144348, 0.8499495387077332, 0.6276869177818298, 0.6327219605445862, 0.714657187461853, 0.09351065754890442, 0.417341947555542, 0.7308747172355652, 0.9490377306938171, 0.4284064471721649, 0.11575694382190704, 0.8318687081336975, 0.4003535807132721]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0309427976608276, 1.261652946472168, 0.8861920833587646, 1.197663426399231, 1.4346141815185547, 0.30167585611343384, 0.9902563095092773, 0.935066819190979, 1.0130903720855713, 1.5141546726226807, 0.873332142829895, 0.8643785119056702, 0.3521927297115326, 0.4285038411617279, -0.06074712425470352, 0.698718786239624]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [4.1147295632981695e-06, 0.758569598197937, 1.3506627082824707, 1.176571249961853, 0.34918487071990967, 0.5109067559242249, 1.17732834815979, 1.2785288095474243, 0.4277729392051697, 0.28225743770599365, 0.7918605208396912, 0.8749561309814453, 0.8798059821128845, 0.8407654762268066, 0.7414852380752563, 0.919482946395874]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8059479594230652, 0.9848358631134033, 1.1096822023391724, 0.34666597843170166, 1.0518784523010254, -0.16897960007190704, 1.1397252082824707, 1.9539588689804077, 1.087736964225769, 0.876163125038147, 1.4149842262268066, 1.263386607170105, 0.07425440847873688, 0.7892792224884033, 1.0548630952835083, 0.5930224061012268]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7233914136886597, 0.7664479613304138, 1.0008997917175293, 0.7524359226226807, 1.0011411905288696, 0.9403954148292542, 0.3656458556652069, 1.2606674432754517, 0.6768785119056702, 0.761466383934021, -0.2825797498226166, 0.6528909802436829, 0.903035044670105, 1.0110384225845337, 1.1446518898010254, 0.3274577260017395]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.363592267036438, 1.3168013095855713, 1.1085630655288696, 1.0030393600463867, 0.894816517829895, 0.9637245535850525, 1.483130931854248, 1.099982500076294, 1.1398788690567017, 1.080012321472168, 0.7348906993865967, -0.37020495533943176, 1.056755781173706, 0.897998571395874, 1.0079880952835083, 1.4286623001098633]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6837089657783508, -0.00031957734609022737, 0.7199364304542542, 0.8306102752685547, 0.7124393582344055, 0.34503173828125, 0.44297534227371216, 0.8456208109855652, -1.0926212072372437, 2.3306925296783447, -1.0259220600128174, 0.48469218611717224, 0.7970395684242249, 0.8890230059623718, 0.7779081463813782, 0.21133165061473846]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.05882057547569275, 0.759341835975647, 1.4925495386123657, 1.389396071434021, 1.4785375595092773, 1.4988588094711304, 1.4087352752685547, 1.385627031326294, 1.951041579246521, 1.4369020462036133, 2.376711845397949, 1.6361838579177856, 1.4183751344680786, 1.4154231548309326, 0.44942721724510193, 1.384090781211853]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4530372619628906, 1.274446964263916, 1.5227571725845337, 1.8391414880752563, 0.9174420833587646, 1.171940803527832, 2.0519661903381348, 1.728581428527832, 2.5164589881896973, 1.4262640476226807, 1.1523327827453613, 0.7617283463478088, 1.1197606325149536, 1.079809308052063, 1.4701545238494873, 1.5090852975845337]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.48380446434021, 1.3390976190567017, 1.5993021726608276, 1.326896071434021, 1.6358848810195923, 1.0129696130752563, 1.3150455951690674, 1.2851343154907227, 1.976913571357727, 1.4198780059814453, 1.6965633630752563, 1.5236350297927856, 2.001415491104126, 1.539940357208252, 1.4974104166030884, 1.5282654762268066]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.140795111656189, 0.9198450446128845, 0.8847436904907227, 0.7942882180213928, -0.09598155319690704, 1.234726071357727, 1.0675034523010254, 1.4745171070098877, 0.5972914099693298, 1.218311071395874, 1.2085893154144287, 1.053881049156189, 0.9882593154907227, 1.25, 0.7337221503257751, 0.30580225586891174]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.192854642868042, 2.3744733333587646, 2.2931880950927734, 2.0655722618103027, 2.0481040477752686, 2.2083041667938232, 2.025763750076294, 2.3376052379608154, 2.0495522022247314, 2.211639642715454, 2.4532127380371094, 2.8387465476989746, 1.7802844047546387, 2.4757723808288574, 2.225245714187622, 2.2004916667938232]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4233672618865967, 1.417617678642273, 2.0617098808288574, 1.0310195684432983, 1.477352499961853, 1.6164852380752563, 1.2801307439804077, 1.4151158332824707, 1.5142204761505127, 1.666520357131958, 1.6277211904525757, 2.158268928527832, 1.4957865476608276, 1.6264922618865967, 1.2350553274154663, 1.726913571357727]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4867889881134033, 1.8884304761886597, 1.055916428565979, 1.465853214263916, 1.6404932737350464, 1.4299508333206177, 1.3678897619247437, 1.367538571357727, 1.4817415475845337, 1.444873571395874, 1.4833216667175293, 1.4931092262268066, 1.1607707738876343, 0.6111324429512024, 1.2297664880752563, 1.482575535774231]
Running loglikelihood requests:  79%|██████████████████████████████████████▌          | 397/504 [21:46<05:32,  3.11s/it]Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.4628686904907227, 2.4677844047546387, 2.388167142868042, 2.1063027381896973, 2.6923279762268066, 2.2759830951690674, 2.480600357055664, 2.131232500076294, 2.1169240474700928, 2.2027738094329834, 2.9003686904907227, 2.147998571395874, 2.365431785583496, 2.5240519046783447, 2.2827422618865967, 2.3099544048309326]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.698867678642273, 1.7251579761505127, 1.582470178604126, 1.8627984523773193, 1.3450227975845337, 1.6827598810195923, 1.7380179166793823, 2.1456284523010254, 1.7249385118484497, 1.6183286905288696, 1.6738061904907227, 1.5356829166412354, 1.6918013095855713, 1.536955714225769, 2.13614821434021, 2.0469627380371094]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8607356548309326, 1.9835877418518066, 1.8008689880371094, 1.7814255952835083, 1.765405535697937, 1.9404159784317017, 1.8186131715774536, 1.8185569047927856, 1.863362193107605, 1.978449821472168, 2.5205845832824707, 1.7652080059051514, 1.7661077976226807, 2.0120480060577393, 1.9160594940185547, 1.877717137336731]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6786999702453613, 1.7290422916412354, 1.6783488988876343, 1.748771071434021, 1.5973490476608276, 1.623771071434021, 1.861767053604126, 1.780525803565979, 1.6314518451690674, 1.722502589225769, 1.3672313690185547, 1.6532435417175293, 1.76228928565979, 2.106346607208252, 1.657829999923706, 1.6900895833969116]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1196014881134033, 2.1778440475463867, 2.536736249923706, 2.0605688095092773, 2.2118592262268066, 2.1434779167175293, 2.6782829761505127, 2.247542142868042, 2.0992801189422607, 2.0782127380371094, 2.063816785812378, 1.8920295238494873, 2.041306495666504, 1.9212604761123657, 2.42130446434021, 2.080714464187622]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.090940952301025, 5.630003452301025, 5.100509166717529, 4.917661666870117, 4.993591785430908, 4.948736190795898, 5.042573928833008, 5.248595714569092, 5.353055000305176, 5.0715413093566895, 5.001580238342285, 4.962956428527832, 5.096646785736084, 5.189255714416504, 5.283093452453613, 5.038184642791748]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.6981215476989746, 3.5306355953216553, 3.447770357131958, 3.303941249847412, 3.55916428565979, 3.5711026191711426, 3.2798454761505127, 3.6567766666412354, 3.7311270236968994, 3.5550386905670166, 3.381232500076294, 2.9987106323242188, 3.29801607131958, 3.548454999923706, 3.585498571395874, 3.6184604167938232]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.5372190475463867, 2.683725357055664, 2.6436972618103027, 2.5402915477752686, 2.5191361904144287, 2.575930595397949, 2.589097499847412, 2.260270357131958, 2.540642499923706, 2.3153090476989746, 3.1534409523010254, 1.855754017829895, 2.608672857284546, 2.494381904602051, 3.113412857055664, 2.531688928604126]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06873167306184769, 0.08473865687847137, 0.08646821230649948, -0.27083030343055725, -0.2345046103000641, -0.08922962844371796, 0.1133950874209404, -0.06542283296585083, 0.07666461169719696, 0.06839726120233536, 0.07922226190567017, 0.0579986572265625, 0.08499038219451904, 0.09768693894147873, -1.0693249702453613, 0.1094888374209404]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07971324026584625, 0.03838494047522545, 0.03530386462807655, 0.03386896103620529, 0.06542454659938812, 0.014556113630533218, 0.03832412138581276, 0.0730532556772232, 0.025520239025354385, 0.06219276785850525, -0.2333878129720688, 0.032174743711948395, 0.03425529599189758, -0.050801824778318405, 0.023463260382413864, 0.00523316441103816]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06969884783029556, 0.06195976957678795, 0.0875992476940155, 0.10802450776100159, 0.10039151459932327, 0.09674929827451706, 0.07824758440256119, -0.10429159551858902, 0.09502659738063812, 0.08294806629419327, 0.03591156005859375, 0.08045187592506409, -0.2118811458349228, 0.013790687546133995, -0.02970130182802677, 0.11137201637029648]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13057546317577362, 0.1440230756998062, 0.13143929839134216, 0.17357464134693146, 0.12799140810966492, 0.13290782272815704, 0.015909260138869286, 0.21378447115421295, 0.19675925374031067, -0.528775691986084, 0.014092949219048023, 0.08427480608224869, 0.2526683211326599, -0.25844889879226685, -0.016358792781829834, -0.07533521205186844]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0660373792052269, 0.10316154360771179, 0.12364308536052704, 0.17840594053268433, 0.0487559475004673, -0.07206126302480698, 0.035371843725442886, 0.03500331938266754, -0.07394580543041229, 0.05893390253186226, -0.22310681641101837, 0.1312180459499359, -0.020709091797471046, -0.21231421828269958, 0.12484107166528702, -0.026258833706378937]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.056833673268556595, 0.121339350938797, 0.022335780784487724, 0.1060221791267395, -0.08270872384309769, -0.08461804687976837, 0.20834556221961975, 0.028024394065141678, 0.14002390205860138, -0.08246878534555435, 0.21208001673221588, 0.06177288666367531, -0.2705705761909485, 0.12139704078435898, 0.14374807476997375, 0.045216165482997894]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.1007719561457634, -0.057733770459890366, 0.0955173596739769, 0.4043407738208771, 0.15689978003501892, 0.1866249293088913, -0.1735675185918808, -0.08764922618865967, 0.3622649013996124, 0.14511828124523163, -0.6132922172546387, 0.349864661693573, 0.1826213002204895, -0.2596958577632904, 0.22221392393112183, 0.2326289862394333]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.42733660340309143, 0.18948467075824738, 0.21624481678009033, 0.12837064266204834, -0.9352341294288635, 0.4023970663547516, 0.1875533163547516, -0.29707661271095276, 0.5425658226013184, 0.15896503627300262, -0.28405898809432983, 0.22518955171108246, 0.3090475797653198, 0.15938028693199158, -0.7704159021377563, -0.056644052267074585]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.23828056454658508, 0.2677728831768036, -0.23875975608825684, 0.28363654017448425, 0.5845274925231934, 0.24666844308376312, 0.23384128510951996, -0.3369253873825073, 0.1415012627840042, 0.7508147358894348, 0.01319456659257412, 0.3806646168231964, 0.33358660340309143, -0.6573795080184937, -0.6017716526985168, 0.3358982503414154]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.285253643989563, 0.05042635276913643, 0.4661330282688141, 0.9531291127204895, 0.26244980096817017, 0.1527775079011917, 0.40733492374420166, 0.44858402013778687, 0.502416729927063, 0.0361451581120491, 0.17858612537384033, 0.639428973197937, 0.9115477204322815, 0.08447162806987762, 0.624495267868042, 0.5961200594902039]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1223117113113403, 0.8371609449386597, 0.5430839657783508, 0.7065374851226807, 0.7864729762077332, 0.538141131401062, 0.5857207775115967, 0.6726211309432983, 0.08175830543041229, 0.42284607887268066, 0.6922566294670105, 0.8828399181365967, 0.4501390755176544, 0.1581619828939438, 0.6767118573188782, 0.3006879687309265]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.8837342262268066, 1.071475625038147, 0.8820444345474243, 0.9915394186973572, 1.3603817224502563, 0.32229578495025635, 0.8730030059814453, 0.8551614880561829, 0.9810832142829895, 1.3286077976226807, 0.7981944680213928, 0.7735362648963928, 0.23759743571281433, 0.366113543510437, -0.2620578706264496, 0.6299184560775757]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.03205820173025131, 0.7006644010543823, 1.1780086755752563, 1.098314642906189, 0.24676668643951416, 0.5809915661811829, 1.0323939323425293, 1.0722628831863403, 0.4334608018398285, 0.36463361978530884, 0.638835072517395, 0.8996444940567017, 0.8526213765144348, 0.8233190178871155, 0.5619403719902039, 0.888386607170105]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.5369436740875244, 0.811995267868042, 0.7845143675804138, 0.23985719680786133, 0.9066230654716492, -0.30430346727371216, 1.0682276487350464, 2.1165292263031006, 0.9287383556365967, 0.8687456250190735, 1.046078085899353, 1.2156776189804077, -0.19385382533073425, 0.689868688583374, 1.0873968601226807, 0.43359649181365967]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.577938437461853, 0.7115848660469055, 0.8397230505943298, 0.7697726488113403, 0.908993124961853, 0.27403464913368225, 0.10145003348588943, 1.078087329864502, 0.825406551361084, 0.8024271130561829, -0.4691614806652069, 0.6269819140434265, 0.8747928738594055, 1.012629508972168, 1.201522946357727, 0.5195208191871643]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.3072098195552826, 1.24471116065979, 0.9002150893211365, 0.9822902083396912, 0.6238835453987122, 0.8314826488494873, 0.9825741052627563, 0.971207857131958, 0.869140625, 0.9485604166984558, 0.685327410697937, -0.38502827286720276, 0.794702410697937, 0.8520233631134033, 0.8385270237922668, 1.1016803979873657]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.7237436771392822, 0.1446598321199417, 0.8174939155578613, 0.8614817261695862, 0.763594388961792, 0.43312740325927734, 0.17910508811473846, 0.8276034593582153, -1.150365948677063, 2.666651964187622, -1.0061851739883423, 0.5212459564208984, 0.7889308333396912, 0.8369797468185425, 0.8435003757476807, 0.2427251636981964]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.07098937034606934, 0.6211842894554138, 1.3440133333206177, 1.2289106845855713, 1.3267643451690674, 1.3254038095474243, 1.236547589302063, 1.2815133333206177, 1.7584270238876343, 1.3947123289108276, 2.03401517868042, 1.3981302976608276, 1.370622992515564, 1.3006056547164917, 0.47604817152023315, 1.3328322172164917]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.387508749961853, 1.2509655952453613, 1.3888474702835083, 1.6688904762268066, 0.9051308035850525, 1.069555401802063, 1.8855336904525757, 1.579792857170105, 2.198472499847412, 1.3333919048309326, 1.0698845386505127, 0.7630622386932373, 1.2067843675613403, 1.073352575302124, 1.3620961904525757, 1.3970329761505127]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.2128686904907227, 1.1245172023773193, 1.2015010118484497, 1.132373571395874, 1.3490169048309326, 0.7278352975845337, 1.0049266815185547, 1.032764196395874, 1.6546919345855713, 1.3122586011886597, 1.3839536905288696, 1.2789896726608276, 1.8965940475463867, 1.283488392829895, 1.1924595832824707, 1.1871049404144287]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.045914888381958, 0.8970329761505127, 0.876228928565979, 0.8038480877876282, 0.0055329399183392525, 1.1361262798309326, 1.036385178565979, 1.4395408630371094, 0.7778292894363403, 1.130859375, 1.256786584854126, 1.1078388690948486, 1.002172589302063, 1.2713526487350464, 0.8457909226417542, 0.4431111216545105]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [1.9783180952072144, 2.1840765476226807, 1.9954792261123657, 1.885489821434021, 1.8417749404907227, 1.9877984523773193, 1.8261499404907227, 2.096207857131958, 1.878555178642273, 1.9565484523773193, 2.2470154762268066, 2.6181530952453613, 1.6529362201690674, 2.2011938095092773, 2.0244908332824707, 1.995874285697937]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3581022024154663, 1.290642499923706, 1.8074086904525757, 1.0401268005371094, 1.3379125595092773, 1.47265625, 1.219627857208252, 1.313948392868042, 1.4114290475845337, 1.5035990476608276, 1.5581109523773193, 2.0534584522247314, 1.426439642906189, 1.46875, 1.12361741065979, 1.5885709524154663]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5091731548309326, 1.6875877380371094, 0.841275691986084, 1.281601071357727, 1.3331724405288696, 1.238632321357727, 1.1547577381134033, 1.1949174404144287, 1.3034805059432983, 1.2942415475845337, 1.2922664880752563, 1.3354547023773193, 0.9904977083206177, 0.47291067242622375, 1.1161999702453613, 1.2257286310195923]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.395277500152588, 2.30135178565979, 2.2936270236968994, 2.06697678565979, 2.6545822620391846, 2.1253511905670166, 2.4439079761505127, 2.1375527381896973, 2.1063027381896973, 2.186183214187622, 2.7817766666412354, 2.0754916667938232, 2.231478214263916, 2.3776333332061768, 2.2974894046783447, 2.2461376190185547]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.563289999961853, 1.60603928565979, 1.4643170833587646, 1.6791608333587646, 1.251448392868042, 1.5230424404144287, 1.5710586309432983, 1.9747190475463867, 1.583699107170105, 1.4945576190948486, 1.5191361904144287, 1.391414999961853, 1.500219464302063, 1.4399139881134033, 1.8085498809814453, 1.9312236309051514]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.7469276189804077, 1.7154438495635986, 1.6111525297164917, 1.655460000038147, 1.5957469940185547, 1.6814266443252563, 1.598070502281189, 1.65665602684021, 1.6790318489074707, 1.808242678642273, 2.166114330291748, 1.5380091667175293, 1.53963303565979, 1.8973182439804077, 1.8198022842407227, 1.631200909614563]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Running loglikelihood requests:  80%|██████████████████████████████████████▉          | 401/504 [21:58<05:15,  3.07s/it]Layer: gate_27 - Captured router_logits: [1.4958194494247437, 1.6227753162384033, 1.512954592704773, 1.6730902194976807, 1.513825535774231, 1.523624062538147, 1.702318549156189, 1.6121689081192017, 1.5199553966522217, 1.618065357208252, 1.2942854166030884, 1.5971789360046387, 1.5603054761886597, 1.847957968711853, 1.5179951190948486, 1.527294397354126]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.967740535736084, 2.0431442260742188, 2.3561599254608154, 1.8906688690185547, 2.0390515327453613, 1.9462122917175293, 2.4569435119628906, 2.02717924118042, 1.949569821357727, 1.906601071357727, 1.8831197023391724, 1.729810357093811, 1.9491090774536133, 1.7600947618484497, 2.208040714263916, 1.9408137798309326]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.0090413093566895, 5.413009166717529, 5.031074523925781, 4.938904285430908, 4.9544854164123535, 4.8190836906433105, 4.994381904602051, 5.282654285430908, 5.276861190795898, 4.971208095550537, 4.9400458335876465, 4.8967695236206055, 5.1117448806762695, 5.024754047393799, 5.1508073806762695, 4.996664524078369]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.87833571434021, 3.6744205951690674, 3.5718047618865967, 3.5355842113494873, 3.7516677379608154, 3.7798454761505127, 3.530942678451538, 3.7294154167175293, 3.8085498809814453, 3.81583571434021, 3.5746138095855713, 3.2670295238494873, 3.61604642868042, 3.7224369049072266, 3.7296347618103027, 3.845461845397949]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.5126843452453613, 2.6686270236968994, 2.5186972618103027, 2.4507548809051514, 2.425473928451538, 2.5877809524536133, 2.524139642715454, 2.3561928272247314, 2.5153615474700928, 2.4266152381896973, 3.1461551189422607, 2.139012098312378, 2.589712142944336, 2.4196364879608154, 2.9340765476226807, 2.506232500076294]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.05769972503185272, 0.06998538970947266, 0.0879773199558258, -0.2510831952095032, -0.24349142611026764, -0.08871230483055115, 0.09658293426036835, -0.04620777443051338, 0.06967613846063614, 0.05585028976202011, 0.06640589982271194, 0.04900550842285156, 0.07052291184663773, 0.09127876907587051, -0.9639892578125, 0.09796350449323654]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.08524565398693085, 0.04393575340509415, 0.030569272115826607, 0.036028340458869934, 0.06466574966907501, 0.02568713016808033, 0.05009044334292412, 0.08563804626464844, 0.06366196274757385, 0.060652125626802444, -0.18540512025356293, 0.04425517097115517, 0.017712680622935295, -0.029435157775878906, 0.030855806544423103, 0.03605223819613457]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07999038696289062, 0.03187073394656181, 0.09094350785017014, 0.07604312896728516, 0.14301668107509613, 0.08819684386253357, 0.07399051636457443, -0.08501653373241425, 0.08975289016962051, 0.11491194367408752, 0.036954186856746674, 0.0892663449048996, -0.1851910650730133, 0.025898413732647896, -0.0036565607879310846, 0.10536757111549377]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.14504632353782654, 0.13974623382091522, 0.1268608719110489, 0.1545548439025879, 0.14641223847866058, 0.11852610856294632, -0.011155301705002785, 0.18005648255348206, 0.17340750992298126, -0.4934057891368866, 0.011653900146484375, 0.08155432343482971, 0.2854217290878296, -0.31897667050361633, 0.048377469182014465, -0.0728045403957367]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0595773346722126, 0.09280404448509216, 0.09719666838645935, 0.21728463470935822, 0.05420546233654022, -0.026424061506986618, 0.024455158039927483, 0.029011379927396774, -0.1093701422214508, 0.021826831623911858, -0.25942090153694153, 0.11886093765497208, 0.06055658683180809, -0.22506704926490784, 0.13983847200870514, -0.11213891953229904]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.036833155900239944, 0.15828634798526764, 0.045077063143253326, 0.15218110382556915, -0.0987170860171318, -0.1134796142578125, 0.16378021240234375, 0.018894195556640625, 0.16792131960391998, -0.05275760963559151, 0.05906573310494423, 0.09685949981212616, -0.2569469213485718, 0.1624346673488617, 0.125105082988739, 0.0892767459154129]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.1203988716006279, -0.05197559669613838, 0.11568450927734375, 0.3474114239215851, 0.16111339628696442, 0.14967761933803558, -0.21506673097610474, -0.08615805953741074, 0.3414902985095978, 0.16720928251743317, -0.590247392654419, 0.3497905731201172, 0.231048583984375, -0.34633877873420715, 0.2512078583240509, 0.2185540646314621]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.46770337224006653, 0.16949385404586792, 0.2073974609375, 0.17072366178035736, -0.8454645276069641, 0.3366221487522125, 0.20974107086658478, -0.4522198736667633, 0.5377910137176514, 0.17390303313732147, -0.2617229223251343, 0.23398035764694214, 0.20699726045131683, 0.21842193603515625, -0.7387815117835999, -0.21145647764205933]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.26877734065055847, 0.25717657804489136, -0.32653358578681946, 0.3044385015964508, 0.5178388953208923, 0.31511029601097107, 0.23474207520484924, -0.30150845646858215, 0.193271204829216, 0.6951687932014465, 0.11849212646484375, 0.3655950427055359, 0.4566539525985718, -0.6183221936225891, -0.6631664037704468, 0.38215333223342896]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.36730852723121643, 0.19384627044200897, 0.5420005321502686, 0.9717546105384827, 0.34636828303337097, 0.14486069977283478, 0.505821943283081, 0.5749400854110718, 0.699371337890625, 0.17115922272205353, 0.1558782458305359, 0.693115234375, 0.8416874408721924, -0.05511127784848213, 0.7306795716285706, 0.6307095885276794]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.092705488204956, 0.8783125281333923, 0.49313631653785706, 0.6857743859291077, 0.7881913781166077, 0.6152324080467224, 0.6068309545516968, 0.7213578820228577, 0.0483221560716629, 0.42490246891975403, 0.7282770276069641, 0.9518876671791077, 0.5136510729789734, 0.14907003939151764, 0.7715259790420532, 0.3823894262313843]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9866832494735718, 1.2623401880264282, 0.8919788599014282, 1.2146967649459839, 1.4540294408798218, 0.24317099153995514, 0.9777055382728577, 0.8946810364723206, 1.0005770921707153, 1.490434169769287, 0.8546308875083923, 0.8534823060035706, 0.2634929418563843, 0.4630713164806366, -0.07777543365955353, 0.6907681822776794]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.02260797657072544, 0.7591996788978577, 1.3029896020889282, 1.1733620166778564, 0.3898870348930359, 0.5269498229026794, 1.1648074388504028, 1.2319058179855347, 0.3830358386039734, 0.25563743710517883, 0.8299068212509155, 0.8506746888160706, 0.8368474841117859, 0.89599609375, 0.7323663830757141, 0.8945978283882141]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7958096861839294, 1.0123401880264282, 1.1538752317428589, 0.3833174407482147, 1.084228515625, -0.15517772734165192, 1.1544300317764282, 2.0174005031585693, 1.1228693723678589, 0.9113103747367859, 1.4371448755264282, 1.2752574682235718, 0.13437999784946442, 0.8211038708686829, 1.0700017213821411, 0.6418973803520203]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7397377490997314, 0.7306129932403564, 0.9937189221382141, 0.76220703125, 1.0072354078292847, 0.8598909974098206, 0.34686556458473206, 1.2738037109375, 0.6386497020721436, 0.7302911877632141, -0.3547663390636444, 0.617804765701294, 0.890126645565033, 1.018754482269287, 1.1394931077957153, 0.31432273983955383]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.32018765807151794, 1.3214222192764282, 1.0409046411514282, 1.005293369293213, 0.8430841565132141, 0.9114879369735718, 1.3895519971847534, 1.0900657176971436, 1.11474609375, 1.0572620630264282, 0.6963445544242859, -0.38372907042503357, 1.0319157838821411, 0.8765980005264282, 0.9838423132896423, 1.4038914442062378]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.6712785363197327, 0.045075155794620514, 0.7604217529296875, 0.8086881041526794, 0.6543759703636169, 0.36174261569976807, 0.4322325885295868, 0.8143491148948669, -1.1240900754928589, 2.345181465148926, -1.0008018016815186, 0.48265907168388367, 0.7857776880264282, 0.8765147924423218, 0.7979403138160706, 0.22068925201892853]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.039833761751651764, 0.7502736449241638, 1.4680842161178589, 1.3607121706008911, 1.430464267730713, 1.494184970855713, 1.4069768190383911, 1.3323308229446411, 1.9064608812332153, 1.4343205690383911, 2.34130859375, 1.6413477659225464, 1.3950618505477905, 1.2852894067764282, 0.40229102969169617, 1.3511574268341064]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5256569385528564, 1.3524502515792847, 1.594926357269287, 1.870561122894287, 0.989990234375, 1.211137294769287, 2.1226918697357178, 1.8172053098678589, 2.522860527038574, 1.5393288135528564, 1.1990078687667847, 0.8062307238578796, 1.207231044769287, 1.1229358911514282, 1.5316051244735718, 1.5510920286178589]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.4826881885528564, 1.3295010328292847, 1.5868252515792847, 1.340087890625, 1.6384943723678589, 1.0052989721298218, 1.293412685394287, 1.2629505395889282, 1.9657536745071411, 1.4030095338821411, 1.6550959348678589, 1.5205744504928589, 2.0046608448028564, 1.544389247894287, 1.537642002105713, 1.5075905323028564]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.140638828277588, 0.9337047338485718, 0.8961625695228577, 0.8218272924423218, -0.10078707337379456, 1.2470036745071411, 1.0650967359542847, 1.464585781097412, 0.6031133532524109, 1.2150212526321411, 1.237593173980713, 1.0457097291946411, 1.0109196901321411, 1.2815607786178589, 0.7515591979026794, 0.2673558294773102]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.1790659427642822, 2.3900034427642822, 2.277787685394287, 2.1043589115142822, 2.015580654144287, 2.2109375, 2.037109375, 2.2970526218414307, 2.0544655323028564, 2.2024147510528564, 2.4638671875, 2.8103692531585693, 1.7804954051971436, 2.4641335010528564, 2.239169120788574, 2.2268288135528564]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4363459348678589, 1.410600185394287, 2.0673828125, 1.07159423828125, 1.4932528734207153, 1.6287287473678589, 1.3004482984542847, 1.428178310394287, 1.5491832494735718, 1.7008610963821411, 1.637162685394287, 2.1728515625, 1.5218838453292847, 1.640092372894287, 1.2347079515457153, 1.6942027807235718]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.535111904144287, 1.8988369703292847, 1.0806552171707153, 1.475142002105713, 1.6307705640792847, 1.472212314605713, 1.43603515625, 1.4022105932235718, 1.5068359375, 1.4755859375, 1.5329368114471436, 1.5202414989471436, 1.2112926244735718, 0.6270391345024109, 1.322221279144287, 1.4720791578292847]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.453125, 2.4718573093414307, 2.3958628177642822, 2.150301933288574, 2.690873622894287, 2.2667791843414307, 2.482954502105713, 2.1633522510528564, 2.150301933288574, 2.227982997894287, 2.8880503177642822, 2.1715199947357178, 2.359463691711426, 2.519975185394287, 2.324751377105713, 2.3220880031585693]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7024147510528564, 1.7330433130264282, 1.5940607786178589, 1.854936122894287, 1.3364924192428589, 1.6877219676971436, 1.7478693723678589, 2.1276190280914307, 1.7276722192764282, 1.6257102489471436, 1.6828835010528564, 1.50634765625, 1.6955344676971436, 1.5428355932235718, 2.1101739406585693, 2.0665838718414307]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8721591234207153, 1.9751337766647339, 1.7854225635528564, 1.765580654144287, 1.7701526880264282, 1.932173252105713, 1.8282359838485718, 1.8060191869735718, 1.8643244504928589, 1.9696377515792847, 2.4863059520721436, 1.724897861480713, 1.748002529144287, 2.0066583156585693, 1.9081476926803589, 1.8705458641052246]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6640846729278564, 1.7371271848678589, 1.663818359375, 1.7732599973678589, 1.6060458421707153, 1.6064453125, 1.8694069385528564, 1.765625, 1.6337890625, 1.7271728515625, 1.3567560911178589, 1.6605557203292847, 1.731977939605713, 2.085360527038574, 1.673051357269287, 1.6727184057235718]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.124423027038574, 2.195223808288574, 2.5374200344085693, 2.0816762447357178, 2.1993963718414307, 2.153364658355713, 2.663174629211426, 2.266201972961426, 2.124112129211426, 2.0803444385528564, 2.092862129211426, 1.8800159692764282, 2.0497381687164307, 1.9343928098678589, 2.4206764698028564, 2.0862481594085693]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.100230693817139, 5.587091445922852, 5.1198506355285645, 4.913618564605713, 4.968661308288574, 4.967240810394287, 5.091708183288574, 5.284978866577148, 5.349254131317139, 5.0563740730285645, 5.006924629211426, 4.9873046875, 5.1716084480285645, 5.179154872894287, 5.270596504211426, 5.085316181182861]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.6897194385528564, 3.4937856197357178, 3.4103338718414307, 3.2917258739471436, 3.4886362552642822, 3.5494496822357178, 3.240633964538574, 3.59521484375, 3.713423252105713, 3.5559303760528564, 3.370694160461426, 2.9930307865142822, 3.294145107269287, 3.5257456302642822, 3.5403940677642822, 3.6032936573028564]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  80%|███████████████████████████████████████▍         | 405/504 [22:10<05:03,  3.07s/it]Layer: gate_31 - Captured router_logits: [2.558638095855713, 2.6946909427642822, 2.685102939605713, 2.5226385593414307, 2.5548651218414307, 2.6091086864471436, 2.576615810394287, 2.3097479343414307, 2.574130058288574, 2.37255859375, 3.1075994968414307, 1.83941650390625, 2.65234375, 2.5325815677642822, 3.1442649364471436, 2.572798252105713]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.059966329485177994, 0.07778673619031906, 0.07809674739837646, -0.23972035944461823, -0.23755735158920288, -0.08709929883480072, 0.09999563544988632, -0.10349716991186142, 0.05767272412776947, 0.06451416015625, 0.07244446873664856, 0.0606510266661644, 0.07888562977313995, 0.08746302127838135, -0.9326285719871521, 0.10406778007745743]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.08050110936164856, 0.044786497950553894, 0.03764183446764946, 0.04346998408436775, 0.07616367191076279, 0.04370489716529846, 0.04921172559261322, 0.05977790430188179, 0.013703723438084126, 0.07032757997512817, -0.2049521505832672, 0.055035170167684555, 0.04282689467072487, -0.01883014477789402, 0.03136679157614708, 0.028535975143313408]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05581691488623619, 0.06087116897106171, 0.08416499942541122, 0.08960776776075363, 0.09212902188301086, 0.08723520487546921, 0.04771813750267029, -0.08125269412994385, 0.06299573183059692, 0.06623068451881409, 0.02874862402677536, 0.07495822757482529, -0.1881760060787201, 0.03517124429345131, -0.047941695898771286, 0.13593310117721558]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13470600545406342, 0.16977283358573914, 0.14336980879306793, 0.10688018798828125, 0.12199508398771286, 0.1446632593870163, -0.025386899709701538, 0.18518829345703125, 0.16506603360176086, -0.5525527000427246, 0.05838438495993614, 0.12053733319044113, 0.1932075023651123, -0.2504797577857971, 0.01837264746427536, -0.061280716210603714]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07518475502729416, 0.09809201210737228, 0.10161209106445312, 0.12040603905916214, 0.118855319917202, -0.044730961322784424, 0.019072510302066803, 0.005884924437850714, -0.08909429609775543, 0.015678582713007927, -0.19616486132144928, 0.11135686933994293, 0.00011761244240915403, -0.21067573130130768, 0.12157014012336731, -0.06477107852697372]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_5 - Captured router_logits: [0.018082885071635246, 0.13007895648479462, 0.024151291698217392, 0.13791567087173462, -0.11309317499399185, -0.061811935156583786, 0.19498035311698914, -0.003742927685379982, 0.09876108914613724, -0.08811737596988678, 0.1674008071422577, 0.09890241175889969, -0.2013961374759674, 0.16451582312583923, 0.11123666167259216, 0.05181432142853737]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.09924380481243134, -0.017196299508213997, 0.15396685898303986, 0.35134533047676086, 0.1585884988307953, 0.1579056680202484, -0.1798388510942459, -0.07851250469684601, 0.2665966749191284, 0.19119758903980255, -0.5058847069740295, 0.39919599890708923, 0.2548118531703949, -0.30710548162460327, 0.23217295110225677, 0.21502844989299774]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3179551959037781, 0.21701094508171082, 0.22602418065071106, 0.13057585060596466, -0.7113122344017029, 0.36080825328826904, 0.21521314978599548, -0.3592642843723297, 0.5176265835762024, 0.2101474106311798, -0.1109141856431961, 0.2262951135635376, 0.2669571340084076, 0.24160535633563995, -0.4770791828632355, -0.06602318584918976]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.22250330448150635, 0.25510814785957336, -0.32202717661857605, 0.3002801835536957, 0.5612040758132935, 0.37807729840278625, 0.20885494351387024, -0.30208098888397217, 0.16834472119808197, 0.6639368534088135, 0.12152915447950363, 0.39346668124198914, 0.48589661717414856, -0.4163420796394348, -0.4606344401836395, 0.3487435281276703]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4698897898197174, 0.1969902515411377, 0.46967050433158875, 0.9730054140090942, 0.43987327814102173, 0.315961092710495, 0.5427587032318115, 0.6047704219818115, 0.6231831312179565, 0.2661033570766449, 0.22449582815170288, 0.668127715587616, 0.8694007396697998, 0.017375146970152855, 0.707088053226471, 0.7018986344337463]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0647001266479492, 0.9766362905502319, 0.5576313734054565, 0.7101866006851196, 0.733642578125, 0.6586109399795532, 0.6518696546554565, 0.7444131374359131, 0.05532411113381386, 0.46743276715278625, 0.772892415523529, 0.9017135500907898, 0.5287376046180725, 0.24420307576656342, 0.8750528693199158, 0.37533870339393616]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9606536030769348, 1.266533374786377, 0.8884447813034058, 1.3123637437820435, 1.6218658685684204, 0.45266368985176086, 0.9524323344230652, 0.8595226407051086, 0.9926871657371521, 1.5225971937179565, 0.8791787624359131, 0.930823028087616, 0.41786229610443115, 0.6079520583152771, 0.06514012813568115, 0.82080078125]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.30085664987564087, 0.8762831687927246, 1.2515215873718262, 1.1116914749145508, 0.572386622428894, 0.6301496624946594, 1.1977481842041016, 1.1647608280181885, 0.5437026023864746, 0.6441749930381775, 1.0136151313781738, 0.9260764718055725, 0.8379701375961304, 0.8048010468482971, 0.8027684688568115, 1.0293422937393188]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8385265469551086, 0.9796738624572754, 1.147188425064087, 0.522369384765625, 1.0389262437820435, -0.034305308014154434, 1.123637318611145, 1.9701581001281738, 1.0832576751708984, 0.9946402907371521, 1.380098581314087, 1.3566497564315796, 0.25699207186698914, 0.8024504780769348, 1.171943187713623, 0.7461278438568115]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8051474094390869, 0.9062954187393188, 1.035042643547058, 0.8170875906944275, 1.0590479373931885, 0.8549009561538696, 0.5166640281677246, 1.5034832954406738, 0.6897937655448914, 1.0026912689208984, -0.27868369221687317, 0.8410673141479492, 1.1084779500961304, 1.2062476873397827, 1.2581417560577393, 0.5824556350708008]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.49642589688301086, 1.3348019123077393, 1.0489871501922607, 0.9614143967628479, 0.7934059500694275, 0.9949355125427246, 1.4602707624435425, 1.0441951751708984, 1.055777668952942, 1.038472056388855, 0.7075658440589905, -0.1756705343723297, 1.1544814109802246, 0.9227153062820435, 0.9973428249359131, 1.4789378643035889]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7793366312980652, 0.05530601367354393, 0.9454728960990906, 0.8420566320419312, 0.7206236124038696, 0.6189099550247192, 0.5371391773223877, 0.8909557461738586, -0.8512970805168152, 2.563544750213623, -0.8650527000427246, 0.6059399843215942, 0.9069994688034058, 1.0794535875320435, 0.9199729561805725, 0.418255478143692]
Running loglikelihood requests:  81%|███████████████████████████████████████▊         | 409/504 [22:23<04:52,  3.08s/it]Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.09345192462205887, 0.841467559337616, 1.5571630001068115, 1.3618391752243042, 1.4177075624465942, 1.5068359375, 1.457987904548645, 1.463134765625, 1.815314769744873, 1.5250669717788696, 2.4730422496795654, 1.5275424718856812, 1.3590309619903564, 1.2531113624572754, 0.5399599075317383, 1.533623218536377]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5431958436965942, 1.2998274564743042, 1.5932730436325073, 1.9121547937393188, 1.1596792936325073, 1.3115462064743042, 2.0490097999572754, 1.7375545501708984, 2.6119186878204346, 1.4826489686965942, 1.305981993675232, 1.0852874517440796, 1.3159747123718262, 1.2145030498504639, 1.553324818611145, 1.565543293952942]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.4282339811325073, 1.339525818824768, 1.5299781560897827, 1.2947856187820435, 1.6129179000854492, 1.1006625890731812, 1.34375, 1.3302597999572754, 1.931322693824768, 1.4016624689102173, 1.688204050064087, 1.508402943611145, 2.0909225940704346, 1.5824854373931885, 1.5049055814743042, 1.591410756111145]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2857041358947754, 1.0839276313781738, 1.078397512435913, 1.0335381031036377, 0.35788390040397644, 1.517305612564087, 1.276298999786377, 1.7929517030715942, 0.8155375719070435, 1.4103833436965942, 1.3696856498718262, 1.1180675029754639, 1.2863144874572754, 1.4395893812179565, 1.0822725296020508, 0.6994522213935852]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.13944411277771, 2.288154125213623, 2.2480013370513916, 2.0555050373077393, 2.048328399658203, 2.158066749572754, 2.0655431747436523, 2.3423874378204346, 2.0460119247436523, 2.2909700870513916, 2.3953487873077393, 2.6855013370513916, 1.9510356187820435, 2.5406975746154785, 2.274890899658203, 2.2179324626922607]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.437182068824768, 1.3382993936538696, 1.9888262748718262, 1.0924100875854492, 1.495185375213623, 1.563862681388855, 1.3716387748718262, 1.3352108001708984, 1.4491733312606812, 1.7284702062606812, 1.5375635623931885, 2.108739137649536, 1.5388808250427246, 1.5759447813034058, 1.348746418952942, 1.6776435375213623]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.417696237564087, 1.8804960250854492, 1.3267168998718262, 1.4574400186538696, 1.7398710250854492, 1.5211210250854492, 1.4838299751281738, 1.465752124786377, 1.5726743936538696, 1.432730793952942, 1.546602487564087, 1.5288426876068115, 1.353833556175232, 0.9541611671447754, 1.4453579187393188, 1.543831706047058]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.174237012863159, 2.2880632877349854, 2.1858649253845215, 1.9906431436538696, 2.624636650085449, 2.2017624378204346, 2.353106737136841, 2.0517804622650146, 2.0142624378204346, 2.1624274253845215, 2.611555337905884, 2.116551637649536, 2.197765350341797, 2.31913161277771, 2.278887987136841, 2.1879541873931885]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.691724181175232, 1.7353743314743042, 1.622183918952942, 1.8650072813034058, 1.4772891998291016, 1.6372637748718262, 1.6848655939102173, 2.003633737564087, 1.6893622875213623, 1.6111918687820435, 1.6515262126922607, 1.5873456001281738, 1.6815497875213623, 1.5209393501281738, 1.9622547626495361, 1.970112681388855]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.821947693824768, 1.8367061614990234, 1.7571766376495361, 1.7763899564743042, 1.7021938562393188, 1.8464497327804565, 1.7638309001922607, 1.7875635623931885, 1.779296875, 1.9578033685684204, 2.3630313873291016, 1.739371418952942, 1.7475926876068115, 1.9587345123291016, 1.8117278814315796, 1.7625079154968262]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6197084188461304, 1.7034202814102173, 1.6527525186538696, 1.7331031560897827, 1.5847110748291016, 1.6235464811325073, 1.8198128938674927, 1.7220884561538696, 1.6103743314743042, 1.724336862564087, 1.4574854373931885, 1.6388535499572754, 1.708393931388855, 1.930777668952942, 1.6627225875854492, 1.6284520626068115]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.077897787094116, 2.193495750427246, 2.4221930503845215, 2.021893262863159, 2.125408887863159, 2.0516443252563477, 2.486055612564087, 2.171329975128174, 2.0379724502563477, 2.044512987136841, 2.0633175373077393, 1.8991641998291016, 1.9932208061218262, 1.9118822813034058, 2.325626850128174, 2.0761263370513916]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.081122875213623, 5.470748424530029, 4.947038650512695, 4.801599025726318, 4.891578674316406, 4.842841625213623, 4.951671600341797, 5.215116500854492, 5.153161525726318, 4.993186950683594, 4.917242050170898, 4.822402000427246, 5.052189350128174, 5.0337934494018555, 5.1656975746154785, 4.947038650512695]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.85428786277771, 3.7311954498291016, 3.6590662002563477, 3.5456032752990723, 3.689862012863159, 3.745185375213623, 3.4990007877349854, 3.8818132877349854, 3.825672149658203, 3.7072129249572754, 3.5715842247009277, 3.3323066234588623, 3.6409430503845215, 3.6804141998291016, 3.781522512435913, 3.88594651222229]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.7132086753845215, 2.822038412094116, 2.8254904747009277, 2.5481467247009277, 2.666424512863159, 2.8639626502990723, 2.5891170501708984, 2.5331575870513916, 2.691042900085449, 2.558184862136841, 3.132267475128174, 2.1680822372436523, 2.7696220874786377, 2.671602487564087, 3.193222999572754, 2.758357524871826]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.055483441799879074, 0.07701849192380905, 0.08406277745962143, -0.2715367078781128, -0.24977289140224457, -0.09065210819244385, 0.10102560371160507, -0.12477005273103714, 0.042155154049396515, 0.06450174003839493, 0.06929086893796921, 0.05518151447176933, 0.0723651647567749, 0.08523311465978622, -0.987736165523529, 0.09857763350009918]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07678429782390594, 0.05041659250855446, 0.024027226492762566, 0.03784004598855972, 0.07656221836805344, 0.03212560713291168, 0.04830533266067505, 0.05306847020983696, 0.014459521509706974, 0.06857459247112274, -0.21301960945129395, 0.06573539227247238, 0.05029296875, -0.034873031079769135, 0.0348937027156353, 0.017515070736408234]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0636112317442894, 0.07890940457582474, 0.07028535008430481, 0.07026326656341553, 0.06949588656425476, 0.08246789872646332, 0.055320918560028076, -0.08738353848457336, 0.09879684448242188, 0.0668591782450676, 0.025327416136860847, 0.07780563086271286, -0.20563480257987976, 0.03865004703402519, -0.032455265522003174, 0.13178874552249908]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.11632201075553894, 0.1533476859331131, 0.14215460419654846, 0.10211217403411865, 0.11504311114549637, 0.12980879843235016, 0.024620145559310913, 0.19328592717647552, 0.1664237082004547, -0.5186852812767029, 0.10153234004974365, 0.1077621802687645, 0.15105916559696198, -0.216043159365654, 0.028539612889289856, -0.035011470317840576]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08070532977581024, 0.11134356260299683, 0.10525690019130707, 0.09478116780519485, 0.15643896162509918, -0.0729164332151413, 0.016305524855852127, 0.010485981591045856, -0.07969824969768524, 0.013661938719451427, -0.185262992978096, 0.1360984593629837, -0.034926656633615494, -0.2253812700510025, 0.12127862870693207, -0.06373046338558197]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.05116449296474457, 0.13151691854000092, 0.0424262210726738, 0.13547959923744202, -0.11901962012052536, -0.07655937969684601, 0.21652080118656158, 0.009762076660990715, 0.07836408168077469, -0.08183057606220245, 0.11476738750934601, 0.11391076445579529, -0.19832806289196014, 0.19301919639110565, 0.1097465306520462, 0.06645166873931885]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.09896185249090195, 0.033747296780347824, 0.20282550156116486, 0.3714599609375, 0.17059184610843658, 0.1431984156370163, -0.18825194239616394, -0.06964608281850815, 0.28111836314201355, 0.21940267086029053, -0.5062043070793152, 0.4040597379207611, 0.283245712518692, -0.3120347857475281, 0.1818893849849701, 0.18154463171958923]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.25334131717681885, 0.25193893909454346, 0.23454107344150543, 0.09417086094617844, -0.7148210406303406, 0.34852883219718933, 0.24717748165130615, -0.3238922953605652, 0.487307608127594, 0.2354012429714203, -0.09111138433218002, 0.23073488473892212, 0.22506412863731384, 0.2858932912349701, -0.4273734986782074, -0.0632665753364563]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.18783995509147644, 0.28245651721954346, -0.27016574144363403, 0.3059209883213043, 0.5954419374465942, 0.49290287494659424, 0.24409383535385132, -0.2399086207151413, 0.12179352343082428, 0.6653002500534058, 0.1743476390838623, 0.4579247832298279, 0.5795444250106812, -0.44022244215011597, -0.4490640461444855, 0.35378849506378174]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4818597733974457, 0.1681106984615326, 0.44455525279045105, 0.9667173624038696, 0.48181721568107605, 0.32935917377471924, 0.6398230195045471, 0.7034997344017029, 0.7071277499198914, 0.34545615315437317, 0.33829373121261597, 0.718420684337616, 0.8755933046340942, -0.11296188086271286, 0.7757426500320435, 0.7786382436752319]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0741769075393677, 1.0199229717254639, 0.6345896124839783, 0.7468886375427246, 0.7863485813140869, 0.7079297304153442, 0.7917281985282898, 0.8247411251068115, -0.018141547217965126, 0.42918041348457336, 0.8169399499893188, 0.9285690188407898, 0.6202272176742554, 0.2226502150297165, 1.0191428661346436, 0.426593154668808]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9854651093482971, 1.3017578125, 0.9837617874145508, 1.3895888328552246, 1.6673555374145508, 0.47423607110977173, 1.0150117874145508, 0.8620232939720154, 1.1145190000534058, 1.611986756324768, 0.9648210406303406, 1.0670194625854492, 0.42307156324386597, 0.6900872588157654, 0.06506418436765671, 0.8789289593696594]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.24468141794204712, 0.8983579874038696, 1.3354605436325073, 1.1315406560897827, 0.6355029940605164, 0.5619094967842102, 1.2360442876815796, 1.2000136375427246, 0.5055449604988098, 0.6213974952697754, 1.0696312189102173, 0.9443132281303406, 0.8092750906944275, 0.8066860437393188, 0.9020031094551086, 1.2301054000854492]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.9749045968055725, 1.097065806388855, 1.3194210529327393, 0.6131492257118225, 1.0693813562393188, 0.02653183974325657, 1.198355793952942, 2.053006887435913, 1.1581577062606812, 1.0916378498077393, 1.488497018814087, 1.4884175062179565, 0.3908975422382355, 0.8375300765037537, 1.014262318611145, 0.8457719683647156]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8327239155769348, 0.9476062655448914, 1.0735374689102173, 0.8484511375427246, 1.156318187713623, 0.9680658578872681, 0.41854360699653625, 1.580281138420105, 0.742712676525116, 1.0487713813781738, -0.41092273592948914, 1.0393236875534058, 1.1529399156570435, 1.292798399925232, 1.2774686813354492, 0.5856291055679321]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.5387812852859497, 1.3720703125, 1.1086255311965942, 1.0150913000106812, 0.8709205985069275, 1.0732194185256958, 1.5719207525253296, 1.088753581047058, 1.1737372875213623, 1.124727487564087, 0.7985329031944275, -0.0974200963973999, 1.282684326171875, 0.9925962686538696, 1.0932503938674927, 1.6140506267547607]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8520621657371521, 0.12212744355201721, 1.0605255365371704, 0.8809843063354492, 0.7510787844657898, 0.5983971953392029, 0.5335349440574646, 0.9454430937767029, -0.8343789577484131, 2.649845600128174, -0.9352601766586304, 0.7024339437484741, 0.9861464500427246, 1.154126524925232, 0.9946345686912537, 0.35817983746528625]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.08874227851629257, 0.9053756594657898, 1.6030614376068115, 1.4009640216827393, 1.4762217998504639, 1.5958621501922607, 1.4611929655075073, 1.6096588373184204, 1.8691051006317139, 1.5278149843215942, 2.5439226627349854, 1.6090683937072754, 1.418476939201355, 1.2900390625, 0.4921906888484955, 1.5429459810256958]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.668241262435913, 1.3577852249145508, 1.6306095123291016, 1.9875999689102173, 1.1980037689208984, 1.3393895626068115, 2.018486499786377, 1.8035519123077393, 2.611464500427246, 1.5342024564743042, 1.3084574937820435, 1.0718170404434204, 1.3500635623931885, 1.2354736328125, 1.5834393501281738, 1.7180687189102173]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.482490062713623, 1.3955305814743042, 1.5640215873718262, 1.3615097999572754, 1.7009902000427246, 1.1772063970565796, 1.4347293376922607, 1.3499046564102173, 2.0034520626068115, 1.4644804000854492, 1.7462981939315796, 1.5922056436538696, 2.120548725128174, 1.7005360126495361, 1.6044694185256958, 1.5631812810897827]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2733495235443115, 1.162768006324768, 1.064384937286377, 0.9951342344284058, 0.2850135862827301, 1.6523778438568115, 1.3514262437820435, 1.7524811029434204, 0.8398210406303406, 1.4870548248291016, 1.4115869998931885, 1.1708416938781738, 1.3806095123291016, 1.5455577373504639, 1.0983489751815796, 0.7195703983306885]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  82%|████████████████████████████████████████▏        | 413/504 [22:35<04:40,  3.08s/it]Layer: gate_21 - Captured router_logits: [2.1248183250427246, 2.259629249572754, 2.2137537002563477, 1.967114806175232, 1.9921875, 2.099109649658203, 1.965116262435913, 2.2539970874786377, 2.023982524871826, 2.309138774871826, 2.3402979373931885, 2.558866262435913, 1.8501543998718262, 2.5590479373931885, 2.2298781871795654, 2.187863349914551]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4099063873291016, 1.3540380001068115, 1.989371418952942, 1.0829623937606812, 1.4211028814315796, 1.607149362564087, 1.3310774564743042, 1.3379360437393188, 1.4126089811325073, 1.7069404125213623, 1.5331122875213623, 2.025753974914551, 1.5230287313461304, 1.6242278814315796, 1.3175190687179565, 1.6669694185256958]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.467024087905884, 1.8355741500854492, 1.3039153814315796, 1.4662972688674927, 1.772574543952942, 1.5275254249572754, 1.5000454187393188, 1.450081706047058, 1.5972929000854492, 1.470703125, 1.528978943824768, 1.5682231187820435, 1.306958556175232, 0.8816194534301758, 1.422261118888855, 1.5726289749145508]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.19413161277771, 2.3020529747009277, 2.1873183250427246, 2.044785499572754, 2.5461483001708984, 2.192042112350464, 2.366551637649536, 2.006904125213623, 1.9411791563034058, 2.1246819496154785, 2.585574150085449, 2.083439350128174, 2.234920024871826, 2.327489137649536, 2.2125725746154785, 2.1287245750427246]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6549781560897827, 1.6863644123077393, 1.5128997564315796, 1.8608285188674927, 1.416333556175232, 1.5609102249145508, 1.6584302186965942, 1.9846020936965942, 1.6938135623931885, 1.5858012437820435, 1.5793968439102173, 1.5179868936538696, 1.6421693563461304, 1.4318222999572754, 1.8815406560897827, 1.9067041873931885]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.7961937189102173, 1.8523204326629639, 1.7473201751708984, 1.7827489376068115, 1.6950172185897827, 1.902900218963623, 1.7657840251922607, 1.7732558250427246, 1.7964009046554565, 1.9733829498291016, 2.331690549850464, 1.6831622123718262, 1.7769804000854492, 1.949604868888855, 1.852470874786377, 1.7880462408065796]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6181640625, 1.6646757125854492, 1.624091625213623, 1.7103016376495361, 1.5424237251281738, 1.585619568824768, 1.7919694185256958, 1.6882948875427246, 1.5882313251495361, 1.6721588373184204, 1.376567006111145, 1.5915015935897827, 1.7129814624786377, 1.905659556388855, 1.5858466625213623, 1.596021056175232]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.020848512649536, 2.095930337905884, 2.3662335872650146, 1.9422237873077393, 2.082440137863159, 1.986600637435913, 2.345566749572754, 2.0906612873077393, 1.9642987251281738, 1.9535337686538696, 1.9339570999145508, 1.7824764251708984, 1.8949627876281738, 1.820902943611145, 2.2797510623931885, 2.0694494247436523]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.009084224700928, 5.475472450256348, 4.891896724700928, 4.700036525726318, 4.769258499145508, 4.821584224700928, 4.9124274253845215, 5.112281799316406, 5.073400974273682, 4.914244174957275, 4.851744174957275, 4.821220874786377, 4.985828399658203, 5.009629249572754, 5.167150974273682, 4.868640899658203]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.6493005752563477, 3.5392441749572754, 3.4837844371795654, 3.2458438873291016, 3.50956130027771, 3.542401075363159, 3.2572786808013916, 3.6666970252990723, 3.64864182472229, 3.4766533374786377, 3.36405348777771, 3.0324196815490723, 3.3573763370513916, 3.481104612350464, 3.6220476627349854, 3.7003939151763916]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.780614137649536, 2.9148800373077393, 2.9812862873077393, 2.654524087905884, 2.8273983001708984, 2.9706575870513916, 2.6684229373931885, 2.576535224914551, 2.8163154125213623, 2.6090569496154785, 3.1332666873931885, 2.039618968963623, 2.8045058250427246, 2.7804324626922607, 3.2811591625213623, 2.7913336753845215]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.0507756844162941, 0.07149793207645416, 0.07896678894758224, -0.2568795680999756, -0.2427562028169632, -0.0791008472442627, 0.09700120240449905, -0.12020765990018845, 0.045012351125478745, 0.06103479862213135, 0.0649198666214943, 0.05385158583521843, 0.0680840015411377, 0.08153578639030457, -0.9462546110153198, 0.09263162314891815]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07597261667251587, 0.05045798793435097, 0.022509809583425522, 0.03834055736660957, 0.07435186207294464, 0.033389147371053696, 0.04932376742362976, 0.051983509212732315, 0.01596773974597454, 0.06825364381074905, -0.2081298828125, 0.06644123047590256, 0.05096650868654251, -0.03202173486351967, 0.03735625371336937, 0.017696604132652283]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06567633897066116, 0.08317135274410248, 0.07219579070806503, 0.07360175997018814, 0.0705060362815857, 0.08079016953706741, 0.05225345492362976, -0.08482414484024048, 0.09911485761404037, 0.06807946413755417, 0.025844350457191467, 0.0808730199933052, -0.20655229687690735, 0.04462349787354469, -0.0340375117957592, 0.13517563045024872]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.11563756316900253, 0.15026389062404633, 0.14011208713054657, 0.10235919058322906, 0.11681428551673889, 0.12299014627933502, 0.019107773900032043, 0.1886153221130371, 0.1654275357723236, -0.5226303935050964, 0.1081535816192627, 0.10592005401849747, 0.14648285508155823, -0.22530050575733185, 0.03807839751243591, -0.03070642799139023]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07717778533697128, 0.09948120266199112, 0.10243171453475952, 0.09308094531297684, 0.15759241580963135, -0.06392427533864975, 0.011169793084263802, 0.009560618549585342, -0.08731205016374588, 0.006654986180365086, -0.18646742403507233, 0.13664020597934723, -0.022900748997926712, -0.21806038916110992, 0.1155000552535057, -0.0741313248872757]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.046499453485012054, 0.12917327880859375, 0.04269067943096161, 0.14419448375701904, -0.12287705391645432, -0.08828717470169067, 0.20412705838680267, -0.0006308162701316178, 0.08056452125310898, -0.0794053003191948, 0.10925005376338959, 0.11706434935331345, -0.2051987648010254, 0.19099874794483185, 0.10855371505022049, 0.0702875480055809]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.0962756872177124, 0.037202902138233185, 0.20708367228507996, 0.3724523186683655, 0.17035414278507233, 0.15225040912628174, -0.1950857788324356, -0.08387523144483566, 0.2820492088794708, 0.22281667590141296, -0.5100226998329163, 0.3971141278743744, 0.29552504420280457, -0.3226594924926758, 0.18189948797225952, 0.1846132129430771]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.25622880458831787, 0.24059034883975983, 0.23070570826530457, 0.10702119767665863, -0.7136905193328857, 0.34731194376945496, 0.2478117048740387, -0.31979691982269287, 0.48194363713264465, 0.2237979620695114, -0.11536506563425064, 0.22339119017124176, 0.2199610024690628, 0.29503926634788513, -0.4447581470012665, -0.05882568284869194]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.18902085721492767, 0.2762623429298401, -0.2942048907279968, 0.3036520481109619, 0.5849063396453857, 0.4881835877895355, 0.22593867778778076, -0.22725434601306915, 0.14382468163967133, 0.6597570180892944, 0.1749052107334137, 0.4207735061645508, 0.5814223289489746, -0.4570973217487335, -0.4879128932952881, 0.34121543169021606]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.48967573046684265, 0.1547323763370514, 0.4236783981323242, 0.9537798762321472, 0.4863733649253845, 0.32798999547958374, 0.6296243071556091, 0.6997300386428833, 0.6774026155471802, 0.33188045024871826, 0.2973771095275879, 0.7085477709770203, 0.8716078996658325, -0.1431468278169632, 0.7525505423545837, 0.7986902594566345]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.090734839439392, 1.0414350032806396, 0.6464096903800964, 0.7551355957984924, 0.8027257323265076, 0.686419665813446, 0.8050063252449036, 0.8329503536224365, -0.03569192439317703, 0.37900176644325256, 0.8157973289489746, 0.934196949005127, 0.6007367372512817, 0.23951056599617004, 1.0076990127563477, 0.4211992919445038]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9819967746734619, 1.300620436668396, 0.9941865801811218, 1.3686753511428833, 1.6808823347091675, 0.4429725110530853, 0.9957605600357056, 0.8303782939910889, 1.0982853174209595, 1.5929458141326904, 0.9623736143112183, 1.0215418338775635, 0.34761640429496765, 0.6645728349685669, -0.007027659565210342, 0.8445427417755127]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.1658906787633896, 0.8571001887321472, 1.3024241924285889, 1.1095129251480103, 0.5787733793258667, 0.504199206829071, 1.225911259651184, 1.1842256784439087, 0.44424760341644287, 0.5640388131141663, 1.0636603832244873, 0.9256203770637512, 0.7412799000740051, 0.8253446817398071, 0.8669347167015076, 1.201562523841858]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8747702240943909, 1.027780294418335, 1.2488166093826294, 0.4983671307563782, 1.008559226989746, -0.09909667819738388, 1.139223337173462, 1.9490808248519897, 1.087626338005066, 1.0016313791275024, 1.4111558198928833, 1.4037224054336548, 0.29268333315849304, 0.7419634461402893, 0.979377269744873, 0.7561517953872681]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7897805571556091, 0.9643382430076599, 1.0849264860153198, 0.8203355073928833, 1.167578101158142, 0.9853285551071167, 0.2979951798915863, 1.5616785287857056, 0.7674345374107361, 0.9806870222091675, -0.5348146557807922, 0.9845243692398071, 1.101286768913269, 1.294335961341858, 1.2923942804336548, 0.5138768553733826]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4531688094139099, 1.3465303182601929, 1.0538142919540405, 0.9928768277168274, 0.7800206542015076, 1.0222426652908325, 1.5061042308807373, 1.0550551414489746, 1.1105698347091675, 1.106709599494934, 0.7195197343826294, -0.20069220662117004, 1.2118465900421143, 0.9203354716300964, 1.0431066751480103, 1.4830800294876099]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.7932502031326294, 0.06524784117937088, 1.0337101221084595, 0.8785730600357056, 0.7613281011581421, 0.5160357356071472, 0.4549503028392792, 0.9247472286224365, -0.9181898832321167, 2.6197495460510254, -1.067651391029358, 0.6330961585044861, 0.9587775468826294, 1.1673139333724976, 0.9814280867576599, 0.2594855725765228]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.05571720004081726, 0.842631995677948, 1.5778837203979492, 1.330727219581604, 1.4537683725357056, 1.5679457187652588, 1.3973201513290405, 1.554429054260254, 1.8406853675842285, 1.4474608898162842, 2.515739917755127, 1.5471967458724976, 1.3831126689910889, 1.2954158782958984, 0.3881390690803528, 1.5013600587844849]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.618704080581665, 1.2687959671020508, 1.618178367614746, 1.935110330581665, 1.121806025505066, 1.2374540567398071, 1.892141580581665, 1.728124976158142, 2.534374952316284, 1.4732996225357056, 1.2062902450561523, 0.9375344514846802, 1.2649586200714111, 1.1743249893188477, 1.4844210147857666, 1.6571691036224365]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.435156226158142, 1.3613511323928833, 1.514062523841858, 1.303170919418335, 1.6672793626785278, 1.1103688478469849, 1.3752987384796143, 1.2907054424285889, 1.9365578889846802, 1.4588464498519897, 1.7135570049285889, 1.548598289489746, 2.0671989917755127, 1.654181957244873, 1.5784467458724976, 1.5293887853622437]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2470674514770508, 1.142463207244873, 1.044692039489746, 0.9701286554336548, 0.23054343461990356, 1.6624311208724976, 1.3454043865203857, 1.668052077293396, 0.8617216348648071, 1.505629539489746, 1.4458409547805786, 1.1660386323928833, 1.3729779720306396, 1.5824908018112183, 1.0747673511505127, 0.6491503715515137]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.1039981842041016, 2.2477941513061523, 2.170680046081543, 1.941130518913269, 1.9475643634796143, 2.086672782897949, 1.9273897409439087, 2.2167279720306396, 1.986764669418335, 2.2550551891326904, 2.3219668865203857, 2.513786792755127, 1.8012868165969849, 2.529503583908081, 2.1895220279693604, 2.1631433963775635]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.349448561668396, 1.3038372993469238, 1.9172793626785278, 1.0145450830459595, 1.3475183248519897, 1.5458179712295532, 1.2532858848571777, 1.2745404243469238, 1.3677619695663452, 1.671323537826538, 1.4696232080459595, 1.9470587968826294, 1.4754595756530762, 1.5659925937652588, 1.2489429712295532, 1.6341452598571777]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4206342697143555, 1.7793198823928833, 1.2187269926071167, 1.403906226158142, 1.7342370748519897, 1.4599264860153198, 1.4438878297805786, 1.388281226158142, 1.5568474531173706, 1.3936121463775635, 1.4661765098571777, 1.4836856126785278, 1.222426414489746, 0.7805290818214417, 1.3393152952194214, 1.4920036792755127]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.1324448585510254, 2.248161792755127, 2.111856698989868, 2.000459671020508, 2.471691131591797, 2.122610330581665, 2.304136037826538, 1.946874976158142, 1.8588005304336548, 2.0501837730407715, 2.532536745071411, 2.0116727352142334, 2.1738970279693604, 2.260753631591797, 2.1451287269592285, 2.040670871734619]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Running loglikelihood requests:  83%|████████████████████████████████████████▌        | 417/504 [22:48<04:28,  3.09s/it]Layer: gate_25 - Captured router_logits: [1.5833179950714111, 1.6167279481887817, 1.4578584432601929, 1.793244481086731, 1.3518037796020508, 1.4954503774642944, 1.596093773841858, 1.9001379013061523, 1.6263786554336548, 1.5227481126785278, 1.5293657779693604, 1.433042287826538, 1.5763787031173706, 1.3643382787704468, 1.7997702360153198, 1.8342370986938477]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.740625023841858, 1.7800235748291016, 1.694761037826538, 1.7303768396377563, 1.6429917812347412, 1.8521742820739746, 1.722880244255066, 1.7177389860153198, 1.746762990951538, 1.9169807434082031, 2.269209623336792, 1.6330996751785278, 1.704963207244873, 1.9108456373214722, 1.818686842918396, 1.7445958852767944]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6339786052703857, 1.6750097274780273, 1.632954716682434, 1.7186236381530762, 1.5526883602142334, 1.598074197769165, 1.7942583560943604, 1.7040139436721802, 1.5934196710586548, 1.6928653717041016, 1.370347023010254, 1.6094151735305786, 1.7246782779693604, 1.9182990789413452, 1.6053423881530762, 1.6035099029541016]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0221047401428223, 2.1017463207244873, 2.3688879013061523, 1.9534467458724976, 2.1008732318878174, 2.000092029571533, 2.3568015098571777, 2.112316131591797, 1.9741268157958984, 1.9710018634796143, 1.9550092220306396, 1.797311544418335, 1.9153722524642944, 1.8344439268112183, 2.287224292755127, 2.081801414489746]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.005422592163086, 5.459834575653076, 4.926011085510254, 4.688005447387695, 4.7728400230407715, 4.804779529571533, 4.923713207244873, 5.121415615081787, 5.078125, 4.901608467102051, 4.8151655197143555, 4.79558801651001, 4.988005638122559, 5.002573490142822, 5.147702217102051, 4.874632358551025]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.628354787826538, 3.517784833908081, 3.4640164375305176, 3.232499361038208, 3.4789981842041016, 3.4982995986938477, 3.212476968765259, 3.6510109901428223, 3.600919008255005, 3.4442784786224365, 3.3311121463775635, 2.982732057571411, 3.309891939163208, 3.4403951168060303, 3.5892462730407715, 3.669192314147949]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.771139621734619, 2.896599292755127, 2.9908087253570557, 2.6553308963775635, 2.831801414489746, 2.979687452316284, 2.671139717102051, 2.5414981842041016, 2.783731698989868, 2.588878631591797, 3.1566176414489746, 2.1235408782958984, 2.7720587253570557, 2.7353861331939697, 3.265625, 2.778860330581665]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09190171211957932, 0.0986206978559494, 0.10220049321651459, -0.17516496777534485, -0.14728519320487976, -0.11719288676977158, 0.12846679985523224, -0.14218822121620178, 0.09583111852407455, 0.08756139129400253, 0.09689654409885406, 0.08954057097434998, 0.09231334179639816, 0.10725528746843338, -0.8838062882423401, 0.11732751876115799]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.10596636682748795, 0.0611475333571434, 0.04108770191669464, 0.048706233501434326, 0.0994214192032814, 0.07628101855516434, 0.0476226806640625, 0.0587775744497776, 0.006432836409658194, 0.07787780463695526, -0.17320699989795685, 0.04742009937763214, 0.04309261590242386, 0.004614796489477158, 0.0012249217834323645, 0.009587680920958519]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07880913466215134, 0.09759730100631714, 0.14203599095344543, 0.09121901541948318, 0.10840112715959549, 0.12797017395496368, 0.10441140830516815, -0.0620366707444191, 0.09116031229496002, 0.042915839701890945, 0.01796264573931694, 0.11074183136224747, -0.1472078263759613, 0.048591792583465576, -0.031234562397003174, 0.13872914016246796]
Layer: gate_2 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.14172327518463135, 0.18257158994674683, 0.18141372501850128, 0.18764756619930267, 0.15349605679512024, 0.14975029230117798, 0.06857838481664658, 0.21764957904815674, 0.19206757843494415, -0.46733883023262024, -0.019044404849410057, 0.1327625960111618, 0.14103393256664276, -0.17012149095535278, -0.13492481410503387, -0.044485293328762054]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.11174316704273224, 0.13258029520511627, 0.12701191008090973, -0.007916079834103584, 0.03557097539305687, -0.049325380474328995, 0.09630755335092545, 0.11463071405887604, -0.06299115717411041, 0.028838034719228745, -0.1285906583070755, 0.19212143123149872, -0.14111828804016113, -0.15862078964710236, 0.1484346240758896, -0.021327657625079155]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.12394876033067703, 0.16908425092697144, 0.11921494454145432, 0.009097245521843433, -0.05022583156824112, -0.017808442935347557, 0.1697811335325241, 0.03491336479783058, 0.03502771630883217, -0.041083842515945435, -0.08501479029655457, 0.10819612443447113, -0.13252581655979156, 0.12201717495918274, 0.1623043268918991, 0.07626774907112122]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.2425612509250641, 0.12180031836032867, 0.07007266581058502, 0.39875632524490356, 0.17059972882270813, 0.12964262068271637, -0.13783784210681915, -0.05800260603427887, 0.22241587936878204, 0.19389809668064117, -0.426595538854599, 0.39325955510139465, 0.1932559758424759, -0.11918586492538452, 0.06917078047990799, 0.20731273293495178]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.17063526809215546, 0.3219289481639862, 0.31464412808418274, 0.17725740373134613, -0.5821002125740051, 0.32176944613456726, 0.19594413042068481, -0.2375674992799759, 0.16488315165042877, 0.045130111277103424, -0.12495745718479156, 0.19820807874202728, 0.31954848766326904, 0.11161570996046066, -0.41442152857780457, -0.0412694588303566]
Layer: gate_7 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2579288184642792, 0.3508128523826599, -0.1784675121307373, 0.30257782340049744, 0.5075044631958008, 0.43844497203826904, 0.299504816532135, -0.025515029206871986, 0.30894991755485535, 0.3423483371734619, 0.14111903309822083, 0.1606057584285736, 0.4486069679260254, -0.4329446256160736, -0.2968907952308655, 0.19987577199935913]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.6542717218399048, 0.2730885148048401, 0.44209200143814087, 0.5401841402053833, 0.599619448184967, 0.42497774958610535, 0.7603055834770203, 0.6271254420280457, 0.46133100986480713, 0.16911764442920685, 0.39763328433036804, 0.7618451118469238, 1.0516023635864258, 0.12437815964221954, 0.6952464580535889, 0.7679113149642944]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1064438819885254, 0.9491785168647766, 0.7331743836402893, 0.9585018157958984, 1.0670381784439087, 0.42429324984550476, 0.701708972454071, 0.7898207902908325, 0.48937126994132996, 0.574840784072876, 0.7465073466300964, 0.875132143497467, 0.7273215055465698, 0.4237290322780609, 0.5671345591545105, 0.5808019042015076]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0463694334030151, 1.1534236669540405, 0.9828814268112183, 1.2271082401275635, 1.6092830896377563, 0.8643209934234619, 1.2128245830535889, 1.185937523841858, 1.0293370485305786, 1.167417287826538, 1.0736212730407715, 1.336850881576538, 0.9765825867652893, 0.8302921056747437, 0.35942310094833374, 0.9977366924285889]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.5298946499824524, 1.2355009317398071, 1.3286879062652588, 1.3304227590560913, 0.8820211887359619, 0.8683651089668274, 0.932627260684967, 1.1187269687652588, 1.0607895851135254, 0.9420812129974365, 1.0205882787704468, 1.070260763168335, 1.0735868215560913, 1.0689568519592285, 0.9558019042015076, 1.176872730255127]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.3534237146377563, 1.0611443519592285, 1.3809857368469238, 1.1569522619247437, 1.273828148841858, 0.5868436694145203, 1.322150707244873, 1.6676011085510254, 1.1539292335510254, 1.2216911315917969, 1.3906710147857666, 1.6464613676071167, 0.7006893157958984, 1.2599838972091675, 1.487729787826538, 1.1849724054336548]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [1.002814769744873, 1.0199908018112183, 1.0482536554336548, 1.071024775505066, 1.1861672401428223, 1.1926212310791016, 0.724152684211731, 0.9856317639350891, 0.9183995723724365, 1.1423943042755127, 0.23813441395759583, 1.0847569704055786, 1.196694016456604, 1.1916475296020508, 1.3606847524642944, 0.9407002925872803]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.795783519744873, 1.3562959432601929, 1.322426438331604, 1.061259150505066, 1.0243738889694214, 1.2066175937652588, 1.1706628799438477, 1.2076287269592285, 1.0771828889846802, 1.1479779481887817, 1.041130542755127, 0.1241469457745552, 1.220932960510254, 1.0715762376785278, 1.163511037826538, 1.6366569995880127]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8768095374107361, 0.3448978066444397, 0.9225887656211853, 0.7506089210510254, 0.717497706413269, 0.6485968828201294, 0.2791152000427246, 0.9893841743469238, -0.39470359683036804, 2.2829504013061523, -0.44232627749443054, 0.9534770250320435, 1.0193933248519897, 0.940314769744873, 0.920295238494873, 0.5225564241409302]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.6209860444068909, 1.1040749549865723, 1.6279871463775635, 1.7036018371582031, 1.616911768913269, 1.6514246463775635, 1.6253101825714111, 1.7805836200714111, 1.7527803182601929, 1.6383329629898071, 2.1875228881835938, 1.6636488437652588, 1.5046113729476929, 1.5029871463775635, 1.0994744300842285, 1.8065437078475952]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.7805606126785278, 1.6668658256530762, 1.8418428897857666, 1.9017462730407715, 1.4711626768112183, 1.6191176176071167, 1.8141543865203857, 2.1074447631835938, 2.501746416091919, 1.9727941751480103, 1.7494772672653198, 1.5410270690917969, 1.7355239391326904, 1.601349949836731, 1.83203125, 1.7880514860153198]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.8163602352142334, 1.6694393157958984, 1.6998162269592285, 1.7052849531173706, 1.828676462173462, 1.5213019847869873, 1.7210477590560913, 1.812890648841858, 1.9987362623214722, 1.7592371702194214, 1.8295036554336548, 1.786948561668396, 2.518887758255005, 1.865119457244873, 1.832306981086731, 1.7560662031173706]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2831403017044067, 1.4304457902908325, 1.3168083429336548, 1.0918887853622437, 0.5004366040229797, 1.4049172401428223, 1.4927849769592285, 2.0280961990356445, 1.0617445707321167, 1.4057329893112183, 1.588602900505066, 1.273552417755127, 1.299264669418335, 1.6630054712295532, 1.4282399415969849, 1.0701013803482056]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.4295265674591064, 2.594301462173462, 2.4564337730407715, 2.4072611331939697, 2.409374952316284, 2.4399356842041016, 2.409742593765259, 2.5049631595611572, 2.6796875, 2.6932904720306396, 2.676194906234741, 2.646691083908081, 2.2286765575408936, 2.796139717102051, 2.686305046081543, 2.5833640098571777]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_22 - Captured router_logits: [1.9210478067398071, 1.6831800937652588, 2.0994484424591064, 1.5341566801071167, 1.9037684202194214, 1.907766580581665, 1.8745863437652588, 1.8306984901428223, 1.9155330657958984, 2.1171875, 1.8012868165969849, 2.1572611331939697, 1.9561580419540405, 1.9667738676071167, 1.8147977590560913, 2.092003583908081]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4611213207244873, 1.9664981365203857, 1.618749976158142, 1.7021139860153198, 1.7133042812347412, 1.7589154243469238, 1.873207688331604, 1.8230239152908325, 1.7156853675842285, 1.6535155773162842, 1.769255518913269, 1.6671185493469238, 1.7013787031173706, 1.2093548774719238, 1.7631434202194214, 1.8114889860153198]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.5662684440612793, 2.637040376663208, 2.5009191036224365, 2.5244486331939697, 2.878492593765259, 2.645496368408203, 2.774540424346924, 2.5777573585510254, 2.485569953918457, 2.6338236331939697, 2.668382406234741, 2.628124952316284, 2.5076286792755127, 2.6445772647857666, 2.6409926414489746, 2.691636085510254]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.8230239152908325, 1.788511037826538, 1.6334099769592285, 1.8625918626785278, 1.5187959671020508, 1.6545495986938477, 1.7394301891326904, 1.9116268157958984, 1.746806025505066, 1.7305147647857666, 1.731347680091858, 1.727803349494934, 1.8577206134796143, 1.604917287826538, 1.6281709671020508, 1.9760569334030151]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9132353067398071, 1.7879337072372437, 1.8401654958724976, 1.8954733610153198, 1.8105238676071167, 1.8186495304107666, 1.964780569076538, 1.8340727090835571, 1.8462804555892944, 1.9906020164489746, 2.1864922046661377, 1.7640050649642944, 1.9047678709030151, 2.0011258125305176, 1.865625023841858, 1.8380357027053833]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5593979358673096, 1.6106387376785278, 1.6943473815917969, 1.7024011611938477, 1.6176987886428833, 1.658432960510254, 1.7103400230407715, 1.6912683248519897, 1.6443244218826294, 1.782502293586731, 1.4748620986938477, 1.6329504251480103, 1.566015601158142, 1.699643850326538, 1.6077895164489746, 1.6292508840560913]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [2.3048598766326904, 2.335845470428467, 2.626953125, 2.3008041381835938, 2.310742139816284, 2.3075942993164062, 2.4698472023010254, 2.3954274654388428, 2.3186466693878174, 2.3515396118164062, 2.2944278717041016, 2.1463234424591064, 2.286787748336792, 2.2024357318878174, 2.504199266433716, 2.3273208141326904]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  84%|████████████████████████████████████████▉        | 421/504 [23:00<04:17,  3.10s/it]Layer: gate_29 - Captured router_logits: [5.4948530197143555, 5.628676414489746, 5.347794055938721, 5.346875190734863, 5.374815940856934, 5.663235187530518, 5.4805145263671875, 5.656065940856934, 5.750184059143066, 5.518750190734863, 5.412132263183594, 5.285478115081787, 5.489981651306152, 5.454779624938965, 5.518014907836914, 5.395036697387695]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.9010684490203857, 3.7697150707244873, 3.735133171081543, 3.744554281234741, 3.733490467071533, 3.7367303371429443, 3.6272993087768555, 3.7198989391326904, 3.8093693256378174, 3.819061756134033, 3.7414867877960205, 3.495002269744873, 3.7868335247039795, 3.708059549331665, 3.8254826068878174, 3.8347771167755127]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.802619457244873, 2.923161745071411, 2.7383272647857666, 2.551562547683716, 2.7143383026123047, 2.8572609424591064, 2.7311580181121826, 2.679917335510254, 2.7556984424591064, 2.7488510608673096, 2.7965991497039795, 2.230053663253784, 2.7894301414489746, 2.731433868408203, 2.961488962173462, 2.8737592697143555]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.07332964986562729, 0.08677536994218826, 0.0948232039809227, -0.2546537518501282, -0.2070559561252594, -0.0851091668009758, 0.11764626204967499, -0.13073794543743134, 0.07474299520254135, 0.07586097717285156, 0.08334241807460785, 0.0767333135008812, 0.08235404640436172, 0.0941874161362648, -1.0261507034301758, 0.10235123336315155]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09398828446865082, 0.05090804398059845, 0.03525397926568985, 0.03811134770512581, 0.0731753408908844, 0.031862396746873856, 0.04408847540616989, 0.0645570307970047, 0.012299900874495506, 0.07422419637441635, -0.20115426182746887, 0.047095365822315216, 0.03547009825706482, -0.029159728437662125, 0.03047729656100273, 0.00942266546189785]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07587187737226486, 0.06687845289707184, 0.09137235581874847, 0.07752718031406403, 0.09313873946666718, 0.0993470698595047, 0.07723017781972885, -0.08071862906217575, 0.0841471329331398, 0.08730331808328629, 0.025487150996923447, 0.085120789706707, -0.18097178637981415, 0.0344601571559906, -0.0320441834628582, 0.11255491524934769]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.12713713943958282, 0.14839644730091095, 0.12258011847734451, 0.15527130663394928, 0.1153920516371727, 0.12281998991966248, 0.0604952871799469, 0.20201537013053894, 0.1779530793428421, -0.4785970151424408, 0.0243817288428545, 0.10082262754440308, 0.2016470730304718, -0.21294675767421722, -0.024202438071370125, -0.035344261676073074]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0769224613904953, 0.11128289252519608, 0.12143252789974213, 0.0970662459731102, 0.05702609196305275, -0.03626437485218048, 0.04406556487083435, 0.037755876779556274, -0.026152338832616806, 0.039602961391210556, -0.15815189480781555, 0.13878223299980164, -0.054951123893260956, -0.2044939249753952, 0.12591488659381866, -0.0092777069658041]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.05351584404706955, 0.12852934002876282, 0.05936858803033829, 0.07732754945755005, -0.061772119253873825, -0.0396968312561512, 0.15871429443359375, 0.0706256702542305, 0.11381103843450546, -0.05460575595498085, 0.05873071402311325, 0.06926827132701874, -0.15531376004219055, 0.12710843980312347, 0.17713892459869385, 0.06383687257766724]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.10195114463567734, 0.04702722281217575, 0.1239064559340477, 0.3801770806312561, 0.1796162873506546, 0.1054534912109375, -0.1283336877822876, -0.0555013008415699, 0.3864346444606781, 0.1550859659910202, -0.5052250623703003, 0.35423997044563293, 0.184846431016922, -0.1982080340385437, 0.16850697994232178, 0.18983858823776245]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3413144052028656, 0.2711663842201233, 0.263795405626297, 0.1099952757358551, -0.7713739275932312, 0.3793472945690155, 0.1935838907957077, -0.2257777601480484, 0.44127100706100464, 0.23764745891094208, -0.1102868914604187, 0.23444947600364685, 0.3109988272190094, 0.19796571135520935, -0.4802420437335968, -0.0894717276096344]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2656097412109375, 0.2954617440700531, -0.2011311799287796, 0.2964593768119812, 0.5397266149520874, 0.3063150942325592, 0.2521767318248749, -0.2419644296169281, 0.1483672857284546, 0.6086934208869934, 0.07692427933216095, 0.3258114755153656, 0.415707528591156, -0.4220297634601593, -0.3791067898273468, 0.30346035957336426]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4020770788192749, 0.1275155246257782, 0.48511695861816406, 0.8133225440979004, 0.3834795355796814, 0.18392980098724365, 0.5749802589416504, 0.5078948140144348, 0.3998623788356781, 0.2100030779838562, 0.329438716173172, 0.6956380009651184, 1.0541177988052368, 0.1592610627412796, 0.7232884168624878, 0.6415259838104248]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0714735984802246, 0.8673211932182312, 0.6602492332458496, 0.7829357385635376, 0.8036484718322754, 0.5179752111434937, 0.6130284070968628, 0.6924641728401184, 0.2218133807182312, 0.6363379955291748, 0.7152274250984192, 0.8803420066833496, 0.673403799533844, 0.2550680935382843, 0.5713421106338501, 0.4838198721408844]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.881036639213562, 0.9699299931526184, 0.9009486436843872, 0.9816371202468872, 1.3399702310562134, 0.23931866884231567, 0.7958083152770996, 0.9157453179359436, 0.857354998588562, 1.1423550844192505, 0.8400006890296936, 0.8047339916229248, 0.3205181360244751, 0.4244101345539093, -0.1538260281085968, 0.702517569065094]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.24124109745025635, 0.8231084942817688, 1.0500255823135376, 1.1194196939468384, 0.38551512360572815, 0.5191504955291748, 0.8577357530593872, 0.8955979347229004, 0.46584901213645935, 0.45740237832069397, 0.7740972638130188, 0.8007347583770752, 0.7752482295036316, 0.8926246166229248, 0.732293963432312, 0.930536150932312]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7239176630973816, 0.8809639811515808, 0.9271356463432312, 0.4477364718914032, 0.912958025932312, -0.086196169257164, 1.0622209310531616, 1.7937593460083008, 0.8593517541885376, 0.8540736436843872, 0.9664509892463684, 1.213750958442688, 0.012674967758357525, 0.7527251243591309, 0.9687790870666504, 0.5923054814338684]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5381208062171936, 0.7560308575630188, 0.8271484375, 0.6919293999671936, 0.9089471697807312, 0.2065051794052124, 0.0841645747423172, 0.6396887898445129, 0.6684454083442688, 0.7452886700630188, -0.3724488615989685, 0.5505131483078003, 0.7808024287223816, 0.9987560510635376, 1.113165020942688, 0.3942420482635498]
Running loglikelihood requests:  84%|█████████████████████████████████████████▎       | 425/504 [23:13<04:08,  3.15s/it]Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.29534295201301575, 1.2165178060531616, 0.9131091833114624, 0.9022042155265808, 0.670770525932312, 0.8016880750656128, 0.8421630859375, 0.9048316478729248, 0.9033203125, 0.9662853479385376, 0.6287318468093872, -0.4624866247177124, 0.8847947120666504, 0.8506556749343872, 0.8604445457458496, 1.069838047027588]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.7110798954963684, 0.050492241978645325, 0.8107532262802124, 0.8109421730041504, 0.8079078197479248, 0.4199117124080658, 0.06802985817193985, 0.8940051794052124, -1.1399259567260742, 2.413516044616699, -0.9466160535812378, 0.38519996404647827, 0.913202166557312, 0.9184744954109192, 0.8074224591255188, 0.1687244176864624]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.0664236918091774, 0.5751459002494812, 1.3340773582458496, 1.292730450630188, 1.3236607313156128, 1.376953125, 1.209100604057312, 1.3417503833770752, 1.530157208442688, 1.3990246057510376, 1.9189453125, 1.2825521230697632, 1.222947597503662, 1.2640904188156128, 0.4389139711856842, 1.4310600757598877]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.408575177192688, 1.1841052770614624, 1.4312889575958252, 1.6781529188156128, 0.888427734375, 1.1234421730041504, 1.6212332248687744, 1.6707589626312256, 2.0914249420166016, 1.3595609664916992, 1.1497628688812256, 0.7555033564567566, 1.1890287399291992, 1.26496160030365, 1.3597005605697632, 1.398669958114624]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.4087611436843872, 1.272228479385376, 1.2614396810531616, 1.2748093605041504, 1.4995349645614624, 0.931881844997406, 1.2179943323135376, 1.190197229385376, 1.5292271375656128, 1.3500279188156128, 1.5079519748687744, 1.4483119249343872, 2.1597843170166016, 1.4805152416229248, 1.3751394748687744, 1.4415922164916992]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.9612775444984436, 1.0265763998031616, 0.984258770942688, 0.8014293909072876, -0.0413760244846344, 1.2344679832458496, 1.1309988498687744, 1.383661150932312, 0.8084281086921692, 1.2096353769302368, 1.3152902126312256, 1.0394810438156128, 1.0604771375656128, 1.4184337854385376, 0.9235185980796814, 0.5882989764213562]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_21 - Captured router_logits: [2.088681221008301, 2.2486979961395264, 2.1387648582458496, 2.0071613788604736, 1.9448474645614624, 2.112537145614624, 1.9242931604385376, 2.1570870876312256, 1.9347097873687744, 2.106212854385376, 2.3475632667541504, 2.6451823711395264, 1.7041480541229248, 2.3568637371063232, 2.173549175262451, 2.094214916229248]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4285714626312256, 1.4035993814468384, 1.7481398582458496, 1.0230422019958496, 1.487537145614624, 1.5856584310531616, 1.328125, 1.3973214626312256, 1.5060453414916992, 1.6741071939468384, 1.5204613208770752, 2.059523820877075, 1.499255895614624, 1.5319010019302368, 1.262439489364624, 1.6621558666229248]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.389136791229248, 1.8012462854385376, 1.1349050998687744, 1.4145275354385376, 1.5460844039916992, 1.4651228189468384, 1.4000650644302368, 1.4308035373687744, 1.5180199146270752, 1.3807663917541504, 1.5196242332458496, 1.486560583114624, 1.2716238498687744, 0.7415030598640442, 1.3817429542541504, 1.551990270614624]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.237816333770752, 2.330357074737549, 2.2030320167541504, 2.0926339626312256, 2.5965402126312256, 2.218471050262451, 2.4454054832458496, 2.1183035373687744, 2.093703508377075, 2.150390625, 2.5472469329833984, 2.1170945167541504, 2.244140625, 2.3890438079833984, 2.3067336082458496, 2.262648820877075]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.5846818685531616, 1.6364396810531616, 1.4673548936843872, 1.7201451063156128, 1.2495814561843872, 1.5075334310531616, 1.5786830186843872, 1.9145740270614624, 1.6126302480697632, 1.5139974355697632, 1.5796595811843872, 1.4629371166229248, 1.5913318395614624, 1.4208984375, 1.6294642686843872, 1.9402902126312256]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.785900354385376, 1.6997593641281128, 1.6202566623687744, 1.6875464916229248, 1.6284761428833008, 1.73028564453125, 1.6643836498260498, 1.685546875, 1.728302001953125, 1.8816267251968384, 2.1104910373687744, 1.5967378616333008, 1.6176525354385376, 1.8113607168197632, 1.7465181350708008, 1.7231038808822632]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5538853406906128, 1.626954197883606, 1.612224817276001, 1.7125098705291748, 1.5616164207458496, 1.559917688369751, 1.7022850513458252, 1.6457411050796509, 1.5585969686508179, 1.6610398292541504, 1.3377046585083008, 1.5764799118041992, 1.5924478769302368, 1.800958514213562, 1.5716726779937744, 1.582513689994812]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0045108795166016, 2.0067429542541504, 2.4249675273895264, 1.9591703414916992, 2.043178081512451, 2.0225539207458496, 2.4268972873687744, 2.092331647872925, 1.9611234664916992, 1.981212854385376, 1.956798791885376, 1.8076171875, 1.957042932510376, 1.8184523582458496, 2.2486050128936768, 2.031226634979248]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.010137557983398, 5.290271759033203, 4.93368673324585, 4.868140697479248, 4.885137557983398, 4.859096050262451, 5.0050225257873535, 5.214332103729248, 5.228236675262451, 4.9952569007873535, 4.983351707458496, 4.82021951675415, 5.038899898529053, 5.003441333770752, 5.119512557983398, 4.933221817016602]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.8560733795166016, 3.708705425262451, 3.7158203125, 3.5525949001312256, 3.7241909503936768, 3.801013708114624, 3.5725505352020264, 3.694800853729248, 3.8299152851104736, 3.8013858795166016, 3.6541574001312256, 3.382289409637451, 3.6950161457061768, 3.743350028991699, 3.8013393878936768, 3.828822612762451]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.7407925128936768, 2.845935583114624, 2.7162387371063232, 2.5826823711395264, 2.640625, 2.842959403991699, 2.729445695877075, 2.5743582248687744, 2.6888020038604736, 2.6673641204833984, 3.037621021270752, 2.290829658508301, 2.758975028991699, 2.717075824737549, 3.031064033508301, 2.759579658508301]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.060786329209804535, 0.07296725362539291, 0.07905790209770203, -0.22709545493125916, -0.2080681174993515, -0.07561998814344406, 0.0989803671836853, -0.06816156208515167, 0.08120102435350418, 0.05981730297207832, 0.07096545398235321, 0.05082794651389122, 0.0725342184305191, 0.08582131564617157, -0.9546001553535461, 0.09068427234888077]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0841667428612709, 0.045590274035930634, 0.034065015614032745, 0.0421564020216465, 0.06714041531085968, 0.02906886674463749, 0.052774589508771896, 0.06925807893276215, 0.013899101875722408, 0.07022223621606827, -0.197708860039711, 0.03915400803089142, 0.03377775475382805, -0.02645111083984375, 0.02655879594385624, 0.012463489547371864]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08258056640625, 0.062168970704078674, 0.09194128215312958, 0.08814499527215958, 0.10428628325462341, 0.10024325549602509, 0.08693575859069824, -0.08261917531490326, 0.09156449884176254, 0.07773516327142715, 0.025920270010828972, 0.07915322482585907, -0.1786896139383316, 0.017821529880166054, -0.034562718123197556, 0.1064094603061676]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.12949812412261963, 0.15070204436779022, 0.12888620793819427, 0.1582365781068802, 0.14042460918426514, 0.13508109748363495, 0.023161187767982483, 0.2001931071281433, 0.17129626870155334, -0.4950583577156067, 0.041277091950178146, 0.08928678184747696, 0.23077043890953064, -0.21633212268352509, -0.012955447658896446, -0.06026136875152588]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07271677255630493, 0.10747693479061127, 0.11219906806945801, 0.12865254282951355, 0.06583119928836823, -0.03835158795118332, 0.037867166101932526, 0.027658484876155853, -0.05616103112697601, 0.050440043210983276, -0.18092474341392517, 0.13513514399528503, -0.027229033410549164, -0.19842717051506042, 0.11891275644302368, -0.03378921002149582]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.02798645757138729, 0.14203974604606628, 0.046439848840236664, 0.09857656061649323, -0.08240922540426254, -0.035186585038900375, 0.1452454775571823, 0.03473157808184624, 0.12160225212574005, -0.09175679832696915, 0.1174309030175209, 0.07311763614416122, -0.21615315973758698, 0.13139177858829498, 0.16609229147434235, 0.056035786867141724]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.0903732106089592, 0.000409229687647894, 0.11033428460359573, 0.37727078795433044, 0.15110906958580017, 0.12488344311714172, -0.12002894282341003, -0.07941555976867676, 0.4071228802204132, 0.15381520986557007, -0.5264421701431274, 0.3543657064437866, 0.17269714176654816, -0.2571639120578766, 0.1986221820116043, 0.1825363039970398]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3845435380935669, 0.2366049885749817, 0.2628135085105896, 0.0973854511976242, -0.7790674567222595, 0.3719584345817566, 0.17366541922092438, -0.2452399879693985, 0.49011707305908203, 0.25457248091697693, -0.2047516256570816, 0.22084541618824005, 0.3035653233528137, 0.19301816821098328, -0.5691947340965271, -0.07038898020982742]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.28679758310317993, 0.28481945395469666, -0.21794551610946655, 0.2978559732437134, 0.5022649168968201, 0.29366880655288696, 0.22281913459300995, -0.2516722083091736, 0.1515396237373352, 0.7112628817558289, 0.07831288874149323, 0.357908695936203, 0.41303738951683044, -0.4806275963783264, -0.516842782497406, 0.3452032506465912]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.33344608545303345, 0.22125685214996338, 0.4947053790092468, 0.8941194415092468, 0.29761698842048645, 0.15843769907951355, 0.4667136073112488, 0.48021799325942993, 0.47776517271995544, 0.17876535654067993, 0.2901795208454132, 0.6456960439682007, 0.9765889644622803, 0.15434761345386505, 0.6745914220809937, 0.6213996410369873]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.138550877571106, 0.8884424567222595, 0.5697242021560669, 0.7874329090118408, 0.8359463214874268, 0.6184059977531433, 0.6210349202156067, 0.6969243884086609, 0.25723597407341003, 0.5892767906188965, 0.7134671211242676, 0.8887483477592468, 0.6345714926719666, 0.24262577295303345, 0.6981065273284912, 0.4274637699127197]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9817571043968201, 1.1339832544326782, 0.9058617353439331, 1.1327654123306274, 1.4675263166427612, 0.47916606068611145, 0.8914324045181274, 0.9580901861190796, 0.9437947273254395, 1.3685994148254395, 0.9233104586601257, 0.9527602791786194, 0.48558542132377625, 0.5615590810775757, -0.01718985289335251, 0.7425404787063599]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.2304047793149948, 0.8195300698280334, 1.1707220077514648, 1.2588008642196655, 0.41604799032211304, 0.656214714050293, 1.0933911800384521, 1.1488714218139648, 0.5596166253089905, 0.5342237949371338, 0.8170048594474792, 0.9006259441375732, 0.8861833810806274, 0.9376647472381592, 0.7046427726745605, 0.9825395345687866]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.5720957517623901, 0.8224126696586609, 0.8984275460243225, 0.2939850091934204, 0.8850597739219666, -0.2685090899467468, 1.0204254388809204, 1.9018731117248535, 0.9034909009933472, 0.8068288564682007, 1.031667709350586, 1.1884647607803345, -0.11331360787153244, 0.6440407633781433, 1.1719985008239746, 0.42973455786705017]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6057496666908264, 0.7818441987037659, 0.8894131183624268, 0.7166439294815063, 0.9554663896560669, 0.36549028754234314, 0.16739222407341003, 0.9316207766532898, 0.7088196277618408, 0.7823089361190796, -0.44638025760650635, 0.5846785306930542, 0.8869379162788391, 1.0157661437988281, 1.1969126462936401, 0.35741010308265686]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.30850255489349365, 1.2848974466323853, 1.0132718086242676, 0.9664203524589539, 0.8159767985343933, 0.8980609774589539, 1.0884819030761719, 0.9864457845687866, 0.9800451993942261, 1.020896077156067, 0.6786285638809204, -0.33995258808135986, 0.9631612300872803, 0.8272543549537659, 0.9056146740913391, 1.236336588859558]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.7617518305778503, 0.13411042094230652, 0.8637195229530334, 0.8790121674537659, 0.8264660239219666, 0.43124204874038696, 0.11465932428836823, 0.8559687733650208, -1.1413471698760986, 2.521554946899414, -1.0625793933868408, 0.49019548296928406, 0.8784356117248535, 0.9375529289245605, 0.8619605302810669, 0.14312045276165009]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.009359704330563545, 0.6974436044692993, 1.3948136568069458, 1.3105703592300415, 1.3617281913757324, 1.4926581382751465, 1.2620011568069458, 1.393448829650879, 1.773955225944519, 1.4123564958572388, 2.1243410110473633, 1.399025797843933, 1.2630770206451416, 1.1954301595687866, 0.38500645756721497, 1.457548975944519]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4571723937988281, 1.2473174333572388, 1.4789361953735352, 1.7562123537063599, 0.9630376696586609, 1.1472374200820923, 1.8582454919815063, 1.730986475944519, 2.3570690155029297, 1.4300639629364014, 1.1422722339630127, 0.772859513759613, 1.216208577156067, 1.1337110996246338, 1.4146273136138916, 1.4762800931930542]
Running loglikelihood requests:  85%|█████████████████████████████████████████▋       | 429/504 [23:26<03:55,  3.14s/it]Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.4415944814682007, 1.328125, 1.3724232912063599, 1.3226186037063599, 1.5520989894866943, 0.9078913331031799, 1.251953125, 1.2406755685806274, 1.781838297843933, 1.411662220954895, 1.585066795349121, 1.4845868349075317, 2.104456901550293, 1.5344032049179077, 1.4352880716323853, 1.404744029045105]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.019261360168457, 1.004988670349121, 0.9183688163757324, 0.8114557862281799, -0.13633893430233002, 1.2095961570739746, 1.1257530450820923, 1.4654452800750732, 0.7239063382148743, 1.2130788564682007, 1.3595397472381592, 1.0689829587936401, 1.0417215824127197, 1.373917579650879, 0.8442294597625732, 0.4471685588359833]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.107492446899414, 2.3020520210266113, 2.144672393798828, 1.9917168617248535, 1.953125, 2.09375, 1.910673975944519, 2.1790285110473633, 1.9600903987884521, 2.124152898788452, 2.368222951889038, 2.6889119148254395, 1.6513084173202515, 2.3706700801849365, 2.123023271560669, 2.0921497344970703]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3943430185317993, 1.3793768882751465, 1.8213478326797485, 1.063441276550293, 1.4374058246612549, 1.5707831382751465, 1.269013524055481, 1.3865776062011719, 1.4951995611190796, 1.651920199394226, 1.568429946899414, 2.0944089889526367, 1.4885165691375732, 1.525320053100586, 1.2355045080184937, 1.6570971012115479]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4105327129364014, 1.8074171543121338, 1.0303131341934204, 1.3204066753387451, 1.4847514629364014, 1.403802752494812, 1.3040286302566528, 1.322195053100586, 1.4572665691375732, 1.3547157049179077, 1.4385353326797485, 1.4006965160369873, 1.1028331518173218, 0.5505878329277039, 1.2148908376693726, 1.403426170349121]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3188064098358154, 2.3549511432647705, 2.246140718460083, 2.063535451889038, 2.611163377761841, 2.164156675338745, 2.4049322605133057, 2.100527048110962, 2.046875, 2.1362011432647705, 2.689570665359497, 2.0779366493225098, 2.2251505851745605, 2.412274122238159, 2.2855799198150635, 2.2595067024230957]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.5840078592300415, 1.6592620611190796, 1.4782568216323853, 1.7390342950820923, 1.2254329919815063, 1.5400508642196655, 1.6209526062011719, 1.9720444679260254, 1.6262706518173218, 1.5500752925872803, 1.5636529922485352, 1.4796216487884521, 1.572853922843933, 1.4450771808624268, 1.805628776550293, 1.9757623672485352]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.8152296543121338, 1.7741817235946655, 1.6920887231826782, 1.7267507314682007, 1.668039321899414, 1.7760436534881592, 1.6926063299179077, 1.716314435005188, 1.7837355136871338, 1.946324348449707, 2.241152048110962, 1.6290473937988281, 1.6439194679260254, 1.9033790826797485, 1.840232014656067, 1.7448289394378662]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5184605121612549, 1.6463578939437866, 1.5780308246612549, 1.6970685720443726, 1.5296733379364014, 1.5481750965118408, 1.71728515625, 1.653255581855774, 1.5547823905944824, 1.6545087099075317, 1.2930629253387451, 1.5674415826797485, 1.5803134441375732, 1.8697553873062134, 1.567900538444519, 1.5878112316131592]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0356738567352295, 2.086737632751465, 2.4555370807647705, 1.981410026550293, 2.0892083644866943, 2.0599114894866943, 2.5143542289733887, 2.142436981201172, 2.001082420349121, 1.9887518882751465, 1.9930346012115479, 1.8113234043121338, 1.9678322076797485, 1.8485033512115479, 2.2834620475769043, 2.0309441089630127]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.0708770751953125, 5.4605607986450195, 5.051299095153809, 4.925169467926025, 4.924039840698242, 4.9359941482543945, 5.050263404846191, 5.285391330718994, 5.309111595153809, 5.044051170349121, 4.979480266571045, 4.907379627227783, 5.166415691375732, 5.092808723449707, 5.197571754455566, 5.020236968994141]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.8913779258728027, 3.7424697875976562, 3.6790285110473633, 3.5695595741271973, 3.744870185852051, 3.8235127925872803, 3.573136329650879, 3.7123963832855225, 3.8941075801849365, 3.8062877655029297, 3.6284356117248535, 3.309429168701172, 3.6557323932647705, 3.7201619148254395, 3.8049697875976562, 3.8547627925872803]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.7670369148254395, 2.9184863567352295, 2.7884976863861084, 2.664156675338745, 2.6715927124023438, 2.893448829650879, 2.722797393798828, 2.5569465160369873, 2.7507529258728027, 2.6506965160369873, 3.2662839889526367, 2.279202699661255, 2.830854654312134, 2.7027485370635986, 3.1953125, 2.756965398788452]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.068926140666008, 0.08657064288854599, 0.08775563538074493, -0.2649419903755188, -0.2558692991733551, -0.048192597925662994, 0.10702228546142578, -0.15191802382469177, 0.05356788635253906, 0.07378005981445312, 0.0831836685538292, 0.05649995803833008, 0.08151912689208984, 0.10034780204296112, -1.0668213367462158, 0.10850343853235245]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.08464431762695312, 0.05126776546239853, 0.04435748979449272, 0.05188922956585884, 0.07943077385425568, 0.03706622123718262, 0.06052055209875107, 0.05743560940027237, 0.0137923713773489, 0.06525363773107529, -0.19476699829101562, 0.07458903640508652, 0.03035869635641575, -0.010318565182387829, 0.039366960525512695, 0.027556801214814186]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07966737449169159, 0.06378159672021866, 0.08464517444372177, 0.07520341873168945, 0.11279620975255966, 0.08919429779052734, 0.03267974779009819, -0.0804658904671669, 0.10290195792913437, 0.09558868408203125, 0.036117054522037506, 0.08343806117773056, -0.20473594963550568, 0.0418214313685894, -0.0369415283203125, 0.1157756820321083]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13803796470165253, 0.16158179938793182, 0.14530257880687714, 0.11924400180578232, 0.1169523224234581, 0.12459945678710938, -0.003754043485969305, 0.18749161064624786, 0.1904933899641037, -0.5390625, 0.08808211982250214, 0.11921081691980362, 0.1525733917951584, -0.2840145230293274, 0.0431976318359375, -0.025435637682676315]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08322601020336151, 0.11487903445959091, 0.09832048416137695, 0.10153064876794815, 0.12407417595386505, -0.05412788316607475, -0.012735891155898571, 0.021532202139496803, -0.11568355560302734, 0.02817382849752903, -0.17539215087890625, 0.12766847014427185, -0.02795886993408203, -0.18573859333992004, 0.1325943022966385, -0.09261779487133026]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_5 - Captured router_logits: [0.027933120727539062, 0.12923526763916016, 0.06278343498706818, 0.14280185103416443, -0.12959404289722443, -0.05750012397766113, 0.22330017387866974, 0.005751228425651789, 0.08114121109247208, -0.05713977664709091, 0.09549865871667862, 0.09434060752391815, -0.1987152099609375, 0.18540000915527344, 0.10496482998132706, 0.07699918746948242]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.131779283285141, -0.02569293975830078, 0.21174851059913635, 0.37701454758644104, 0.17372894287109375, 0.16966858506202698, -0.12189541012048721, -0.05123557895421982, 0.30151596665382385, 0.21490898728370667, -0.4972084164619446, 0.4078513979911804, 0.21848145127296448, -0.3123420774936676, 0.1712353676557541, 0.18432311713695526]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.2519416809082031, 0.2160572111606598, 0.23158493638038635, 0.12406444549560547, -0.7004638910293579, 0.3790748715400696, 0.2286529541015625, -0.27112045884132385, 0.4824031889438629, 0.23433074355125427, -0.15143470466136932, 0.21019820868968964, 0.24316787719726562, 0.2875385284423828, -0.5134795904159546, -0.11338834464550018]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.22854852676391602, 0.2778587341308594, -0.32554855942726135, 0.29789429903030396, 0.6026336550712585, 0.448263555765152, 0.21695537865161896, -0.24793167412281036, 0.18537673354148865, 0.7099853754043579, 0.1462160050868988, 0.4138549864292145, 0.520007312297821, -0.5227142572402954, -0.4854232668876648, 0.3419864773750305]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.48233717679977417, 0.202626034617424, 0.47004395723342896, 0.9183624386787415, 0.3793579041957855, 0.30519333481788635, 0.6201446652412415, 0.640148937702179, 0.6248115301132202, 0.30186158418655396, 0.24404525756835938, 0.708508312702179, 0.9103927612304688, 0.009100723080337048, 0.7387450933456421, 0.776928722858429]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1062042713165283, 0.995806872844696, 0.5747283697128296, 0.7442382574081421, 0.7603057622909546, 0.6871042251586914, 0.7263946533203125, 0.7734619379043579, 0.0009254455799236894, 0.5009182095527649, 0.7963012456893921, 0.915667712688446, 0.6094650030136108, 0.296224981546402, 0.9378231167793274, 0.409707635641098]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0681335926055908, 1.2653076648712158, 1.032373070716858, 1.383203148841858, 1.691552758216858, 0.4287765622138977, 0.991040050983429, 0.8822372555732727, 1.06964111328125, 1.5662109851837158, 0.957928478717804, 1.0820434093475342, 0.37885743379592896, 0.6322402954101562, 0.021494293585419655, 0.8336547613143921]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.16528625786304474, 0.894274890422821, 1.2830932140350342, 1.1348145008087158, 0.5605506896972656, 0.5869690179824829, 1.229071021080017, 1.16180419921875, 0.44137269258499146, 0.5519256591796875, 1.0570557117462158, 0.9232422113418579, 0.817340075969696, 0.8754028081893921, 0.8267822265625, 1.1393554210662842]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8924560546875, 0.994433581829071, 1.2155883312225342, 0.4859611392021179, 1.087060570716858, -0.07253418117761612, 1.1599609851837158, 2.0914063453674316, 1.1122558116912842, 1.0384521484375, 1.387304663658142, 1.440185546875, 0.2778823971748352, 0.821881115436554, 1.0808837413787842, 0.7271484136581421]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.807354748249054, 0.96923828125, 1.0032227039337158, 0.818066418170929, 1.105566382408142, 0.6542426943778992, 0.3648529052734375, 1.4610869884490967, 0.7400878667831421, 0.944226086139679, -0.38154298067092896, 0.8984740972518921, 1.106115698814392, 1.276208519935608, 1.273278832435608, 0.4627731442451477]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4469657838344574, 1.427978515625, 1.093115210533142, 1.0536010265350342, 0.8225372433662415, 1.015380859375, 1.4383224248886108, 1.0668456554412842, 1.1218750476837158, 1.127783179283142, 0.740509033203125, -0.23748931288719177, 1.1952606439590454, 0.9352051019668579, 1.047631859779358, 1.4192031621932983]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.7571929693222046, 0.03670043870806694, 0.9865967035293579, 0.8632446527481079, 0.7409576177597046, 0.4917152523994446, 0.5027000308036804, 0.9327636957168579, -0.9112335443496704, 2.578564405441284, -1.023799180984497, 0.5649811029434204, 0.9201415777206421, 1.100500464439392, 0.9540466070175171, 0.2755245268344879]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.00942077673971653, 0.8662475347518921, 1.57647705078125, 1.372949242591858, 1.4958007335662842, 1.582299828529358, 1.4786651134490967, 1.543768286705017, 1.8274872303009033, 1.5342895984649658, 2.432861328125, 1.5606476068496704, 1.341619849205017, 1.340368628501892, 0.48716431856155396, 1.53887939453125]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.616601586341858, 1.3207519054412842, 1.5952590703964233, 1.950292944908142, 1.1725342273712158, 1.24871826171875, 1.9938476085662842, 1.831933617591858, 2.49365234375, 1.5343749523162842, 1.2589843273162842, 1.0196014642715454, 1.3376953601837158, 1.267248511314392, 1.538671851158142, 1.609765648841858]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.473779320716858, 1.3939940929412842, 1.533837914466858, 1.3680908679962158, 1.669824242591858, 1.0965576171875, 1.3348388671875, 1.272070288658142, 1.8798096179962158, 1.445581078529358, 1.7164306640625, 1.6133544445037842, 2.213207960128784, 1.645483374595642, 1.5830566883087158, 1.5082275867462158]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.2225525379180908, 1.170446753501892, 1.0950927734375, 0.933337390422821, 0.22306784987449646, 1.624914526939392, 1.298242211341858, 1.6434752941131592, 0.9337829351425171, 1.424902319908142, 1.4737060070037842, 1.1652038097381592, 1.3087890148162842, 1.4958984851837158, 1.064550757408142, 0.5760505795478821]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.217334032058716, 2.3721680641174316, 2.2577147483825684, 2.043505907058716, 2.0542969703674316, 2.198046922683716, 2.059765577316284, 2.336132764816284, 2.073046922683716, 2.29296875, 2.4380860328674316, 2.64892578125, 1.911718726158142, 2.592968702316284, 2.304443359375, 2.2691407203674316]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.410742163658142, 1.4316895008087158, 1.950292944908142, 1.0397460460662842, 1.435449242591858, 1.59375, 1.2863037586212158, 1.34033203125, 1.4629395008087158, 1.706933617591858, 1.52099609375, 2.0435547828674316, 1.513037085533142, 1.589941382408142, 1.245458960533142, 1.663476586341858]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Running loglikelihood requests:  86%|██████████████████████████████████████████       | 433/504 [23:38<03:42,  3.14s/it]Layer: gate_23 - Captured router_logits: [2.4373536109924316, 1.836572289466858, 1.196813941001892, 1.4290039539337158, 1.7424805164337158, 1.4977538585662842, 1.487060546875, 1.449365258216858, 1.5892822742462158, 1.418701171875, 1.482812523841858, 1.569360375404358, 1.2461426258087158, 0.8128776550292969, 1.3943603038787842, 1.558007836341858]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.1932616233825684, 2.3570313453674316, 2.2164063453674316, 2.064257860183716, 2.603515625, 2.1649413108825684, 2.404003858566284, 2.0298829078674316, 2.010693311691284, 2.1539063453674316, 2.60498046875, 2.07568359375, 2.239062547683716, 2.365429639816284, 2.230664014816284, 2.1605467796325684]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.624755859375, 1.654541015625, 1.4729492664337158, 1.806542992591858, 1.366357445716858, 1.5382812023162842, 1.622412085533142, 1.9284179210662842, 1.654150366783142, 1.526123046875, 1.6021728515625, 1.443994164466858, 1.6266601085662842, 1.398681640625, 1.830908179283142, 1.874365210533142]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.752685546875, 1.8160613775253296, 1.694189429283142, 1.7291259765625, 1.6985352039337158, 1.8503021001815796, 1.7472503185272217, 1.727441430091858, 1.7522140741348267, 1.9263794422149658, 2.25994873046875, 1.631555199623108, 1.7123534679412842, 1.9298584461212158, 1.8249633312225342, 1.7787185907363892]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.651586890220642, 1.6837432384490967, 1.639257788658142, 1.7170593738555908, 1.5735595226287842, 1.607031226158142, 1.8037109375, 1.7171509265899658, 1.627984642982483, 1.6920287609100342, 1.4017822742462158, 1.6321289539337158, 1.7503173351287842, 1.9384949207305908, 1.621789574623108, 1.6112823486328125]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.07293701171875, 2.1535887718200684, 2.4620940685272217, 2.004638671875, 2.1311707496643066, 2.067340135574341, 2.4186768531799316, 2.175122022628784, 2.0319581031799316, 2.0137696266174316, 2.0002684593200684, 1.8335449695587158, 1.958764672279358, 1.856909155845642, 2.334521532058716, 2.1186156272888184]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.050390720367432, 5.477734565734863, 4.957421779632568, 4.786035060882568, 4.826269626617432, 4.88671875, 5.036913871765137, 5.204882621765137, 5.182031154632568, 4.944140434265137, 4.90625, 4.8505859375, 5.019140720367432, 5.042578220367432, 5.232031345367432, 4.951855659484863]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.822924852371216, 3.7162108421325684, 3.6588377952575684, 3.4519410133361816, 3.614001512527466, 3.681884765625, 3.4157943725585938, 3.7716064453125, 3.7335267066955566, 3.6261963844299316, 3.525390625, 3.1920409202575684, 3.509927272796631, 3.6120362281799316, 3.731372117996216, 3.84979248046875]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.7373046875, 2.8709473609924316, 2.886035203933716, 2.6024413108825684, 2.7734375, 2.8936524391174316, 2.661328077316284, 2.55419921875, 2.7662110328674316, 2.5957274436950684, 3.1357421875, 2.1297850608825684, 2.7721190452575684, 2.7119140625, 3.22998046875, 2.75732421875]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.060040008276700974, 0.0803644210100174, 0.08514795452356339, -0.20513954758644104, -0.21787360310554504, -0.08327308297157288, 0.10373178869485855, -0.11165677756071091, 0.06732627749443054, 0.06414716690778732, 0.07124093919992447, 0.06242585927248001, 0.0802469477057457, 0.09343166649341583, -0.9173678159713745, 0.09744144976139069]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07843956351280212, 0.049035146832466125, 0.02519196644425392, 0.037313997745513916, 0.07154088467359543, 0.03934510424733162, 0.049495402723550797, 0.06525930017232895, 0.020847221836447716, 0.06538410484790802, -0.17818978428840637, 0.056858208030462265, 0.04262039065361023, -0.020915299654006958, 0.02723953127861023, 0.03258318826556206]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06959181278944016, 0.05655193328857422, 0.080282062292099, 0.08171042054891586, 0.09858918935060501, 0.08455758541822433, 0.05324065312743187, -0.07854442298412323, 0.09389789402484894, 0.06358376890420914, 0.03629518300294876, 0.09043943136930466, -0.1873677521944046, 0.03763531520962715, -0.03046417236328125, 0.12618274986743927]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13543701171875, 0.1552499681711197, 0.13572873175144196, 0.11389316618442535, 0.11546129733324051, 0.14791439473628998, 0.018313676118850708, 0.1823914349079132, 0.188201904296875, -0.5284079313278198, 0.03408891707658768, 0.11149147152900696, 0.1782790869474411, -0.22224308550357819, -0.0039357892237603664, -0.03459206596016884]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08762848377227783, 0.09871507436037064, 0.10278438031673431, 0.10348041355609894, 0.09771434962749481, -0.07041999697685242, 0.022250639274716377, 0.02849285490810871, -0.09742345660924911, 0.05506623163819313, -0.19644321501255035, 0.11438017338514328, -0.02694936841726303, -0.18764011561870575, 0.12189190089702606, -0.03299928456544876]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_5 - Captured router_logits: [0.012582974508404732, 0.12634766101837158, 0.0426279716193676, 0.10574878752231598, -0.11981514096260071, -0.0375264473259449, 0.22785831987857819, 0.015450110659003258, 0.11592669039964676, -0.07169909030199051, 0.1728891283273697, 0.10078097879886627, -0.18757981061935425, 0.16482661664485931, 0.1490253061056137, 0.08001581579446793]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.12209652364253998, -0.013644878752529621, 0.14122283458709717, 0.37062034010887146, 0.14800673723220825, 0.16397662460803986, -0.145824134349823, -0.08881378173828125, 0.3088736832141876, 0.21564346551895142, -0.5168183445930481, 0.39132454991340637, 0.23722957074642181, -0.3178896903991699, 0.1790701001882553, 0.20538701117038727]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.32061767578125, 0.20477294921875, 0.2388986498117447, 0.12853866815567017, -0.7008025050163269, 0.38064849376678467, 0.21593455970287323, -0.30065056681632996, 0.49001917243003845, 0.18560008704662323, -0.13331955671310425, 0.2214997112751007, 0.29500168561935425, 0.24374154210090637, -0.5012175440788269, -0.05820024758577347]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.23360012471675873, 0.23767325282096863, -0.3375111222267151, 0.2689913213253021, 0.5678523182868958, 0.3558818995952606, 0.2081981599330902, -0.27036383748054504, 0.23356159031391144, 0.6866447329521179, 0.11912654340267181, 0.42185407876968384, 0.4763621687889099, -0.49195900559425354, -0.4752776324748993, 0.3103395104408264]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.47179314494132996, 0.21462777256965637, 0.46381086111068726, 0.9433155655860901, 0.3276742696762085, 0.2946002781391144, 0.5393316745758057, 0.6061198115348816, 0.5595718622207642, 0.2532145082950592, 0.24882115423679352, 0.6534204483032227, 0.9307641983032227, 0.09137041121721268, 0.7218174338340759, 0.7163774371147156]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1506214141845703, 0.9782464504241943, 0.5423239469528198, 0.7447165250778198, 0.7203149795532227, 0.6535221934318542, 0.6277888417243958, 0.7777694463729858, 0.06133035570383072, 0.5345854163169861, 0.7894318699836731, 0.9064315557479858, 0.6163972020149231, 0.3007984757423401, 0.8055737018585205, 0.3693798780441284]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0592072010040283, 1.2627453804016113, 0.9311898946762085, 1.2859951257705688, 1.6412259340286255, 0.4723886251449585, 0.9814202785491943, 0.9095521569252014, 1.0191806554794312, 1.5334786176681519, 0.9045472741127014, 0.9912985563278198, 0.43742096424102783, 0.6405850648880005, 0.003512651426717639, 0.7892628312110901]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.22292855381965637, 0.860276460647583, 1.2192633152008057, 1.1156350374221802, 0.5326788425445557, 0.5982634425163269, 1.1480118036270142, 1.1351850032806396, 0.49424391984939575, 0.6258200407028198, 0.9773512482643127, 0.9556039571762085, 0.846266508102417, 0.827223539352417, 0.7355644106864929, 1.0061849355697632]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7948843240737915, 0.9454126358032227, 1.1251940727233887, 0.44266295433044434, 1.0461488962173462, -0.05438467115163803, 1.1162359714508057, 2.0028045177459717, 1.0730669498443604, 0.965745210647583, 1.2920422554016113, 1.3576723337173462, 0.17433518171310425, 0.8058769106864929, 1.1554737091064453, 0.6708076596260071]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7738475203514099, 0.8934044241905212, 0.9710035920143127, 0.7809996008872986, 1.029371976852417, 0.6086488366127014, 0.4493235945701599, 1.3287009000778198, 0.7024739384651184, 0.944411039352417, -0.27583977580070496, 0.7488043308258057, 1.105440616607666, 1.1496268510818481, 1.2292543649673462, 0.4526335895061493]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.39113596081733704, 1.395282506942749, 1.0497045516967773, 0.9890449643135071, 0.7667016983032227, 0.9570813179016113, 1.3106533288955688, 1.038010835647583, 1.0102163553237915, 1.028395414352417, 0.6989433169364929, -0.2327849566936493, 1.0731391906738281, 0.8763270974159241, 0.9873297214508057, 1.3833078145980835]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.818922758102417, 0.08604822307825089, 0.9526273012161255, 0.8904454112052917, 0.7402093410491943, 0.5357751846313477, 0.44913268089294434, 0.9690880179405212, -0.9091616868972778, 2.5055792331695557, -0.9169792532920837, 0.580643892288208, 0.9249549508094788, 1.1174629926681519, 0.9508338570594788, 0.3976675271987915]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.06290259957313538, 0.8623825311660767, 1.5301607847213745, 1.4147073030471802, 1.4156399965286255, 1.5574419498443604, 1.5043600797653198, 1.4737579822540283, 1.8354053497314453, 1.555739164352417, 2.3665614128112793, 1.5461488962173462, 1.3369390964508057, 1.3069661855697632, 0.5229163765907288, 1.5544120073318481]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5592948198318481, 1.3217147588729858, 1.6371945142745972, 1.9164162874221802, 1.1588040590286255, 1.286358118057251, 2.0451722145080566, 1.8323317766189575, 2.5291154384613037, 1.5404146909713745, 1.26220703125, 1.0640164613723755, 1.3589743375778198, 1.311232328414917, 1.558443546295166, 1.5870893001556396]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.4270333051681519, 1.3528646230697632, 1.4505959749221802, 1.3008813858032227, 1.5591946840286255, 1.0179568529129028, 1.2514523267745972, 1.2360903024673462, 1.8253705501556396, 1.3790814876556396, 1.6049178838729858, 1.5160256624221802, 2.1182267665863037, 1.5273687839508057, 1.4594351053237915, 1.4227514266967773]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1534862518310547, 1.0594701766967773, 1.087064266204834, 0.9005549550056458, 0.2123820036649704, 1.3765774965286255, 1.2196013927459717, 1.601393461227417, 0.8474371433258057, 1.2893630266189575, 1.4000900983810425, 1.0728853940963745, 1.1680939197540283, 1.4171173572540283, 1.0268023014068604, 0.5199854969978333]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.159254789352417, 2.3484575748443604, 2.2282652854919434, 2.0421674251556396, 1.9722555875778198, 2.2033252716064453, 2.080679178237915, 2.3035857677459717, 2.0715644359588623, 2.237680196762085, 2.4138622283935547, 2.7612178325653076, 1.9097555875778198, 2.53125, 2.2722856998443604, 2.250901460647583]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3989382982254028, 1.2990283966064453, 1.9341446161270142, 1.0447466373443604, 1.4596604108810425, 1.5403646230697632, 1.2718474864959717, 1.3001302480697632, 1.4644430875778198, 1.6878255605697632, 1.5090645551681519, 2.0433192253112793, 1.5164263248443604, 1.5482271909713745, 1.2508889436721802, 1.6915063858032227]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4491686820983887, 1.8603765964508057, 1.2550830841064453, 1.4472155570983887, 1.7030248641967773, 1.5120192766189575, 1.5111678838729858, 1.4867788553237915, 1.5590444803237915, 1.456279993057251, 1.524338960647583, 1.5160256624221802, 1.2829025983810425, 0.8435003757476807, 1.4008914232254028, 1.531099796295166]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.248798131942749, 2.3420472145080566, 2.2878105640411377, 2.073617696762085, 2.6321113109588623, 2.2089343070983887, 2.4295873641967773, 2.0770232677459717, 2.0864884853363037, 2.1895031929016113, 2.5813801288604736, 2.1104767322540283, 2.2949719429016113, 2.3595752716064453, 2.287109375, 2.19921875]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6413260698318481, 1.657001256942749, 1.499248743057251, 1.778545618057251, 1.3729467391967773, 1.630859375, 1.6283053159713745, 1.9598357677459717, 1.6297576427459717, 1.5397635698318481, 1.5940505266189575, 1.4506710767745972, 1.6102263927459717, 1.4322916269302368, 1.8587239980697632, 1.9132611751556396]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.7995291948318481, 1.8274880647659302, 1.6859976053237915, 1.7500500679016113, 1.7293169498443604, 1.8139617443084717, 1.7962207794189453, 1.7543069124221802, 1.7620443105697632, 1.9176682233810425, 2.2939953804016113, 1.665940523147583, 1.6909555196762085, 1.9465895891189575, 1.8177709579467773, 1.7596795558929443]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Running loglikelihood requests:  87%|██████████████████████████████████████████▍      | 437/504 [23:51<03:29,  3.13s/it]Layer: gate_27 - Captured router_logits: [1.587665319442749, 1.6451572179794312, 1.5937750339508057, 1.6824920177459717, 1.548828125, 1.5778244733810425, 1.7702323198318481, 1.6741536855697632, 1.6075971126556396, 1.6743414402008057, 1.380784273147583, 1.584810733795166, 1.6483373641967773, 1.9224759340286255, 1.6553235054016113, 1.5624499320983887]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0209834575653076, 2.1071715354919434, 2.439753532409668, 1.985276460647583, 2.073317289352417, 2.0258915424346924, 2.4043469429016113, 2.137470006942749, 1.9873297214508057, 1.973557710647583, 1.982572078704834, 1.8000801801681519, 1.9569185972213745, 1.798677921295166, 2.3092448711395264, 2.0458734035491943]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.098156929016113, 5.52734375, 5.040965557098389, 4.90895414352417, 4.879507064819336, 4.95823335647583, 5.101362228393555, 5.325721263885498, 5.293869972229004, 5.006810665130615, 4.988531589508057, 4.917668342590332, 5.083033084869385, 5.112179279327393, 5.25901460647583, 5.181690692901611]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.894381046295166, 3.7561097145080566, 3.6960887908935547, 3.538987398147583, 3.7109625339508057, 3.778470516204834, 3.482628345489502, 3.824519157409668, 3.837627649307251, 3.698692798614502, 3.6246495246887207, 3.2846806049346924, 3.609938383102417, 3.699068546295166, 3.7782952785491943, 3.8889849185943604]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.7203526496887207, 2.840444803237915, 2.8315303325653076, 2.57421875, 2.7086338996887207, 2.8415465354919434, 2.6420271396636963, 2.5469250679016113, 2.740084171295166, 2.5907952785491943, 3.1392228603363037, 2.1131434440612793, 2.770432710647583, 2.716045618057251, 3.225661039352417, 2.7489984035491943]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.06045453995466232, 0.07597772032022476, 0.07658855617046356, -0.24333445727825165, -0.23947691917419434, -0.09665034711360931, 0.10313904285430908, -0.12621913850307465, 0.04985246807336807, 0.06432283669710159, 0.07355323433876038, 0.05741657316684723, 0.07570932060480118, 0.08321556448936462, -0.9103065133094788, 0.10401016473770142]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.0806335061788559, 0.044401511549949646, 0.03496531769633293, 0.04299124702811241, 0.06990843266248703, 0.04737619310617447, 0.049653664231300354, 0.05108368769288063, 0.017030373215675354, 0.06935256719589233, -0.20478859543800354, 0.05026242509484291, 0.050482627004384995, -0.018248338252305984, 0.025588255375623703, 0.0267054233700037]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05560009181499481, 0.06422475725412369, 0.08482751995325089, 0.08651327341794968, 0.09181135147809982, 0.09235557913780212, 0.04044048488140106, -0.07562803477048874, 0.07091306895017624, 0.06134766712784767, 0.02992727793753147, 0.07819248735904694, -0.18845152854919434, 0.03740868344902992, -0.05107859894633293, 0.14195330440998077]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.12968836724758148, 0.16680839657783508, 0.14868399500846863, 0.11416645348072052, 0.12000489979982376, 0.13965743780136108, -0.02355448342859745, 0.18913425505161285, 0.16829466819763184, -0.5139973759651184, 0.04729989916086197, 0.12273074686527252, 0.16238754987716675, -0.22363632917404175, 0.0019622216932475567, -0.06121513247489929]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08624252676963806, 0.10205430537462234, 0.10459655523300171, 0.0946248397231102, 0.11763704568147659, -0.03488432988524437, 0.034707486629486084, 0.004129654262214899, -0.07226444780826569, 0.024784186854958534, -0.20374786853790283, 0.11077646166086197, -0.015864836052060127, -0.21884159743785858, 0.12973570823669434, -0.06329815089702606]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_5 - Captured router_logits: [0.0200475063174963, 0.14145323634147644, 0.035144317895174026, 0.12265914678573608, -0.10562837868928909, -0.0731668695807457, 0.21281589567661285, -0.007893049158155918, 0.07677215337753296, -0.07201366126537323, 0.12372256815433502, 0.10900174826383591, -0.20677459239959717, 0.1756104677915573, 0.11793366819620132, 0.060700833797454834]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.09209011495113373, -0.025430532172322273, 0.15469399094581604, 0.3598977029323578, 0.1641826182603836, 0.14541126787662506, -0.18244346976280212, -0.04664650931954384, 0.28159743547439575, 0.2075214833021164, -0.5033193826675415, 0.42952942848205566, 0.25686410069465637, -0.2964383661746979, 0.21136474609375, 0.21564307808876038]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.25903084874153137, 0.19297927618026733, 0.23562251031398773, 0.10532653331756592, -0.6964831352233887, 0.36863356828689575, 0.22456398606300354, -0.3566315472126007, 0.47365278005599976, 0.2257353961467743, -0.08962934464216232, 0.2087051123380661, 0.2880116105079651, 0.24151493608951569, -0.4496929347515106, 0.006754361558705568]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20125091075897217, 0.27459716796875, -0.2932426333427429, 0.2991342842578888, 0.6075220108032227, 0.4069386124610901, 0.21897339820861816, -0.27021437883377075, 0.1665409803390503, 0.6187791228294373, 0.14472061395645142, 0.46946755051612854, 0.5062475204467773, -0.4429055154323578, -0.44591033458709717, 0.33829382061958313]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.5062263607978821, 0.17274239659309387, 0.4388701617717743, 0.9137338399887085, 0.46000123023986816, 0.3891660273075104, 0.5532445907592773, 0.6199731826782227, 0.6048536896705627, 0.32952409982681274, 0.2734281122684479, 0.7076321840286255, 0.9857302904129028, -0.06545159965753555, 0.7151692509651184, 0.7393078804016113]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.068464994430542, 1.0442270040512085, 0.5730927586555481, 0.7604917883872986, 0.7810090184211731, 0.7032493948936462, 0.6974189877510071, 0.7869340777397156, -0.008990459144115448, 0.5003873109817505, 0.8043995499610901, 0.9522799253463745, 0.7031218409538269, 0.29400596022605896, 0.8534999489784241, 0.3932636082172394]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9789413213729858, 1.3077173233032227, 0.9346704483032227, 1.3601324558258057, 1.7780448198318481, 0.5744910836219788, 0.9983223080635071, 0.8848313689231873, 1.0418920516967773, 1.5298227071762085, 0.9421073794364929, 0.9578825831413269, 0.44873282313346863, 0.6892665028572083, 0.07036023586988449, 0.9077774286270142]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.33921071887016296, 0.9007912874221802, 1.2768930196762085, 1.1204928159713745, 0.6257199048995972, 0.5962414741516113, 1.1873465776443481, 1.1539150476455688, 0.5981742739677429, 0.7582225203514099, 1.0801407098770142, 0.9195963740348816, 0.8425355553627014, 0.8357371687889099, 0.8956330418586731, 1.0822817087173462]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.0171023607254028, 1.0413161516189575, 1.2506885528564453, 0.7250397801399231, 1.0917468070983887, 0.1603870838880539, 1.183518648147583, 2.036358118057251, 1.1237479448318481, 1.0859625339508057, 1.481370210647583, 1.507061243057251, 0.3656318783760071, 0.9082328677177429, 1.0334033966064453, 0.8748849034309387]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8312174677848816, 0.9663461446762085, 1.0292718410491943, 0.8325070142745972, 1.1018379926681519, 0.8274614214897156, 0.5027559995651245, 1.4533135890960693, 0.7105180621147156, 1.103928804397583, -0.2467091828584671, 0.8831818699836731, 1.1746200323104858, 1.2463442087173462, 1.305401086807251, 0.5974500775337219]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.5249336361885071, 1.412184476852417, 1.0917468070983887, 0.9449995160102844, 0.8441130518913269, 1.071439266204834, 1.367478609085083, 1.0595452785491943, 1.1206930875778198, 1.0873397588729858, 0.7903395295143127, -0.11431415379047394, 1.1841332912445068, 0.9690755009651184, 1.0654296875, 1.5995593070983887]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.8612279891967773, 0.09553645551204681, 1.0203090906143188, 0.8680388331413269, 0.7232571840286255, 0.7004480361938477, 0.6155677437782288, 1.000413179397583, -0.8004588484764099, 2.6788361072540283, -0.8192858695983887, 0.7600254416465759, 0.9563301205635071, 1.1528820991516113, 0.9893266558647156, 0.5374247431755066]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.19012822210788727, 0.9153207540512085, 1.6538586616516113, 1.4366486072540283, 1.4617513418197632, 1.5628255605697632, 1.4845972061157227, 1.5621118545532227, 1.7921252250671387, 1.6564878225326538, 2.5886919498443604, 1.6111059188842773, 1.3693231344223022, 1.2935822010040283, 0.5748103260993958, 1.649564266204834]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6891025304794312, 1.4347455501556396, 1.7187249660491943, 2.07421875, 1.3568960428237915, 1.427584171295166, 2.114182710647583, 1.8812099695205688, 2.7550079822540283, 1.6263521909713745, 1.427584171295166, 1.2740103006362915, 1.5095151662826538, 1.4512908458709717, 1.6853466033935547, 1.7181991338729858]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.5185797214508057, 1.4193209409713745, 1.6219701766967773, 1.3903244733810425, 1.7151442766189575, 1.1778219938278198, 1.4627153873443604, 1.3718700408935547, 1.9654947519302368, 1.4408053159713745, 1.7984274625778198, 1.6005859375, 2.2384815216064453, 1.7024239301681519, 1.6046173572540283, 1.6929837465286255]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.3750375509262085, 1.1331630945205688, 1.1233474016189575, 1.051297664642334, 0.4060453772544861, 1.587177038192749, 1.336838960647583, 1.7851214408874512, 0.8496969938278198, 1.4809695482254028, 1.450270414352417, 1.165452241897583, 1.3961087465286255, 1.5598458051681519, 1.1596804857254028, 0.7853514552116394]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.2549078464508057, 2.4357972145080566, 2.3979365825653076, 2.177433967590332, 2.1790363788604736, 2.2664263248443604, 2.2164463996887207, 2.4992988109588623, 2.167217493057251, 2.4383013248443604, 2.5181291103363037, 2.752704381942749, 2.0797276496887207, 2.709735631942749, 2.413461446762085, 2.375601053237915]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.5361578464508057, 1.419170618057251, 2.0281450748443604, 1.1711488962173462, 1.5264923572540283, 1.6315103769302368, 1.4351462125778198, 1.3889724016189575, 1.5159755945205688, 1.8081930875778198, 1.5856369733810425, 2.121694803237915, 1.6421774625778198, 1.6625601053237915, 1.4278595447540283, 1.7556841373443604]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5205328464508057, 1.942157506942749, 1.4400540590286255, 1.5376602411270142, 1.8394931554794312, 1.6186898946762085, 1.5777243375778198, 1.5786257982254028, 1.6323617696762085, 1.5021533966064453, 1.6512420177459717, 1.6155349016189575, 1.4475661516189575, 1.0445212125778198, 1.5407651662826538, 1.6303085088729858]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.2138421535491943, 2.337440013885498, 2.2345752716064453, 2.0556890964508057, 2.6627604961395264, 2.25390625, 2.4241786003112793, 2.1028645038604736, 2.0447216033935547, 2.244891881942749, 2.6220953464508057, 2.212139368057251, 2.239483118057251, 2.368990421295166, 2.36328125, 2.2008213996887207]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7358273267745972, 1.7661257982254028, 1.6698217391967773, 1.9261819124221802, 1.5272936820983887, 1.6465845108032227, 1.7301682233810425, 2.0478765964508057, 1.7321213483810425, 1.6686698198318481, 1.666766881942749, 1.6088742017745972, 1.7112880945205688, 1.5472255945205688, 1.9390023946762085, 2.0043067932128906]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8373898267745972, 1.8591089248657227, 1.7986278533935547, 1.8224658966064453, 1.737454891204834, 1.8861303329467773, 1.802959680557251, 1.8300280570983887, 1.8148819208145142, 1.9958934783935547, 2.3465795516967773, 1.7760666608810425, 1.795673131942749, 1.9734325408935547, 1.8599759340286255, 1.8006622791290283]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6528195142745972, 1.7255358695983887, 1.6799379587173462, 1.7628204822540283, 1.6079727411270142, 1.6526943445205688, 1.8500601053237915, 1.7562350034713745, 1.6392477750778198, 1.7505133152008057, 1.4777143001556396, 1.6613832712173462, 1.741436243057251, 1.9365484714508057, 1.6954126358032227, 1.6623096466064453]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1477863788604736, 2.2741386890411377, 2.4910356998443604, 2.0843348503112793, 2.2099859714508057, 2.1225459575653076, 2.4910857677459717, 2.2369792461395264, 2.1137821674346924, 2.1028645038604736, 2.1477363109588623, 1.9605368375778198, 2.049354076385498, 1.982271671295166, 2.3852665424346924, 2.1761317253112793]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.18870210647583, 5.582531929016113, 5.028846263885498, 4.853565692901611, 4.975560665130615, 4.928084850311279, 5.011818885803223, 5.241185665130615, 5.211738586425781, 5.058493614196777, 4.972155570983887, 4.888621807098389, 5.137419700622559, 5.114583492279053, 5.243189334869385, 5.015224456787109]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.944110631942749, 3.8133013248443604, 3.744591236114502, 3.6336138248443604, 3.7571113109588623, 3.8256208896636963, 3.580679178237915, 3.9319911003112793, 3.8561699390411377, 3.7871594429016113, 3.6447315216064453, 3.3907158374786377, 3.701472282409668, 3.7504005432128906, 3.8865184783935547, 4.02854585647583]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Running loglikelihood requests:  88%|██████████████████████████████████████████▉      | 441/504 [24:03<03:17,  3.13s/it]Layer: gate_31 - Captured router_logits: [2.741586446762085, 2.8871192932128906, 2.861177921295166, 2.5666065216064453, 2.6988182067871094, 2.956730842590332, 2.589543342590332, 2.5596954822540283, 2.735276460647583, 2.5961036682128906, 3.1062700748443604, 2.2355518341064453, 2.8039863109588623, 2.7140424251556396, 3.2087340354919434, 2.778245210647583]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.06221979483962059, 0.07658891379833221, 0.08705773204565048, -0.22865335643291473, -0.19741781055927277, -0.09386097639799118, 0.09815186262130737, -0.04947097599506378, 0.07673466950654984, 0.06058967858552933, 0.06560833752155304, 0.06595265120267868, 0.07669998705387115, 0.09344403445720673, -0.9290850758552551, 0.10347123444080353]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.08286137133836746, 0.04246758669614792, 0.028114616870880127, 0.033824969083070755, 0.07758460193872452, 0.03420728072524071, 0.05451390519738197, 0.08162064850330353, 0.05416657030582428, 0.058789242058992386, -0.18777664005756378, 0.055002037435770035, 0.03214908018708229, -0.03881855681538582, 0.03139064833521843, 0.03439568728208542]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08178309351205826, 0.05451304093003273, 0.08718505501747131, 0.08971201628446579, 0.12061260640621185, 0.08717960864305496, 0.07827401906251907, -0.07696929574012756, 0.08094609528779984, 0.09516173601150513, 0.027149002999067307, 0.08491615206003189, -0.19136027991771698, 0.013671280816197395, -0.015480933710932732, 0.11773166060447693]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1341671198606491, 0.1474839299917221, 0.13066358864307404, 0.14033527672290802, 0.1446128934621811, 0.14155597984790802, 0.01264022197574377, 0.18997827172279358, 0.17144536972045898, -0.48870930075645447, 0.007422509137541056, 0.09544392675161362, 0.2626243829727173, -0.26993313431739807, 0.008080421015620232, -0.07963264733552933]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.05860771983861923, 0.0966496616601944, 0.10218345373868942, 0.16612333059310913, 0.048811279237270355, -0.06445629894733429, 0.027472756803035736, 0.028261655941605568, -0.09501786530017853, 0.021577315405011177, -0.19150136411190033, 0.13941262662410736, 0.022181127220392227, -0.19611720740795135, 0.12147501856088638, -0.04790625721216202]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.04617111384868622, 0.15447919070720673, 0.031057778745889664, 0.12344558537006378, -0.09832525998353958, -0.06053577736020088, 0.21251648664474487, 0.041647378355264664, 0.15121221542358398, -0.07048846781253815, 0.12280748784542084, 0.09280741959810257, -0.17987456917762756, 0.1469845473766327, 0.13942807912826538, 0.07124616205692291]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.10061328113079071, -0.0027533196844160557, 0.10064934939146042, 0.36718273162841797, 0.1456746757030487, 0.1689746379852295, -0.1644754260778427, -0.07980465888977051, 0.31897449493408203, 0.14480452239513397, -0.4950965642929077, 0.3764680027961731, 0.23044021427631378, -0.2501070201396942, 0.21973606944084167, 0.19843876361846924]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.42181792855262756, 0.24217627942562103, 0.24357149004936218, 0.16246865689754486, -0.7286804914474487, 0.35450467467308044, 0.1673964411020279, -0.2868192493915558, 0.5507336854934692, 0.16710950434207916, -0.16916726529598236, 0.21400630474090576, 0.2847658693790436, 0.20497427880764008, -0.5579025745391846, -0.15135768055915833]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.24337173998355865, 0.2358366698026657, -0.27407440543174744, 0.3253062963485718, 0.5452626943588257, 0.32302024960517883, 0.2352115511894226, -0.30450499057769775, 0.16947560012340546, 0.6692401766777039, 0.1080435961484909, 0.37285980582237244, 0.4567966163158417, -0.48030704259872437, -0.5150004029273987, 0.35469648241996765]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.38792914152145386, 0.24493804574012756, 0.4978725016117096, 0.9649451971054077, 0.34353044629096985, 0.16923226416110992, 0.5289766192436218, 0.5524173378944397, 0.6335607767105103, 0.1966409981250763, 0.2656765282154083, 0.7021738290786743, 0.9005626440048218, 0.09072519093751907, 0.7321935892105103, 0.6259860992431641]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1190059185028076, 0.8803583979606628, 0.5524109601974487, 0.7214640974998474, 0.7195553183555603, 0.6751839518547058, 0.635869026184082, 0.6999543309211731, 0.1706336885690689, 0.5770501494407654, 0.7222440838813782, 0.8968331217765808, 0.6117743849754333, 0.15486600995063782, 0.7908018231391907, 0.4000438451766968]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0010273456573486, 1.280996322631836, 0.8564833402633667, 1.2866528034210205, 1.4927709102630615, 0.31731465458869934, 1.0001394748687744, 0.9942230582237244, 0.9853642582893372, 1.5201400518417358, 0.8911576867103577, 0.9488002061843872, 0.4513343572616577, 0.5573461055755615, 0.10721102356910706, 0.7338930368423462]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.11588594317436218, 0.7700639367103577, 1.1939301490783691, 1.1533583402633667, 0.4801841974258423, 0.6053799986839294, 1.122133731842041, 1.1503715515136719, 0.43684685230255127, 0.4603762924671173, 0.8425610065460205, 0.8342887759208679, 0.880922794342041, 0.8186891078948975, 0.7066254019737244, 0.9165483117103577]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7607294917106628, 0.9001623392105103, 1.0584288835525513, 0.3343701958656311, 1.0061384439468384, -0.091796875, 1.0903257131576538, 1.8967126607894897, 1.0401785373687744, 0.8684557676315308, 1.2840908765792847, 1.2910916805267334, 0.18780913949012756, 0.7702224254608154, 1.2274502515792847, 0.6151178479194641]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6947037577629089, 0.7258396148681641, 0.931513786315918, 0.7391690611839294, 0.9762580990791321, 0.7093014121055603, 0.5120072960853577, 1.1913460493087769, 0.6491920948028564, 0.8117311000823975, -0.2746613621711731, 0.6642115116119385, 0.9524132013320923, 1.0255554914474487, 1.1502131223678589, 0.3839261829853058]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.37495720386505127, 1.357421875, 1.052074909210205, 0.9732523560523987, 0.8335087895393372, 0.9334161877632141, 1.3809727430343628, 1.0236150026321411, 1.0262529850006104, 1.0048701763153076, 0.6912413835525513, -0.23531192541122437, 1.0887688398361206, 0.8369520902633667, 0.9660866260528564, 1.3409708738327026]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.7156015634536743, 0.16351298987865448, 0.7706362009048462, 0.8373579382896423, 0.6818279027938843, 0.39336445927619934, 0.3684858977794647, 0.9018364548683167, -0.954648494720459, 2.438793659210205, -0.9114387631416321, 0.46696412563323975, 0.8574345707893372, 0.9481058716773987, 0.8078137636184692, 0.3194469213485718]
Running loglikelihood requests:  88%|███████████████████████████████████████████▎     | 445/504 [24:16<03:04,  3.13s/it]Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.1413193792104721, 0.8188515901565552, 1.4919846057891846, 1.3957297801971436, 1.4818130731582642, 1.5068233013153076, 1.467392921447754, 1.3885830640792847, 1.9095854759216309, 1.478198528289795, 2.346996784210205, 1.5698622465133667, 1.3766796588897705, 1.3513976335525513, 0.5101968050003052, 1.4746220111846924]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4528206586837769, 1.2850040197372437, 1.5105773210525513, 1.8400466442108154, 0.9775136709213257, 1.233715534210205, 2.010653495788574, 1.717329502105713, 2.4708807468414307, 1.4403916597366333, 1.171088695526123, 0.8505970239639282, 1.2029474973678589, 1.172895908355713, 1.4919846057891846, 1.4964995384216309]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.4405691623687744, 1.3195515871047974, 1.5372108221054077, 1.3191710710525513, 1.6110998392105103, 1.031991958618164, 1.3305346965789795, 1.3159877061843872, 1.8725649118423462, 1.4364092350006104, 1.6888190507888794, 1.5184658765792847, 2.1765928268432617, 1.5495890378952026, 1.4814326763153076, 1.493227481842041]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1208908557891846, 0.9637784361839294, 0.909369945526123, 0.820746898651123, 0.07105443626642227, 1.260488510131836, 1.1084110736846924, 1.4800074100494385, 0.6812633275985718, 1.2029221057891846, 1.2630884647369385, 1.0188781023025513, 1.0562094449996948, 1.3089488744735718, 0.8099920153617859, 0.4269167482852936]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.1952109336853027, 2.3845372200012207, 2.305093288421631, 2.1091721057891846, 2.0666091442108154, 2.2148945331573486, 2.0607244968414307, 2.3265016078948975, 2.0548903942108154, 2.230620861053467, 2.4620535373687744, 2.8066153526306152, 1.8381696939468384, 2.4970576763153076, 2.2642552852630615, 2.235288143157959]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4953835010528564, 1.4037134647369385, 2.0572240352630615, 1.1146509647369385, 1.510298252105713, 1.6260145902633667, 1.358715534210205, 1.4602272510528564, 1.560673713684082, 1.7444703578948975, 1.6396104097366333, 2.195819854736328, 1.5345982313156128, 1.6223620176315308, 1.3128043413162231, 1.783583641052246]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.3884942531585693, 1.8460328578948975, 1.0531084537506104, 1.4326298236846924, 1.5641740560531616, 1.3972200155258179, 1.319399356842041, 1.326704502105713, 1.438413143157959, 1.353135108947754, 1.4271509647369385, 1.4147727489471436, 1.137631893157959, 0.6083362102508545, 1.220829963684082, 1.4426237344741821]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3461849689483643, 2.378246784210205, 2.3677961826324463, 2.0797483921051025, 2.670454502105713, 2.2529423236846924, 2.4322240352630615, 2.141538143157959, 2.089691638946533, 2.199979782104492, 2.7818586826324463, 2.1036932468414307, 2.3471996784210205, 2.440645217895508, 2.252638101577759, 2.2924106121063232]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6769987344741821, 1.7629870176315308, 1.616528034210205, 1.8705357313156128, 1.3382203578948975, 1.6491477489471436, 1.7028206586837769, 2.1115057468414307, 1.7257001399993896, 1.6218546628952026, 1.6834923028945923, 1.5665584802627563, 1.7039873600006104, 1.540635108947754, 2.0500710010528564, 2.020799398422241]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8343141078948975, 1.9451333284378052, 1.7953530550003052, 1.756239891052246, 1.7687956094741821, 1.886962890625, 1.7713701725006104, 1.7794743776321411, 1.817446231842041, 1.970170497894287, 2.443105697631836, 1.7117998600006104, 1.739448070526123, 1.9949015378952026, 1.9142717123031616, 1.8208388090133667]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6555143594741821, 1.737241268157959, 1.671748161315918, 1.7409192323684692, 1.5965908765792847, 1.6097807884216309, 1.8493810892105103, 1.7410967350006104, 1.6422483921051025, 1.7151988744735718, 1.3675932884216309, 1.6610440015792847, 1.7386870384216309, 2.0446934700012207, 1.646357536315918, 1.697798252105713]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.053926467895508, 2.1528003215789795, 2.5043628215789795, 2.033076286315918, 2.113433361053467, 2.0925323963165283, 2.590097427368164, 2.2098214626312256, 2.0540788173675537, 2.035004138946533, 2.0273945331573486, 1.8477576971054077, 2.0107929706573486, 1.8980317115783691, 2.360643148422241, 2.010551929473877]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.042613506317139, 5.575284004211426, 5.078632354736328, 4.888291358947754, 4.954038143157959, 4.912540435791016, 5.068689346313477, 5.224837779998779, 5.326298713684082, 5.049614429473877, 4.967329502105713, 4.922788143157959, 5.0634636878967285, 5.145292282104492, 5.239346504211426, 5.005580425262451]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.822544574737549, 3.6661932468414307, 3.5539772510528564, 3.4258828163146973, 3.6724836826324463, 3.7577109336853027, 3.425933361053467, 3.742593288421631, 3.8697240352630615, 3.697138786315918, 3.5282061100006104, 3.194399356842041, 3.5394175052642822, 3.6689326763153076, 3.6927759647369385, 3.7421367168426514]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.5727474689483643, 2.690340995788574, 2.7241275310516357, 2.5302352905273438, 2.5700080394744873, 2.6096794605255127, 2.555093288421631, 2.3252334594726562, 2.6242897510528564, 2.3919947147369385, 3.202719211578369, 1.9007331132888794, 2.618607997894287, 2.5604708194732666, 3.141538143157959, 2.581777572631836]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.061780061572790146, 0.07503727823495865, 0.07958350330591202, -0.23913870751857758, -0.23385818302631378, -0.08590421080589294, 0.10586488246917725, -0.11648916453123093, 0.048096247017383575, 0.06545128673315048, 0.07961798459291458, 0.0639653354883194, 0.07232767343521118, 0.08514832705259323, -0.9894290566444397, 0.0963592529296875]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07811930030584335, 0.04194403439760208, 0.02553132362663746, 0.03584210202097893, 0.06278109550476074, 0.026178237050771713, 0.04674965888261795, 0.06416954845190048, 0.015732109546661377, 0.06712262332439423, -0.1832307130098343, 0.043826859444379807, 0.03158054128289223, -0.02640424482524395, 0.03143731504678726, 0.019917871803045273]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05165892839431763, 0.043818190693855286, 0.07976878434419632, 0.07038522511720657, 0.08635833114385605, 0.08426260203123093, 0.03522402420639992, -0.09286003559827805, 0.07918509095907211, 0.08298036456108093, 0.014968723058700562, 0.08274821192026138, -0.19067461788654327, 0.030309058725833893, -0.031272195279598236, 0.11018311977386475]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.12402254343032837, 0.16152280569076538, 0.1381923109292984, 0.12430284917354584, 0.10275902599096298, 0.1368527114391327, 0.030181884765625, 0.18976226449012756, 0.17885203659534454, -0.45152103900909424, 0.051399726420640945, 0.07915417850017548, 0.20503799617290497, -0.22348617017269135, -0.005016970913857222, -0.04735377058386803]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0683942511677742, 0.1067744642496109, 0.10474078357219696, 0.1364920437335968, 0.07132631540298462, -0.07003942877054214, 0.03408298268914223, 0.028760043904185295, -0.06660481542348862, 0.05849704518914223, -0.17384833097457886, 0.1488778293132782, -0.031942687928676605, -0.17233464121818542, 0.11979813873767853, -0.018473835662007332]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06557345390319824, 0.14192239940166473, 0.05957893282175064, 0.08838445693254471, -0.06535002589225769, -0.03583011403679848, 0.20780053734779358, 0.03273862227797508, 0.13253962993621826, -0.06945959478616714, 0.09441068768501282, 0.10048229992389679, -0.1605982631444931, 0.1618390828371048, 0.12416651099920273, 0.09673938155174255]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.09762008488178253, 0.018321247771382332, 0.14124248921871185, 0.3612441122531891, 0.1848580539226532, 0.13111858069896698, -0.13638047873973846, -0.045227646827697754, 0.3767438530921936, 0.18926279246807098, -0.47628507018089294, 0.4072170555591583, 0.24123789370059967, -0.23937353491783142, 0.23810884356498718, 0.17878346145153046]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.36438819766044617, 0.20781251788139343, 0.2256556898355484, 0.12258296459913254, -0.7494007349014282, 0.3763253390789032, 0.22050431370735168, -0.2773263156414032, 0.4877649247646332, 0.15477168560028076, -0.07994654029607773, 0.2075340896844864, 0.28806692361831665, 0.2409842312335968, -0.4604840874671936, 0.022025467827916145]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.19816133379936218, 0.28823021054267883, -0.28147193789482117, 0.3170720934867859, 0.5847595930099487, 0.3619844615459442, 0.25203943252563477, -0.26513513922691345, 0.17001540958881378, 0.6418060660362244, 0.1253776103258133, 0.38436374068260193, 0.5376483798027039, -0.4157595932483673, -0.36342552304267883, 0.3038003146648407]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4255894124507904, 0.2614040672779083, 0.3884834051132202, 0.9727133512496948, 0.36747047305107117, 0.34586086869239807, 0.5571082830429077, 0.5921266078948975, 0.5840224027633667, 0.28205376863479614, 0.36923733353614807, 0.6966822147369385, 0.93896484375, 0.18242377042770386, 0.7380149364471436, 0.7190163135528564]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0545217990875244, 0.9383434057235718, 0.5644721388816833, 0.7632914185523987, 0.8352209329605103, 0.6293855905532837, 0.679389476776123, 0.7539823651313782, 0.15915372967720032, 0.5564020276069641, 0.7539189457893372, 0.9205116033554077, 0.6948939561843872, 0.2261931151151657, 0.8608065247535706, 0.4117082953453064]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9202516078948975, 1.1508725881576538, 0.8375513553619385, 1.2470766305923462, 1.4498655796051025, 0.49098026752471924, 0.8687094449996948, 0.9417106509208679, 0.942255973815918, 1.4364346265792847, 0.8469730019569397, 0.9051339030265808, 0.38474565744400024, 0.565826416015625, 0.031498897820711136, 0.7930384874343872]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.236572265625, 0.8204139471054077, 1.1540305614471436, 1.0765016078948975, 0.49845588207244873, 0.5795485973358154, 1.0843790769577026, 1.1071618795394897, 0.5604073405265808, 0.6227551698684692, 1.0567611455917358, 0.8263747692108154, 0.8170149922370911, 0.8139458298683167, 0.724545955657959, 1.0321377515792847]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8792359828948975, 0.9665305614471436, 1.1942851543426514, 0.5480512976646423, 1.0640219449996948, 0.07927911728620529, 1.1173269748687744, 2.0010147094726562, 0.9859856963157654, 0.9905641078948975, 1.3401100635528564, 1.373833179473877, 0.23766297101974487, 0.8819881081581116, 0.989448070526123, 0.7746264934539795]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6439858675003052, 0.8256899118423462, 0.8866933584213257, 0.7712053656578064, 1.0682071447372437, 0.6467328667640686, 0.4179108738899231, 1.2103540897369385, 0.6110681295394897, 0.8585569858551025, -0.3195180594921112, 0.7330116033554077, 1.0192046165466309, 1.1437956094741821, 1.1551592350006104, 0.44765642285346985]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4240310490131378, 1.3128551244735718, 1.0800527334213257, 0.9570439457893372, 0.8641753792762756, 1.0112367868423462, 1.3090931177139282, 1.0403815507888794, 1.0772626399993896, 1.1221083402633667, 0.6677911877632141, -0.22221869230270386, 1.2268383502960205, 0.9330610632896423, 0.9981229901313782, 1.4130074977874756]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.7914135456085205, 0.19699452817440033, 0.9294354319572449, 0.8550502061843872, 0.6775833964347839, 0.5135164856910706, 0.3707786798477173, 0.916079044342041, -0.9156795144081116, 2.511997699737549, -0.8590642809867859, 0.6380504369735718, 0.8783101439476013, 0.9845398664474487, 0.9088055491447449, 0.3069782853126526]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.11395184695720673, 0.7869889140129089, 1.468369483947754, 1.4048200845718384, 1.4031554460525513, 1.4580966234207153, 1.3212382793426514, 1.3614169359207153, 1.7257888317108154, 1.4978249073028564, 2.2728288173675537, 1.4121243953704834, 1.2870463132858276, 1.1981661319732666, 0.44924411177635193, 1.4501826763153076]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5478389263153076, 1.3196022510528564, 1.5218901634216309, 1.8806310892105103, 1.0375279188156128, 1.2282112836837769, 1.930600643157959, 1.855215072631836, 2.471388101577759, 1.471996784210205, 1.2508243322372437, 0.9538130164146423, 1.2332842350006104, 1.2265799045562744, 1.4736708402633667, 1.538149356842041]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.4883320331573486, 1.2543628215789795, 1.4365360736846924, 1.2629616260528564, 1.6070921421051025, 1.0393668413162231, 1.3103946447372437, 1.263075828552246, 1.8737317323684692, 1.334238052368164, 1.5995839834213257, 1.5263545513153076, 2.2225546836853027, 1.563286304473877, 1.4689022302627563, 1.4296367168426514]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Running loglikelihood requests:  89%|███████████████████████████████████████████▋     | 449/504 [24:28<02:52,  3.14s/it]Layer: gate_20 - Captured router_logits: [1.0551789999008179, 1.035815715789795, 0.9300679564476013, 0.8128321170806885, 0.13174715638160706, 1.3537439107894897, 1.1287540197372437, 1.5301719903945923, 0.6891666054725647, 1.2346540689468384, 1.2455863952636719, 1.0416687726974487, 1.1369470357894897, 1.3793374300003052, 1.0073908567428589, 0.5261048078536987]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.091010570526123, 2.252739429473877, 2.1716721057891846, 2.010044574737549, 1.9597200155258179, 2.1193182468414307, 1.9517552852630615, 2.2203733921051025, 1.9670759439468384, 2.2285916805267334, 2.348721504211426, 2.6418426036834717, 1.867441177368164, 2.4382102489471436, 2.2117490768432617, 2.1500608921051025]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4080256223678589, 1.3411628007888794, 1.9490665197372437, 1.0566152334213257, 1.484121322631836, 1.560673713684082, 1.3418222665786743, 1.3666801452636719, 1.4248173236846924, 1.7204748392105103, 1.5500203371047974, 2.0918221473693848, 1.4695616960525513, 1.5527597665786743, 1.2850801944732666, 1.6800426244735718]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.435673713684082, 1.8220373392105103, 1.1652114391326904, 1.3933137655258179, 1.588017463684082, 1.432173252105713, 1.376572608947754, 1.3604910373687744, 1.4991376399993896, 1.3588169813156128, 1.4574878215789795, 1.4665685892105103, 1.2448508739471436, 0.8506255745887756, 1.3691660165786743, 1.5449979305267334]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.293222427368164, 2.4129464626312256, 2.279423713684082, 2.100142002105713, 2.665482997894287, 2.26430606842041, 2.566152572631836, 2.119521141052246, 2.144683361053467, 2.2301137447357178, 2.6435673236846924, 2.188007354736328, 2.2542612552642822, 2.448559284210205, 2.2990057468414307, 2.3408076763153076]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7047990560531616, 1.7644075155258179, 1.584517002105713, 1.8930600881576538, 1.4472402334213257, 1.6209415197372437, 1.6814123392105103, 2.0747768878936768, 1.721743106842041, 1.640371322631836, 1.6903916597366333, 1.6066862344741821, 1.7126623392105103, 1.557021141052246, 1.911576747894287, 2.0029423236846924]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8419743776321411, 1.8236924409866333, 1.7766335010528564, 1.788808822631836, 1.6762886047363281, 1.8262099027633667, 1.7176085710525513, 1.7556818723678589, 1.8031401634216309, 1.9603286981582642, 2.309380054473877, 1.666928768157959, 1.7452313899993896, 1.8973467350006104, 1.8299639225006104, 1.7436903715133667]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6509486436843872, 1.6986353397369385, 1.6646205186843872, 1.7548701763153076, 1.6088676452636719, 1.610541820526123, 1.8081371784210205, 1.7380529642105103, 1.6629464626312256, 1.7414900064468384, 1.470829963684082, 1.6353744268417358, 1.7196123600006104, 1.9263900518417358, 1.627333641052246, 1.6648234128952026]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.015878677368164, 2.0957791805267334, 2.4629158973693848, 1.9752434492111206, 2.0550425052642822, 2.030184745788574, 2.473569393157959, 2.113027572631836, 1.9744318723678589, 1.9838676452636719, 1.957284927368164, 1.8581575155258179, 1.9247158765792847, 1.829951286315918, 2.3043324947357178, 2.081879138946533]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.011160850524902, 5.385348796844482, 4.91923713684082, 4.8908281326293945, 4.831371784210205, 4.8242692947387695, 4.993912220001221, 5.231128215789795, 5.2226057052612305, 4.974634647369385, 4.9446024894714355, 4.792410850524902, 5.023335933685303, 5.001217365264893, 5.1564531326293945, 4.904017925262451]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.7567977905273438, 3.6523945331573486, 3.5732548236846924, 3.4658076763153076, 3.6427557468414307, 3.677861213684082, 3.481534004211426, 3.7223012447357178, 3.751420497894287, 3.6152596473693848, 3.5545859336853027, 3.3604910373687744, 3.596083641052246, 3.6569602489471436, 3.7227070331573486, 3.8461849689483643]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.6448862552642822, 2.7607548236846924, 2.7050528526306152, 2.5458602905273438, 2.5860390663146973, 2.7702414989471436, 2.6228692531585693, 2.5203428268432617, 2.5961344242095947, 2.614701747894287, 3.0861403942108154, 2.146230697631836, 2.6969358921051025, 2.6431615352630615, 3.138291358947754, 2.664975643157959]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.06323549151420593, 0.07691390812397003, 0.0827854722738266, -0.22510775923728943, -0.20428228378295898, -0.08558634668588638, 0.10566245764493942, -0.10251296311616898, 0.07120028138160706, 0.06166661158204079, 0.0765647366642952, 0.07514794915914536, 0.07300438731908798, 0.08818549662828445, -0.9506439566612244, 0.09551149606704712]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0898393914103508, 0.048244278877973557, 0.035026054829359055, 0.03845028951764107, 0.0674569234251976, 0.034101907163858414, 0.054804518818855286, 0.06901451200246811, 0.01140762958675623, 0.07240235805511475, -0.19078539311885834, 0.036606431007385254, 0.04268993064761162, -0.03172985836863518, 0.036122433841228485, 0.004452395718544722]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07963582128286362, 0.05670379102230072, 0.09795686602592468, 0.0941554456949234, 0.10690010339021683, 0.10756970942020416, 0.08866337686777115, -0.080169677734375, 0.10478309541940689, 0.07843490689992905, 0.03158034384250641, 0.07948402315378189, -0.17979411780834198, 0.030419088900089264, -0.037619851529598236, 0.11863411217927933]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.1382184773683548, 0.15327076613903046, 0.13597264885902405, 0.15883289277553558, 0.131758451461792, 0.12410874664783478, 0.0393262580037117, 0.21185460686683655, 0.17548875510692596, -0.48145800828933716, 0.009499438107013702, 0.11018113791942596, 0.20224910974502563, -0.2184174805879593, -0.040879882872104645, -0.0639670267701149]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07387394458055496, 0.10857421159744263, 0.12051064521074295, 0.10014234483242035, 0.04987196624279022, -0.052283499389886856, 0.030808882787823677, 0.03954974189400673, -0.022107409313321114, 0.038081973791122437, -0.15200617909431458, 0.14445753395557404, -0.06679108738899231, -0.2009059339761734, 0.11442657560110092, -0.010225766338407993]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.03407188504934311, 0.14762531220912933, 0.035962291061878204, 0.07405486702919006, -0.059979721903800964, -0.028826428577303886, 0.15655101835727692, 0.0635283812880516, 0.08906599879264832, -0.07064660638570786, 0.13371573388576508, 0.06879662722349167, -0.19303201138973236, 0.12195532768964767, 0.17247217893600464, 0.05795307829976082]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.07855650782585144, 0.02275105193257332, 0.09809301048517227, 0.41145622730255127, 0.16188980638980865, 0.10245583206415176, -0.09416297823190689, -0.06858885288238525, 0.4201628565788269, 0.14132413268089294, -0.48920905590057373, 0.37927523255348206, 0.17110294103622437, -0.19691844284534454, 0.16028030216693878, 0.19300426542758942]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.32935047149658203, 0.27663061022758484, 0.27949029207229614, 0.10439211130142212, -0.7505263090133667, 0.40583986043930054, 0.1723729968070984, -0.16967515647411346, 0.4237097203731537, 0.24668903648853302, -0.1306619942188263, 0.21166536211967468, 0.33195120096206665, 0.1744154840707779, -0.4624467194080353, -0.059542570263147354]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.25938117504119873, 0.28646373748779297, -0.18486201763153076, 0.3016258478164673, 0.5049684047698975, 0.28894200921058655, 0.2293047159910202, -0.22935347259044647, 0.14944101870059967, 0.619248628616333, 0.056733764708042145, 0.4018729031085968, 0.4002971053123474, -0.44074398279190063, -0.401059627532959, 0.32011789083480835]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.34361523389816284, 0.19887037575244904, 0.5118535161018372, 0.8211146593093872, 0.2964065372943878, 0.19340971112251282, 0.5444033741950989, 0.4789457321166992, 0.43692710995674133, 0.2286931872367859, 0.3597412109375, 0.6818562150001526, 1.0655628442764282, 0.14241087436676025, 0.6744210124015808, 0.6377270221710205]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1069347858428955, 0.9010437726974487, 0.5951007008552551, 0.7981939911842346, 0.8141043782234192, 0.5846613049507141, 0.6218357086181641, 0.6686663031578064, 0.27358970046043396, 0.6489320993423462, 0.7138671875, 0.9088752865791321, 0.7615419626235962, 0.2345244139432907, 0.5922324657440186, 0.4495770335197449]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.915400505065918, 1.0673320293426514, 0.8711064457893372, 1.0794345140457153, 1.4548498392105103, 0.4013782739639282, 0.8039329051971436, 0.9448622465133667, 0.8656782507896423, 1.238534927368164, 0.8820896148681641, 0.9257685542106628, 0.4427315890789032, 0.49162667989730835, -0.07851499319076538, 0.7141081690788269]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.22613446414470673, 0.8411881327629089, 1.0767426490783691, 1.1504414081573486, 0.41017526388168335, 0.5996854901313782, 0.9499432444572449, 1.0297693014144897, 0.5386090874671936, 0.5599462389945984, 0.7621594667434692, 0.8387910723686218, 0.8239967823028564, 0.8284420371055603, 0.6463702321052551, 0.9111708402633667]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7093014121055603, 0.8465211391448975, 0.894169807434082, 0.4521341621875763, 0.8831676244735718, -0.04695664346218109, 1.0568181276321411, 1.811840534210205, 0.8617149591445923, 0.8198052048683167, 0.9842354655265808, 1.2495434284210205, -0.001263506943359971, 0.7345208525657654, 1.0215128660202026, 0.5518069863319397]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5241350531578064, 0.7097580432891846, 0.8465909361839294, 0.6959086060523987, 0.9143161773681641, 0.2875826060771942, 0.15526393055915833, 0.7567241787910461, 0.6757019758224487, 0.7790939807891846, -0.26704227924346924, 0.5770984888076782, 0.8574028611183167, 0.9989346861839294, 1.1871448755264282, 0.40514543652534485]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.3387625515460968, 1.3130580186843872, 1.0010145902633667, 0.9276582598686218, 0.797417163848877, 0.8640929460525513, 0.9358116388320923, 0.9649451971054077, 0.9594155550003052, 1.002079963684082, 0.6845195889472961, -0.3300083577632904, 0.9359654188156128, 0.7964438199996948, 0.9227628111839294, 1.1828930377960205]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.7693917155265808, 0.14119334518909454, 0.7936282753944397, 0.7856762409210205, 0.7877482771873474, 0.4181118309497833, 0.20667356252670288, 0.8652502298355103, -1.0974605083465576, 2.430752754211426, -0.8590658903121948, 0.4533627927303314, 0.8981330990791321, 0.9333591461181641, 0.8036919236183167, 0.28803521394729614]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.17447811365127563, 0.6655653715133667, 1.352729320526123, 1.3405539989471436, 1.3173954486846924, 1.4090908765792847, 1.2378754615783691, 1.3668323755264282, 1.6574674844741821, 1.4143351316452026, 2.025669574737549, 1.3553926944732666, 1.2803075313568115, 1.190746784210205, 0.4702517092227936, 1.494673252105713]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4361809492111206, 1.2951501607894897, 1.4821460247039795, 1.7549716234207153, 0.9495992064476013, 1.1741578578948975, 1.8398945331573486, 1.7768871784210205, 2.262986898422241, 1.4255783557891846, 1.2348061800003052, 0.8481996059417725, 1.258725643157959, 1.3519619703292847, 1.4192370176315308, 1.4896003007888794]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.4550020694732666, 1.3289873600006104, 1.350903034210205, 1.331904411315918, 1.5451501607894897, 0.9720094203948975, 1.2917131185531616, 1.2447746992111206, 1.6706955432891846, 1.4046519994735718, 1.5531655550003052, 1.5297534465789795, 2.190645217895508, 1.533989429473877, 1.4215706586837769, 1.3708908557891846]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0297399759292603, 1.0189224481582642, 0.968876838684082, 0.899848461151123, -0.029050355777144432, 1.195693016052246, 1.1540939807891846, 1.477846622467041, 0.7708629369735718, 1.2146154642105103, 1.325791358947754, 1.0190112590789795, 1.0546875, 1.386059284210205, 0.8886449337005615, 0.5508288145065308]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.1116578578948975, 2.2724227905273438, 2.1554384231567383, 2.1015625, 1.9996448755264282, 2.1265218257904053, 1.9889914989471436, 2.222504138946533, 1.972402572631836, 2.169236898422241, 2.387986898422241, 2.7295048236846924, 1.7407163381576538, 2.3891031742095947, 2.1816153526306152, 2.121245861053467]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4829038381576538, 1.3961546421051025, 1.7919033765792847, 1.0703125, 1.4794541597366333, 1.568486213684082, 1.3655134439468384, 1.4307020902633667, 1.480215072631836, 1.7013493776321411, 1.5118709802627563, 2.075385570526123, 1.5233360528945923, 1.523183822631836, 1.2815543413162231, 1.7041395902633667]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.376674175262451, 1.7456371784210205, 1.0394176244735718, 1.303267002105713, 1.4405945539474487, 1.3897626399993896, 1.2717634439468384, 1.375, 1.4006696939468384, 1.318739891052246, 1.447392463684082, 1.3802759647369385, 1.1969865560531616, 0.6528887152671814, 1.2853084802627563, 1.451958179473877]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  90%|████████████████████████████████████████████     | 453/504 [24:41<02:40,  3.15s/it]Layer: gate_24 - Captured router_logits: [2.336850643157959, 2.4216721057891846, 2.3204140663146973, 2.1435673236846924, 2.729809284210205, 2.2608563899993896, 2.5078125, 2.2452313899993896, 2.2206778526306152, 2.279423713684082, 2.6858766078948975, 2.1945009231567383, 2.309760570526123, 2.4772727489471436, 2.3784496784210205, 2.440239429473877]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6483867168426514, 1.703784465789795, 1.5356636047363281, 1.771103858947754, 1.320058822631836, 1.5961850881576538, 1.6492998600006104, 2.0230824947357178, 1.668628215789795, 1.6067878007888794, 1.6208654642105103, 1.5387581586837769, 1.621753215789795, 1.5488027334213257, 1.7759740352630615, 2.046621322631836]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.833730697631836, 1.7446986436843872, 1.7227070331573486, 1.7245585918426514, 1.6582285165786743, 1.7535765171051025, 1.6668812036514282, 1.7416421175003052, 1.7720614671707153, 1.9382482767105103, 2.1705939769744873, 1.6070287227630615, 1.6430093050003052, 1.905590534210205, 1.8038631677627563, 1.7259489297866821]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.5392400026321411, 1.648369312286377, 1.5748686790466309, 1.7182109355926514, 1.5696784257888794, 1.5688539743423462, 1.716073989868164, 1.6629931926727295, 1.59002685546875, 1.6880707740783691, 1.3312195539474487, 1.5951894521713257, 1.5901734828948975, 1.8275161981582642, 1.5721261501312256, 1.6055701971054077]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.016893148422241, 2.068486213684082, 2.450068473815918, 2.0168426036834717, 2.085747241973877, 2.058187961578369, 2.5019278526306152, 2.1269278526306152, 2.0185165405273438, 2.018770217895508, 1.9739245176315308, 1.8344155550003052, 1.984311580657959, 1.8758116960525513, 2.297128677368164, 2.0370333194732666]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.076907634735107, 5.4257307052612305, 5.0613837242126465, 5.024249076843262, 4.987215995788574, 4.8757100105285645, 5.05600643157959, 5.303064346313477, 5.3457794189453125, 5.078530788421631, 5.015625, 4.897625923156738, 5.154829502105713, 5.082792282104492, 5.205255508422852, 5.004667282104492]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.005529403686523, 3.847909927368164, 3.823051929473877, 3.7268223762512207, 3.8623173236846924, 3.963778495788574, 3.7517502307891846, 3.846083641052246, 4.015726566314697, 3.957284927368164, 3.7713067531585693, 3.5162971019744873, 3.8596034049987793, 3.8638393878936768, 3.9150772094726562, 3.978135108947754]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.812094211578369, 2.955458641052246, 2.7900772094726562, 2.6220576763153076, 2.68100643157959, 2.914975643157959, 2.716923713684082, 2.6253550052642822, 2.7673497200012207, 2.778003215789795, 3.2230112552642822, 2.4312093257904053, 2.8656656742095947, 2.77475643157959, 3.183745861053467, 2.8187906742095947]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.059029947966337204, 0.0735187754034996, 0.07706929743289948, -0.2423972636461258, -0.22170573472976685, -0.05064656585454941, 0.1027228832244873, -0.1285628229379654, 0.060767415910959244, 0.06192423403263092, 0.06662678718566895, 0.046565450727939606, 0.06988627463579178, 0.09706217795610428, -1.0220181941986084, 0.09724731743335724]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0683060735464096, 0.04174237698316574, 0.018836263567209244, 0.042723871767520905, 0.0688094049692154, 0.03015808016061783, 0.04992512986063957, 0.04931030422449112, 0.007322387769818306, 0.05890767276287079, -0.20426350831985474, 0.055889230221509933, 0.04633809253573418, -0.02804972417652607, 0.049043428152799606, 0.01879374124109745]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05851887911558151, 0.056697312742471695, 0.0694352239370346, 0.0812721773982048, 0.0892181396484375, 0.07401061803102493, 0.05038323625922203, -0.08374593406915665, 0.1076405867934227, 0.08648020774126053, 0.026319148018956184, 0.07456807792186737, -0.18046101927757263, 0.020028864964842796, -0.04084228351712227, 0.11687865853309631]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.12699981033802032, 0.1566479504108429, 0.1371980756521225, 0.12329813838005066, 0.10550890862941742, 0.11394164711236954, -0.018787434324622154, 0.19129475951194763, 0.1711564064025879, -0.5294140577316284, 0.0812288448214531, 0.12434061616659164, 0.15269674360752106, -0.24027587473392487, 0.03300943970680237, -0.012488149106502533]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07112634927034378, 0.11401967704296112, 0.11158569157123566, 0.10113586485385895, 0.13200785219669342, -0.060622964054346085, 0.005703328642994165, 0.028584085404872894, -0.08931764960289001, -0.0033056640531867743, -0.1643448919057846, 0.11037841439247131, -0.02332763746380806, -0.17988306283950806, 0.1329117864370346, -0.08137024194002151]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_5 - Captured router_logits: [0.0036197917070239782, 0.14193114638328552, 0.0883268192410469, 0.14589543640613556, -0.13796550035476685, -0.0662841796875, 0.2245979756116867, -0.008417154662311077, 0.07662831991910934, -0.07675658911466599, 0.14196939766407013, 0.09119811654090881, -0.20290690660476685, 0.17172770202159882, 0.1110852062702179, 0.08099156618118286]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11337819695472717, -0.0002868652227334678, 0.22980305552482605, 0.37960124015808105, 0.16488748788833618, 0.12872599065303802, -0.14942607283592224, -0.0865681990981102, 0.3498763144016266, 0.20601481199264526, -0.48702067136764526, 0.41966837644577026, 0.22072266042232513, -0.32698893547058105, 0.16626159846782684, 0.19358378648757935]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.2707991600036621, 0.21628427505493164, 0.22555501759052277, 0.10802002251148224, -0.6795426607131958, 0.38738688826560974, 0.22722534835338593, -0.26102620363235474, 0.4819042980670929, 0.25442707538604736, -0.16420410573482513, 0.19852375984191895, 0.2717691957950592, 0.2738338112831116, -0.4850943982601166, -0.06884053349494934]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.22895100712776184, 0.2828995883464813, -0.2997477352619171, 0.31569334864616394, 0.6008951663970947, 0.4578515589237213, 0.21632497012615204, -0.24754638969898224, 0.16811014711856842, 0.6893538236618042, 0.15432454645633698, 0.48691487312316895, 0.5179036259651184, -0.4510384202003479, -0.4891113340854645, 0.3486838638782501]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4916341006755829, 0.25821614265441895, 0.46990397572517395, 0.9426204562187195, 0.42469239234924316, 0.32554036378860474, 0.647718071937561, 0.6518229246139526, 0.6875325441360474, 0.3390299379825592, 0.2874568700790405, 0.7271614670753479, 0.9721581935882568, -0.045340169221162796, 0.7725390791893005, 0.8118749856948853]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1462198495864868, 1.0221158266067505, 0.6050781011581421, 0.7933202981948853, 0.8066243529319763, 0.7473996877670288, 0.7630370855331421, 0.78104168176651, 0.09332600980997086, 0.4894433617591858, 0.7821744680404663, 0.9488866925239563, 0.6372233033180237, 0.2532820701599121, 0.9093745946884155, 0.4683534801006317]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0554687976837158, 1.339270830154419, 1.0240625143051147, 1.4872689247131348, 1.7646875381469727, 0.5719372630119324, 1.0185155868530273, 0.9749218821525574, 1.0854296684265137, 1.5882030725479126, 1.0314843654632568, 1.153476595878601, 0.5165494680404663, 0.7407003045082092, 0.14616699516773224, 0.8883593678474426]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.19025227427482605, 0.9278255105018616, 1.3115625381469727, 1.1810417175292969, 0.6944026947021484, 0.6002538800239563, 1.2999999523162842, 1.1886588335037231, 0.51357501745224, 0.6780533790588379, 1.0335807800292969, 0.9226301908493042, 0.8450390696525574, 0.90604168176651, 0.8446354269981384, 1.2017707824707031]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.0309635400772095, 1.076145887374878, 1.333483099937439, 0.5676122903823853, 1.12932288646698, 0.11928222328424454, 1.240390658378601, 2.137239694595337, 1.1854556798934937, 1.1240885257720947, 1.5045051574707031, 1.5548958778381348, 0.47215819358825684, 0.9075325727462769, 1.091406226158142, 0.8426106572151184]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8189843893051147, 0.9648958444595337, 0.9920833110809326, 0.823437511920929, 1.1471874713897705, 0.7346419095993042, 0.47013673186302185, 1.4209342002868652, 0.7092838287353516, 0.9712370038032532, -0.366437166929245, 0.9592838287353516, 1.139430284500122, 1.2600911855697632, 1.27985680103302, 0.4853645861148834]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.5001725554466248, 1.471093773841858, 1.1246614456176758, 1.0057681798934937, 0.9207812547683716, 1.078671932220459, 1.506077527999878, 1.096093773841858, 1.2422916889190674, 1.141614556312561, 0.7758203148841858, -0.1338411420583725, 1.197695255279541, 0.9646093845367432, 1.0969010591506958, 1.4879565238952637]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.8554362058639526, 0.1224873885512352, 1.0065380334854126, 0.8731250166893005, 0.703819990158081, 0.5105013251304626, 0.6320485472679138, 0.9820247292518616, -0.8010677099227905, 2.646500587463379, -0.9314957857131958, 0.6113867163658142, 0.9758594036102295, 1.114804744720459, 0.95145183801651, 0.34406086802482605]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.10099121183156967, 0.9654866456985474, 1.5819921493530273, 1.4798632860183716, 1.5012760162353516, 1.602369785308838, 1.4778255224227905, 1.5898046493530273, 1.8767708539962769, 1.6342447996139526, 2.513098955154419, 1.6078938245773315, 1.381617784500122, 1.3176822662353516, 0.56319659948349, 1.6345572471618652]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6564583778381348, 1.3748958110809326, 1.5920573472976685, 1.9780728816986084, 1.2594531774520874, 1.3266927003860474, 2.0737760066986084, 1.9097917079925537, 2.5848958492279053, 1.5871354341506958, 1.351259708404541, 1.1074340343475342, 1.3800650835037231, 1.3902978897094727, 1.6100521087646484, 1.684999942779541]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.541041612625122, 1.4537500143051147, 1.5891666412353516, 1.4108333587646484, 1.731458306312561, 1.2053662538528442, 1.451744794845581, 1.391627550125122, 1.963880181312561, 1.461744785308838, 1.7645572423934937, 1.6229166984558105, 2.334843635559082, 1.71916663646698, 1.642552137374878, 1.5801823139190674]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1966276168823242, 1.15388023853302, 1.0166406631469727, 0.9154752492904663, 0.2986027002334595, 1.5475780963897705, 1.29255211353302, 1.5827832221984863, 0.8260791301727295, 1.3558332920074463, 1.452734351158142, 1.0964908599853516, 1.2819792032241821, 1.5504167079925537, 1.0394678115844727, 0.5935115814208984]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.2666666507720947, 2.4289584159851074, 2.3179166316986084, 2.124479055404663, 2.113906145095825, 2.240000009536743, 2.1072916984558105, 2.3734374046325684, 2.1247916221618652, 2.385937452316284, 2.486041784286499, 2.7249999046325684, 1.9806771278381348, 2.652916669845581, 2.3642709255218506, 2.347916603088379]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4353646039962769, 1.4074479341506958, 1.98479163646698, 1.104895830154419, 1.4432551860809326, 1.6164582967758179, 1.3048046827316284, 1.3620573282241821, 1.5173437595367432, 1.7811458110809326, 1.5391666889190674, 2.0910415649414062, 1.5315624475479126, 1.6252604722976685, 1.2990885972976685, 1.6913541555404663]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.534895896911621, 1.8669791221618652, 1.2660937309265137, 1.4558333158493042, 1.726406216621399, 1.552760362625122, 1.4950000047683716, 1.466770887374878, 1.5759375095367432, 1.4185937643051147, 1.5334374904632568, 1.5604687929153442, 1.2734895944595337, 0.8462435007095337, 1.4251302480697632, 1.5653125047683716]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.2125000953674316, 2.372499942779541, 2.2527084350585938, 2.081979274749756, 2.655677080154419, 2.2440624237060547, 2.413749933242798, 2.0529167652130127, 2.0445313453674316, 2.2004165649414062, 2.6066665649414062, 2.1061458587646484, 2.2655208110809326, 2.3971874713897705, 2.2362499237060547, 2.2109375]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.658177137374878, 1.6770312786102295, 1.502343773841858, 1.8346874713897705, 1.3871614933013916, 1.564062476158142, 1.6229166984558105, 1.9772917032241821, 1.6545312404632568, 1.5784895420074463, 1.616614580154419, 1.4767708778381348, 1.6361979246139526, 1.4331250190734863, 1.8476041555404663, 1.9526041746139526]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.822291612625122, 1.859606146812439, 1.7714062929153442, 1.7622395753860474, 1.6892187595367432, 1.8945931196212769, 1.7840429544448853, 1.7834374904632568, 1.8089323043823242, 1.9894791841506958, 2.3460676670074463, 1.683984398841858, 1.7619792222976685, 1.9897135496139526, 1.8401042222976685, 1.8099788427352905]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6618489027023315, 1.6861978769302368, 1.6578906774520874, 1.7353124618530273, 1.5785937309265137, 1.6047916412353516, 1.8046875, 1.7269010543823242, 1.6302604675292969, 1.7213151454925537, 1.4103515148162842, 1.6191927194595337, 1.7473437786102295, 1.9583854675292969, 1.6266145706176758, 1.6301041841506958]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  91%|████████████████████████████████████████████▍    | 457/504 [24:53<02:27,  3.14s/it]Layer: gate_28 - Captured router_logits: [2.1069271564483643, 2.192552089691162, 2.5314061641693115, 2.0388541221618652, 2.157864570617676, 2.1098437309265137, 2.4881250858306885, 2.214270830154419, 2.0805728435516357, 2.060885429382324, 2.052968740463257, 1.8727604150772095, 2.0010156631469727, 1.9189062118530273, 2.382760524749756, 2.154583215713501]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.160208225250244, 5.595624923706055, 5.033958435058594, 4.887395858764648, 4.932291507720947, 4.997499942779541, 5.115833282470703, 5.288125038146973, 5.270625114440918, 5.0785417556762695, 4.987083435058594, 4.943124771118164, 5.151666641235352, 5.1508331298828125, 5.336249828338623, 5.011458396911621]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.9872395992279053, 3.8369791507720947, 3.8153645992279053, 3.6140103340148926, 3.7560417652130127, 3.8285417556762695, 3.5736589431762695, 3.951979160308838, 3.90317702293396, 3.782916784286499, 3.700000047683716, 3.307506561279297, 3.651881456375122, 3.7602603435516357, 3.8809375762939453, 4.036015510559082]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.772916555404663, 2.8827083110809326, 2.893749952316284, 2.5888540744781494, 2.794062614440918, 2.907604217529297, 2.6423957347869873, 2.534583330154419, 2.73520827293396, 2.581458330154419, 3.1619791984558105, 2.1080517768859863, 2.770937442779541, 2.707604169845581, 3.236875057220459, 2.788749933242798]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.06643656641244888, 0.07999379187822342, 0.08521749079227448, -0.24900206923484802, -0.21943765878677368, -0.07432372868061066, 0.10773050785064697, -0.1256689429283142, 0.04641059786081314, 0.07031489163637161, 0.07638844847679138, 0.05471048876643181, 0.08491414040327072, 0.09271891415119171, -0.9662890434265137, 0.10580983757972717]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08828368782997131, 0.04321485012769699, 0.03400431200861931, 0.051896996796131134, 0.06827881187200546, 0.040078263729810715, 0.05236399173736572, 0.0687522366642952, 0.021284382790327072, 0.06524393707513809, -0.19100341200828552, 0.05634918063879013, 0.04267822206020355, -0.01615346223115921, 0.039946697652339935, 0.025410309433937073]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06676346063613892, 0.06538935005664825, 0.08324340730905533, 0.0892854854464531, 0.09489338845014572, 0.09276184439659119, 0.05797363445162773, -0.08314066380262375, 0.09644012153148651, 0.07197163999080658, 0.025552164763212204, 0.10148055851459503, -0.1871427446603775, 0.041548870503902435, -0.02774251252412796, 0.12970763444900513]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.12632547318935394, 0.15695230662822723, 0.1518212854862213, 0.1308007836341858, 0.12213795632123947, 0.11704020202159882, -0.0011700439499691129, 0.1861572265625, 0.17898274958133698, -0.5401822924613953, 0.07234863191843033, 0.10787434875965118, 0.16320475935935974, -0.2443314641714096, 0.01747436448931694, -0.029137777164578438]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07571553438901901, 0.11350799351930618, 0.11019795387983322, 0.08988036960363388, 0.10937388241291046, -0.057067565619945526, 0.013843434862792492, 0.02125345915555954, -0.11827433109283447, 0.030868427827954292, -0.14743992686271667, 0.13459350168704987, -0.024510497227311134, -0.18523165583610535, 0.1214078739285469, -0.05122721195220947]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.014973144978284836, 0.12129781395196915, 0.03857218474149704, 0.11247151345014572, -0.09653401374816895, -0.06602335721254349, 0.25651559233665466, 0.03364501893520355, 0.06207328662276268, -0.0669645145535469, 0.2746695876121521, 0.08141296356916428, -0.1641666740179062, 0.145880788564682, 0.11530377715826035, 0.059815265238285065]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.12416341155767441, -0.022317148745059967, 0.18596598505973816, 0.41776856780052185, 0.15800857543945312, 0.15310099720954895, -0.16090922057628632, -0.0822957381606102, 0.2811751365661621, 0.20051798224449158, -0.492413729429245, 0.4428125023841858, 0.23062825202941895, -0.27792438864707947, 0.1685192883014679, 0.17541849613189697]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.2657886743545532, 0.23862600326538086, 0.24395956099033356, 0.13559488952159882, -0.7041276097297668, 0.37489989399909973, 0.21968261897563934, -0.2691357433795929, 0.45869627594947815, 0.21971191465854645, -0.15735839307308197, 0.2144813984632492, 0.30074217915534973, 0.22428487241268158, -0.44643229246139526, 0.003064778633415699]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.22036702930927277, 0.26363179087638855, -0.3335343301296234, 0.3437638282775879, 0.6230322122573853, 0.43999674916267395, 0.2119661420583725, -0.2512166202068329, 0.13751505315303802, 0.5925707817077637, 0.11149048060178757, 0.46398600935935974, 0.48777344822883606, -0.44837647676467896, -0.4903255105018616, 0.29634857177734375]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4500439465045929, 0.18090982735157013, 0.49452635645866394, 0.8637499809265137, 0.43677082657814026, 0.27313750982284546, 0.5946744680404663, 0.6156673431396484, 0.5382226705551147, 0.28138914704322815, 0.27430135011672974, 0.7139974236488342, 0.9788012504577637, -0.10596028715372086, 0.690625011920929, 0.73263019323349]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.115221381187439, 1.0177929401397705, 0.6476432085037231, 0.7787500023841858, 0.736132800579071, 0.6789302825927734, 0.7282812595367432, 0.767981767654419, 0.04146891459822655, 0.41504883766174316, 0.7611588835716248, 0.9207617044448853, 0.6814941167831421, 0.21798501908779144, 0.7545036673545837, 0.3925614356994629]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9899349212646484, 1.3064583539962769, 0.9617447853088379, 1.3667675256729126, 1.73828125, 0.48167115449905396, 0.9884244799613953, 0.930468738079071, 1.031822919845581, 1.4528385400772095, 0.9299218654632568, 1.0381380319595337, 0.43803671002388, 0.6570011377334595, 0.09392252564430237, 0.8031575679779053]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.13605956733226776, 0.8822526335716248, 1.1828385591506958, 1.104062557220459, 0.5750138163566589, 0.5968164205551147, 1.1704199314117432, 1.1143815517425537, 0.46323567628860474, 0.6968489289283752, 0.9731249809265137, 0.9027604460716248, 0.7956966161727905, 0.8226302266120911, 0.8158203363418579, 1.1321874856948853]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8698437213897705, 0.9449478983879089, 1.1069140434265137, 0.429080605506897, 0.9723437428474426, -0.015634765848517418, 1.1476562023162842, 1.9014062881469727, 1.0250650644302368, 0.9637760519981384, 1.3119921684265137, 1.4244791269302368, 0.2371484339237213, 0.7240495085716248, 1.0054166316986084, 0.7457454204559326]
Running loglikelihood requests:  91%|████████████████████████████████████████████▊    | 461/504 [25:06<02:14,  3.13s/it]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6719661355018616, 0.8529947996139526, 0.9302343726158142, 0.7559505105018616, 1.0493229627609253, 0.489361971616745, 0.3166275918483734, 1.163759708404541, 0.6335514187812805, 0.8804947733879089, -0.40520018339157104, 0.81298828125, 1.0416308641433716, 1.1357030868530273, 1.208020806312561, 0.42127034068107605]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.4403393566608429, 1.4188021421432495, 1.05427086353302, 0.9430989623069763, 0.8018164038658142, 0.993151068687439, 1.280126929283142, 1.0418750047683716, 1.1168750524520874, 1.0579687356948853, 0.6827148199081421, -0.1836889684200287, 1.0995874404907227, 0.9039322733879089, 1.0227603912353516, 1.384142279624939]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.8198567628860474, 0.07827311009168625, 0.9658105373382568, 0.8375016450881958, 0.7175114154815674, 0.4791845679283142, 0.5035400390625, 0.9356510639190674, -0.9062174558639526, 2.6249217987060547, -0.9474959373474121, 0.588610053062439, 0.9409635663032532, 1.1310546398162842, 0.9320898652076721, 0.359967440366745]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.09796549379825592, 0.868420422077179, 1.536940097808838, 1.3843213319778442, 1.4782551527023315, 1.5774739980697632, 1.4438931941986084, 1.5169661045074463, 1.7551546096801758, 1.5897525548934937, 2.430260419845581, 1.5087956190109253, 1.3711944818496704, 1.2821223735809326, 0.5032299757003784, 1.6047916412353516]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.669531226158142, 1.35197913646698, 1.671067714691162, 1.9483333826065063, 1.188645839691162, 1.369140625, 2.0501041412353516, 1.8773958683013916, 2.5844271183013916, 1.57770836353302, 1.3387629985809326, 1.092861294746399, 1.3421093225479126, 1.3919625282287598, 1.5846874713897705, 1.6940103769302368]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.470677137374878, 1.3672916889190674, 1.520078182220459, 1.3524478673934937, 1.6437499523162842, 1.107434868812561, 1.3697395324707031, 1.334010362625122, 1.856471300125122, 1.4518228769302368, 1.6985937356948853, 1.5792968273162842, 2.301041603088379, 1.6291146278381348, 1.5741145610809326, 1.5036197900772095]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1819075345993042, 1.0942057371139526, 1.022031307220459, 0.9093213081359863, 0.25427937507629395, 1.4727864265441895, 1.2664583921432495, 1.5775814056396484, 0.8160221576690674, 1.3626562356948853, 1.3772135972976685, 1.0607227087020874, 1.2440885305404663, 1.488437533378601, 1.0157356262207031, 0.6154516339302063]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.2381250858306885, 2.3971874713897705, 2.296562433242798, 2.1044790744781494, 2.0932812690734863, 2.2439582347869873, 2.085416555404663, 2.3526041507720947, 2.0782811641693115, 2.397812604904175, 2.4623959064483643, 2.7048957347869873, 1.9666146039962769, 2.5953125953674316, 2.351614475250244, 2.299166679382324]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4408854246139526, 1.3537499904632568, 1.9537500143051147, 1.0859375, 1.470729112625122, 1.604062557220459, 1.3619270324707031, 1.337499976158142, 1.5203125476837158, 1.7589062452316284, 1.5422916412353516, 2.070937395095825, 1.5280729532241821, 1.6276041269302368, 1.316380262374878, 1.7184895277023315]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5275521278381348, 1.872499942779541, 1.2533073425292969, 1.4834375381469727, 1.7100521326065063, 1.5219792127609253, 1.4338542222976685, 1.4656771421432495, 1.5993229150772095, 1.4349479675292969, 1.5162500143051147, 1.5194270610809326, 1.3126041889190674, 0.8332307934761047, 1.4109375476837158, 1.599583387374878]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.2389583587646484, 2.3784375190734863, 2.276458263397217, 2.073437452316284, 2.6491665840148926, 2.2713541984558105, 2.42104172706604, 2.0634374618530273, 2.049427032470703, 2.1840624809265137, 2.6078124046325684, 2.149791717529297, 2.291562557220459, 2.3842709064483643, 2.271250009536743, 2.2263541221618652]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6710416078567505, 1.7108854055404663, 1.5327603816986084, 1.834375023841858, 1.4080989360809326, 1.5645833015441895, 1.6675000190734863, 2.0003645420074463, 1.6829687356948853, 1.5885416269302368, 1.6340104341506958, 1.5109374523162842, 1.649427056312561, 1.4669270515441895, 1.844739556312561, 1.9421875476837158]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.809166669845581, 1.8347444534301758, 1.7502083778381348, 1.7679166793823242, 1.69572913646698, 1.8575325012207031, 1.72966468334198, 1.745104193687439, 1.7776399850845337, 1.9572396278381348, 2.291796922683716, 1.6746094226837158, 1.744114637374878, 1.9668489694595337, 1.8236783742904663, 1.7989355325698853]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.599765658378601, 1.6416666507720947, 1.6303906440734863, 1.6953645944595337, 1.5391666889190674, 1.5649479627609253, 1.7604687213897705, 1.6815625429153442, 1.5779427289962769, 1.668515682220459, 1.3650000095367432, 1.5905729532241821, 1.6628645658493042, 1.910729169845581, 1.5693750381469727, 1.5797916650772095]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.971093773841858, 2.0344791412353516, 2.3840625286102295, 1.8987499475479126, 2.0060417652130127, 1.9610416889190674, 2.3058853149414062, 2.0703125, 1.9265625476837158, 1.9076042175292969, 1.882239580154419, 1.719739556312561, 1.8894987106323242, 1.7487499713897705, 2.234323024749756, 2.017864465713501]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.9613542556762695, 5.406146049499512, 4.8882293701171875, 4.734010219573975, 4.767083168029785, 4.789583206176758, 4.980833530426025, 5.126874923706055, 5.13895845413208, 4.931458473205566, 4.851354122161865, 4.815625190734863, 4.971875190734863, 5.008124828338623, 5.128958225250244, 4.8823957443237305]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.7752604484558105, 3.623593807220459, 3.589270830154419, 3.3842709064483643, 3.598281145095825, 3.660963535308838, 3.375, 3.684999942779541, 3.738997459411621, 3.6376302242279053, 3.5031771659851074, 3.1399738788604736, 3.4941015243530273, 3.5873959064483643, 3.6854166984558105, 3.8147852420806885]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.7715625762939453, 2.885937452316284, 2.868854284286499, 2.608750104904175, 2.753958225250244, 2.903437614440918, 2.65541672706604, 2.550520896911621, 2.7641665935516357, 2.5908854007720947, 3.1231250762939453, 2.112929582595825, 2.770416736602783, 2.734375, 3.2344791889190674, 2.7869791984558105]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.05492391064763069, 0.0676707997918129, 0.07643920928239822, -0.19080159068107605, -0.1923014372587204, -0.054929353296756744, 0.0932236760854721, -0.11492106318473816, 0.05076995864510536, 0.058523762971162796, 0.06628987938165665, 0.0720900446176529, 0.07053344696760178, 0.08740295469760895, -0.915442705154419, 0.08781656622886658]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07701700925827026, 0.028755340725183487, 0.019287211820483208, 0.03575475141406059, 0.06683024019002914, 0.037397611886262894, 0.03670817241072655, 0.06660354882478714, -0.005836893804371357, 0.06482808291912079, -0.14156128466129303, 0.05660873278975487, 0.02812815271317959, -0.006913146935403347, 0.03418217971920967, 0.027462005615234375]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06298574060201645, 0.05294102802872658, 0.08244191855192184, 0.07144897431135178, 0.0941365584731102, 0.07551229000091553, 0.03278239071369171, -0.07491088658571243, 0.07384826987981796, 0.08281758427619934, 0.023454487323760986, 0.09279398620128632, -0.1770503669977188, 0.06154073029756546, -0.04541625827550888, 0.11362671107053757]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.11525492370128632, 0.14065292477607727, 0.1286100298166275, 0.13021321594715118, 0.10181355476379395, 0.12750732898712158, 0.014912922866642475, 0.17738647758960724, 0.1682714819908142, -0.45907390117645264, 0.06395548582077026, 0.09396891295909882, 0.19612345099449158, -0.23687419295310974, -0.0068505858071148396, -0.003884277306497097]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08090230077505112, 0.09576059877872467, 0.10470397770404816, 0.1272517889738083, 0.09557515382766724, -0.06627807766199112, -0.0021298725623637438, 0.032477568835020065, -0.08814432471990585, 0.04179850220680237, -0.14182861149311066, 0.1398250311613083, -0.049956053495407104, -0.1512535810470581, 0.13232503831386566, -0.04352864623069763]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.037403564900159836, 0.13729217648506165, 0.07190897315740585, 0.08112263679504395, -0.12617146968841553, -0.027041422203183174, 0.21058471500873566, 0.0031355794053524733, 0.12758484482765198, -0.049062907695770264, 0.0378621406853199, 0.1000068187713623, -0.10990641266107559, 0.16813965141773224, 0.1309574395418167, 0.12010370939970016]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11740753054618835, -0.02265218086540699, 0.14006103575229645, 0.39581623673439026, 0.1635286509990692, 0.12718668580055237, -0.1375771015882492, -0.06952555477619171, 0.3271045386791229, 0.20002929866313934, -0.46858641505241394, 0.3751426041126251, 0.2374446541070938, -0.2608084976673126, 0.21145954728126526, 0.16346679627895355]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3318863809108734, 0.1940511018037796, 0.19098062813282013, 0.13294148445129395, -0.7810611724853516, 0.3711966872215271, 0.24006225168704987, -0.2794303297996521, 0.43078023195266724, 0.1688246726989746, -0.12816151976585388, 0.23486939072608948, 0.22682617604732513, 0.2859016954898834, -0.44327229261398315, -0.12297912687063217]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2086653709411621, 0.2599422335624695, -0.3508593738079071, 0.3542610704898834, 0.5490702390670776, 0.35435953736305237, 0.2338113933801651, -0.22090454399585724, 0.15604105591773987, 0.6025423407554626, 0.16094858944416046, 0.3636051416397095, 0.549720048904419, -0.41693684458732605, -0.4455289840698242, 0.2957128882408142]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.37583211064338684, 0.21621257066726685, 0.44440755248069763, 0.8605175614356995, 0.37321531772613525, 0.20283295214176178, 0.5523665547370911, 0.6126757860183716, 0.47871580719947815, 0.24016845226287842, 0.29412850737571716, 0.6665754914283752, 0.9327304363250732, 0.10935547202825546, 0.7176692485809326, 0.7153776288032532]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0755257606506348, 0.8976367115974426, 0.5786523222923279, 0.7385677099227905, 0.7406119704246521, 0.5641809105873108, 0.6686718463897705, 0.7849348783493042, 0.11673665046691895, 0.4368961453437805, 0.7218359112739563, 0.8813086152076721, 0.6332194209098816, 0.21225829422473907, 0.7330597043037415, 0.3772261440753937]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.8977603912353516, 1.1237272024154663, 0.9594791531562805, 1.1817635297775269, 1.4138411283493042, 0.30473795533180237, 0.840247392654419, 0.9102132320404053, 0.9136198163032532, 1.382968783378601, 0.8689160346984863, 0.8887012004852295, 0.3173063099384308, 0.5249654054641724, -0.0319010429084301, 0.7125651240348816]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.17803364992141724, 0.7860937714576721, 1.127343773841858, 1.0799479484558105, 0.4087044298648834, 0.5776228904724121, 1.0119433403015137, 1.0392447710037231, 0.4609423875808716, 0.5124365091323853, 1.0206217765808105, 0.8257291913032532, 0.759204089641571, 0.8777604103088379, 0.6980989575386047, 1.0158593654632568]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.837597668170929, 0.9442708492279053, 1.14509117603302, 0.4415470361709595, 0.9814843535423279, -0.04981119930744171, 1.0806770324707031, 1.8607292175292969, 0.9417968988418579, 0.9401041865348816, 1.1847655773162842, 1.3282291889190674, 0.14922526478767395, 0.7739322781562805, 1.013281226158142, 0.7305110692977905]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5684179663658142, 0.8038411736488342, 0.8873958587646484, 0.7281249761581421, 1.0589061975479126, 0.47652506828308105, 0.20287109911441803, 0.9547086358070374, 0.6378971338272095, 0.7819271087646484, -0.3907812535762787, 0.6180696487426758, 0.9067773222923279, 1.140781283378601, 1.1240625381469727, 0.35201171040534973]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4070125222206116, 1.308750033378601, 1.0227603912353516, 1.0009765625, 0.8152864575386047, 0.962890625, 1.1615201234817505, 1.0091146230697632, 1.069166660308838, 1.170468807220459, 0.6679297089576721, -0.2988907992839813, 1.2447069883346558, 0.8746614456176758, 0.9750000238418579, 1.216404676437378]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.8159961104393005, 0.16895776987075806, 0.9616943597793579, 0.8645963668823242, 0.7290114164352417, 0.4475862681865692, 0.3112141788005829, 0.8851627707481384, -0.922228991985321, 2.373763084411621, -0.8308044672012329, 0.5031445026397705, 0.9304166436195374, 0.9839192628860474, 0.9318945407867432, 0.2614888548851013]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.04431315138936043, 0.7731510400772095, 1.4738541841506958, 1.3322200775146484, 1.5310156345367432, 1.4800000190734863, 1.2992838621139526, 1.393613338470459, 1.6883902549743652, 1.4868098497390747, 2.1349480152130127, 1.3898470401763916, 1.2956331968307495, 1.2124089002609253, 0.43586426973342896, 1.4391601085662842]
Running loglikelihood requests:  92%|█████████████████████████████████████████████▏   | 465/504 [25:19<02:02,  3.15s/it]Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.522291660308838, 1.213854193687439, 1.5137890577316284, 1.8067187070846558, 0.9589322805404663, 1.2110416889190674, 1.81291663646698, 1.7686458826065063, 2.2853646278381348, 1.40276038646698, 1.171067714691162, 0.8179329633712769, 1.1776301860809326, 1.148707628250122, 1.4239062070846558, 1.4692187309265137]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.5066145658493042, 1.3106249570846558, 1.4391145706176758, 1.343177080154419, 1.654843807220459, 1.0558788776397705, 1.3342708349227905, 1.3403385877609253, 1.7683594226837158, 1.4323698282241821, 1.6374739408493042, 1.5648176670074463, 2.1446614265441895, 1.6208332777023315, 1.5774999856948853, 1.4558333158493042]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0061734914779663, 1.084557294845581, 0.9548437595367432, 0.8070247173309326, -0.028640950098633766, 1.3765103816986084, 1.1554166078567505, 1.3850008249282837, 0.8011653423309326, 1.27190101146698, 1.1933072805404663, 1.059069037437439, 1.16559898853302, 1.3515104055404663, 0.8003450632095337, 0.4897322654724121]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.188124895095825, 2.3359375, 2.1982290744781494, 2.0389063358306885, 2.0184895992279053, 2.284895896911621, 2.003124952316284, 2.276562452316284, 1.9887499809265137, 2.235729217529297, 2.431666612625122, 2.6756250858306885, 1.8092708587646484, 2.4541666507720947, 2.259739637374878, 2.1895833015441895]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.439843773841858, 1.4272395372390747, 1.8550000190734863, 1.0147526264190674, 1.478489637374878, 1.6002082824707031, 1.3561458587646484, 1.3856250047683716, 1.5114582777023315, 1.6744791269302368, 1.5616666078567505, 2.0941667556762695, 1.4890625476837158, 1.590833306312561, 1.2635416984558105, 1.6665104627609253]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.340729236602783, 1.8284895420074463, 1.1149349212646484, 1.3928124904632568, 1.5368750095367432, 1.444322943687439, 1.3881770372390747, 1.3990625143051147, 1.5118489265441895, 1.3584896326065063, 1.4651042222976685, 1.4560155868530273, 1.2553906440734863, 0.7371044754981995, 1.3306770324707031, 1.5281771421432495]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3332290649414062, 2.491041660308838, 2.3298957347869873, 2.1722917556762695, 2.67104172706604, 2.3115625381469727, 2.5007290840148926, 2.143437385559082, 2.1528124809265137, 2.2666666507720947, 2.647083282470703, 2.2119791507720947, 2.342395782470703, 2.4973957538604736, 2.297187566757202, 2.383125066757202]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.666041612625122, 1.7078646421432495, 1.5304166078567505, 1.818385362625122, 1.328932285308838, 1.5670833587646484, 1.6476041078567505, 2.037656307220459, 1.663489580154419, 1.5889583826065063, 1.688072919845581, 1.497656226158142, 1.6931771039962769, 1.4952083826065063, 1.8663021326065063, 1.954531192779541]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.820468783378601, 1.8151921033859253, 1.737708330154419, 1.7209374904632568, 1.6944271326065063, 1.8502799272537231, 1.6956933736801147, 1.7211979627609253, 1.7941471338272095, 1.9602603912353516, 2.2623698711395264, 1.6370443105697632, 1.6947916746139526, 1.9007291793823242, 1.8287402391433716, 1.7953646183013916]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6296939849853516, 1.6771337985992432, 1.630358099937439, 1.7266536951065063, 1.5688021183013916, 1.5814718008041382, 1.7709261178970337, 1.7031445503234863, 1.6124576330184937, 1.68694007396698, 1.3953125476837158, 1.5982812643051147, 1.6872395277023315, 1.8816406726837158, 1.581699252128601, 1.6416927576065063]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [1.9733333587646484, 2.0291666984558105, 2.426406145095825, 1.9165104627609253, 2.0227603912353516, 2.0066146850585938, 2.397031307220459, 2.108593702316284, 1.933750033378601, 1.9332292079925537, 1.9077603816986084, 1.7952604293823242, 1.9256510734558105, 1.7893229722976685, 2.25083327293396, 2.013124942779541]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.8729166984558105, 5.220729351043701, 4.791666507720947, 4.729739665985107, 4.6854166984558105, 4.673437595367432, 4.872395992279053, 5.025989532470703, 5.0883331298828125, 4.832708358764648, 4.797968864440918, 4.698853969573975, 4.943047046661377, 4.892499923706055, 5.024479389190674, 4.744999885559082]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.720468759536743, 3.6328125, 3.546354055404663, 3.3922526836395264, 3.585624933242798, 3.646145820617676, 3.3784115314483643, 3.6033332347869873, 3.6630728244781494, 3.593177080154419, 3.4749999046325684, 3.203176975250244, 3.4904167652130127, 3.609375, 3.6482291221618652, 3.7439844608306885]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.633958339691162, 2.7604687213897705, 2.7370834350585938, 2.56333327293396, 2.633958339691162, 2.7221875190734863, 2.6232292652130127, 2.4462499618530273, 2.621666669845581, 2.5283334255218506, 2.9923958778381348, 2.061009168624878, 2.684687614440918, 2.5813541412353516, 3.0710415840148926, 2.682708263397217]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.060974325984716415, 0.07220957428216934, 0.08366699516773224, -0.23054829239845276, -0.19687825441360474, -0.04906005784869194, 0.10162373632192612, -0.12929850816726685, 0.07197754085063934, 0.06624959409236908, 0.08040791749954224, 0.07326314598321915, 0.07718221098184586, 0.08934488892555237, -0.9587630033493042, 0.09604959934949875]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08105549961328506, 0.049166616052389145, 0.02582804299890995, 0.04294842854142189, 0.06973225623369217, 0.04101257398724556, 0.044824421405792236, 0.07045548409223557, 0.004939625971019268, 0.07415506988763809, -0.1659088134765625, 0.054455794394016266, 0.021999359130859375, -0.01686767488718033, 0.03647923842072487, 0.023267313838005066]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.061470843851566315, 0.056898701936006546, 0.09167928248643875, 0.06578470766544342, 0.08865997195243835, 0.0855095386505127, 0.037764281034469604, -0.07954640686511993, 0.07765187323093414, 0.10612180083990097, 0.024274088442325592, 0.08846087008714676, -0.17032307386398315, 0.01513519324362278, -0.050887856632471085, 0.11874186247587204]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.11655791103839874, 0.14999958872795105, 0.13461263477802277, 0.13450297713279724, 0.1132182851433754, 0.12859049439430237, 0.01987304724752903, 0.17397542297840118, 0.1700236052274704, -0.47422850131988525, 0.07902343571186066, 0.08095499873161316, 0.20043131709098816, -0.2778027355670929, 0.023059895262122154, -0.034493815153837204]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06847544014453888, 0.10040415078401566, 0.10241373628377914, 0.13601325452327728, 0.09950866550207138, -0.06086263060569763, 0.005619710311293602, 0.024176431819796562, -0.09829773008823395, 0.026826579123735428, -0.15386880934238434, 0.13158513605594635, -0.02816670760512352, -0.18096496164798737, 0.12508463859558105, -0.05424397811293602]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.029766641557216644, 0.14515502750873566, 0.07237477600574493, 0.10580240935087204, -0.10773356258869171, -0.0583699531853199, 0.22852051258087158, 0.009052734822034836, 0.12649331986904144, -0.06335082650184631, 0.025498047471046448, 0.10166330635547638, -0.12855204939842224, 0.16383707523345947, 0.1215260848402977, 0.09027150273323059]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.12481231987476349, -0.005712483543902636, 0.16751545667648315, 0.3774869740009308, 0.17594808340072632, 0.12647013366222382, -0.1601942926645279, -0.09005452692508698, 0.36818888783454895, 0.20080728828907013, -0.48026734590530396, 0.37085020542144775, 0.2465071678161621, -0.2838362753391266, 0.22160115838050842, 0.17354878783226013]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3468945324420929, 0.18915466964244843, 0.20705647766590118, 0.1332906037569046, -0.7722200751304626, 0.38720378279685974, 0.2336551994085312, -0.3668261766433716, 0.4682011008262634, 0.18047526478767395, -0.1752384454011917, 0.2408982366323471, 0.19958007335662842, 0.28612080216407776, -0.511371374130249, -0.10483887046575546]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2174353003501892, 0.2779793441295624, -0.3783658742904663, 0.3252897262573242, 0.570239245891571, 0.3523111939430237, 0.23178791999816895, -0.25393229722976685, 0.21113525331020355, 0.6294506788253784, 0.16319356858730316, 0.3602229952812195, 0.5760090947151184, -0.4767610728740692, -0.5380680561065674, 0.32542684674263]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3875984847545624, 0.16308918595314026, 0.4513077735900879, 0.946764349937439, 0.3614949584007263, 0.17958007752895355, 0.5173486471176147, 0.6249804496765137, 0.56229168176651, 0.2148616462945938, 0.18241454660892487, 0.6645312309265137, 0.9102286696434021, -0.035075683146715164, 0.7207421660423279, 0.728528618812561]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.074720025062561, 0.9042773246765137, 0.5580012798309326, 0.723229169845581, 0.7677018046379089, 0.5773372650146484, 0.6410351395606995, 0.750963568687439, 0.01784667931497097, 0.40031901001930237, 0.7192317843437195, 0.8531315326690674, 0.6107422113418579, 0.18459798395633698, 0.8134350776672363, 0.2916274964809418]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.8665755391120911, 1.082838535308838, 0.9497656226158142, 1.1404036283493042, 1.4083235263824463, 0.20500507950782776, 0.8324869871139526, 0.8505468964576721, 0.9131901264190674, 1.4106054306030273, 0.866686224937439, 0.8195247650146484, 0.20553548634052277, 0.426449179649353, -0.136962890625, 0.6090624928474426]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [-0.0050325519405305386, 0.7227083444595337, 1.1627213954925537, 1.0250520706176758, 0.3167968690395355, 0.5304296612739563, 0.9924853444099426, 1.0145963430404663, 0.3725297152996063, 0.4462772607803345, 1.0576106309890747, 0.8316406011581421, 0.6712174415588379, 0.8500000238418579, 0.7342708110809326, 0.9877604246139526]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7312239408493042, 0.911145806312561, 1.1568424701690674, 0.2977831959724426, 0.9821093678474426, -0.10682617127895355, 1.0788542032241821, 1.8605729341506958, 0.9132291674613953, 0.8908593654632568, 1.2082682847976685, 1.3475260734558105, 0.12895019352436066, 0.6796207427978516, 0.9369010329246521, 0.6484602689743042]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6279980540275574, 0.7993879914283752, 0.9033593535423279, 0.7565885186195374, 1.0860155820846558, 0.6292691826820374, 0.19068196415901184, 1.0297428369522095, 0.6773242354393005, 0.78445965051651, -0.47657185792922974, 0.6060937643051147, 0.9586360454559326, 1.19880211353302, 1.1748567819595337, 0.3099682629108429]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_15 - Captured router_logits: [0.3178430199623108, 1.3182291984558105, 1.029843807220459, 0.9678124785423279, 0.7377669215202332, 0.9295833110809326, 1.2147916555404663, 0.9818750023841858, 1.0486979484558105, 1.1249479055404663, 0.6054427027702332, -0.3221012353897095, 1.2368261814117432, 0.8510937690734863, 0.9436979293823242, 1.1660798788070679]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.7304785251617432, 0.10610107332468033, 0.9294351935386658, 0.8666666746139526, 0.6855127215385437, 0.3615063428878784, 0.2894270718097687, 0.9189127683639526, -1.0428824424743652, 2.445286512374878, -1.0231575965881348, 0.48177653551101685, 0.873548150062561, 0.9624609351158142, 0.9200000166893005, 0.18764647841453552]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [-0.04374999925494194, 0.7343554496765137, 1.4176563024520874, 1.2305176258087158, 1.4806119203567505, 1.449609398841858, 1.2541797161102295, 1.3171875476837158, 1.6740299463272095, 1.3718619346618652, 2.1800520420074463, 1.3499023914337158, 1.2243343591690063, 1.2213802337646484, 0.31676921248435974, 1.3912370204925537]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.560520887374878, 1.1945832967758179, 1.5295898914337158, 1.884739637374878, 0.9536588788032532, 1.2009896039962769, 1.8157812356948853, 1.74177086353302, 2.418541669845581, 1.4539583921432495, 1.168138027191162, 0.7873030304908752, 1.1688541173934937, 1.182508111000061, 1.4260936975479126, 1.4893749952316284]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.4293229579925537, 1.2776042222976685, 1.4489322900772095, 1.3061197996139526, 1.6242707967758179, 1.0076041221618652, 1.3155728578567505, 1.2692968845367432, 1.7585417032241821, 1.4113280773162842, 1.6476823091506958, 1.49455726146698, 2.1957812309265137, 1.5770833492279053, 1.5143228769302368, 1.4389322996139526]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.069882869720459, 1.0889973640441895, 0.9459896087646484, 0.832080066204071, -0.032164715230464935, 1.4273698329925537, 1.142031192779541, 1.3942773342132568, 0.7967447638511658, 1.2831250429153442, 1.2754687070846558, 1.0521419048309326, 1.1651041507720947, 1.3981250524520874, 0.8588183522224426, 0.4901863634586334]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.146510362625122, 2.305312395095825, 2.200000047683716, 2.028958320617676, 1.95354163646698, 2.2133333683013916, 1.9405728578567505, 2.2379167079925537, 1.973645806312561, 2.17885422706604, 2.3840625286102295, 2.636666774749756, 1.7767187356948853, 2.4393749237060547, 2.2169792652130127, 2.1948957443237305]
Running loglikelihood requests:  93%|█████████████████████████████████████████████▌   | 469/504 [25:31<01:50,  3.15s/it]Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.3879687786102295, 1.4039583206176758, 1.9550000429153442, 1.0296354293823242, 1.4102083444595337, 1.6079167127609253, 1.3091015815734863, 1.3796354532241821, 1.514635443687439, 1.679114580154419, 1.5519791841506958, 2.103854179382324, 1.4718228578567505, 1.6010416746139526, 1.243190050125122, 1.6770833730697632]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4091665744781494, 1.8522396087646484, 1.106848955154419, 1.404687523841858, 1.6175520420074463, 1.4705208539962769, 1.4264583587646484, 1.3779687881469727, 1.5290625095367432, 1.3928645849227905, 1.4827083349227905, 1.5197917222976685, 1.240781307220459, 0.6873877048492432, 1.3515104055404663, 1.5047396421432495]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.248020887374878, 2.4081249237060547, 2.2830207347869873, 2.0752084255218506, 2.612499952316284, 2.2180209159851074, 2.4473958015441895, 2.08510422706604, 2.0638020038604736, 2.1591665744781494, 2.6455209255218506, 2.1237499713897705, 2.3114583492279053, 2.435312509536743, 2.233020782470703, 2.2691667079925537]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6757291555404663, 1.7135416269302368, 1.546302080154419, 1.8177083730697632, 1.3038541078567505, 1.592604160308838, 1.6503125429153442, 2.0442707538604736, 1.6631771326065063, 1.5733853578567505, 1.700026035308838, 1.518958330154419, 1.706874966621399, 1.5017708539962769, 1.8697395324707031, 1.9180208444595337]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8061457872390747, 1.8093944787979126, 1.7224478721618652, 1.7096353769302368, 1.6888281106948853, 1.8426334857940674, 1.7232877016067505, 1.7129297256469727, 1.7760156393051147, 1.945156216621399, 2.2916276454925537, 1.6394661664962769, 1.693385362625122, 1.913906216621399, 1.83161461353302, 1.7893961668014526]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6220247745513916, 1.6444311141967773, 1.6216862201690674, 1.7192057371139526, 1.557630181312561, 1.5662174224853516, 1.7591561079025269, 1.7110481262207031, 1.5965397357940674, 1.6735286712646484, 1.3546874523162842, 1.6016731262207031, 1.678554654121399, 1.9020832777023315, 1.578515648841858, 1.6293554306030273]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0020833015441895, 2.0615625381469727, 2.469895839691162, 1.9318749904632568, 2.052968740463257, 2.0232813358306885, 2.4272916316986084, 2.1254167556762695, 1.9562499523162842, 1.9548958539962769, 1.9682812690734863, 1.7884374856948853, 1.9216145277023315, 1.804114580154419, 2.296875, 2.007239580154419]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [4.901771068572998, 5.307499885559082, 4.8572916984558105, 4.7265625, 4.716562271118164, 4.751874923706055, 4.906041622161865, 5.064843654632568, 5.126562595367432, 4.872135639190674, 4.8218231201171875, 4.744375228881836, 5.009218692779541, 4.960833549499512, 5.108020782470703, 4.804583549499512]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.747812509536743, 3.6736457347869873, 3.5872395038604736, 3.427760362625122, 3.6357290744781494, 3.716458320617676, 3.393749952316284, 3.654895782470703, 3.700312614440918, 3.595416784286499, 3.487187385559082, 3.1605467796325684, 3.472135305404663, 3.6566667556762695, 3.669583320617676, 3.7607812881469727]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.5609374046325684, 2.6600520610809326, 2.6701042652130127, 2.514479160308838, 2.575937509536743, 2.6599478721618652, 2.5877082347869873, 2.3355207443237305, 2.559166669845581, 2.4432291984558105, 2.9895832538604736, 2.0109505653381348, 2.5928125381469727, 2.49692702293396, 3.0426042079925537, 2.6031250953674316]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.056387823075056076, 0.06841618567705154, 0.08062908798456192, -0.230661541223526, -0.21605785191059113, -0.08353856950998306, 0.09233979880809784, -0.03393389657139778, 0.06661245226860046, 0.05330219864845276, 0.061440546065568924, 0.056211523711681366, 0.07106904685497284, 0.0855337604880333, -0.8991105556488037, 0.09455583244562149]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.08476834744215012, 0.04325949028134346, 0.03420979157090187, 0.032491374760866165, 0.07070066779851913, 0.030433887615799904, 0.051141273230314255, 0.08267521113157272, 0.04509827867150307, 0.059426795691251755, -0.18604257702827454, 0.05287546664476395, 0.024540256708860397, -0.037627555429935455, 0.027426229789853096, 0.039806004613637924]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07876535505056381, 0.03825700655579567, 0.08875728398561478, 0.09335100650787354, 0.12219748646020889, 0.08382601290941238, 0.06550680845975876, -0.0816170945763588, 0.08247746527194977, 0.10229244828224182, 0.029228828847408295, 0.08619112521409988, -0.1892559975385666, 0.01871645078063011, -0.006555505562573671, 0.11268739402294159]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.14117431640625, 0.13965091109275818, 0.1223907470703125, 0.14448589086532593, 0.13536524772644043, 0.14019609987735748, 0.017283672466874123, 0.18946920335292816, 0.1751888394355774, -0.46745094656944275, 0.014690295793116093, 0.09149134159088135, 0.2726758122444153, -0.26590049266815186, 0.027733881026506424, -0.05804494768381119]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.05700209364295006, 0.09475141018629074, 0.10568814724683762, 0.16851311922073364, 0.04803631827235222, -0.034971289336681366, 0.022778691723942757, 0.029682261869311333, -0.09480492025613785, 0.03700380027294159, -0.21557122468948364, 0.12939824163913727, 0.03572453558444977, -0.20005731284618378, 0.1264813393354416, -0.058074951171875]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.028533935546875, 0.155008465051651, 0.02394299954175949, 0.11511560529470444, -0.09486348181962967, -0.06089246645569801, 0.20361410081386566, 0.026632361114025116, 0.1630193293094635, -0.056906212121248245, 0.15439069271087646, 0.08857015520334244, -0.19743883609771729, 0.14470775425434113, 0.1401853859424591, 0.08038587868213654]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.1045614704489708, -0.04090448468923569, 0.11353446543216705, 0.3788617253303528, 0.1520427018404007, 0.1407858431339264, -0.17461910843849182, -0.06672709435224533, 0.35465383529663086, 0.15813177824020386, -0.5083287954330444, 0.3843095004558563, 0.22871357202529907, -0.2807926535606384, 0.24683421850204468, 0.21100203692913055]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.4253595769405365, 0.20805029571056366, 0.21338056027889252, 0.18339373171329498, -0.7300002574920654, 0.3547932505607605, 0.18855738639831543, -0.31538763642311096, 0.5423637628555298, 0.16790771484375, -0.1830221712589264, 0.2234525978565216, 0.2932656705379486, 0.21620693802833557, -0.5886428356170654, -0.12613822519779205]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2537066340446472, 0.23250311613082886, -0.288318932056427, 0.31389227509498596, 0.5514872670173645, 0.2990706264972687, 0.21780642867088318, -0.2800194025039673, 0.17684565484523773, 0.658608078956604, 0.10637685656547546, 0.3973607122898102, 0.4514622092247009, -0.5070281028747559, -0.5449944734573364, 0.35115545988082886]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.34905800223350525, 0.20746015012264252, 0.522255539894104, 0.9257020950317383, 0.3158610463142395, 0.16214938461780548, 0.4652627408504486, 0.5454827547073364, 0.5839942693710327, 0.19739346206188202, 0.22626268863677979, 0.6757944226264954, 0.9460350275039673, 0.07137484103441238, 0.7067540884017944, 0.6162505149841309]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1173276901245117, 0.8855904340744019, 0.5224609375, 0.7311154007911682, 0.7005582451820374, 0.6312299370765686, 0.5891904830932617, 0.7219831943511963, 0.1046898290514946, 0.5409892201423645, 0.7298418879508972, 0.9104267954826355, 0.6526967883110046, 0.19092746078968048, 0.7233639359474182, 0.3583621382713318]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9907094836235046, 1.2396537065505981, 0.8455183506011963, 1.1931350231170654, 1.4791226387023926, 0.27759119868278503, 0.9554608464241028, 0.9472705721855164, 0.9669420123100281, 1.4776710271835327, 0.8888236284255981, 0.8508762717247009, 0.3542991876602173, 0.5329231023788452, -0.001577016431838274, 0.7029798626899719]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.07119668275117874, 0.7231313586235046, 1.1631783246994019, 1.112410306930542, 0.395263671875, 0.5470927357673645, 1.0749149322509766, 1.0955381393432617, 0.34426507353782654, 0.3914893865585327, 0.8320147395133972, 0.8221072554588318, 0.8057432174682617, 0.8415065407752991, 0.6716943979263306, 0.8547033071517944]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7055004239082336, 0.8839210271835327, 1.0125765800476074, 0.2473573386669159, 0.9779085516929626, -0.15918463468551636, 1.0438661575317383, 1.8459670543670654, 1.0133023262023926, 0.8425359129905701, 1.2107923030853271, 1.2273807525634766, 0.07200004160404205, 0.7195245027542114, 1.1792124509811401, 0.5604429244995117]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6358774304389954, 0.7380040884017944, 0.9106313586235046, 0.6919209361076355, 0.9686048626899719, 0.5645487904548645, 0.3226079046726227, 1.0678942203521729, 0.5570431351661682, 0.7150219082832336, -0.39625343680381775, 0.5606887340545654, 0.8651997447013855, 0.9814321398735046, 1.0963894128799438, 0.2899763882160187]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.2715173661708832, 1.3143211603164673, 0.9739495515823364, 0.9538244009017944, 0.7313793301582336, 0.8741025924682617, 1.1792371273040771, 0.9996041059494019, 1.0051203966140747, 0.9890202879905701, 0.6285301446914673, -0.3677392899990082, 0.9949522018432617, 0.7972181439399719, 0.9312447309494019, 1.2340500354766846]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.6861374378204346, 0.13573063910007477, 0.7827313542366028, 0.8097979426383972, 0.7050912976264954, 0.3917995095252991, 0.33749306201934814, 0.8948199152946472, -1.0500199794769287, 2.410789728164673, -0.9767569303512573, 0.4546261429786682, 0.8172376751899719, 0.9214857220649719, 0.8085211515426636, 0.2590859830379486]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.08583976328372955, 0.7598027586936951, 1.4444811344146729, 1.3779362440109253, 1.4200538396835327, 1.476668119430542, 1.4176751375198364, 1.3271023035049438, 1.7999745607376099, 1.4604096412658691, 2.2503695487976074, 1.5243052244186401, 1.3278025388717651, 1.2947107553482056, 0.4458535611629486, 1.4315614700317383]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4768264293670654, 1.2647804021835327, 1.5304582118988037, 1.8469699621200562, 0.9774730801582336, 1.2006440162658691, 1.9930320978164673, 1.7569679021835327, 2.4382917881011963, 1.4697529077529907, 1.1637722253799438, 0.8311371803283691, 1.2236195802688599, 1.1738561391830444, 1.4957242012023926, 1.5232263803482056]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3953758478164673, 1.2484164237976074, 1.4650285243988037, 1.2477566003799438, 1.5418074131011963, 0.952339768409729, 1.2607685327529907, 1.2266812324523926, 1.7678157091140747, 1.3444889783859253, 1.6116976737976074, 1.4510135650634766, 2.096574068069458, 1.4724714756011963, 1.4283678531646729, 1.428605318069458]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0845088958740234, 0.9161211848258972, 0.864310622215271, 0.8102689385414124, 0.014087986201047897, 1.1828943490982056, 1.0541859865188599, 1.3679685592651367, 0.6434170603752136, 1.1501795053482056, 1.2099609375, 0.9944573640823364, 0.9748997092247009, 1.3055849075317383, 0.7678800225257874, 0.3321995139122009]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.086570978164673, 2.304370880126953, 2.214315891265869, 2.0302999019622803, 1.9347550868988037, 2.1549830436706543, 1.9816300868988037, 2.2013301849365234, 1.9790434837341309, 2.140519380569458, 2.3848183155059814, 2.7130489349365234, 1.7416596412658691, 2.410578489303589, 2.1656460762023926, 2.160472869873047]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4487965106964111, 1.4070945978164673, 1.98828125, 1.0859375, 1.4865392446517944, 1.587943434715271, 1.3023384809494019, 1.398490309715271, 1.4967272281646729, 1.712785005569458, 1.6307010650634766, 2.1542441844940186, 1.5036423206329346, 1.5955448150634766, 1.2498416900634766, 1.6871832609176636]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.393105983734131, 1.8444889783859253, 1.0441300868988037, 1.418021559715271, 1.5211676359176636, 1.3935283422470093, 1.3619087934494019, 1.3252216577529907, 1.4193412065505981, 1.3597972393035889, 1.4399281740188599, 1.4557116031646729, 1.1483055353164673, 0.6141843795776367, 1.2530616521835327, 1.3970650434494019]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.351034641265869, 2.3973817825317383, 2.3807010650634766, 2.0829813480377197, 2.6739864349365234, 2.2281460762023926, 2.4399282932281494, 2.1199324131011963, 2.1188766956329346, 2.2131545543670654, 2.7906460762023926, 2.1085305213928223, 2.2903294563293457, 2.4464738368988037, 2.268897771835327, 2.2882180213928223]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.667863130569458, 1.7364864349365234, 1.5525232553482056, 1.8254855871200562, 1.3268053531646729, 1.6260029077529907, 1.6695523262023926, 2.0872044563293457, 1.6790012121200562, 1.5952808856964111, 1.6321790218353271, 1.5056482553482056, 1.6674408912658691, 1.515677809715271, 1.9993137121200562, 2.031461238861084]
Running loglikelihood requests:  94%|█████████████████████████████████████████████▉   | 473/504 [25:44<01:36,  3.12s/it]Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8387880325317383, 1.9179489612579346, 1.7297297716140747, 1.7231841087341309, 1.7173775434494019, 1.8395335674285889, 1.7411713600158691, 1.7513196468353271, 1.779837965965271, 1.9415645599365234, 2.404059410095215, 1.6670185327529907, 1.7065033912658691, 1.9272593259811401, 1.8687314987182617, 1.7731868028640747]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6248152256011963, 1.724662184715271, 1.6456397771835327, 1.7428209781646729, 1.5985536575317383, 1.5887880325317383, 1.8387880325317383, 1.7347180843353271, 1.6322318315505981, 1.7152396440505981, 1.3895164728164673, 1.6368770599365234, 1.6946790218353271, 2.031038761138916, 1.6613175868988037, 1.6510241031646729]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.084406614303589, 2.171769380569458, 2.5485641956329346, 2.061866521835327, 2.1435811519622803, 2.0977089405059814, 2.576277494430542, 2.2040750980377197, 2.0598607063293457, 2.0412795543670654, 2.0338892936706543, 1.8732051849365234, 2.0040383338928223, 1.9069889783859253, 2.3703019618988037, 2.039273738861084]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.0641889572143555, 5.563133239746094, 5.067145347595215, 4.9315876960754395, 4.970016956329346, 4.914273738861084, 5.065033912658691, 5.313450336456299, 5.327069282531738, 5.032833576202393, 5.011507511138916, 4.951330184936523, 5.14210319519043, 5.159839630126953, 5.245460510253906, 5.05320930480957]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.8856630325317383, 3.680109739303589, 3.581820011138916, 3.4775125980377197, 3.7011191844940186, 3.7434544563293457, 3.5057010650634766, 3.7913851737976074, 3.8878800868988037, 3.7421875, 3.5733742713928223, 3.315772771835327, 3.601140260696411, 3.7117819786071777, 3.730574369430542, 3.7955024242401123]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.6221494674682617, 2.7640414237976074, 2.7058699131011963, 2.5806586742401123, 2.578864097595215, 2.679898738861084, 2.6163430213928223, 2.435335636138916, 2.6187710762023926, 2.496304988861084, 3.202913761138916, 2.0775444507598877, 2.710198402404785, 2.5911107063293457, 3.218961238861084, 2.6655404567718506]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.060610875487327576, 0.07627900689840317, 0.08577728271484375, -0.2262418568134308, -0.2387830913066864, -0.05378182604908943, 0.10157383978366852, -0.11723624169826508, 0.070648193359375, 0.060146648436784744, 0.07622739672660828, 0.07175692170858383, 0.07459226995706558, 0.09110239148139954, -0.9705403447151184, 0.09592989087104797]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0769415944814682, 0.035449981689453125, 0.029967837035655975, 0.04468737542629242, 0.0700547993183136, 0.030701637268066406, 0.039679210633039474, 0.06378020346164703, 0.005257633049041033, 0.07454405725002289, -0.18212805688381195, 0.05567121505737305, 0.030122756958007812, -0.014777395874261856, 0.03649335354566574, 0.02108907699584961]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06448131054639816, 0.0549505278468132, 0.08065255731344223, 0.07625452429056168, 0.10335370898246765, 0.0793381780385971, 0.0344407819211483, -0.0781114399433136, 0.0878363698720932, 0.08813868463039398, 0.03650204464793205, 0.08509111404418945, -0.1983625590801239, 0.03631292283535004, -0.0469648577272892, 0.12823444604873657]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.12422137707471848, 0.14369794726371765, 0.1436225026845932, 0.12618933618068695, 0.1175130233168602, 0.10910669714212418, -0.0036438836250454187, 0.17610174417495728, 0.17175203561782837, -0.4546152651309967, 0.07278527319431305, 0.0821770578622818, 0.19949595630168915, -0.2746853232383728, 0.0183275006711483, -0.02438863180577755]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08242961764335632, 0.10424099862575531, 0.11580763757228851, 0.1420305073261261, 0.09032884985208511, -0.06693649291992188, 0.0114529924467206, 0.02568933740258217, -0.08027638494968414, 0.06396102905273438, -0.18407398462295532, 0.1232859268784523, -0.04242171719670296, -0.17852497100830078, 0.12007395178079605, -0.04281022772192955]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.021811166778206825, 0.13478538393974304, 0.05289268493652344, 0.11226102709770203, -0.0823805034160614, -0.05466291680932045, 0.22585763037204742, 0.016670068725943565, 0.10684079676866531, -0.0500895194709301, 0.16441811621189117, 0.08553695678710938, -0.1697913259267807, 0.1509348601102829, 0.10134061425924301, 0.09024545550346375]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.12519676983356476, -0.0382588692009449, 0.16983965039253235, 0.39011213183403015, 0.19094562530517578, 0.12389744818210602, -0.153829887509346, -0.0829010009765625, 0.33768320083618164, 0.1948581337928772, -0.5178900957107544, 0.3754029870033264, 0.2348666787147522, -0.30835554003715515, 0.1934339702129364, 0.17102983593940735]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3377412259578705, 0.1992388814687729, 0.20230001211166382, 0.1441379189491272, -0.7448662519454956, 0.38930341601371765, 0.2289208322763443, -0.2825792133808136, 0.4332271218299866, 0.18736182153224945, -0.20614369213581085, 0.24365870654582977, 0.2274119108915329, 0.2595558166503906, -0.5687488913536072, -0.07937982678413391]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.229287251830101, 0.2778896689414978, -0.3609992265701294, 0.2979939877986908, 0.5545417070388794, 0.3424564003944397, 0.22823207080364227, -0.2997758686542511, 0.1893886923789978, 0.6324030756950378, 0.15008153021335602, 0.4095340371131897, 0.5240952968597412, -0.5278761386871338, -0.5169627070426941, 0.31528452038764954]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3646816611289978, 0.1522267609834671, 0.4961887001991272, 0.9048393964767456, 0.35158538818359375, 0.20889155566692352, 0.5129767656326294, 0.5928310751914978, 0.4897613525390625, 0.2025129497051239, 0.17257732152938843, 0.6743977665901184, 0.8926069736480713, -0.0170474573969841, 0.7111680507659912, 0.66357421875]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0648820400238037, 0.8756035566329956, 0.5112168788909912, 0.6798231601715088, 0.7428147792816162, 0.5818854570388794, 0.6317511796951294, 0.69384765625, 0.01224433071911335, 0.3644239604473114, 0.6749742031097412, 0.8281046748161316, 0.5971611738204956, 0.15054766833782196, 0.6746385097503662, 0.2752394676208496]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9106716513633728, 1.1364203691482544, 0.9083116054534912, 1.1663784980773926, 1.4418904781341553, 0.3350931704044342, 0.8951958417892456, 0.8696882724761963, 0.9083930253982544, 1.4319390058517456, 0.8522474765777588, 0.8138868808746338, 0.2826572060585022, 0.47753143310546875, -0.0914747416973114, 0.6639404296875]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.031944699585437775, 0.7597385048866272, 1.1289604902267456, 1.013427734375, 0.33965471386909485, 0.4892917275428772, 1.053092122077942, 1.0523885488510132, 0.4021492004394531, 0.5030966997146606, 1.0404052734375, 0.8828667402267456, 0.758056640625, 0.7848036289215088, 0.7763943076133728, 1.0020887851715088]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7706434726715088, 0.9262966513633728, 1.0783963203430176, 0.33319389820098877, 0.9153645634651184, -0.1140730082988739, 1.0798135995864868, 1.7707791328430176, 0.9379340410232544, 0.8638508915901184, 1.2331000566482544, 1.2716200351715088, 0.0855645090341568, 0.7259745001792908, 0.9309624433517456, 0.6032443642616272]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.616032063961029, 0.7775065302848816, 0.8729926347732544, 0.7031521201133728, 1.0464409589767456, 0.4912787675857544, 0.2081502228975296, 1.0884568691253662, 0.6219041347503662, 0.7911105751991272, -0.4450395405292511, 0.6249762773513794, 0.9356486201286316, 1.1455349922180176, 1.1143527030944824, 0.2857123613357544]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_15 - Captured router_logits: [0.3072645366191864, 1.3294541835784912, 1.0179849863052368, 0.9332817792892456, 0.7429097294807434, 0.9094509482383728, 1.1665785312652588, 0.9784613847732544, 1.0132378339767456, 1.0692274570465088, 0.6383621692657471, -0.3330654501914978, 1.1445854902267456, 0.839111328125, 0.9349501132965088, 1.216806173324585]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.7530381679534912, 0.08498425036668777, 0.9273783564567566, 0.8921135663986206, 0.7077094316482544, 0.4171176552772522, 0.34369829297065735, 0.9450751543045044, -1.0553029775619507, 2.4606120586395264, -0.9718848466873169, 0.5286526083946228, 0.8577880859375, 0.9869113564491272, 0.9432644248008728, 0.2480572611093521]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.0018903943710029125, 0.7352176308631897, 1.40966796875, 1.2728915214538574, 1.4128960371017456, 1.4757215976715088, 1.2683783769607544, 1.305419921875, 1.6735703945159912, 1.4419962167739868, 2.232042074203491, 1.397421956062317, 1.290557622909546, 1.2530245780944824, 0.3499060869216919, 1.3955374956130981]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4955512285232544, 1.2310655117034912, 1.5000271797180176, 1.8448892831802368, 0.9668511152267456, 1.1356879472732544, 1.8790147304534912, 1.7311197519302368, 2.3583984375, 1.4227430820465088, 1.1453450918197632, 0.8395758867263794, 1.1778836250305176, 1.2009481191635132, 1.4265950918197632, 1.5247938632965088]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3775498867034912, 1.2083333730697632, 1.3697645664215088, 1.2280545234680176, 1.5552300214767456, 0.9447360634803772, 1.2577853202819824, 1.1944037675857544, 1.7354600429534912, 1.3581814765930176, 1.5745443105697632, 1.4557833671569824, 2.1843533515930176, 1.4923502206802368, 1.4312065839767456, 1.3750814199447632]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0409647226333618, 1.0016275644302368, 0.9251844882965088, 0.8190036416053772, 0.08630328625440598, 1.3755425214767456, 1.1146918535232544, 1.3976783752441406, 0.7166938781738281, 1.2527127265930176, 1.2880316972732544, 0.9959038496017456, 1.1441514492034912, 1.3712564706802368, 0.89947509765625, 0.4840749204158783]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.1399738788604736, 2.297308921813965, 2.1985676288604736, 2.0065646171569824, 1.9716254472732544, 2.171006917953491, 1.9767794609069824, 2.246202230453491, 1.9921875, 2.213433265686035, 2.369032144546509, 2.6667752265930176, 1.8339301347732544, 2.454535484313965, 2.2248263359069824, 2.191948890686035]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.39794921875, 1.3146700859069824, 1.9267578125, 1.04052734375, 1.4044053554534912, 1.5493706464767456, 1.2944878339767456, 1.3213975429534912, 1.4537760019302368, 1.6793076992034912, 1.5196397304534912, 2.050347328186035, 1.4401583671569824, 1.5614149570465088, 1.2362196445465088, 1.6271700859069824]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.351508140563965, 1.7727322578430176, 1.0242241621017456, 1.3727213144302368, 1.5837674140930176, 1.4251844882965088, 1.3515625, 1.2924262285232544, 1.4549696445465088, 1.3004556894302368, 1.4161241054534912, 1.4244791269302368, 1.1623263359069824, 0.6690995693206787, 1.2764214277267456, 1.4401041269302368]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.234808921813965, 2.4093966484069824, 2.293836832046509, 2.083875894546509, 2.6404080390930176, 2.1839191913604736, 2.4622395038604736, 2.072374105453491, 2.0621745586395264, 2.1857638359069824, 2.690863609313965, 2.126519203186035, 2.3186848163604736, 2.4404296875, 2.2458767890930176, 2.2702908515930176]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6515841484069824, 1.7126736640930176, 1.5474175214767456, 1.8396267890930176, 1.3435872793197632, 1.5803494453430176, 1.6612412929534912, 2.026801109313965, 1.6856011152267456, 1.6002603769302368, 1.6440972089767456, 1.5132920742034912, 1.6740450859069824, 1.48974609375, 1.9159071445465088, 1.9568142890930176]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8203125, 1.8643171787261963, 1.7928602695465088, 1.7517361640930176, 1.6912435293197632, 1.8403489589691162, 1.7502847909927368, 1.7483724355697632, 1.7913072109222412, 1.9870877265930176, 2.3086750507354736, 1.6769477128982544, 1.7277017831802368, 1.9436849355697632, 1.8597140312194824, 1.7966935634613037]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6401638984680176, 1.707763671875, 1.6817220449447632, 1.7502169609069824, 1.58056640625, 1.6068793535232544, 1.8082139492034912, 1.7472330331802368, 1.6395127773284912, 1.717529296875, 1.3893771171569824, 1.6505533456802368, 1.7160372734069824, 1.9337022304534912, 1.629638671875, 1.69384765625]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.039659261703491, 2.113823890686035, 2.5003254413604736, 1.9876302480697632, 2.0994465351104736, 2.052354574203491, 2.4508464336395264, 2.1598849296569824, 1.9929471015930176, 1.9750434160232544, 2.0003254413604736, 1.8243272304534912, 1.9370388984680176, 1.8655599355697632, 2.30810546875, 2.0305988788604736]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  95%|██████████████████████████████████████████████▍  | 477/504 [25:56<01:25,  3.15s/it]Layer: gate_29 - Captured router_logits: [4.943142414093018, 5.386393070220947, 4.8779296875, 4.752007484436035, 4.770290851593018, 4.819661617279053, 4.933810710906982, 5.127604007720947, 5.1640625, 4.905056476593018, 4.83213996887207, 4.783203125, 5.002604007720947, 4.959418296813965, 5.112955570220947, 4.864040851593018]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.891710042953491, 3.756944417953491, 3.6482205390930176, 3.576280355453491, 3.751953125, 3.7906901836395264, 3.5159504413604736, 3.7956814765930176, 3.827256917953491, 3.716905355453491, 3.6189236640930176, 3.325575113296509, 3.5709636211395264, 3.7652995586395264, 3.765516519546509, 3.960829019546509]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.5393338203430176, 2.698621988296509, 2.6083984375, 2.447808265686035, 2.485731363296509, 2.6328125, 2.5123698711395264, 2.334716796875, 2.5281574726104736, 2.370361328125, 2.9978299140930176, 2.021690845489502, 2.562635660171509, 2.468316078186035, 3.016058921813965, 2.518174886703491]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.057513706386089325, 0.0738024115562439, 0.07477236539125443, -0.22665943205356598, -0.19020789861679077, -0.09081687033176422, 0.1041230782866478, -0.08670473843812943, 0.07535520941019058, 0.05825193226337433, 0.06683177500963211, 0.06496225297451019, 0.07394645363092422, 0.08341270685195923, -0.9275332093238831, 0.09530940651893616]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08549778908491135, 0.0431852750480175, 0.035482488572597504, 0.03893172740936279, 0.07069848477840424, 0.024203555658459663, 0.04398469626903534, 0.05727289617061615, 0.01357430499047041, 0.06717499345541, -0.21098993718624115, 0.039524346590042114, 0.04943310469388962, -0.04284794256091118, 0.021064328029751778, 0.009411073289811611]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07297032326459885, 0.06043267622590065, 0.08995571732521057, 0.09903168678283691, 0.09135351330041885, 0.0976334661245346, 0.07548931241035461, -0.07929713279008865, 0.08922695368528366, 0.0540151447057724, 0.02821371518075466, 0.08193604648113251, -0.17104972898960114, 0.027106205001473427, -0.011507276445627213, 0.11000146716833115]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13129350543022156, 0.15670561790466309, 0.12294285744428635, 0.16300275921821594, 0.11814214289188385, 0.14998529851436615, 0.045418109744787216, 0.21691465377807617, 0.17604795098304749, -0.46858838200569153, 0.014667833223938942, 0.10064482688903809, 0.20883221924304962, -0.18076033890247345, -0.05212380737066269, -0.04331433027982712]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07891792058944702, 0.11187078058719635, 0.12767598032951355, 0.06416299194097519, 0.054332248866558075, -0.046050865203142166, 0.04948382452130318, 0.02291375771164894, -0.00680971797555685, 0.05896912142634392, -0.14530406892299652, 0.13775205612182617, -0.06897660344839096, -0.19031259417533875, 0.10497155040502548, 0.00537109375]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.04248906672000885, 0.1366695612668991, 0.03534800559282303, 0.04144115000963211, -0.06430741399526596, 0.002811915474012494, 0.21076223254203796, 0.07227078080177307, 0.08597435802221298, -0.08545609563589096, 0.21125556528568268, 0.06006568670272827, -0.1843605637550354, 0.10351530462503433, 0.180511474609375, 0.05103892832994461]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.06057438254356384, 0.061651524156332016, 0.11122152954339981, 0.40568608045578003, 0.14755034446716309, 0.09633840620517731, -0.10390999168157578, -0.04990698769688606, 0.42545199394226074, 0.13889279961585999, -0.5162972211837769, 0.41303780674934387, 0.16537605226039886, -0.2189103215932846, 0.18294450640678406, 0.17356915771961212]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.3145219087600708, 0.24754682183265686, 0.2938404381275177, 0.06994199007749557, -0.7633177042007446, 0.4276082217693329, 0.17044755816459656, -0.11776604503393173, 0.44617322087287903, 0.25328075885772705, -0.13291050493717194, 0.19544531404972076, 0.38008740544319153, 0.1538068801164627, -0.48176854848861694, 0.05354846268892288]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.25813227891921997, 0.2659946382045746, -0.16648326814174652, 0.2731993794441223, 0.5335280895233154, 0.27569320797920227, 0.21679085493087769, -0.20050564408302307, 0.12377585470676422, 0.6700843572616577, 0.05045458301901817, 0.4669894278049469, 0.3709923028945923, -0.4758094549179077, -0.40828049182891846, 0.2949637770652771]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3116248846054077, 0.19659768044948578, 0.5059986114501953, 0.8261512517929077, 0.2715591788291931, 0.2501254975795746, 0.4738442897796631, 0.4800339937210083, 0.35221582651138306, 0.22259779274463654, 0.34844842553138733, 0.6879607439041138, 1.17227041721344, 0.21370041370391846, 0.684085488319397, 0.6341604590415955]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.2171441316604614, 0.8988157510757446, 0.5608975887298584, 0.799488365650177, 0.7946708798408508, 0.6135801672935486, 0.5901154279708862, 0.6682988405227661, 0.19060850143432617, 0.6812726855278015, 0.71540766954422, 0.8970826864242554, 0.7079734206199646, 0.20905829966068268, 0.4873446524143219, 0.40186405181884766]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9459245800971985, 1.1145875453948975, 0.911861777305603, 1.0974980592727661, 1.5304522514343262, 0.40710321068763733, 0.8540744185447693, 0.9414200186729431, 0.9057135581970215, 1.2430952787399292, 0.861988365650177, 0.8134353160858154, 0.4095613658428192, 0.49407055974006653, -0.20912879705429077, 0.6889304518699646]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.2283342331647873, 0.7495667338371277, 1.0407404899597168, 1.1492077112197876, 0.3851644992828369, 0.5086858868598938, 0.984791100025177, 0.9842237234115601, 0.4612676203250885, 0.5753947496414185, 0.6753205060958862, 0.8279324173927307, 0.8339155912399292, 0.8736245632171631, 0.6235283017158508, 0.8645466566085815]
Layer: gate_12 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.5628988742828369, 0.7325250506401062, 0.7539131045341492, 0.26883408427238464, 0.7515060901641846, -0.15803377330303192, 0.9776903390884399, 1.7515405416488647, 0.7644971609115601, 0.7297260165214539, 0.87223881483078, 1.1572128534317017, -0.1881619244813919, 0.6439630389213562, 1.0090305805206299, 0.3730829656124115]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.42708662152290344, 0.6446653604507446, 0.7455160617828369, 0.5861988067626953, 0.8413567543029785, 0.04100702702999115, 0.14320524036884308, 0.6820266246795654, 0.6469554901123047, 0.7275906205177307, -0.4188404381275177, 0.3801518976688385, 0.7956474423408508, 0.8947513103485107, 1.1305842399597168, 0.23062348365783691]
Running loglikelihood requests:  95%|██████████████████████████████████████████████▊  | 481/504 [26:09<01:12,  3.14s/it]Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.18862958252429962, 1.268953561782837, 0.8979973793029785, 0.9333736896514893, 0.6429563760757446, 0.7853212952613831, 0.8081579208374023, 0.875, 0.9252310991287231, 0.9381051659584045, 0.6139001846313477, -0.44235703349113464, 0.7706109881401062, 0.677074134349823, 0.7971776127815247, 1.0631017684936523]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.7711817622184753, 0.07545191794633865, 0.8045362234115601, 0.8348302841186523, 0.8924268484115601, 0.4252259135246277, 0.19379177689552307, 0.8859072327613831, -1.208248496055603, 2.4566597938537598, -1.0195965766906738, 0.4678667187690735, 0.8711763024330139, 0.9726837873458862, 0.8178642392158508, 0.28136348724365234]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.04667212441563606, 0.5299433469772339, 1.306007981300354, 1.287356972694397, 1.2400692701339722, 1.3714238405227661, 1.1854093074798584, 1.2448283433914185, 1.5210717916488647, 1.4238418340682983, 1.986960768699646, 1.28628408908844, 1.190155029296875, 1.1870598793029785, 0.30439865589141846, 1.4595896005630493]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.3740097284317017, 1.2189151048660278, 1.4312623739242554, 1.6663732528686523, 0.8866912126541138, 1.0615646839141846, 1.821687936782837, 1.6427156925201416, 2.1686289310455322, 1.3399537801742554, 1.0409605503082275, 0.7935859560966492, 1.2050505876541138, 1.3115904331207275, 1.39761221408844, 1.3946963548660278]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3113446235656738, 1.2162742614746094, 1.1999064683914185, 1.1910486221313477, 1.4026188850402832, 0.7766818404197693, 1.1234869956970215, 1.0685794353485107, 1.5426936149597168, 1.317479133605957, 1.4082856178283691, 1.3711762428283691, 2.1295664310455322, 1.3999229669570923, 1.266505241394043, 1.2596005201339722]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.9718868732452393, 0.9456701278686523, 0.9130309224128723, 0.7763190269470215, -0.1123390719294548, 1.1086872816085815, 1.0828014612197876, 1.3653994798660278, 0.7655562162399292, 1.1649978160858154, 1.2924461364746094, 0.9543766379356384, 1.0190086364746094, 1.3456206321716309, 0.860355019569397, 0.4784950017929077]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.0649757385253906, 2.196082830429077, 2.0931997299194336, 1.9635783433914185, 1.8514524698257446, 2.026848554611206, 1.8803917169570923, 2.112125873565674, 1.8792914152145386, 2.083296537399292, 2.3033671379089355, 2.6817781925201416, 1.6169674396514893, 2.3043572902679443, 2.0501761436462402, 2.011223554611206]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.383692741394043, 1.3097491264343262, 1.6658780574798584, 0.9737566113471985, 1.387929081916809, 1.4903168678283691, 1.2238116264343262, 1.2970950603485107, 1.3934859037399292, 1.6468969583511353, 1.4701805114746094, 2.028499126434326, 1.4481184482574463, 1.4644585847854614, 1.234870195388794, 1.6357834339141846]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.2978103160858154, 1.7395466566085815, 1.014153242111206, 1.3013864755630493, 1.3954665660858154, 1.3167363405227661, 1.2353652715682983, 1.2974802255630493, 1.3931282758712769, 1.229148268699646, 1.4007482528686523, 1.3590998649597168, 1.0664613246917725, 0.5669151544570923, 1.2080765962600708, 1.3543684482574463]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.2002639770507812, 2.278499126434326, 2.1695642471313477, 2.0854973793029785, 2.6505281925201416, 2.1525087356567383, 2.381822109222412, 2.0435738563537598, 2.0490756034851074, 2.1361136436462402, 2.5480854511260986, 2.0566680431365967, 2.188270330429077, 2.333296537399292, 2.2723371982574463, 2.225572109222412]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.5171104669570923, 1.6311619281768799, 1.4222050905227661, 1.6658231019973755, 1.2060409784317017, 1.4685848951339722, 1.5573283433914185, 1.8989876508712769, 1.5709177255630493, 1.4739216566085815, 1.5081701278686523, 1.4172534942626953, 1.5274537801742554, 1.3922204971313477, 1.6528939008712769, 1.9684748649597168]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.7892276048660278, 1.7114086151123047, 1.6446137428283691, 1.6996588706970215, 1.6307905912399292, 1.6999889612197876, 1.6409542560577393, 1.7046380043029785, 1.7327932119369507, 1.8917940855026245, 2.1055238246917725, 1.5759105682373047, 1.594822883605957, 1.8886443376541138, 1.7879638671875, 1.6598759889602661]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.4925270080566406, 1.627293586730957, 1.5565649271011353, 1.6948999166488647, 1.5246891975402832, 1.5227291584014893, 1.6752070188522339, 1.6331297159194946, 1.54897940158844, 1.6834561824798584, 1.3040273189544678, 1.5522942543029785, 1.5357167720794678, 1.7840731143951416, 1.5545568466186523, 1.5603268146514893]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0154874324798584, 2.02899432182312, 2.459273099899292, 1.9719960689544678, 2.0469987392425537, 2.0108108520507812, 2.4814040660858154, 2.098453998565674, 1.9639084339141846, 1.9674845933914185, 1.9414613246917725, 1.8030369281768799, 1.9810876846313477, 1.8282350301742554, 2.2459287643432617, 2.0702850818634033]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.0390625, 5.355633735656738, 4.984485149383545, 4.911394119262695, 4.896787166595459, 4.872359275817871, 5.007262229919434, 5.247469425201416, 5.288182258605957, 5.028609275817871, 4.963083267211914, 4.873129367828369, 5.098536491394043, 5.037521839141846, 5.127310752868652, 4.952849864959717]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.951364517211914, 3.7751431465148926, 3.7341549396514893, 3.616128444671631, 3.8034770488739014, 3.909000873565674, 3.6768264770507812, 3.84930682182312, 3.9098260402679443, 3.9138975143432617, 3.756437063217163, 3.500880241394043, 3.8086488246917725, 3.7892825603485107, 3.8547534942626953, 3.962808132171631]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.822953462600708, 2.9526848793029785, 2.7679357528686523, 2.6607613563537598, 2.6739656925201416, 2.924295663833618, 2.7487895488739014, 2.6522886753082275, 2.79522442817688, 2.771016836166382, 3.225792169570923, 2.457443952560425, 2.8589348793029785, 2.7366857528686523, 3.1540493965148926, 2.8258142471313477]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.05624068155884743, 0.06765561550855637, 0.07700925320386887, -0.23517608642578125, -0.20442242920398712, -0.05520455539226532, 0.10125470906496048, -0.1005423441529274, 0.04843373969197273, 0.06127776950597763, 0.06844351440668106, 0.052120208740234375, 0.06903735548257828, 0.08437434583902359, -0.9283202886581421, 0.09779466688632965]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08544747531414032, 0.040390532463788986, 0.02916194312274456, 0.03801117092370987, 0.06634652614593506, 0.041486795991659164, 0.05624302476644516, 0.05991429090499878, 0.013017122633755207, 0.0681413933634758, -0.20557817816734314, 0.05755915120244026, 0.03495047613978386, -0.017185047268867493, 0.029565047472715378, 0.021630464121699333]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06946675479412079, 0.053055163472890854, 0.07007250189781189, 0.09047818928956985, 0.09650813788175583, 0.08975350856781006, 0.04087219387292862, -0.08233598619699478, 0.08916293829679489, 0.09093540906906128, 0.03230895847082138, 0.07804739475250244, -0.19626377522945404, 0.02415706031024456, -0.04675554484128952, 0.12180045247077942]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.1403544843196869, 0.15566493570804596, 0.14304111897945404, 0.11840090155601501, 0.11329323798418045, 0.11482980102300644, -0.04640219733119011, 0.19183261692523956, 0.1678466796875, -0.46649694442749023, 0.06009979173541069, 0.10447082668542862, 0.16976840794086456, -0.24174630641937256, 0.03770577535033226, -0.0553719662129879]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07564087212085724, 0.12030045688152313, 0.11254838854074478, 0.09021627902984619, 0.10024119913578033, -0.05263519287109375, 0.0013316563563421369, 0.035351019352674484, -0.1155962273478508, 0.01767142117023468, -0.1805630326271057, 0.13319876790046692, -0.013219342567026615, -0.21318364143371582, 0.12597350776195526, -0.08353358507156372]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.03020324744284153, 0.13681422173976898, 0.040693338960409164, 0.13437587022781372, -0.12741874158382416, -0.07686309516429901, 0.2542598247528076, 0.0002990722714457661, 0.058857183903455734, -0.06475568562746048, 0.1980939656496048, 0.08250465244054794, -0.18232770264148712, 0.16565442085266113, 0.11763480305671692, 0.06406925618648529]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.07588021457195282, -0.02806396409869194, 0.21875697374343872, 0.4016217887401581, 0.1654381901025772, 0.14581473171710968, -0.20194461941719055, -0.08498556911945343, 0.33203473687171936, 0.19351021945476532, -0.4673505425453186, 0.45249634981155396, 0.22920270264148712, -0.2849138677120209, 0.19210204482078552, 0.20173165202140808]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.26397356390953064, 0.21700625121593475, 0.24061976373195648, 0.1151842400431633, -0.6667934060096741, 0.4060690701007843, 0.1968536376953125, -0.28376901149749756, 0.4939819276332855, 0.24256068468093872, -0.12502789497375488, 0.1965375691652298, 0.3299578130245209, 0.2240884006023407, -0.4376673996448517, 0.02344992570579052]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20193655788898468, 0.2649753987789154, -0.2963588237762451, 0.3039380609989166, 0.6511387228965759, 0.4162527918815613, 0.21408604085445404, -0.2418997585773468, 0.14069563150405884, 0.6299787163734436, 0.14615881443023682, 0.544830322265625, 0.4856305718421936, -0.4440813362598419, -0.4705217778682709, 0.3285127878189087]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.46482762694358826, 0.17572283744812012, 0.4992985427379608, 0.937084972858429, 0.4351126551628113, 0.39586618542671204, 0.5949742197990417, 0.5989397168159485, 0.6106061935424805, 0.36279821395874023, 0.31994104385375977, 0.7554129362106323, 1.041894555091858, -0.04058794304728508, 0.7385463118553162, 0.7936663031578064]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1583731174468994, 1.0359723567962646, 0.6114537119865417, 0.7966936230659485, 0.8034632802009583, 0.7151861786842346, 0.7453020215034485, 0.7455078363418579, 0.01030099019408226, 0.49272045493125916, 0.7860909700393677, 0.9525181651115417, 0.7705914974212646, 0.2233450710773468, 0.8457628488540649, 0.3935978412628174]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [1.0057199001312256, 1.330691933631897, 0.9659039974212646, 1.3841099739074707, 1.8028459548950195, 0.5565481781959534, 1.0262835025787354, 0.9515904188156128, 1.0862584114074707, 1.5466797351837158, 1.0271484851837158, 1.0323381423950195, 0.42811670899391174, 0.7016257047653198, 0.0837140753865242, 0.8646345138549805]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.23111790418624878, 0.9398995637893677, 1.3084124326705933, 1.1669642925262451, 0.6732509136199951, 0.6058314442634583, 1.227807641029358, 1.1840331554412842, 0.5538386702537537, 0.7628348469734192, 0.9840402007102966, 0.9157087206840515, 0.8446289300918579, 0.8490792512893677, 0.8890625238418579, 1.1518415212631226]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.9630580544471741, 1.0065289735794067, 1.1979701519012451, 0.5298985242843628, 1.041406273841858, 0.0625937357544899, 1.1840959787368774, 2.039341449737549, 1.1168805360794067, 1.0299943685531616, 1.4224190711975098, 1.5053571462631226, 0.3465244770050049, 0.8227957487106323, 1.0477957725524902, 0.8173366189002991]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7498395442962646, 0.9037946462631226, 0.9470981955528259, 0.7762835025787354, 1.044698715209961, 0.5979422330856323, 0.46646031737327576, 1.2945138216018677, 0.6547572612762451, 0.9913085699081421, -0.2995361387729645, 0.8807756900787354, 1.1145716905593872, 1.182742714881897, 1.2793108224868774, 0.4557447135448456]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.532710075378418, 1.4982421398162842, 1.1465401649475098, 0.9687778949737549, 0.8617292046546936, 1.0714285373687744, 1.3865540027618408, 1.1272042989730835, 1.1984933614730835, 1.0859932899475098, 0.7823660969734192, -0.11205357313156128, 1.1185652017593384, 0.9516183137893677, 1.0902622938156128, 1.519873023033142]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_16 - Captured router_logits: [0.901074230670929, 0.1419243961572647, 0.9746948480606079, 0.8875418305397034, 0.709667980670929, 0.5291355848312378, 0.6134292483329773, 1.0017403364181519, -0.7687168717384338, 2.7723214626312256, -0.8606480360031128, 0.6951747536659241, 0.9722377061843872, 1.1655831336975098, 0.9273926019668579, 0.4240094721317291]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.19185267388820648, 0.9862618446350098, 1.6073939800262451, 1.487081527709961, 1.4426339864730835, 1.5815848112106323, 1.490625023841858, 1.5641322135925293, 1.8296282291412354, 1.6615792512893677, 2.5364954471588135, 1.6000208854675293, 1.4081263542175293, 1.3290457725524902, 0.5845044851303101, 1.6863560676574707]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6403459310531616, 1.3559151887893677, 1.6432756185531616, 1.9949777126312256, 1.3030134439468384, 1.4227678775787354, 2.029520034790039, 1.860714316368103, 2.626004457473755, 1.538839340209961, 1.3477120399475098, 1.1523926258087158, 1.3993582725524902, 1.486837387084961, 1.6209263801574707, 1.6872209310531616]
Running loglikelihood requests:  96%|███████████████████████████████████████████████▏ | 485/504 [26:21<00:58,  3.10s/it]Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.5003348588943481, 1.4008928537368774, 1.5330915451049805, 1.3760881423950195, 1.6925222873687744, 1.1727644205093384, 1.4269251823425293, 1.3434569835662842, 1.9033761024475098, 1.4361048936843872, 1.7414621114730835, 1.5848214626312256, 2.379129409790039, 1.6841238737106323, 1.5604352951049805, 1.5335100889205933]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.212015151977539, 1.0782365798950195, 1.0143414735794067, 0.8928152918815613, 0.36019372940063477, 1.4759068489074707, 1.2844866514205933, 1.5928571224212646, 0.7974853515625, 1.348270058631897, 1.4689452648162842, 1.1142089366912842, 1.2725725173950195, 1.5534597635269165, 1.0458461046218872, 0.6257269978523254]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.2389509677886963, 2.454575777053833, 2.3284597396850586, 2.1497209072113037, 2.154296875, 2.2440848350524902, 2.1148996353149414, 2.3983259201049805, 2.137946367263794, 2.406473159790039, 2.488727569580078, 2.7284598350524902, 2.03125, 2.6847097873687744, 2.3865513801574707, 2.374776840209961]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.4762276411056519, 1.3895647525787354, 2.0045759677886963, 1.172098159790039, 1.4742745161056519, 1.6205357313156128, 1.3726004362106323, 1.3440290689468384, 1.4873325824737549, 1.8229352235794067, 1.5753347873687744, 2.095982074737549, 1.5676897764205933, 1.6393972635269165, 1.374776840209961, 1.732421875]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.581752300262451, 1.883426308631897, 1.3135044574737549, 1.437276840209961, 1.7266740798950195, 1.5597097873687744, 1.4620535373687744, 1.4716517925262451, 1.5597097873687744, 1.4440847635269165, 1.5514508485794067, 1.5522321462631226, 1.3036272525787354, 0.9165457487106323, 1.460546851158142, 1.5400111675262451]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.2675223350524902, 2.3602678775787354, 2.3072545528411865, 2.1150670051574707, 2.7093749046325684, 2.312723159790039, 2.471205472946167, 2.1166293621063232, 2.086718797683716, 2.2501115798950195, 2.6796875, 2.1776785850524902, 2.2889509201049805, 2.405245542526245, 2.3130581378936768, 2.248325824737549]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7036272287368774, 1.7503348588943481, 1.5545201301574707, 1.8981026411056519, 1.4353795051574707, 1.5898995399475098, 1.682031273841858, 2.0553572177886963, 1.6916853189468384, 1.636328101158142, 1.633203148841858, 1.5545759201049805, 1.678348183631897, 1.5146205425262451, 1.8893972635269165, 1.9737722873687744]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8386719226837158, 1.865869164466858, 1.7743861675262451, 1.7961496114730835, 1.7056361436843872, 1.8683140277862549, 1.771854043006897, 1.8040179014205933, 1.7914934158325195, 2.0393972396850586, 2.336188554763794, 1.6965681314468384, 1.7764508724212646, 2.0311663150787354, 1.8813616037368774, 1.7942888736724854]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6605747938156128, 1.7299665212631226, 1.6713727712631226, 1.7698661088943481, 1.6102678775787354, 1.6560826301574707, 1.846316933631897, 1.7479074001312256, 1.6508091688156128, 1.7347238063812256, 1.4479910135269165, 1.6638672351837158, 1.7488281726837158, 1.9922432899475098, 1.6491070985794067, 1.6797432899475098]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.1497209072113037, 2.240736722946167, 2.5715959072113037, 2.0675222873687744, 2.2006137371063232, 2.107868194580078, 2.4923548698425293, 2.213392972946167, 2.1166293621063232, 2.0885045528411865, 2.0858259201049805, 1.9037946462631226, 2.030998945236206, 1.9574776887893677, 2.4205915927886963, 2.208035707473755]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.260267734527588, 5.68169641494751, 5.149106979370117, 4.993080139160156, 5.067857265472412, 5.066071510314941, 5.19330358505249, 5.41205358505249, 5.3868303298950195, 5.192634105682373, 5.1004462242126465, 5.047544479370117, 5.303794860839844, 5.235937595367432, 5.3973212242126465, 5.149330139160156]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.135267734527588, 3.936049222946167, 3.9185268878936768, 3.7556920051574707, 3.907924175262451, 3.975781202316284, 3.7796874046325684, 4.066071510314941, 4.0311384201049805, 3.9310266971588135, 3.8470981121063232, 3.4735212326049805, 3.8450334072113037, 3.8909597396850586, 3.9966518878936768, 4.167076110839844]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.8671875, 3.0058035850524902, 2.902790069580078, 2.655468702316284, 2.7860491275787354, 3.0122768878936768, 2.7174108028411865, 2.634486675262451, 2.8348214626312256, 2.705357074737549, 3.247767925262451, 2.3382253646850586, 2.891852617263794, 2.792522430419922, 3.3453125953674316, 2.8921875953674316]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.060329269617795944, 0.07458308339118958, 0.08435855060815811, -0.23344288766384125, -0.21271194517612457, -0.08010908216238022, 0.10163525491952896, -0.07947839051485062, 0.0688551738858223, 0.05993608012795448, 0.06647900491952896, 0.05712096765637398, 0.07341666519641876, 0.09931569546461105, -0.9481502175331116, 0.10235065221786499]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.08400118350982666, 0.04441015422344208, 0.03196528181433678, 0.03678595647215843, 0.07271763682365417, 0.03467946872115135, 0.05698582902550697, 0.0732436254620552, 0.04859117045998573, 0.06177974119782448, -0.19322314858436584, 0.05149730667471886, 0.03872371092438698, -0.03097556345164776, 0.028444981202483177, 0.03442559763789177]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07084401696920395, 0.040664784610271454, 0.08394036442041397, 0.09725197404623032, 0.12288151681423187, 0.08737536519765854, 0.07219408452510834, -0.08405514061450958, 0.09795943647623062, 0.09707199037075043, 0.03821132332086563, 0.08288933336734772, -0.18298517167568207, 0.022858329117298126, -0.003925268072634935, 0.11549753695726395]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13610927760601044, 0.15086564421653748, 0.13379879295825958, 0.1400526911020279, 0.12069259583950043, 0.1210097149014473, 0.02779642678797245, 0.19359157979488373, 0.1714327186346054, -0.45127448439598083, 0.012640469707548618, 0.0996730625629425, 0.2305067926645279, -0.26857590675354004, 0.010459568351507187, -0.05549776554107666]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06712805479764938, 0.09770169109106064, 0.1059800311923027, 0.13978731632232666, 0.0480450876057148, -0.04482833296060562, 0.034284066408872604, 0.031227443367242813, -0.08021257817745209, 0.036569345742464066, -0.1807480901479721, 0.12738379836082458, 0.015614384785294533, -0.18732407689094543, 0.1288912147283554, -0.07235662639141083]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.02933933399617672, 0.17078687250614166, 0.04354393854737282, 0.12440380454063416, -0.086077481508255, -0.050290316343307495, 0.20499099791049957, 0.03424780070781708, 0.1333911120891571, -0.05919569730758667, 0.1261114627122879, 0.09691973775625229, -0.20071478188037872, 0.1573358029127121, 0.15194614231586456, 0.08106784522533417]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.10265891999006271, 0.007663229014724493, 0.11119942367076874, 0.36776047945022583, 0.15347842872142792, 0.13192395865917206, -0.1415122151374817, -0.08146866410970688, 0.37661343812942505, 0.1643243283033371, -0.4896089732646942, 0.40935593843460083, 0.21566462516784668, -0.2546262741088867, 0.18948563933372498, 0.21230348944664001]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.35344308614730835, 0.19451914727687836, 0.22337667644023895, 0.12352807819843292, -0.69792640209198, 0.3816550374031067, 0.17894667387008667, -0.2675073742866516, 0.49857497215270996, 0.16205187141895294, -0.16953177750110626, 0.19769375026226044, 0.3147379457950592, 0.20829883217811584, -0.5606990456581116, -0.07162409275770187]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2418496012687683, 0.2262471467256546, -0.2667466402053833, 0.33597642183303833, 0.5971405506134033, 0.34474071860313416, 0.21996982395648956, -0.23029890656471252, 0.15785349905490875, 0.6018132567405701, 0.12887108325958252, 0.46651381254196167, 0.46450406312942505, -0.49601590633392334, -0.5134525299072266, 0.3189984858036041]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.40940260887145996, 0.2666599452495575, 0.4826323986053467, 0.892185389995575, 0.3730486333370209, 0.2784340977668762, 0.5224415063858032, 0.566300094127655, 0.6225143671035767, 0.27327120304107666, 0.2975083589553833, 0.7208871245384216, 1.06088125705719, 0.05408687889575958, 0.71042799949646, 0.6839687824249268]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1250406503677368, 0.922957718372345, 0.5369890928268433, 0.7764662504196167, 0.7303095459938049, 0.6595733165740967, 0.6161826252937317, 0.6868206262588501, 0.1855652779340744, 0.5610170364379883, 0.7211418747901917, 0.9412151575088501, 0.7796241641044617, 0.17107579112052917, 0.6918611526489258, 0.4355875551700592]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9811763763427734, 1.3150758743286133, 0.8858978748321533, 1.3102333545684814, 1.6187868118286133, 0.48716801404953003, 0.9947208762168884, 1.04664146900177, 1.0008633136749268, 1.4423686265945435, 0.96314537525177, 0.9719627499580383, 0.5225016474723816, 0.6352716088294983, 0.11462048441171646, 0.775518000125885]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.14194908738136292, 0.8301488757133484, 1.2215948104858398, 1.1539572477340698, 0.5804063081741333, 0.5541284680366516, 1.1138827800750732, 1.1400022506713867, 0.47077035903930664, 0.6097226142883301, 0.8860358595848083, 0.8153872489929199, 0.9250877499580383, 0.8616394996643066, 0.787109375, 0.97605299949646]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.9608525633811951, 0.9635133743286133, 1.1690373420715332, 0.47876861691474915, 1.0509793758392334, 0.12629146873950958, 1.1434272527694702, 1.9122508764266968, 1.0691519975662231, 0.9579370617866516, 1.3923375606536865, 1.4108922481536865, 0.3337066173553467, 0.9451285004615784, 1.0519134998321533, 0.7623839378356934]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7009525299072266, 0.7607138752937317, 0.9180253744125366, 0.7738479375839233, 1.0088032484054565, 0.6441526412963867, 0.5185086727142334, 1.0880286693572998, 0.6066646575927734, 0.860457718372345, -0.18969018757343292, 0.6915088891983032, 1.0039911270141602, 1.04173743724823, 1.1652655601501465, 0.39280810952186584]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.4047856032848358, 1.4256397485733032, 1.0958163738250732, 0.9426941871643066, 0.9110408425331116, 1.0088881254196167, 1.2747060060501099, 1.0450917482376099, 1.0913156270980835, 1.02615487575531, 0.7674861550331116, -0.14623670279979706, 1.0851166248321533, 0.8697633743286133, 1.0382416248321533, 1.35213041305542]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.8107379674911499, 0.22227345407009125, 0.7919797897338867, 0.8564311861991882, 0.6977893114089966, 0.4203110933303833, 0.40302973985671997, 0.94667649269104, -0.845809280872345, 2.5704541206359863, -0.8512794375419617, 0.54425048828125, 0.9127109050750732, 0.9990234375, 0.8235323429107666, 0.35142451524734497]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.25694915652275085, 0.9201200008392334, 1.5223335027694702, 1.52997624874115, 1.5195878744125366, 1.5246546268463135, 1.4851816892623901, 1.4493744373321533, 1.8604223728179932, 1.6389974355697632, 2.348562002182007, 1.5768238306045532, 1.3804206848144531, 1.3674988746643066, 0.6120287179946899, 1.6079879999160767]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.50067937374115, 1.3157835006713867, 1.5295233726501465, 1.87788724899292, 1.1030875444412231, 1.2769474983215332, 1.9422553777694702, 1.8877943754196167, 2.4581069946289062, 1.49099862575531, 1.2242555618286133, 0.9446950554847717, 1.30277681350708, 1.3744162321090698, 1.5569519996643066, 1.5474977493286133]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.48539400100708, 1.33305025100708, 1.4914515018463135, 1.3378057479858398, 1.6268682479858398, 1.0548219680786133, 1.3608752489089966, 1.29615318775177, 1.7818303108215332, 1.4168082475662231, 1.6722429990768433, 1.5366564989089966, 2.3331069946289062, 1.5632359981536865, 1.4950181245803833, 1.4996037483215332]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0475578308105469, 0.9752179384231567, 0.8801517486572266, 0.7721141576766968, 0.09611887484788895, 1.2080078125, 1.1311423778533936, 1.4026179313659668, 0.626236617565155, 1.1872735023498535, 1.27683424949646, 1.0280585289001465, 1.0269191265106201, 1.3812839984893799, 0.8179630637168884, 0.43435004353523254]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.2190897464752197, 2.437952995300293, 2.317821502685547, 2.170459747314453, 2.0906362533569336, 2.2736639976501465, 2.111809253692627, 2.3430707454681396, 2.1125452518463135, 2.3166892528533936, 2.5049819946289062, 2.8106884956359863, 1.9376698732376099, 2.5755207538604736, 2.321784496307373, 2.2997055053710938]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.5474977493286133, 1.4723732471466064, 2.0534420013427734, 1.1653079986572266, 1.5596693754196167, 1.6520607471466064, 1.3841712474822998, 1.4652966260910034, 1.58763587474823, 1.8333333730697632, 1.6544950008392334, 2.188066005706787, 1.57082200050354, 1.6627037525177002, 1.3621490001678467, 1.7871942520141602]
Running loglikelihood requests:  97%|███████████████████████████████████████████████▌ | 489/504 [26:33<00:46,  3.09s/it]Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4792232513427734, 1.8477694988250732, 1.1287647485733032, 1.4187613725662231, 1.5699728727340698, 1.4247056245803833, 1.3738677501678467, 1.3830389976501465, 1.4461051225662231, 1.3309556245803833, 1.4846014976501465, 1.4427649974822998, 1.18529212474823, 0.7295275926589966, 1.29415762424469, 1.5018682479858398]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3924365043640137, 2.46637225151062, 2.415987253189087, 2.1436820030212402, 2.728487253189087, 2.31453800201416, 2.5358922481536865, 2.1837635040283203, 2.177196502685547, 2.3031022548675537, 2.75543475151062, 2.182178497314453, 2.354053497314453, 2.5173232555389404, 2.3292572498321533, 2.3743207454681396]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6845561265945435, 1.7446784973144531, 1.5901833772659302, 1.8840579986572266, 1.3697350025177002, 1.6426630020141602, 1.70142662525177, 2.1119792461395264, 1.7039741277694702, 1.6189991235733032, 1.68019700050354, 1.5548007488250732, 1.7248075008392334, 1.54398775100708, 1.97537362575531, 2.0855977535247803]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8323708772659302, 1.8861349821090698, 1.8039515018463135, 1.7476788759231567, 1.7415647506713867, 1.8588265180587769, 1.7719444036483765, 1.7796648740768433, 1.79831862449646, 1.96976900100708, 2.393031120300293, 1.6775645017623901, 1.7341485023498535, 1.9733638763427734, 1.8673149347305298, 1.7790793180465698]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.65692937374115, 1.7136266231536865, 1.6681952476501465, 1.7591712474822998, 1.6079031229019165, 1.6226788759231567, 1.8324275016784668, 1.7523211240768433, 1.6600147485733032, 1.7335965633392334, 1.41355299949646, 1.6527683734893799, 1.7339787483215332, 2.0353827476501465, 1.65625, 1.6826879978179932]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.125962495803833, 2.2332427501678467, 2.620640754699707, 2.1237545013427734, 2.1902740001678467, 2.1749887466430664, 2.6021852493286133, 2.2860054969787598, 2.1312272548675537, 2.125452995300293, 2.1252264976501465, 1.95822012424469, 2.08500337600708, 2.0138134956359863, 2.4772984981536865, 2.1223957538604736]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.16304349899292, 5.616394996643066, 5.155797004699707, 5.026381492614746, 5.070425510406494, 5.052083492279053, 5.1555705070495605, 5.3650360107421875, 5.404664993286133, 5.14741849899292, 5.0529890060424805, 5.008152008056641, 5.212862491607666, 5.225769996643066, 5.35258150100708, 5.112998008728027]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.8157835006713867, 3.6685914993286133, 3.566009998321533, 3.4709012508392334, 3.6175272464752197, 3.64605975151062, 3.485280752182007, 3.734375, 3.7951767444610596, 3.6886322498321533, 3.556612253189087, 3.2247509956359863, 3.526777744293213, 3.628962755203247, 3.6951992511749268, 3.7545855045318604]
Layer: gate_30 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.6350769996643066, 2.7894022464752197, 2.7130887508392334, 2.5086050033569336, 2.6106204986572266, 2.7076539993286133, 2.5681612491607666, 2.4016644954681396, 2.61956524848938, 2.4796762466430664, 3.122056245803833, 2.0755491256713867, 2.6927082538604736, 2.5887680053710938, 3.1447010040283203, 2.639153003692627]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.05250594764947891, 0.06686048209667206, 0.06665312498807907, -0.2248150259256363, -0.22541102766990662, -0.04709363356232643, 0.09570995718240738, -0.11958882212638855, 0.05087325721979141, 0.051148828119039536, 0.06382068246603012, 0.06341324746608734, 0.07132983207702637, 0.0853462815284729, -0.9193242788314819, 0.09033043682575226]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08448996394872665, 0.05411677435040474, 0.03620591759681702, 0.0551491342484951, 0.0665857121348381, 0.030310217291116714, 0.0472470186650753, 0.06392954289913177, 0.030614767223596573, 0.07265893369913101, -0.1819111853837967, 0.04597655311226845, 0.03541678935289383, -0.024947892874479294, 0.03182849660515785, 0.013619493693113327]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06012805551290512, 0.0635494664311409, 0.08519107103347778, 0.09324053674936295, 0.0985105112195015, 0.09076554328203201, 0.042480695992708206, -0.06697651743888855, 0.11166039854288101, 0.07336846739053726, 0.032058943063020706, 0.07995309680700302, -0.1906583458185196, 0.04726284742355347, -0.019795887172222137, 0.13532131910324097]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.12238255143165588, 0.14443650841712952, 0.16222506761550903, 0.14017201960086823, 0.12484194338321686, 0.11918116360902786, 0.00446740910410881, 0.19076037406921387, 0.19291026890277863, -0.43768948316574097, 0.07170195877552032, 0.09576176851987839, 0.17432619631290436, -0.24748024344444275, -0.001395382103510201, -0.028197331354022026]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.09059438854455948, 0.09476368129253387, 0.11822453141212463, 0.10457850247621536, 0.1272701472043991, -0.07371395826339722, 0.008105149492621422, 0.03291653096675873, -0.10000097751617432, 0.03720411658287048, -0.1692996770143509, 0.12173029035329819, -0.06283888220787048, -0.16424639523029327, 0.1334071308374405, -0.04181363806128502]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_5 - Captured router_logits: [0.04862634465098381, 0.1552615910768509, 0.09956860542297363, 0.08557675778865814, -0.12837469577789307, -0.05865478515625, 0.27187883853912354, 0.007003442384302616, 0.08228336274623871, -0.0696679875254631, 0.08804549276828766, 0.12070613354444504, -0.20436938107013702, 0.18321296572685242, 0.1273120492696762, 0.10549642145633698]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11602145433425903, -0.006462438963353634, 0.16299301385879517, 0.3943672776222229, 0.17259147763252258, 0.14282499253749847, -0.13880054652690887, -0.07466421276330948, 0.2978670597076416, 0.24110640585422516, -0.510499894618988, 0.44133520126342773, 0.2244381159543991, -0.3451957106590271, 0.15416227281093597, 0.18562738597393036]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.2636978328227997, 0.16467785835266113, 0.22143463790416718, 0.12318795919418335, -0.7593502402305603, 0.41578012704849243, 0.2652168869972229, -0.2564733624458313, 0.4462924897670746, 0.17489396035671234, -0.18772956728935242, 0.21736600995063782, 0.2510530948638916, 0.2760273814201355, -0.5676014423370361, -0.07341208308935165]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.22409285604953766, 0.2955440580844879, -0.36121150851249695, 0.31960558891296387, 0.6208969950675964, 0.41239359974861145, 0.2525479793548584, -0.27537742257118225, 0.18904592096805573, 0.5835853815078735, 0.21401453018188477, 0.45229601860046387, 0.5611080527305603, -0.5297268629074097, -0.6009594202041626, 0.32558202743530273]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.45043033361434937, 0.1990930289030075, 0.45841047167778015, 0.8845323920249939, 0.351042777299881, 0.23260498046875, 0.5071784853935242, 0.648313581943512, 0.5037441253662109, 0.24793390929698944, 0.22020527720451355, 0.7071041464805603, 1.0190211534500122, -0.030943913385272026, 0.7142170071601868, 0.7389225959777832]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0849263668060303, 0.9088517427444458, 0.509279191493988, 0.7415680289268494, 0.786187469959259, 0.6512380838394165, 0.6976591944694519, 0.7751428484916687, -0.014649348333477974, 0.3815535306930542, 0.7630742788314819, 0.8655914664268494, 0.7260733246803284, 0.14270201325416565, 0.7422175407409668, 0.32710108160972595]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.8959305286407471, 1.1837395429611206, 0.950559675693512, 1.2866429090499878, 1.5900040864944458, 0.4336320161819458, 0.9448606371879578, 0.8988857269287109, 0.9662284255027771, 1.4576143026351929, 0.8890799880027771, 0.9394093751907349, 0.3621145188808441, 0.6218480467796326, -0.022156672552227974, 0.7513774037361145]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.07073838263750076, 0.7531629204750061, 1.1966097354888916, 1.0284223556518555, 0.48283910751342773, 0.4573282301425934, 1.06695556640625, 1.0319277048110962, 0.44972705841064453, 0.6056919097900391, 1.0545672178268433, 0.8865146636962891, 0.7838408350944519, 0.8323519229888916, 0.7610628604888916, 1.0561450719833374]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8354273438453674, 0.9010902643203735, 1.1876603364944458, 0.43611258268356323, 0.9758337140083313, -0.009189890697598457, 1.0888234376907349, 1.85354483127594, 0.9641732573509216, 0.9648146033287048, 1.2721694707870483, 1.4196304082870483, 0.2406352013349533, 0.8395230770111084, 0.9669426083564758, 0.6192207932472229]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7216067910194397, 0.810371994972229, 0.899224579334259, 0.7574335336685181, 1.1097831726074219, 0.5882532000541687, 0.2434920072555542, 1.1523737907409668, 0.5874752402305603, 0.9090339541435242, -0.40849897265434265, 0.6803033351898193, 1.0234994888305664, 1.2165782451629639, 1.1818009614944458, 0.34849798679351807]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_15 - Captured router_logits: [0.3454170823097229, 1.452950119972229, 1.0868703126907349, 0.9707468748092651, 0.807737410068512, 1.0139342546463013, 1.2277158498764038, 1.0071128606796265, 1.1236590147018433, 1.1334538459777832, 0.7399064302444458, -0.16053634881973267, 1.245539903640747, 0.9094566106796265, 1.0265275239944458, 1.2918245792388916]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.8566930890083313, 0.19869904220104218, 1.0610971450805664, 0.9059584736824036, 0.7182653546333313, 0.5091971755027771, 0.40908950567245483, 0.9819846153259277, -0.9572098255157471, 2.6909689903259277, -0.9652063250541687, 0.6169661283493042, 0.9194554686546326, 1.0395435094833374, 1.030339002609253, 0.29795724153518677]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.08109386265277863, 0.8791157603263855, 1.552180528640747, 1.38864266872406, 1.460762619972229, 1.5995218753814697, 1.3505094051361084, 1.4250961542129517, 1.7947214841842651, 1.5079145431518555, 2.377098798751831, 1.4552857875823975, 1.3176779747009277, 1.2391266822814941, 0.40393248200416565, 1.5577483177185059]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6122318506240845, 1.2959421873092651, 1.5637534856796265, 1.9190181493759155, 1.1424323320388794, 1.2677239179611206, 1.9858325719833374, 1.8141324520111084, 2.506908893585205, 1.5216301679611206, 1.2532358169555664, 0.9645376801490784, 1.326375961303711, 1.3554742336273193, 1.5430853366851807, 1.632987380027771]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.4897971153259277, 1.343575119972229, 1.4805562496185303, 1.3188258409500122, 1.6788712739944458, 1.0833102464675903, 1.4032765626907349, 1.3023991584777832, 1.8284894227981567, 1.4251981973648071, 1.7077016830444336, 1.6196653842926025, 2.3390274047851562, 1.6514983177185059, 1.5696711540222168, 1.5044018030166626]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1351956129074097, 1.078897476196289, 0.9981051683425903, 0.9216017127037048, 0.13744023442268372, 1.4748717546463013, 1.2295359373092651, 1.4730570316314697, 0.6936089992523193, 1.3665461540222168, 1.3963385820388794, 1.0913450717926025, 1.304308533668518, 1.49609375, 0.9708853363990784, 0.5798444747924805]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_21 - Captured router_logits: [2.2530317306518555, 2.4377331733703613, 2.3266091346740723, 2.150944471359253, 2.083838701248169, 2.302588701248169, 2.138875961303711, 2.390974760055542, 2.0932836532592773, 2.355177164077759, 2.5171408653259277, 2.7870802879333496, 1.9451959133148193, 2.57416033744812, 2.3577425479888916, 2.3204290866851807]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.5022737979888916, 1.377506971359253, 2.028684616088867, 1.1469799280166626, 1.4913129806518555, 1.666744351387024, 1.3945895433425903, 1.4210004806518555, 1.5721781253814697, 1.8058534860610962, 1.6054104566574097, 2.1937966346740723, 1.5429104566574097, 1.6526352167129517, 1.3575676679611206, 1.7491254806518555]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.499008893966675, 1.8446828126907349, 1.1731139421463013, 1.4056669473648071, 1.630130648612976, 1.506588101387024, 1.416569471359253, 1.400594711303711, 1.555037260055542, 1.3780900239944458, 1.4897388219833374, 1.4714902639389038, 1.252332091331482, 0.7844019532203674, 1.3641557693481445, 1.4917793273925781]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.29302716255188, 2.4607043266296387, 2.328125, 2.126749038696289, 2.698111057281494, 2.239738702774048, 2.502798557281494, 2.136427164077759, 2.114272356033325, 2.266674518585205, 2.713502883911133, 2.1937966346740723, 2.3730177879333496, 2.489622116088867, 2.3176305294036865, 2.2995569705963135]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7311683893203735, 1.788712739944458, 1.6256996393203735, 1.9221081733703613, 1.4102145433425903, 1.6380597352981567, 1.749650239944458, 2.09771466255188, 1.7534397840499878, 1.6620218753814697, 1.7097715139389038, 1.5772504806518555, 1.7291277647018433, 1.5614506006240845, 1.9376165866851807, 2.043376922607422]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8446245193481445, 1.8620970249176025, 1.80276358127594, 1.766849398612976, 1.6999183893203735, 1.88888680934906, 1.7456564903259277, 1.75588858127594, 1.837894320487976, 1.9979010820388794, 2.3444788455963135, 1.687150239944458, 1.7342584133148193, 1.9760669469833374, 1.8739651441574097, 1.7919557094573975]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Running loglikelihood requests:  98%|███████████████████████████████████████████████▉ | 493/504 [26:45<00:33,  3.08s/it]Layer: gate_27 - Captured router_logits: [1.674177885055542, 1.7330049276351929, 1.6842350959777832, 1.7827075719833374, 1.616837739944458, 1.6203941106796265, 1.841592788696289, 1.7704349756240845, 1.67190420627594, 1.738529086112976, 1.4499183893203735, 1.6651411056518555, 1.7663829326629639, 1.9747551679611206, 1.6964201927185059, 1.700967788696289]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.0456507205963135, 2.1610307693481445, 2.5189483165740967, 2.027751922607422, 2.1114156246185303, 2.101854085922241, 2.4980759620666504, 2.2143189907073975, 2.037605047225952, 2.0137593746185303, 2.041161298751831, 1.8711520433425903, 1.980672836303711, 1.8930736780166626, 2.344041585922241, 2.0563199520111084]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.071128845214844, 5.500932693481445, 4.982509136199951, 4.8467817306518555, 4.89272403717041, 4.8257927894592285, 4.991371154785156, 5.20102596282959, 5.2054572105407715, 4.983442306518555, 4.906483173370361, 4.842584133148193, 5.072761058807373, 5.048274040222168, 5.211753845214844, 4.936567306518555]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.9863572120666504, 3.8754663467407227, 3.7440531253814697, 3.6713502407073975, 3.7607858180999756, 3.8965134620666504, 3.642782211303711, 3.8843867778778076, 3.948227643966675, 3.8331973552703857, 3.690298557281494, 3.448868989944458, 3.725804567337036, 3.80859375, 3.874650239944458, 4.074451923370361]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.6283233165740967, 2.7811334133148193, 2.747201442718506, 2.476912260055542, 2.6195194721221924, 2.7707555294036865, 2.5317163467407227, 2.4160447120666504, 2.5959653854370117, 2.4711694717407227, 3.0846548080444336, 2.0903685092926025, 2.643831729888916, 2.583838701248169, 3.1159048080444336, 2.622493028640747]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.05107532814145088, 0.06273304671049118, 0.07357427477836609, -0.21505293250083923, -0.20166015625, -0.05793900787830353, 0.10022056102752686, -0.10353337973356247, 0.06231800466775894, 0.051949795335531235, 0.06319940835237503, 0.0616636797785759, 0.06940890103578568, 0.0805910974740982, -0.8813121318817139, 0.092678003013134]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08353964984416962, 0.03845159336924553, 0.023420576006174088, 0.03739332780241966, 0.070068359375, 0.04103448987007141, 0.051004305481910706, 0.05122014880180359, 0.01772821508347988, 0.064479760825634, -0.19074596464633942, 0.05232197046279907, 0.05366821214556694, -0.042208168655633926, 0.04599921405315399, 0.021391088142991066]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07803954929113388, 0.04279840737581253, 0.08781488239765167, 0.0980416014790535, 0.09577053785324097, 0.08535600453615189, 0.04268188402056694, -0.07868125289678574, 0.1018720418214798, 0.08132116496562958, 0.04046686366200447, 0.08136721700429916, -0.1694047451019287, 0.02250615879893303, -0.03137318044900894, 0.12387584149837494]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.15582525730133057, 0.15243420004844666, 0.13884055614471436, 0.14301647245883942, 0.12018821388483047, 0.14181795716285706, 0.027331819757819176, 0.1988697350025177, 0.16563332080841064, -0.4575772285461426, 0.03682916983962059, 0.12685324251651764, 0.17596545815467834, -0.2409401684999466, 0.0034684615675359964, -0.032165493816137314]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.08878173679113388, 0.10406535118818283, 0.13902559876441956, 0.0868091955780983, 0.08263327926397324, -0.058249734342098236, 0.02403009496629238, 0.031264983117580414, -0.06953013688325882, 0.03744964674115181, -0.1804387867450714, 0.13257168233394623, -0.049837980419397354, -0.2006647288799286, 0.12564364075660706, -0.05473632737994194]
Layer: gate_4 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.003931496292352676, 0.13912339508533478, 0.028935102745890617, 0.06059736758470535, -0.06615211814641953, -0.03907914459705353, 0.2938443124294281, 0.06030994653701782, 0.05381636321544647, -0.05727421119809151, 0.31266868114471436, 0.0715484619140625, -0.1867009997367859, 0.13903918862342834, 0.119014672935009, 0.0606364868581295]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.09554027020931244, -0.038688383996486664, 0.13966508209705353, 0.45359107851982117, 0.16195623576641083, 0.1311207115650177, -0.1862318515777588, -0.08219216018915176, 0.2826865315437317, 0.18165116012096405, -0.5200328230857849, 0.45829829573631287, 0.19153942167758942, -0.32093727588653564, 0.11959283798933029, 0.17127297818660736]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.20292635262012482, 0.185893252491951, 0.22576794028282166, 0.11882545799016953, -0.7486106157302856, 0.4396550953388214, 0.21671143174171448, -0.2926269471645355, 0.40839123725891113, 0.222099170088768, -0.15763938426971436, 0.19124728441238403, 0.35165348649024963, 0.20719465613365173, -0.5590553879737854, 0.02668457105755806]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2477128505706787, 0.24552612006664276, -0.3641601502895355, 0.3043590188026428, 0.6474875807762146, 0.31910067796707153, 0.228892520070076, -0.24527476727962494, 0.13882890343666077, 0.4599243104457855, 0.12071505188941956, 0.5399037003517151, 0.4507812559604645, -0.547394335269928, -0.6094637513160706, 0.2811773121356964]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.38981711864471436, 0.08384788036346436, 0.5679687261581421, 0.7880903482437134, 0.3611871898174286, 0.19402410089969635, 0.45238369703292847, 0.5698863863945007, 0.37995606660842896, 0.2380027025938034, 0.21424782276153564, 0.716601550579071, 1.1480135917663574, -0.08754494041204453, 0.7027521133422852, 0.7401278614997864]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.191057801246643, 0.9693892002105713, 0.6040838360786438, 0.8033202886581421, 0.7320135235786438, 0.5471690893173218, 0.6122469902038574, 0.6955965757369995, 0.009255426935851574, 0.3912220299243927, 0.7214311361312866, 0.8694424629211426, 0.8446666598320007, 0.1456621289253235, 0.5169156193733215, 0.27695977687835693]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9383878111839294, 1.1626064777374268, 0.8850141763687134, 1.1346412897109985, 1.6535155773162842, 0.34438809752464294, 0.8985617756843567, 0.8821732997894287, 0.9152876138687134, 1.3166548013687134, 0.9014914631843567, 0.8078657388687134, 0.32241877913475037, 0.47941699624061584, -0.14163388311862946, 0.658203125]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.04467107728123665, 0.6948330998420715, 1.1183239221572876, 1.0132101774215698, 0.4087557792663574, 0.4781516194343567, 0.9782315492630005, 0.9097567200660706, 0.41591131687164307, 0.7280628681182861, 0.8319646716117859, 0.8588778376579285, 0.7213600873947144, 0.7738281488418579, 0.7455610632896423, 0.9608664512634277]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7226207256317139, 0.814240038394928, 0.915314257144928, 0.23840554058551788, 0.8742542862892151, -0.034401632845401764, 1.0564985275268555, 1.6046874523162842, 0.8514559864997864, 0.7900923490524292, 1.0743430852890015, 1.3500710725784302, -0.0032148880418390036, 0.6335493326187134, 0.9278408885002136, 0.5850053429603577]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5236239433288574, 0.8046875, 0.8081676363945007, 0.6156250238418579, 0.9344105124473572, 0.2260187268257141, 0.18993252515792847, 0.736254870891571, 0.5649769306182861, 0.7795454263687134, -0.4180460274219513, 0.572070300579071, 0.9119406938552856, 1.0564985275268555, 1.1834338903427124, 0.22833362221717834]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.2829267978668213, 1.4803977012634277, 1.0462713241577148, 0.9053799510002136, 0.7113325595855713, 0.9103338122367859, 0.9668989777565002, 1.0013139247894287, 1.0605823993682861, 1.0205610990524292, 0.6223277449607849, -0.37006503343582153, 0.9703124761581421, 0.8440696001052856, 0.9802201986312866, 1.160338282585144]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.8593394756317139, 0.13148553669452667, 0.9254705309867859, 0.906036913394928, 0.7690340876579285, 0.47586560249328613, 0.29757189750671387, 1.0544389486312866, -1.0050026178359985, 2.6561079025268555, -1.0015041828155518, 0.5351740121841431, 0.9385298490524292, 1.0856356620788574, 0.9124200940132141, 0.32055220007896423]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.01922718435525894, 0.6914567351341248, 1.4143288135528564, 1.327574610710144, 1.3074750900268555, 1.4453480243682861, 1.2823264598846436, 1.313538670539856, 1.628966212272644, 1.5602627992630005, 2.213991403579712, 1.4080166816711426, 1.2518665790557861, 1.221271276473999, 0.3779563307762146, 1.5762073993682861]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4933239221572876, 1.2969460487365723, 1.5389559268951416, 1.8654829263687134, 1.0791903734207153, 1.2480823993682861, 1.9089488983154297, 1.7555397748947144, 2.3753550052642822, 1.382244348526001, 1.1605291366577148, 0.8938121199607849, 1.2565340995788574, 1.5363303422927856, 1.4974431991577148, 1.5169744491577148]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.3803976774215698, 1.2166193723678589, 1.3455610275268555, 1.2281605005264282, 1.53515625, 0.9364435076713562, 1.2005681991577148, 1.201740026473999, 1.5811079740524292, 1.3614346981048584, 1.5537642240524292, 1.4783735275268555, 2.372727155685425, 1.4870383739471436, 1.3715908527374268, 1.4200639724731445]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0259499549865723, 0.9406072497367859, 0.9063920378684998, 0.7481223344802856, 0.18555602431297302, 1.2881569862365723, 1.1200283765792847, 1.2238370180130005, 0.7369983792304993, 1.2051846981048584, 1.3620028495788574, 0.9556374549865723, 1.0880327224731445, 1.4777698516845703, 0.8707919120788574, 0.5197110176086426]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_21 - Captured router_logits: [2.187784194946289, 2.3623578548431396, 2.2454545497894287, 2.094815254211426, 2.006960153579712, 2.184375047683716, 2.014275550842285, 2.318323850631714, 2.0018465518951416, 2.2490057945251465, 2.421875, 2.6992897987365723, 1.939985752105713, 2.4643466472625732, 2.271448850631714, 2.2585227489471436]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.367755651473999, 1.3024147748947144, 1.829545497894287, 1.0085937976837158, 1.3964488506317139, 1.4943181276321411, 1.2561079263687134, 1.2513494491577148, 1.3856533765792847, 1.6975852251052856, 1.4676135778427124, 2.030965805053711, 1.4225852489471436, 1.5056818723678589, 1.1975141763687134, 1.6037641763687134]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4443891048431396, 1.8228693008422852, 1.126420497894287, 1.3805397748947144, 1.5293323993682861, 1.4413352012634277, 1.3468040227890015, 1.4026278257369995, 1.4817471504211426, 1.296448826789856, 1.4568182229995728, 1.4696732759475708, 1.2029119729995728, 0.802398145198822, 1.3428977727890015, 1.4813920259475708]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.1572442054748535, 2.3109374046325684, 2.2348010540008545, 2.0609374046325684, 2.6954545974731445, 2.231250047683716, 2.3866477012634277, 2.0355112552642822, 2.0470170974731445, 2.173579454421997, 2.5553977489471436, 2.1031250953674316, 2.210369348526001, 2.3563921451568604, 2.2102272510528564, 2.2073862552642822]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6301136016845703, 1.7186079025268555, 1.5019176006317139, 1.8021306991577148, 1.3717329502105713, 1.5338068008422852, 1.6246448755264282, 1.9948863983154297, 1.6683238744735718, 1.5895596742630005, 1.6061079502105713, 1.4941051006317139, 1.6555397510528564, 1.4580966234207153, 1.8177556991577148, 1.9727272987365723]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.8100851774215698, 1.81494140625, 1.731036901473999, 1.7204545736312866, 1.6663352251052856, 1.7851518392562866, 1.6943248510360718, 1.7306817770004272, 1.747455358505249, 1.9897726774215698, 2.2074928283691406, 1.6503195762634277, 1.6825283765792847, 1.9882102012634277, 1.8005682229995728, 1.7114391326904297]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6143110990524292, 1.6794034242630005, 1.6504971981048584, 1.7570312023162842, 1.6007102727890015, 1.6092329025268555, 1.787642002105713, 1.706605076789856, 1.6302201747894287, 1.7117010354995728, 1.4372869729995728, 1.6383522748947144, 1.6909445524215698, 1.895099401473999, 1.6162997484207153, 1.6325994729995728]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.052130699157715, 2.147869348526001, 2.548579454421997, 2.0039772987365723, 2.076775550842285, 2.0391335487365723, 2.423224449157715, 2.1303977966308594, 2.0065340995788574, 2.0073864459991455, 2.0026988983154297, 1.8372159004211426, 2.02294921875, 1.8785511255264282, 2.3123579025268555, 2.0752129554748535]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.148721694946289, 5.4990057945251465, 5.03579568862915, 4.986434459686279, 4.991051197052002, 4.983238697052002, 5.0774149894714355, 5.3678975105285645, 5.389488697052002, 5.15113639831543, 5.080681800842285, 4.95639181137085, 5.239133358001709, 5.104971408843994, 5.3048295974731445, 5.038636207580566]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [3.960653305053711, 3.8039772510528564, 3.711789846420288, 3.6083807945251465, 3.7637784481048584, 3.8488636016845703, 3.6268465518951416, 3.9046874046325684, 3.8508522510528564, 3.8210227489471436, 3.669034004211426, 3.4844460487365723, 3.7082386016845703, 3.7504260540008545, 3.8164772987365723, 3.9758522510528564]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Running loglikelihood requests:  99%|████████████████████████████████████████████████▎| 497/504 [26:57<00:21,  3.06s/it]Layer: gate_31 - Captured router_logits: [2.6605112552642822, 2.805823802947998, 2.6725852489471436, 2.481534004211426, 2.5514204502105713, 2.735795497894287, 2.5553977489471436, 2.4848365783691406, 2.654829502105713, 2.553835153579712, 3.0897727012634277, 2.2737038135528564, 2.7164063453674316, 2.6401278972625732, 3.133948802947998, 2.748295545578003]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.057388871908187866, 0.07278724759817123, 0.07640838623046875, -0.20913837850093842, -0.20398513972759247, -0.06913361698389053, 0.10525032132863998, -0.13156919181346893, 0.0581207275390625, 0.05914871767163277, 0.06839074194431305, 0.06890219449996948, 0.0696631520986557, 0.08552409708499908, -0.8385506868362427, 0.09921886026859283]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09018904715776443, 0.04951031878590584, 0.03262300789356232, 0.031407251954078674, 0.0675031840801239, 0.04911401495337486, 0.05200619250535965, 0.04858794063329697, 0.024333035573363304, 0.06489506363868713, -0.18955400586128235, 0.049964357167482376, 0.0478617362678051, -0.03262922540307045, 0.018969006836414337, 0.018318459391593933]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.057811737060546875, 0.0613572858273983, 0.08576633036136627, 0.09269092977046967, 0.08964404463768005, 0.09149961173534393, 0.04748139530420303, -0.06857752054929733, 0.09721656888723373, 0.07385889440774918, 0.03276987373828888, 0.09625526517629623, -0.1980217844247818, 0.024577530100941658, -0.0408138707280159, 0.1244879812002182]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.1368086040019989, 0.1591152548789978, 0.1474439799785614, 0.1472880095243454, 0.1166873499751091, 0.13753820955753326, 0.04275229945778847, 0.20162327587604523, 0.16745786368846893, -0.44832921028137207, 0.0381876640021801, 0.12721535563468933, 0.17657357454299927, -0.23081687092781067, -0.005297625437378883, -0.04675038531422615]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.1036393940448761, 0.10975675284862518, 0.14319610595703125, 0.04325583949685097, 0.07658810168504715, -0.04432763159275055, 0.026701750233769417, 0.037383466958999634, -0.07691559940576553, 0.05306441709399223, -0.16564291715621948, 0.13491199910640717, -0.0695512592792511, -0.1903059184551239, 0.12712040543556213, -0.0290256068110466]
Layer: gate_4 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.017310813069343567, 0.1678585410118103, 0.07501856237649918, 0.027544939890503883, -0.09171436727046967, -0.0394473597407341, 0.29050585627555847, 0.05276291444897652, 0.06951487809419632, -0.033629242330789566, 0.23319950699806213, 0.09393338859081268, -0.16571271419525146, 0.1650933176279068, 0.14274682104587555, 0.08480637520551682]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.12843506038188934, -0.02190483920276165, 0.11962777376174927, 0.44150570034980774, 0.15598098933696747, 0.13636384904384613, -0.15021854639053345, -0.04567435756325722, 0.2950819432735443, 0.19426247477531433, -0.48301753401756287, 0.4893052875995636, 0.19526728987693787, -0.27825361490249634, 0.14056622982025146, 0.19484032690525055]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.1787075400352478, 0.2206844687461853, 0.24143247306346893, 0.12726734578609467, -0.6647316217422485, 0.4591539204120636, 0.19322995841503143, -0.22359438240528107, 0.36864331364631653, 0.1589558869600296, -0.14681950211524963, 0.2015753835439682, 0.36267542839050293, 0.20460905134677887, -0.4907768964767456, 0.06099220737814903]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.19712434709072113, 0.24247346818447113, -0.3197970986366272, 0.32004010677337646, 0.6668113470077515, 0.37433990836143494, 0.2174716591835022, -0.19409744441509247, 0.13899457454681396, 0.42953377962112427, 0.1712670475244522, 0.5874904990196228, 0.45823386311531067, -0.5103431940078735, -0.5114248991012573, 0.2486589252948761]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.44038447737693787, 0.1769612580537796, 0.47636017203330994, 0.7372730374336243, 0.3601277768611908, 0.26327627897262573, 0.5280807018280029, 0.5999168157577515, 0.34717699885368347, 0.26318132877349854, 0.27302664518356323, 0.7488787770271301, 1.2491681575775146, -0.10156702250242233, 0.7156756520271301, 0.7487702369689941]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.1703287363052368, 1.0042046308517456, 0.5714699029922485, 0.8314525485038757, 0.7255045771598816, 0.5860753655433655, 0.6700439453125, 0.7260561585426331, 0.083160400390625, 0.45403826236724854, 0.7226020097732544, 0.9081850647926331, 1.003662109375, 0.20407330989837646, 0.46967682242393494, 0.3367767333984375]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.9630534052848816, 1.2311559915542603, 0.9832175970077515, 1.2316148281097412, 1.7690610885620117, 0.46291324496269226, 0.9382776618003845, 0.995879054069519, 0.9609194397926331, 1.3039278984069824, 0.9853515625, 0.9021990895271301, 0.43383562564849854, 0.6131365895271301, -0.041167084127664566, 0.7571071982383728]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.11699873954057693, 0.8114510774612427, 1.0876193046569824, 1.0558087825775146, 0.5256494879722595, 0.46626338362693787, 0.9846824407577515, 0.8989529013633728, 0.4480251669883728, 0.8156783580780029, 0.8533890247344971, 0.8360459804534912, 0.77593994140625, 0.8110260963439941, 0.8316695690155029, 1.0252459049224854]
Layer: gate_12 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8709490895271301, 0.8485966324806213, 1.0374619960784912, 0.428466796875, 0.9103732705116272, 0.1161566823720932, 1.0774377584457397, 1.6973379850387573, 0.8975694179534912, 0.9056712985038757, 1.1242856979370117, 1.4678095579147339, 0.1859470009803772, 0.7685795426368713, 0.9419668912887573, 0.6848370432853699]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5552096962928772, 0.8165147304534912, 0.8103298544883728, 0.6474609375, 0.9747902154922485, 0.14986617863178253, 0.19152380526065826, 0.6428675055503845, 0.5267017483711243, 0.8688241243362427, -0.38917937874794006, 0.5692319273948669, 0.9318305253982544, 1.0561342239379883, 1.2122576236724854, 0.28216326236724854]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.32670876383781433, 1.5427517890930176, 1.0496599674224854, 0.8548900485038757, 0.7229049205780029, 0.9954788684844971, 0.8957157731056213, 1.0053892135620117, 1.1225404739379883, 1.050166368484497, 0.6971390247344971, -0.24871826171875, 0.9922417402267456, 0.7997323274612427, 1.0385560989379883, 1.1872694492340088]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9206452369689941, 0.22062146663665771, 0.9424641728401184, 0.8854528069496155, 0.7283347845077515, 0.5114158391952515, 0.34677577018737793, 1.057219386100769, -0.9213641285896301, 2.7628400325775146, -0.9259586930274963, 0.5647605657577515, 0.9878833889961243, 1.1000615358352661, 0.9461534023284912, 0.41568782925605774]
Running loglikelihood requests:  99%|████████████████████████████████████████████████▋| 501/504 [27:10<00:09,  3.08s/it]Running loglikelihood requests: 100%|█████████████████████████████████████████████████| 504/504 [27:10<00:00,  3.23s/it]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.11318178474903107, 0.7532077431678772, 1.431731104850769, 1.3843903541564941, 1.3168402910232544, 1.4695096015930176, 1.3135037422180176, 1.3798285722732544, 1.582184910774231, 1.5587564706802368, 2.2090566158294678, 1.3839020729064941, 1.2224980592727661, 1.1663411855697632, 0.3794352114200592, 1.6461588144302368]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5300925970077515, 1.3507667779922485, 1.567780613899231, 1.8576388359069824, 1.1489076614379883, 1.2723885774612427, 1.9243345260620117, 1.8077256679534912, 2.364366292953491, 1.4655671119689941, 1.214753270149231, 0.9630545377731323, 1.351200819015503, 1.6052697896957397, 1.5350838899612427, 1.6195746660232544]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.455475926399231, 1.3107638359069824, 1.382197618484497, 1.3102213144302368, 1.6154513359069824, 1.0427879095077515, 1.400752305984497, 1.2760597467422485, 1.5981987714767456, 1.390986680984497, 1.6452546119689941, 1.6093388795852661, 2.590277671813965, 1.6025751829147339, 1.4965277910232544, 1.4754774570465088]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0365759134292603, 1.0366392135620117, 0.9810112714767456, 0.7990586757659912, 0.1917283833026886, 1.306061863899231, 1.1966146230697632, 1.3568521738052368, 0.7471035122871399, 1.260344386100769, 1.41015625, 1.0018174648284912, 1.2281900644302368, 1.5149739980697632, 0.9506474137306213, 0.6347752213478088]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_21 - Captured router_logits: [2.266782522201538, 2.4583332538604736, 2.3161168098449707, 2.2084057331085205, 2.139467477798462, 2.2990450859069824, 2.160590171813965, 2.3994503021240234, 2.084779977798462, 2.3919270038604736, 2.532841444015503, 2.787471055984497, 1.9942853450775146, 2.619936227798462, 2.3881654739379883, 2.3693575859069824]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.5178674459457397, 1.3653067350387573, 1.8360822200775146, 1.133210301399231, 1.4787325859069824, 1.5912904739379883, 1.384186863899231, 1.3582175970077515, 1.4782986640930176, 1.8259549140930176, 1.5532407760620117, 2.1640625, 1.5292245149612427, 1.6090856790542603, 1.3361544609069824, 1.7111544609069824]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4439380168914795, 1.8224825859069824, 1.1579138040542603, 1.4055989980697632, 1.5143228769302368, 1.5048466920852661, 1.3653790950775146, 1.4027777910232544, 1.4714264869689941, 1.2956452369689941, 1.4845197200775146, 1.4291810989379883, 1.2659143209457397, 0.7947658896446228, 1.3435691595077515, 1.5175057649612427]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.2598378658294678, 2.38671875, 2.288339138031006, 2.1637730598449707, 2.7643229961395264, 2.286313772201538, 2.5345776081085205, 2.145254611968994, 2.1728878021240234, 2.3373842239379883, 2.5486111640930176, 2.194154977798462, 2.3466434478759766, 2.4473378658294678, 2.3061342239379883, 2.3475115299224854]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.6447482109069824, 1.7458044290542603, 1.5196036100387573, 1.825810194015503, 1.3963396549224854, 1.5520833730697632, 1.6354166269302368, 2.0169270038604736, 1.6628327369689941, 1.6181279420852661, 1.6373697519302368, 1.5193142890930176, 1.7080439329147339, 1.4970341920852661, 1.7554253339767456, 1.996672511100769]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.860966444015503, 1.7769639492034912, 1.8146700859069824, 1.7361111640930176, 1.6838830709457397, 1.7863633632659912, 1.7027181386947632, 1.7677228450775146, 1.7704038619995117, 2.024522542953491, 2.2185330390930176, 1.6473524570465088, 1.71875, 2.007631540298462, 1.8084852695465088, 1.7432318925857544]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.6415654420852661, 1.7039930820465088, 1.6942998170852661, 1.7832754850387573, 1.6315103769302368, 1.6038049459457397, 1.8069299459457397, 1.7353876829147339, 1.683991551399231, 1.7527127265930176, 1.4785879850387573, 1.643952488899231, 1.702727198600769, 1.8993779420852661, 1.659794569015503, 1.7126736640930176]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [2.133680582046509, 2.225404977798462, 2.6623263359069824, 2.1348378658294678, 2.1767940521240234, 2.1653645038604736, 2.4900896549224854, 2.256654977798462, 2.112557888031006, 2.122251272201538, 2.1208043098449707, 1.9356192350387573, 2.0612339973449707, 1.9761284589767456, 2.4224536418914795, 2.1576244831085205]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [5.226273059844971, 5.573495388031006, 5.1056132316589355, 5.062789440155029, 5.066840171813965, 5.067708492279053, 5.190682888031006, 5.415798664093018, 5.446469783782959, 5.224826335906982, 5.076967716217041, 5.033565044403076, 5.412905216217041, 5.224247455596924, 5.3712382316589355, 5.138309955596924]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.037615776062012, 3.9059605598449707, 3.8463542461395264, 3.7532551288604736, 3.8104746341705322, 3.912904977798462, 3.7802371978759766, 3.948495388031006, 3.9462528228759766, 3.9198496341705322, 3.7826967239379883, 3.5753672122955322, 3.8384692668914795, 3.848090171813965, 3.918836832046509, 4.057074546813965]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_31 - Captured router_logits: [2.7595486640930176, 2.955873727798462, 2.7505786418914795, 2.557870388031006, 2.66796875, 2.8262441158294678, 2.6490161418914795, 2.5798611640930176, 2.6940104961395264, 2.642939805984497, 3.0904223918914795, 2.3755786418914795, 2.801649332046509, 2.703558921813965, 3.1556713581085205, 2.8110532760620117]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
{'gate_0': [10.43655577301979, 12.285117015242577, 12.643037177622318, -30.57570916414261, -26.939604230225086, -13.9039960000664, 14.65974435210228, -17.031257734633982, 9.50034423917532, 10.471574991941452, 11.43256390094757, 9.191953528672457, 10.98190937191248, 12.938804179430008, -129.7072583436966, 14.323790952563286], 'gate_1': [10.893136478960514, 6.678319530561566, 4.510218909010291, 5.906390832737088, 9.711199190467596, 4.275940561201423, 6.381633393466473, 9.116557661443949, 2.5423965050722472, 8.78019766509533, -22.972466230392456, 6.471608951687813, 2.2201348706439603, -2.8207746051921276, 2.6125371058878954, 3.200483025982976], 'gate_2': [10.05173933878541, 7.865251369774342, 11.737056352198124, 8.605761347338557, 13.680877469480038, 12.805180668830872, 8.839563518762589, -9.960745207965374, 10.289375387132168, 10.902438906021416, 2.3798921764973784, 11.142438560724258, -21.439139492809772, 3.742857208424539, -3.1633864770410582, 13.880326971411705], 'gate_3': [16.450524240732193, 20.199453622102737, 16.59987334907055, 18.24657978117466, 16.77475231140852, 15.981928631663322, 4.477095305046532, 23.458605960011482, 22.79249081015587, -58.52380168437958, 1.8602427792502567, 12.583093672990799, 24.459719821810722, -28.64323753118515, -2.848898929893039, -5.992663963814266], 'gate_4': [10.040008094161749, 13.194371581077576, 12.086394861340523, 16.167471577151446, 7.88672100007534, -7.132705230033025, 5.5907903698971495, 6.187464457936585, -8.897794169577537, 2.782798212778289, -20.388100296258926, 18.266727209091187, -5.097018745269452, -22.48734661191702, 16.345801033079624, -5.709401144646108], 'gate_5': [8.612545735901222, 17.886975325644016, 8.795624578371644, 14.443028568755835, -10.117786338581936, -3.3952711005404126, 15.80876437301049, 2.9877364145650063, 16.52306817471981, -9.137731119059026, -9.662854617461562, 12.171279612928629, -20.935045678168535, 18.241725228726864, 17.504020757973194, 9.884813636541367], 'gate_6': [14.707765348255634, 4.719553906208603, 18.20614193379879, 42.8145694732666, 22.946811884641647, 17.931309185922146, -12.086248174222419, -5.826608587987721, 45.668201848864555, 23.15218634903431, -59.12982687354088, 40.431214064359665, 27.810424581170082, -24.354783153161407, 24.028709808830172, 23.440494067966938], 'gate_7': [46.07598511874676, 31.993840724229813, 34.72982630133629, 18.664920926094055, -98.16770207881927, 40.93697239458561, 25.787006855010986, -29.73519482697884, 53.326725997030735, 21.797449864912778, -15.358635138371028, 33.17673970758915, 33.14234238117933, 24.795929051935673, -60.69492529332638, -22.36123051540926], 'gate_8': [33.994922786951065, 40.41843627393246, -25.53022559452802, 41.725864350795746, 61.61094503104687, 45.13979463279247, 34.641305670142174, -26.837450795108452, 23.405520491302013, 83.63105589151382, 12.365640857780818, 30.051815200597048, 56.75871151685715, -49.3216841458343, -47.02785921189934, 40.37649239599705], 'gate_9': [59.73922681808472, 35.69058337435126, 59.18547251820564, 109.60132640600204, 54.507130831480026, 38.18960744142532, 82.15521749854088, 73.53096655011177, 81.91620667278767, 24.917668405221775, 43.9592601954937, 88.90635174512863, 101.26820421218872, 36.438099099788815, 94.27259331941605, 88.78682500123978], 'gate_10': [129.36747348308563, 111.35751760005951, 78.52138012647629, 100.75830227136612, 119.40734845399857, 74.89235171675682, 86.4811259508133, 91.52522224187851, 37.65089261403773, 80.4830790758133, 95.7199736237526, 114.93933749198914, 65.25584926456213, 34.803542494773865, 105.2508379817009, 62.30615943670273], 'gate_11': [127.32808446884155, 145.00946003198624, 120.24786907434464, 155.26697927713394, 173.98233783245087, 67.53318287432194, 120.76962381601334, 133.79275023937225, 129.43162834644318, 175.53035736083984, 119.36109739542007, 134.6842595934868, 71.8437709659338, 77.23222211003304, 14.187465987750329, 107.61515128612518], 'gate_12': [37.83478996963913, 118.01651531457901, 155.8667813539505, 148.7983351945877, 65.07458370178938, 91.83594918251038, 133.44337844848633, 150.64085084199905, 87.38372525572777, 59.74739155173302, 113.36565655469894, 115.34889620542526, 121.89785915613174, 113.41711378097534, 91.63387402892113, 127.94984072446823], 'gate_13': [115.82806128263474, 127.44042557477951, 144.63836652040482, 80.51140277087688, 147.90671283006668, 9.839900693390518, 149.4501270055771, 258.2918031215668, 137.2469503879547, 131.97855788469315, 167.75203520059586, 170.34881246089935, 31.082730429479852, 126.54411596059799, 152.7592608332634, 104.97116687893867], 'gate_14': [100.8526548743248, 105.96287643909454, 125.3211452960968, 114.27149724960327, 133.18333852291107, 129.86056792736053, 60.43849143292755, 157.6385633945465, 101.6737768650055, 116.34082460403442, -12.259191949851811, 104.43720635771751, 130.9246277809143, 141.64558655023575, 148.69793820381165, 81.01122218370438], 'gate_15': [65.9192861020565, 155.18381768465042, 136.05421829223633, 124.60115593671799, 105.32289999723434, 122.86085134744644, 168.0888838171959, 135.67287695407867, 124.19023019075394, 133.94590878486633, 100.40092474222183, -25.32335533760488, 136.9910204410553, 118.83086150884628, 126.73351180553436, 179.02699494361877], 'gate_16': [96.93212449550629, 33.78248275857186, 110.3131445646286, 100.76236283779144, 91.46257400512695, 68.25287318229675, 43.817765055340715, 112.85054636001587, -97.14076210930943, 292.71310901641846, -90.82782900333405, 83.63295704126358, 114.0223628282547, 116.78943109512329, 109.26954835653305, 52.45431824028492], 'gate_17': [27.84545379143674, 102.1059921681881, 186.34132194519043, 176.03286284208298, 187.586172580719, 187.83268690109253, 183.48128807544708, 186.99897134304047, 225.65341758728027, 187.55882847309113, 272.1367870569229, 189.17323851585388, 180.50547790527344, 164.23128616809845, 88.24927744269371, 178.99513018131256], 'gate_18': [192.99291670322418, 173.470099568367, 194.96224069595337, 225.57137823104858, 137.2833097577095, 168.06553786993027, 237.8670346736908, 229.96018075942993, 288.8437201976776, 191.46557581424713, 163.08054274320602, 130.82149028778076, 170.43194818496704, 151.8617160320282, 195.2157506942749, 196.43896412849426], 'gate_19': [193.0746853351593, 176.92400753498077, 185.5810351371765, 177.26472544670105, 205.6779670715332, 142.36358177661896, 169.9977849125862, 174.40388095378876, 243.5815944671631, 190.71863389015198, 207.01616883277893, 195.24583792686462, 253.04887104034424, 199.46099746227264, 192.50930452346802, 190.21448600292206], 'gate_20': [136.42129170894623, 142.8191316127777, 137.01980966329575, 123.8697424530983, 13.243654631367463, 167.298912525177, 156.9219331741333, 226.3077986240387, 110.29608601331711, 162.41977524757385, 164.55216217041016, 144.9320297241211, 146.03904736042023, 175.79514634609222, 133.76057213544846, 70.4017224162817], 'gate_21': [270.51507461071014, 289.3117940425873, 274.31037187576294, 259.13108944892883, 257.1290498971939, 273.0370478630066, 256.07913959026337, 282.9588311910629, 257.78192377090454, 277.5027128458023, 302.661563873291, 336.7664623260498, 234.18157041072845, 310.53259897232056, 286.53676414489746, 276.76399993896484], 'gate_22': [191.2687268257141, 180.3163390159607, 242.80707573890686, 147.43834257125854, 196.84582448005676, 206.70305740833282, 179.40140235424042, 185.55196833610535, 199.11408245563507, 213.52936100959778, 201.05712020397186, 266.58742678165436, 201.10766458511353, 205.74341690540314, 171.14352756738663, 218.94103813171387], 'gate_23': [301.57435297966003, 229.8403217792511, 159.00534456968307, 188.79722845554352, 204.074502825737, 193.47986161708832, 186.65158486366272, 189.0693517923355, 195.43895554542542, 186.4992152452469, 199.29718494415283, 192.95154345035553, 173.67957735061646, 109.09274247288704, 182.12182676792145, 198.44780158996582], 'gate_24': [300.8309180736542, 306.9479932785034, 291.50154423713684, 279.6400992870331, 339.8325786590576, 296.12316036224365, 323.5502734184265, 283.6120744943619, 282.0226607322693, 290.8795071840286, 340.6225402355194, 284.3483191728592, 294.1676650047302, 312.12233328819275, 301.2096059322357, 302.7523822784424], 'gate_25': [213.4820773601532, 218.20799720287323, 197.02273881435394, 223.28713834285736, 177.2047120332718, 202.3722882270813, 209.5419499874115, 244.57658755779266, 210.21045994758606, 201.982235789299, 207.10834109783173, 194.17071986198425, 206.31709706783295, 191.6612708568573, 226.44749748706818, 250.44800889492035], 'gate_26': [232.51770496368408, 225.31877064704895, 220.0389734506607, 224.75741243362427, 218.72060179710388, 228.48564839363098, 219.11324560642242, 221.8437852859497, 227.1184619665146, 239.10245776176453, 281.64393866062164, 211.72109735012054, 218.65574717521667, 234.14996123313904, 229.98712372779846, 225.31604838371277], 'gate_27': [201.7092660665512, 208.6942048072815, 204.6682459115982, 215.63237726688385, 201.20955514907837, 200.92854762077332, 220.73446118831635, 213.92966413497925, 202.76720690727234, 216.13627970218658, 177.4655681848526, 208.5226525068283, 207.39869701862335, 232.24456644058228, 203.0385843515396, 204.44910776615143], 'gate_28': [262.6080673933029, 271.18580543994904, 310.35455083847046, 260.5741186141968, 271.1687433719635, 264.7134962081909, 316.3416357040405, 274.75066471099854, 261.43794083595276, 261.67192482948303, 256.4815766811371, 241.33479022979736, 255.89150857925415, 242.95710492134094, 294.60696387290955, 265.0441623926163], 'gate_29': [659.3155002593994, 701.2020802497864, 654.6063270568848, 647.6133470535278, 647.2977514266968, 640.6768069267273, 659.4208550453186, 687.916356086731, 682.1207489967346, 657.3825387954712, 653.9346632957458, 642.3912782669067, 661.1935629844666, 662.840922832489, 676.9284625053406, 653.0358362197876], 'gate_30': [487.69743251800537, 470.8118906021118, 466.09358978271484, 449.8996171951294, 470.77901697158813, 472.89546823501587, 446.68706345558167, 475.0325779914856, 481.188597202301, 476.0990288257599, 460.0677812099457, 423.85263657569885, 462.8587098121643, 470.07510900497437, 478.4903974533081, 483.47697353363037], 'gate_31': [357.7453718185425, 369.49005699157715, 358.0618453025818, 338.2703547477722, 349.20234537124634, 364.40229749679565, 351.8476140499115, 334.088219165802, 353.16279888153076, 345.10497307777405, 390.92642402648926, 283.67029535770416, 358.9190921783447, 349.580913066864, 393.49659848213196, 358.88091135025024]}
{'gate_0': [0, 1, 0, 0, 0, 0, 94, 0, 0, 0, 0, 0, 0, 4, 0, 27], 'gate_1': [80, 0, 0, 0, 11, 0, 0, 35, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_2': [0, 0, 7, 0, 39, 25, 1, 0, 0, 10, 0, 0, 0, 0, 0, 44], 'gate_3': [0, 1, 0, 0, 0, 0, 0, 51, 16, 0, 0, 0, 58, 0, 0, 0], 'gate_4': [0, 0, 2, 55, 2, 0, 0, 0, 0, 0, 0, 56, 0, 0, 11, 0], 'gate_5': [1, 11, 0, 14, 0, 0, 36, 0, 33, 0, 4, 0, 0, 10, 17, 0], 'gate_6': [0, 1, 0, 27, 0, 0, 1, 0, 73, 0, 0, 21, 1, 0, 2, 0], 'gate_7': [7, 13, 6, 0, 0, 3, 3, 0, 80, 0, 0, 0, 14, 0, 0, 0], 'gate_8': [1, 0, 0, 1, 6, 8, 0, 0, 0, 103, 0, 0, 7, 0, 0, 0], 'gate_9': [0, 1, 0, 63, 0, 0, 8, 0, 5, 0, 0, 1, 42, 0, 1, 5], 'gate_10': [59, 1, 0, 0, 39, 0, 0, 0, 2, 3, 0, 0, 0, 0, 21, 1], 'gate_11': [0, 0, 0, 8, 58, 0, 0, 1, 0, 50, 0, 8, 1, 0, 0, 0], 'gate_12': [0, 7, 59, 27, 0, 1, 0, 31, 0, 0, 0, 0, 1, 0, 0, 0], 'gate_13': [3, 0, 0, 0, 0, 0, 0, 120, 0, 1, 0, 1, 0, 0, 1, 0], 'gate_14': [0, 0, 0, 0, 1, 20, 0, 65, 0, 0, 0, 0, 1, 4, 35, 0], 'gate_15': [0, 19, 2, 0, 5, 0, 39, 1, 0, 0, 0, 0, 1, 1, 0, 58], 'gate_16': [0, 0, 0, 0, 0, 0, 0, 0, 0, 124, 0, 0, 2, 0, 0, 0], 'gate_17': [0, 0, 1, 6, 0, 0, 0, 1, 0, 0, 118, 0, 0, 0, 0, 0], 'gate_18': [0, 0, 0, 0, 0, 0, 0, 5, 120, 0, 0, 0, 0, 0, 1, 0], 'gate_19': [7, 0, 0, 0, 0, 0, 0, 0, 46, 0, 0, 0, 73, 0, 0, 0], 'gate_20': [0, 0, 0, 0, 0, 1, 0, 121, 0, 0, 0, 0, 0, 4, 0, 0], 'gate_21': [0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 101, 0, 22, 0, 0], 'gate_22': [0, 0, 7, 0, 4, 0, 0, 0, 0, 1, 0, 103, 0, 0, 0, 11], 'gate_23': [122, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3], 'gate_24': [0, 0, 0, 0, 59, 0, 1, 0, 0, 0, 59, 1, 0, 0, 0, 6], 'gate_25': [0, 0, 0, 0, 0, 0, 0, 49, 0, 0, 0, 0, 0, 0, 12, 65], 'gate_26': [2, 0, 3, 0, 1, 0, 0, 0, 0, 0, 119, 0, 1, 0, 0, 0], 'gate_27': [0, 0, 0, 5, 0, 1, 4, 0, 0, 17, 0, 0, 0, 97, 1, 1], 'gate_28': [0, 0, 48, 0, 0, 0, 74, 0, 1, 0, 0, 0, 0, 0, 3, 0], 'gate_29': [0, 70, 0, 2, 0, 1, 0, 41, 12, 0, 0, 0, 0, 0, 0, 0], 'gate_30': [53, 1, 1, 0, 0, 0, 3, 0, 35, 6, 0, 0, 4, 0, 0, 23], 'gate_31': [1, 7, 0, 0, 0, 1, 0, 0, 4, 3, 50, 0, 1, 1, 55, 3]}
{'mmlu_formal_logic': {'alias': 'formal_logic', 'acc,none': 0.6507936507936508, 'acc_stderr,none': 0.04263906892795132}}
