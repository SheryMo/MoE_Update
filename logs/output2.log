/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|                                                                                                                                                         | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|█████████████████████████████                                                                                                                    | 1/5 [00:00<00:03,  1.10it/s]Loading checkpoint shards:  40%|██████████████████████████████████████████████████████████                                                                                       | 2/5 [00:01<00:02,  1.14it/s]Loading checkpoint shards:  60%|███████████████████████████████████████████████████████████████████████████████████████                                                          | 3/5 [00:02<00:01,  1.16it/s]Loading checkpoint shards:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                             | 4/5 [00:03<00:00,  1.15it/s]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.29it/s]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.22it/s]
Some weights of PhiMoEForCausalLM were not initialized from the model checkpoint at /root/autodl-tmp/lm-evaluation-harness/phiMergedMoE and are newly initialized: ['model.layers.0.block_sparse_moe.experts.1.w1.weight', 'model.layers.0.block_sparse_moe.experts.1.w2.weight', 'model.layers.0.block_sparse_moe.experts.1.w3.weight', 'model.layers.0.block_sparse_moe.experts.10.w1.weight', 'model.layers.0.block_sparse_moe.experts.10.w2.weight', 'model.layers.0.block_sparse_moe.experts.10.w3.weight', 'model.layers.0.block_sparse_moe.experts.11.w1.weight', 'model.layers.0.block_sparse_moe.experts.11.w2.weight', 'model.layers.0.block_sparse_moe.experts.11.w3.weight', 'model.layers.0.block_sparse_moe.experts.12.w1.weight', 'model.layers.0.block_sparse_moe.experts.12.w2.weight', 'model.layers.0.block_sparse_moe.experts.12.w3.weight', 'model.layers.0.block_sparse_moe.experts.14.w1.weight', 'model.layers.0.block_sparse_moe.experts.14.w2.weight', 'model.layers.0.block_sparse_moe.experts.14.w3.weight', 'model.layers.0.block_sparse_moe.experts.2.w1.weight', 'model.layers.0.block_sparse_moe.experts.2.w2.weight', 'model.layers.0.block_sparse_moe.experts.2.w3.weight', 'model.layers.0.block_sparse_moe.experts.3.w1.weight', 'model.layers.0.block_sparse_moe.experts.3.w2.weight', 'model.layers.0.block_sparse_moe.experts.3.w3.weight', 'model.layers.0.block_sparse_moe.experts.4.w1.weight', 'model.layers.0.block_sparse_moe.experts.4.w2.weight', 'model.layers.0.block_sparse_moe.experts.4.w3.weight', 'model.layers.0.block_sparse_moe.experts.5.w1.weight', 'model.layers.0.block_sparse_moe.experts.5.w2.weight', 'model.layers.0.block_sparse_moe.experts.5.w3.weight', 'model.layers.0.block_sparse_moe.experts.7.w1.weight', 'model.layers.0.block_sparse_moe.experts.7.w2.weight', 'model.layers.0.block_sparse_moe.experts.7.w3.weight', 'model.layers.0.block_sparse_moe.experts.8.w1.weight', 'model.layers.0.block_sparse_moe.experts.8.w2.weight', 'model.layers.0.block_sparse_moe.experts.8.w3.weight', 'model.layers.0.block_sparse_moe.experts.9.w1.weight', 'model.layers.0.block_sparse_moe.experts.9.w2.weight', 'model.layers.0.block_sparse_moe.experts.9.w3.weight', 'model.layers.1.block_sparse_moe.experts.10.w1.weight', 'model.layers.1.block_sparse_moe.experts.10.w2.weight', 'model.layers.1.block_sparse_moe.experts.10.w3.weight', 'model.layers.1.block_sparse_moe.experts.11.w1.weight', 'model.layers.1.block_sparse_moe.experts.11.w2.weight', 'model.layers.1.block_sparse_moe.experts.11.w3.weight', 'model.layers.1.block_sparse_moe.experts.12.w1.weight', 'model.layers.1.block_sparse_moe.experts.12.w2.weight', 'model.layers.1.block_sparse_moe.experts.12.w3.weight', 'model.layers.1.block_sparse_moe.experts.13.w1.weight', 'model.layers.1.block_sparse_moe.experts.13.w2.weight', 'model.layers.1.block_sparse_moe.experts.13.w3.weight', 'model.layers.1.block_sparse_moe.experts.14.w1.weight', 'model.layers.1.block_sparse_moe.experts.14.w2.weight', 'model.layers.1.block_sparse_moe.experts.14.w3.weight', 'model.layers.1.block_sparse_moe.experts.15.w1.weight', 'model.layers.1.block_sparse_moe.experts.15.w2.weight', 'model.layers.1.block_sparse_moe.experts.15.w3.weight', 'model.layers.1.block_sparse_moe.experts.2.w1.weight', 'model.layers.1.block_sparse_moe.experts.2.w2.weight', 'model.layers.1.block_sparse_moe.experts.2.w3.weight', 'model.layers.1.block_sparse_moe.experts.3.w1.weight', 'model.layers.1.block_sparse_moe.experts.3.w2.weight', 'model.layers.1.block_sparse_moe.experts.3.w3.weight', 'model.layers.1.block_sparse_moe.experts.5.w1.weight', 'model.layers.1.block_sparse_moe.experts.5.w2.weight', 'model.layers.1.block_sparse_moe.experts.5.w3.weight', 'model.layers.1.block_sparse_moe.experts.6.w1.weight', 'model.layers.1.block_sparse_moe.experts.6.w2.weight', 'model.layers.1.block_sparse_moe.experts.6.w3.weight', 'model.layers.1.block_sparse_moe.experts.8.w1.weight', 'model.layers.1.block_sparse_moe.experts.8.w2.weight', 'model.layers.1.block_sparse_moe.experts.8.w3.weight', 'model.layers.1.block_sparse_moe.experts.9.w1.weight', 'model.layers.1.block_sparse_moe.experts.9.w2.weight', 'model.layers.1.block_sparse_moe.experts.9.w3.weight', 'model.layers.10.block_sparse_moe.experts.10.w1.weight', 'model.layers.10.block_sparse_moe.experts.10.w2.weight', 'model.layers.10.block_sparse_moe.experts.10.w3.weight', 'model.layers.10.block_sparse_moe.experts.12.w1.weight', 'model.layers.10.block_sparse_moe.experts.12.w2.weight', 'model.layers.10.block_sparse_moe.experts.12.w3.weight', 'model.layers.10.block_sparse_moe.experts.13.w1.weight', 'model.layers.10.block_sparse_moe.experts.13.w2.weight', 'model.layers.10.block_sparse_moe.experts.13.w3.weight', 'model.layers.10.block_sparse_moe.experts.14.w1.weight', 'model.layers.10.block_sparse_moe.experts.14.w2.weight', 'model.layers.10.block_sparse_moe.experts.14.w3.weight', 'model.layers.10.block_sparse_moe.experts.15.w1.weight', 'model.layers.10.block_sparse_moe.experts.15.w2.weight', 'model.layers.10.block_sparse_moe.experts.15.w3.weight', 'model.layers.10.block_sparse_moe.experts.2.w1.weight', 'model.layers.10.block_sparse_moe.experts.2.w2.weight', 'model.layers.10.block_sparse_moe.experts.2.w3.weight', 'model.layers.10.block_sparse_moe.experts.3.w1.weight', 'model.layers.10.block_sparse_moe.experts.3.w2.weight', 'model.layers.10.block_sparse_moe.experts.3.w3.weight', 'model.layers.10.block_sparse_moe.experts.5.w1.weight', 'model.layers.10.block_sparse_moe.experts.5.w2.weight', 'model.layers.10.block_sparse_moe.experts.5.w3.weight', 'model.layers.10.block_sparse_moe.experts.6.w1.weight', 'model.layers.10.block_sparse_moe.experts.6.w2.weight', 'model.layers.10.block_sparse_moe.experts.6.w3.weight', 'model.layers.10.block_sparse_moe.experts.7.w1.weight', 'model.layers.10.block_sparse_moe.experts.7.w2.weight', 'model.layers.10.block_sparse_moe.experts.7.w3.weight', 'model.layers.10.block_sparse_moe.experts.8.w1.weight', 'model.layers.10.block_sparse_moe.experts.8.w2.weight', 'model.layers.10.block_sparse_moe.experts.8.w3.weight', 'model.layers.10.block_sparse_moe.experts.9.w1.weight', 'model.layers.10.block_sparse_moe.experts.9.w2.weight', 'model.layers.10.block_sparse_moe.experts.9.w3.weight', 'model.layers.11.block_sparse_moe.experts.1.w1.weight', 'model.layers.11.block_sparse_moe.experts.1.w2.weight', 'model.layers.11.block_sparse_moe.experts.1.w3.weight', 'model.layers.11.block_sparse_moe.experts.10.w1.weight', 'model.layers.11.block_sparse_moe.experts.10.w2.weight', 'model.layers.11.block_sparse_moe.experts.10.w3.weight', 'model.layers.11.block_sparse_moe.experts.11.w1.weight', 'model.layers.11.block_sparse_moe.experts.11.w2.weight', 'model.layers.11.block_sparse_moe.experts.11.w3.weight', 'model.layers.11.block_sparse_moe.experts.12.w1.weight', 'model.layers.11.block_sparse_moe.experts.12.w2.weight', 'model.layers.11.block_sparse_moe.experts.12.w3.weight', 'model.layers.11.block_sparse_moe.experts.13.w1.weight', 'model.layers.11.block_sparse_moe.experts.13.w2.weight', 'model.layers.11.block_sparse_moe.experts.13.w3.weight', 'model.layers.11.block_sparse_moe.experts.14.w1.weight', 'model.layers.11.block_sparse_moe.experts.14.w2.weight', 'model.layers.11.block_sparse_moe.experts.14.w3.weight', 'model.layers.11.block_sparse_moe.experts.15.w1.weight', 'model.layers.11.block_sparse_moe.experts.15.w2.weight', 'model.layers.11.block_sparse_moe.experts.15.w3.weight', 'model.layers.11.block_sparse_moe.experts.2.w1.weight', 'model.layers.11.block_sparse_moe.experts.2.w2.weight', 'model.layers.11.block_sparse_moe.experts.2.w3.weight', 'model.layers.11.block_sparse_moe.experts.5.w1.weight', 'model.layers.11.block_sparse_moe.experts.5.w2.weight', 'model.layers.11.block_sparse_moe.experts.5.w3.weight', 'model.layers.11.block_sparse_moe.experts.6.w1.weight', 'model.layers.11.block_sparse_moe.experts.6.w2.weight', 'model.layers.11.block_sparse_moe.experts.6.w3.weight', 'model.layers.11.block_sparse_moe.experts.7.w1.weight', 'model.layers.11.block_sparse_moe.experts.7.w2.weight', 'model.layers.11.block_sparse_moe.experts.7.w3.weight', 'model.layers.11.block_sparse_moe.experts.8.w1.weight', 'model.layers.11.block_sparse_moe.experts.8.w2.weight', 'model.layers.11.block_sparse_moe.experts.8.w3.weight', 'model.layers.12.block_sparse_moe.experts.1.w1.weight', 'model.layers.12.block_sparse_moe.experts.1.w2.weight', 'model.layers.12.block_sparse_moe.experts.1.w3.weight', 'model.layers.12.block_sparse_moe.experts.10.w1.weight', 'model.layers.12.block_sparse_moe.experts.10.w2.weight', 'model.layers.12.block_sparse_moe.experts.10.w3.weight', 'model.layers.12.block_sparse_moe.experts.11.w1.weight', 'model.layers.12.block_sparse_moe.experts.11.w2.weight', 'model.layers.12.block_sparse_moe.experts.11.w3.weight', 'model.layers.12.block_sparse_moe.experts.12.w1.weight', 'model.layers.12.block_sparse_moe.experts.12.w2.weight', 'model.layers.12.block_sparse_moe.experts.12.w3.weight', 'model.layers.12.block_sparse_moe.experts.13.w1.weight', 'model.layers.12.block_sparse_moe.experts.13.w2.weight', 'model.layers.12.block_sparse_moe.experts.13.w3.weight', 'model.layers.12.block_sparse_moe.experts.14.w1.weight', 'model.layers.12.block_sparse_moe.experts.14.w2.weight', 'model.layers.12.block_sparse_moe.experts.14.w3.weight', 'model.layers.12.block_sparse_moe.experts.15.w1.weight', 'model.layers.12.block_sparse_moe.experts.15.w2.weight', 'model.layers.12.block_sparse_moe.experts.15.w3.weight', 'model.layers.12.block_sparse_moe.experts.4.w1.weight', 'model.layers.12.block_sparse_moe.experts.4.w2.weight', 'model.layers.12.block_sparse_moe.experts.4.w3.weight', 'model.layers.12.block_sparse_moe.experts.5.w1.weight', 'model.layers.12.block_sparse_moe.experts.5.w2.weight', 'model.layers.12.block_sparse_moe.experts.5.w3.weight', 'model.layers.12.block_sparse_moe.experts.6.w1.weight', 'model.layers.12.block_sparse_moe.experts.6.w2.weight', 'model.layers.12.block_sparse_moe.experts.6.w3.weight', 'model.layers.12.block_sparse_moe.experts.8.w1.weight', 'model.layers.12.block_sparse_moe.experts.8.w2.weight', 'model.layers.12.block_sparse_moe.experts.8.w3.weight', 'model.layers.12.block_sparse_moe.experts.9.w1.weight', 'model.layers.12.block_sparse_moe.experts.9.w2.weight', 'model.layers.12.block_sparse_moe.experts.9.w3.weight', 'model.layers.13.block_sparse_moe.experts.1.w1.weight', 'model.layers.13.block_sparse_moe.experts.1.w2.weight', 'model.layers.13.block_sparse_moe.experts.1.w3.weight', 'model.layers.13.block_sparse_moe.experts.12.w1.weight', 'model.layers.13.block_sparse_moe.experts.12.w2.weight', 'model.layers.13.block_sparse_moe.experts.12.w3.weight', 'model.layers.13.block_sparse_moe.experts.13.w1.weight', 'model.layers.13.block_sparse_moe.experts.13.w2.weight', 'model.layers.13.block_sparse_moe.experts.13.w3.weight', 'model.layers.13.block_sparse_moe.experts.14.w1.weight', 'model.layers.13.block_sparse_moe.experts.14.w2.weight', 'model.layers.13.block_sparse_moe.experts.14.w3.weight', 'model.layers.13.block_sparse_moe.experts.15.w1.weight', 'model.layers.13.block_sparse_moe.experts.15.w2.weight', 'model.layers.13.block_sparse_moe.experts.15.w3.weight', 'model.layers.13.block_sparse_moe.experts.2.w1.weight', 'model.layers.13.block_sparse_moe.experts.2.w2.weight', 'model.layers.13.block_sparse_moe.experts.2.w3.weight', 'model.layers.13.block_sparse_moe.experts.3.w1.weight', 'model.layers.13.block_sparse_moe.experts.3.w2.weight', 'model.layers.13.block_sparse_moe.experts.3.w3.weight', 'model.layers.13.block_sparse_moe.experts.4.w1.weight', 'model.layers.13.block_sparse_moe.experts.4.w2.weight', 'model.layers.13.block_sparse_moe.experts.4.w3.weight', 'model.layers.13.block_sparse_moe.experts.5.w1.weight', 'model.layers.13.block_sparse_moe.experts.5.w2.weight', 'model.layers.13.block_sparse_moe.experts.5.w3.weight', 'model.layers.13.block_sparse_moe.experts.6.w1.weight', 'model.layers.13.block_sparse_moe.experts.6.w2.weight', 'model.layers.13.block_sparse_moe.experts.6.w3.weight', 'model.layers.13.block_sparse_moe.experts.8.w1.weight', 'model.layers.13.block_sparse_moe.experts.8.w2.weight', 'model.layers.13.block_sparse_moe.experts.8.w3.weight', 'model.layers.13.block_sparse_moe.experts.9.w1.weight', 'model.layers.13.block_sparse_moe.experts.9.w2.weight', 'model.layers.13.block_sparse_moe.experts.9.w3.weight', 'model.layers.14.block_sparse_moe.experts.1.w1.weight', 'model.layers.14.block_sparse_moe.experts.1.w2.weight', 'model.layers.14.block_sparse_moe.experts.1.w3.weight', 'model.layers.14.block_sparse_moe.experts.10.w1.weight', 'model.layers.14.block_sparse_moe.experts.10.w2.weight', 'model.layers.14.block_sparse_moe.experts.10.w3.weight', 'model.layers.14.block_sparse_moe.experts.11.w1.weight', 'model.layers.14.block_sparse_moe.experts.11.w2.weight', 'model.layers.14.block_sparse_moe.experts.11.w3.weight', 'model.layers.14.block_sparse_moe.experts.12.w1.weight', 'model.layers.14.block_sparse_moe.experts.12.w2.weight', 'model.layers.14.block_sparse_moe.experts.12.w3.weight', 'model.layers.14.block_sparse_moe.experts.15.w1.weight', 'model.layers.14.block_sparse_moe.experts.15.w2.weight', 'model.layers.14.block_sparse_moe.experts.15.w3.weight', 'model.layers.14.block_sparse_moe.experts.2.w1.weight', 'model.layers.14.block_sparse_moe.experts.2.w2.weight', 'model.layers.14.block_sparse_moe.experts.2.w3.weight', 'model.layers.14.block_sparse_moe.experts.3.w1.weight', 'model.layers.14.block_sparse_moe.experts.3.w2.weight', 'model.layers.14.block_sparse_moe.experts.3.w3.weight', 'model.layers.14.block_sparse_moe.experts.4.w1.weight', 'model.layers.14.block_sparse_moe.experts.4.w2.weight', 'model.layers.14.block_sparse_moe.experts.4.w3.weight', 'model.layers.14.block_sparse_moe.experts.5.w1.weight', 'model.layers.14.block_sparse_moe.experts.5.w2.weight', 'model.layers.14.block_sparse_moe.experts.5.w3.weight', 'model.layers.14.block_sparse_moe.experts.6.w1.weight', 'model.layers.14.block_sparse_moe.experts.6.w2.weight', 'model.layers.14.block_sparse_moe.experts.6.w3.weight', 'model.layers.14.block_sparse_moe.experts.8.w1.weight', 'model.layers.14.block_sparse_moe.experts.8.w2.weight', 'model.layers.14.block_sparse_moe.experts.8.w3.weight', 'model.layers.14.block_sparse_moe.experts.9.w1.weight', 'model.layers.14.block_sparse_moe.experts.9.w2.weight', 'model.layers.14.block_sparse_moe.experts.9.w3.weight', 'model.layers.15.block_sparse_moe.experts.10.w1.weight', 'model.layers.15.block_sparse_moe.experts.10.w2.weight', 'model.layers.15.block_sparse_moe.experts.10.w3.weight', 'model.layers.15.block_sparse_moe.experts.11.w1.weight', 'model.layers.15.block_sparse_moe.experts.11.w2.weight', 'model.layers.15.block_sparse_moe.experts.11.w3.weight', 'model.layers.15.block_sparse_moe.experts.12.w1.weight', 'model.layers.15.block_sparse_moe.experts.12.w2.weight', 'model.layers.15.block_sparse_moe.experts.12.w3.weight', 'model.layers.15.block_sparse_moe.experts.13.w1.weight', 'model.layers.15.block_sparse_moe.experts.13.w2.weight', 'model.layers.15.block_sparse_moe.experts.13.w3.weight', 'model.layers.15.block_sparse_moe.experts.14.w1.weight', 'model.layers.15.block_sparse_moe.experts.14.w2.weight', 'model.layers.15.block_sparse_moe.experts.14.w3.weight', 'model.layers.15.block_sparse_moe.experts.2.w1.weight', 'model.layers.15.block_sparse_moe.experts.2.w2.weight', 'model.layers.15.block_sparse_moe.experts.2.w3.weight', 'model.layers.15.block_sparse_moe.experts.3.w1.weight', 'model.layers.15.block_sparse_moe.experts.3.w2.weight', 'model.layers.15.block_sparse_moe.experts.3.w3.weight', 'model.layers.15.block_sparse_moe.experts.4.w1.weight', 'model.layers.15.block_sparse_moe.experts.4.w2.weight', 'model.layers.15.block_sparse_moe.experts.4.w3.weight', 'model.layers.15.block_sparse_moe.experts.5.w1.weight', 'model.layers.15.block_sparse_moe.experts.5.w2.weight', 'model.layers.15.block_sparse_moe.experts.5.w3.weight', 'model.layers.15.block_sparse_moe.experts.7.w1.weight', 'model.layers.15.block_sparse_moe.experts.7.w2.weight', 'model.layers.15.block_sparse_moe.experts.7.w3.weight', 'model.layers.15.block_sparse_moe.experts.8.w1.weight', 'model.layers.15.block_sparse_moe.experts.8.w2.weight', 'model.layers.15.block_sparse_moe.experts.8.w3.weight', 'model.layers.15.block_sparse_moe.experts.9.w1.weight', 'model.layers.15.block_sparse_moe.experts.9.w2.weight', 'model.layers.15.block_sparse_moe.experts.9.w3.weight', 'model.layers.16.block_sparse_moe.experts.1.w1.weight', 'model.layers.16.block_sparse_moe.experts.1.w2.weight', 'model.layers.16.block_sparse_moe.experts.1.w3.weight', 'model.layers.16.block_sparse_moe.experts.10.w1.weight', 'model.layers.16.block_sparse_moe.experts.10.w2.weight', 'model.layers.16.block_sparse_moe.experts.10.w3.weight', 'model.layers.16.block_sparse_moe.experts.11.w1.weight', 'model.layers.16.block_sparse_moe.experts.11.w2.weight', 'model.layers.16.block_sparse_moe.experts.11.w3.weight', 'model.layers.16.block_sparse_moe.experts.14.w1.weight', 'model.layers.16.block_sparse_moe.experts.14.w2.weight', 'model.layers.16.block_sparse_moe.experts.14.w3.weight', 'model.layers.16.block_sparse_moe.experts.15.w1.weight', 'model.layers.16.block_sparse_moe.experts.15.w2.weight', 'model.layers.16.block_sparse_moe.experts.15.w3.weight', 'model.layers.16.block_sparse_moe.experts.2.w1.weight', 'model.layers.16.block_sparse_moe.experts.2.w2.weight', 'model.layers.16.block_sparse_moe.experts.2.w3.weight', 'model.layers.16.block_sparse_moe.experts.3.w1.weight', 'model.layers.16.block_sparse_moe.experts.3.w2.weight', 'model.layers.16.block_sparse_moe.experts.3.w3.weight', 'model.layers.16.block_sparse_moe.experts.4.w1.weight', 'model.layers.16.block_sparse_moe.experts.4.w2.weight', 'model.layers.16.block_sparse_moe.experts.4.w3.weight', 'model.layers.16.block_sparse_moe.experts.5.w1.weight', 'model.layers.16.block_sparse_moe.experts.5.w2.weight', 'model.layers.16.block_sparse_moe.experts.5.w3.weight', 'model.layers.16.block_sparse_moe.experts.6.w1.weight', 'model.layers.16.block_sparse_moe.experts.6.w2.weight', 'model.layers.16.block_sparse_moe.experts.6.w3.weight', 'model.layers.16.block_sparse_moe.experts.7.w1.weight', 'model.layers.16.block_sparse_moe.experts.7.w2.weight', 'model.layers.16.block_sparse_moe.experts.7.w3.weight', 'model.layers.16.block_sparse_moe.experts.8.w1.weight', 'model.layers.16.block_sparse_moe.experts.8.w2.weight', 'model.layers.16.block_sparse_moe.experts.8.w3.weight', 'model.layers.17.block_sparse_moe.experts.1.w1.weight', 'model.layers.17.block_sparse_moe.experts.1.w2.weight', 'model.layers.17.block_sparse_moe.experts.1.w3.weight', 'model.layers.17.block_sparse_moe.experts.12.w1.weight', 'model.layers.17.block_sparse_moe.experts.12.w2.weight', 'model.layers.17.block_sparse_moe.experts.12.w3.weight', 'model.layers.17.block_sparse_moe.experts.13.w1.weight', 'model.layers.17.block_sparse_moe.experts.13.w2.weight', 'model.layers.17.block_sparse_moe.experts.13.w3.weight', 'model.layers.17.block_sparse_moe.experts.14.w1.weight', 'model.layers.17.block_sparse_moe.experts.14.w2.weight', 'model.layers.17.block_sparse_moe.experts.14.w3.weight', 'model.layers.17.block_sparse_moe.experts.15.w1.weight', 'model.layers.17.block_sparse_moe.experts.15.w2.weight', 'model.layers.17.block_sparse_moe.experts.15.w3.weight', 'model.layers.17.block_sparse_moe.experts.2.w1.weight', 'model.layers.17.block_sparse_moe.experts.2.w2.weight', 'model.layers.17.block_sparse_moe.experts.2.w3.weight', 'model.layers.17.block_sparse_moe.experts.3.w1.weight', 'model.layers.17.block_sparse_moe.experts.3.w2.weight', 'model.layers.17.block_sparse_moe.experts.3.w3.weight', 'model.layers.17.block_sparse_moe.experts.4.w1.weight', 'model.layers.17.block_sparse_moe.experts.4.w2.weight', 'model.layers.17.block_sparse_moe.experts.4.w3.weight', 'model.layers.17.block_sparse_moe.experts.5.w1.weight', 'model.layers.17.block_sparse_moe.experts.5.w2.weight', 'model.layers.17.block_sparse_moe.experts.5.w3.weight', 'model.layers.17.block_sparse_moe.experts.6.w1.weight', 'model.layers.17.block_sparse_moe.experts.6.w2.weight', 'model.layers.17.block_sparse_moe.experts.6.w3.weight', 'model.layers.17.block_sparse_moe.experts.7.w1.weight', 'model.layers.17.block_sparse_moe.experts.7.w2.weight', 'model.layers.17.block_sparse_moe.experts.7.w3.weight', 'model.layers.17.block_sparse_moe.experts.9.w1.weight', 'model.layers.17.block_sparse_moe.experts.9.w2.weight', 'model.layers.17.block_sparse_moe.experts.9.w3.weight', 'model.layers.18.block_sparse_moe.experts.1.w1.weight', 'model.layers.18.block_sparse_moe.experts.1.w2.weight', 'model.layers.18.block_sparse_moe.experts.1.w3.weight', 'model.layers.18.block_sparse_moe.experts.10.w1.weight', 'model.layers.18.block_sparse_moe.experts.10.w2.weight', 'model.layers.18.block_sparse_moe.experts.10.w3.weight', 'model.layers.18.block_sparse_moe.experts.11.w1.weight', 'model.layers.18.block_sparse_moe.experts.11.w2.weight', 'model.layers.18.block_sparse_moe.experts.11.w3.weight', 'model.layers.18.block_sparse_moe.experts.12.w1.weight', 'model.layers.18.block_sparse_moe.experts.12.w2.weight', 'model.layers.18.block_sparse_moe.experts.12.w3.weight', 'model.layers.18.block_sparse_moe.experts.13.w1.weight', 'model.layers.18.block_sparse_moe.experts.13.w2.weight', 'model.layers.18.block_sparse_moe.experts.13.w3.weight', 'model.layers.18.block_sparse_moe.experts.14.w1.weight', 'model.layers.18.block_sparse_moe.experts.14.w2.weight', 'model.layers.18.block_sparse_moe.experts.14.w3.weight', 'model.layers.18.block_sparse_moe.experts.15.w1.weight', 'model.layers.18.block_sparse_moe.experts.15.w2.weight', 'model.layers.18.block_sparse_moe.experts.15.w3.weight', 'model.layers.18.block_sparse_moe.experts.2.w1.weight', 'model.layers.18.block_sparse_moe.experts.2.w2.weight', 'model.layers.18.block_sparse_moe.experts.2.w3.weight', 'model.layers.18.block_sparse_moe.experts.3.w1.weight', 'model.layers.18.block_sparse_moe.experts.3.w2.weight', 'model.layers.18.block_sparse_moe.experts.3.w3.weight', 'model.layers.18.block_sparse_moe.experts.4.w1.weight', 'model.layers.18.block_sparse_moe.experts.4.w2.weight', 'model.layers.18.block_sparse_moe.experts.4.w3.weight', 'model.layers.18.block_sparse_moe.experts.5.w1.weight', 'model.layers.18.block_sparse_moe.experts.5.w2.weight', 'model.layers.18.block_sparse_moe.experts.5.w3.weight', 'model.layers.18.block_sparse_moe.experts.9.w1.weight', 'model.layers.18.block_sparse_moe.experts.9.w2.weight', 'model.layers.18.block_sparse_moe.experts.9.w3.weight', 'model.layers.19.block_sparse_moe.experts.1.w1.weight', 'model.layers.19.block_sparse_moe.experts.1.w2.weight', 'model.layers.19.block_sparse_moe.experts.1.w3.weight', 'model.layers.19.block_sparse_moe.experts.11.w1.weight', 'model.layers.19.block_sparse_moe.experts.11.w2.weight', 'model.layers.19.block_sparse_moe.experts.11.w3.weight', 'model.layers.19.block_sparse_moe.experts.13.w1.weight', 'model.layers.19.block_sparse_moe.experts.13.w2.weight', 'model.layers.19.block_sparse_moe.experts.13.w3.weight', 'model.layers.19.block_sparse_moe.experts.14.w1.weight', 'model.layers.19.block_sparse_moe.experts.14.w2.weight', 'model.layers.19.block_sparse_moe.experts.14.w3.weight', 'model.layers.19.block_sparse_moe.experts.15.w1.weight', 'model.layers.19.block_sparse_moe.experts.15.w2.weight', 'model.layers.19.block_sparse_moe.experts.15.w3.weight', 'model.layers.19.block_sparse_moe.experts.2.w1.weight', 'model.layers.19.block_sparse_moe.experts.2.w2.weight', 'model.layers.19.block_sparse_moe.experts.2.w3.weight', 'model.layers.19.block_sparse_moe.experts.3.w1.weight', 'model.layers.19.block_sparse_moe.experts.3.w2.weight', 'model.layers.19.block_sparse_moe.experts.3.w3.weight', 'model.layers.19.block_sparse_moe.experts.4.w1.weight', 'model.layers.19.block_sparse_moe.experts.4.w2.weight', 'model.layers.19.block_sparse_moe.experts.4.w3.weight', 'model.layers.19.block_sparse_moe.experts.5.w1.weight', 'model.layers.19.block_sparse_moe.experts.5.w2.weight', 'model.layers.19.block_sparse_moe.experts.5.w3.weight', 'model.layers.19.block_sparse_moe.experts.6.w1.weight', 'model.layers.19.block_sparse_moe.experts.6.w2.weight', 'model.layers.19.block_sparse_moe.experts.6.w3.weight', 'model.layers.19.block_sparse_moe.experts.7.w1.weight', 'model.layers.19.block_sparse_moe.experts.7.w2.weight', 'model.layers.19.block_sparse_moe.experts.7.w3.weight', 'model.layers.19.block_sparse_moe.experts.9.w1.weight', 'model.layers.19.block_sparse_moe.experts.9.w2.weight', 'model.layers.19.block_sparse_moe.experts.9.w3.weight', 'model.layers.2.block_sparse_moe.experts.1.w1.weight', 'model.layers.2.block_sparse_moe.experts.1.w2.weight', 'model.layers.2.block_sparse_moe.experts.1.w3.weight', 'model.layers.2.block_sparse_moe.experts.10.w1.weight', 'model.layers.2.block_sparse_moe.experts.10.w2.weight', 'model.layers.2.block_sparse_moe.experts.10.w3.weight', 'model.layers.2.block_sparse_moe.experts.11.w1.weight', 'model.layers.2.block_sparse_moe.experts.11.w2.weight', 'model.layers.2.block_sparse_moe.experts.11.w3.weight', 'model.layers.2.block_sparse_moe.experts.12.w1.weight', 'model.layers.2.block_sparse_moe.experts.12.w2.weight', 'model.layers.2.block_sparse_moe.experts.12.w3.weight', 'model.layers.2.block_sparse_moe.experts.13.w1.weight', 'model.layers.2.block_sparse_moe.experts.13.w2.weight', 'model.layers.2.block_sparse_moe.experts.13.w3.weight', 'model.layers.2.block_sparse_moe.experts.14.w1.weight', 'model.layers.2.block_sparse_moe.experts.14.w2.weight', 'model.layers.2.block_sparse_moe.experts.14.w3.weight', 'model.layers.2.block_sparse_moe.experts.2.w1.weight', 'model.layers.2.block_sparse_moe.experts.2.w2.weight', 'model.layers.2.block_sparse_moe.experts.2.w3.weight', 'model.layers.2.block_sparse_moe.experts.3.w1.weight', 'model.layers.2.block_sparse_moe.experts.3.w2.weight', 'model.layers.2.block_sparse_moe.experts.3.w3.weight', 'model.layers.2.block_sparse_moe.experts.6.w1.weight', 'model.layers.2.block_sparse_moe.experts.6.w2.weight', 'model.layers.2.block_sparse_moe.experts.6.w3.weight', 'model.layers.2.block_sparse_moe.experts.7.w1.weight', 'model.layers.2.block_sparse_moe.experts.7.w2.weight', 'model.layers.2.block_sparse_moe.experts.7.w3.weight', 'model.layers.2.block_sparse_moe.experts.8.w1.weight', 'model.layers.2.block_sparse_moe.experts.8.w2.weight', 'model.layers.2.block_sparse_moe.experts.8.w3.weight', 'model.layers.2.block_sparse_moe.experts.9.w1.weight', 'model.layers.2.block_sparse_moe.experts.9.w2.weight', 'model.layers.2.block_sparse_moe.experts.9.w3.weight', 'model.layers.20.block_sparse_moe.experts.1.w1.weight', 'model.layers.20.block_sparse_moe.experts.1.w2.weight', 'model.layers.20.block_sparse_moe.experts.1.w3.weight', 'model.layers.20.block_sparse_moe.experts.10.w1.weight', 'model.layers.20.block_sparse_moe.experts.10.w2.weight', 'model.layers.20.block_sparse_moe.experts.10.w3.weight', 'model.layers.20.block_sparse_moe.experts.11.w1.weight', 'model.layers.20.block_sparse_moe.experts.11.w2.weight', 'model.layers.20.block_sparse_moe.experts.11.w3.weight', 'model.layers.20.block_sparse_moe.experts.12.w1.weight', 'model.layers.20.block_sparse_moe.experts.12.w2.weight', 'model.layers.20.block_sparse_moe.experts.12.w3.weight', 'model.layers.20.block_sparse_moe.experts.14.w1.weight', 'model.layers.20.block_sparse_moe.experts.14.w2.weight', 'model.layers.20.block_sparse_moe.experts.14.w3.weight', 'model.layers.20.block_sparse_moe.experts.15.w1.weight', 'model.layers.20.block_sparse_moe.experts.15.w2.weight', 'model.layers.20.block_sparse_moe.experts.15.w3.weight', 'model.layers.20.block_sparse_moe.experts.2.w1.weight', 'model.layers.20.block_sparse_moe.experts.2.w2.weight', 'model.layers.20.block_sparse_moe.experts.2.w3.weight', 'model.layers.20.block_sparse_moe.experts.3.w1.weight', 'model.layers.20.block_sparse_moe.experts.3.w2.weight', 'model.layers.20.block_sparse_moe.experts.3.w3.weight', 'model.layers.20.block_sparse_moe.experts.4.w1.weight', 'model.layers.20.block_sparse_moe.experts.4.w2.weight', 'model.layers.20.block_sparse_moe.experts.4.w3.weight', 'model.layers.20.block_sparse_moe.experts.6.w1.weight', 'model.layers.20.block_sparse_moe.experts.6.w2.weight', 'model.layers.20.block_sparse_moe.experts.6.w3.weight', 'model.layers.20.block_sparse_moe.experts.8.w1.weight', 'model.layers.20.block_sparse_moe.experts.8.w2.weight', 'model.layers.20.block_sparse_moe.experts.8.w3.weight', 'model.layers.20.block_sparse_moe.experts.9.w1.weight', 'model.layers.20.block_sparse_moe.experts.9.w2.weight', 'model.layers.20.block_sparse_moe.experts.9.w3.weight', 'model.layers.21.block_sparse_moe.experts.1.w1.weight', 'model.layers.21.block_sparse_moe.experts.1.w2.weight', 'model.layers.21.block_sparse_moe.experts.1.w3.weight', 'model.layers.21.block_sparse_moe.experts.12.w1.weight', 'model.layers.21.block_sparse_moe.experts.12.w2.weight', 'model.layers.21.block_sparse_moe.experts.12.w3.weight', 'model.layers.21.block_sparse_moe.experts.14.w1.weight', 'model.layers.21.block_sparse_moe.experts.14.w2.weight', 'model.layers.21.block_sparse_moe.experts.14.w3.weight', 'model.layers.21.block_sparse_moe.experts.15.w1.weight', 'model.layers.21.block_sparse_moe.experts.15.w2.weight', 'model.layers.21.block_sparse_moe.experts.15.w3.weight', 'model.layers.21.block_sparse_moe.experts.2.w1.weight', 'model.layers.21.block_sparse_moe.experts.2.w2.weight', 'model.layers.21.block_sparse_moe.experts.2.w3.weight', 'model.layers.21.block_sparse_moe.experts.3.w1.weight', 'model.layers.21.block_sparse_moe.experts.3.w2.weight', 'model.layers.21.block_sparse_moe.experts.3.w3.weight', 'model.layers.21.block_sparse_moe.experts.4.w1.weight', 'model.layers.21.block_sparse_moe.experts.4.w2.weight', 'model.layers.21.block_sparse_moe.experts.4.w3.weight', 'model.layers.21.block_sparse_moe.experts.5.w1.weight', 'model.layers.21.block_sparse_moe.experts.5.w2.weight', 'model.layers.21.block_sparse_moe.experts.5.w3.weight', 'model.layers.21.block_sparse_moe.experts.6.w1.weight', 'model.layers.21.block_sparse_moe.experts.6.w2.weight', 'model.layers.21.block_sparse_moe.experts.6.w3.weight', 'model.layers.21.block_sparse_moe.experts.7.w1.weight', 'model.layers.21.block_sparse_moe.experts.7.w2.weight', 'model.layers.21.block_sparse_moe.experts.7.w3.weight', 'model.layers.21.block_sparse_moe.experts.8.w1.weight', 'model.layers.21.block_sparse_moe.experts.8.w2.weight', 'model.layers.21.block_sparse_moe.experts.8.w3.weight', 'model.layers.21.block_sparse_moe.experts.9.w1.weight', 'model.layers.21.block_sparse_moe.experts.9.w2.weight', 'model.layers.21.block_sparse_moe.experts.9.w3.weight', 'model.layers.22.block_sparse_moe.experts.1.w1.weight', 'model.layers.22.block_sparse_moe.experts.1.w2.weight', 'model.layers.22.block_sparse_moe.experts.1.w3.weight', 'model.layers.22.block_sparse_moe.experts.10.w1.weight', 'model.layers.22.block_sparse_moe.experts.10.w2.weight', 'model.layers.22.block_sparse_moe.experts.10.w3.weight', 'model.layers.22.block_sparse_moe.experts.12.w1.weight', 'model.layers.22.block_sparse_moe.experts.12.w2.weight', 'model.layers.22.block_sparse_moe.experts.12.w3.weight', 'model.layers.22.block_sparse_moe.experts.13.w1.weight', 'model.layers.22.block_sparse_moe.experts.13.w2.weight', 'model.layers.22.block_sparse_moe.experts.13.w3.weight', 'model.layers.22.block_sparse_moe.experts.14.w1.weight', 'model.layers.22.block_sparse_moe.experts.14.w2.weight', 'model.layers.22.block_sparse_moe.experts.14.w3.weight', 'model.layers.22.block_sparse_moe.experts.3.w1.weight', 'model.layers.22.block_sparse_moe.experts.3.w2.weight', 'model.layers.22.block_sparse_moe.experts.3.w3.weight', 'model.layers.22.block_sparse_moe.experts.4.w1.weight', 'model.layers.22.block_sparse_moe.experts.4.w2.weight', 'model.layers.22.block_sparse_moe.experts.4.w3.weight', 'model.layers.22.block_sparse_moe.experts.5.w1.weight', 'model.layers.22.block_sparse_moe.experts.5.w2.weight', 'model.layers.22.block_sparse_moe.experts.5.w3.weight', 'model.layers.22.block_sparse_moe.experts.6.w1.weight', 'model.layers.22.block_sparse_moe.experts.6.w2.weight', 'model.layers.22.block_sparse_moe.experts.6.w3.weight', 'model.layers.22.block_sparse_moe.experts.7.w1.weight', 'model.layers.22.block_sparse_moe.experts.7.w2.weight', 'model.layers.22.block_sparse_moe.experts.7.w3.weight', 'model.layers.22.block_sparse_moe.experts.8.w1.weight', 'model.layers.22.block_sparse_moe.experts.8.w2.weight', 'model.layers.22.block_sparse_moe.experts.8.w3.weight', 'model.layers.22.block_sparse_moe.experts.9.w1.weight', 'model.layers.22.block_sparse_moe.experts.9.w2.weight', 'model.layers.22.block_sparse_moe.experts.9.w3.weight', 'model.layers.23.block_sparse_moe.experts.10.w1.weight', 'model.layers.23.block_sparse_moe.experts.10.w2.weight', 'model.layers.23.block_sparse_moe.experts.10.w3.weight', 'model.layers.23.block_sparse_moe.experts.11.w1.weight', 'model.layers.23.block_sparse_moe.experts.11.w2.weight', 'model.layers.23.block_sparse_moe.experts.11.w3.weight', 'model.layers.23.block_sparse_moe.experts.12.w1.weight', 'model.layers.23.block_sparse_moe.experts.12.w2.weight', 'model.layers.23.block_sparse_moe.experts.12.w3.weight', 'model.layers.23.block_sparse_moe.experts.13.w1.weight', 'model.layers.23.block_sparse_moe.experts.13.w2.weight', 'model.layers.23.block_sparse_moe.experts.13.w3.weight', 'model.layers.23.block_sparse_moe.experts.14.w1.weight', 'model.layers.23.block_sparse_moe.experts.14.w2.weight', 'model.layers.23.block_sparse_moe.experts.14.w3.weight', 'model.layers.23.block_sparse_moe.experts.15.w1.weight', 'model.layers.23.block_sparse_moe.experts.15.w2.weight', 'model.layers.23.block_sparse_moe.experts.15.w3.weight', 'model.layers.23.block_sparse_moe.experts.3.w1.weight', 'model.layers.23.block_sparse_moe.experts.3.w2.weight', 'model.layers.23.block_sparse_moe.experts.3.w3.weight', 'model.layers.23.block_sparse_moe.experts.5.w1.weight', 'model.layers.23.block_sparse_moe.experts.5.w2.weight', 'model.layers.23.block_sparse_moe.experts.5.w3.weight', 'model.layers.23.block_sparse_moe.experts.6.w1.weight', 'model.layers.23.block_sparse_moe.experts.6.w2.weight', 'model.layers.23.block_sparse_moe.experts.6.w3.weight', 'model.layers.23.block_sparse_moe.experts.7.w1.weight', 'model.layers.23.block_sparse_moe.experts.7.w2.weight', 'model.layers.23.block_sparse_moe.experts.7.w3.weight', 'model.layers.23.block_sparse_moe.experts.8.w1.weight', 'model.layers.23.block_sparse_moe.experts.8.w2.weight', 'model.layers.23.block_sparse_moe.experts.8.w3.weight', 'model.layers.23.block_sparse_moe.experts.9.w1.weight', 'model.layers.23.block_sparse_moe.experts.9.w2.weight', 'model.layers.23.block_sparse_moe.experts.9.w3.weight', 'model.layers.24.block_sparse_moe.experts.1.w1.weight', 'model.layers.24.block_sparse_moe.experts.1.w2.weight', 'model.layers.24.block_sparse_moe.experts.1.w3.weight', 'model.layers.24.block_sparse_moe.experts.11.w1.weight', 'model.layers.24.block_sparse_moe.experts.11.w2.weight', 'model.layers.24.block_sparse_moe.experts.11.w3.weight', 'model.layers.24.block_sparse_moe.experts.12.w1.weight', 'model.layers.24.block_sparse_moe.experts.12.w2.weight', 'model.layers.24.block_sparse_moe.experts.12.w3.weight', 'model.layers.24.block_sparse_moe.experts.13.w1.weight', 'model.layers.24.block_sparse_moe.experts.13.w2.weight', 'model.layers.24.block_sparse_moe.experts.13.w3.weight', 'model.layers.24.block_sparse_moe.experts.14.w1.weight', 'model.layers.24.block_sparse_moe.experts.14.w2.weight', 'model.layers.24.block_sparse_moe.experts.14.w3.weight', 'model.layers.24.block_sparse_moe.experts.15.w1.weight', 'model.layers.24.block_sparse_moe.experts.15.w2.weight', 'model.layers.24.block_sparse_moe.experts.15.w3.weight', 'model.layers.24.block_sparse_moe.experts.2.w1.weight', 'model.layers.24.block_sparse_moe.experts.2.w2.weight', 'model.layers.24.block_sparse_moe.experts.2.w3.weight', 'model.layers.24.block_sparse_moe.experts.3.w1.weight', 'model.layers.24.block_sparse_moe.experts.3.w2.weight', 'model.layers.24.block_sparse_moe.experts.3.w3.weight', 'model.layers.24.block_sparse_moe.experts.5.w1.weight', 'model.layers.24.block_sparse_moe.experts.5.w2.weight', 'model.layers.24.block_sparse_moe.experts.5.w3.weight', 'model.layers.24.block_sparse_moe.experts.7.w1.weight', 'model.layers.24.block_sparse_moe.experts.7.w2.weight', 'model.layers.24.block_sparse_moe.experts.7.w3.weight', 'model.layers.24.block_sparse_moe.experts.8.w1.weight', 'model.layers.24.block_sparse_moe.experts.8.w2.weight', 'model.layers.24.block_sparse_moe.experts.8.w3.weight', 'model.layers.24.block_sparse_moe.experts.9.w1.weight', 'model.layers.24.block_sparse_moe.experts.9.w2.weight', 'model.layers.24.block_sparse_moe.experts.9.w3.weight', 'model.layers.25.block_sparse_moe.experts.1.w1.weight', 'model.layers.25.block_sparse_moe.experts.1.w2.weight', 'model.layers.25.block_sparse_moe.experts.1.w3.weight', 'model.layers.25.block_sparse_moe.experts.10.w1.weight', 'model.layers.25.block_sparse_moe.experts.10.w2.weight', 'model.layers.25.block_sparse_moe.experts.10.w3.weight', 'model.layers.25.block_sparse_moe.experts.11.w1.weight', 'model.layers.25.block_sparse_moe.experts.11.w2.weight', 'model.layers.25.block_sparse_moe.experts.11.w3.weight', 'model.layers.25.block_sparse_moe.experts.12.w1.weight', 'model.layers.25.block_sparse_moe.experts.12.w2.weight', 'model.layers.25.block_sparse_moe.experts.12.w3.weight', 'model.layers.25.block_sparse_moe.experts.13.w1.weight', 'model.layers.25.block_sparse_moe.experts.13.w2.weight', 'model.layers.25.block_sparse_moe.experts.13.w3.weight', 'model.layers.25.block_sparse_moe.experts.15.w1.weight', 'model.layers.25.block_sparse_moe.experts.15.w2.weight', 'model.layers.25.block_sparse_moe.experts.15.w3.weight', 'model.layers.25.block_sparse_moe.experts.2.w1.weight', 'model.layers.25.block_sparse_moe.experts.2.w2.weight', 'model.layers.25.block_sparse_moe.experts.2.w3.weight', 'model.layers.25.block_sparse_moe.experts.3.w1.weight', 'model.layers.25.block_sparse_moe.experts.3.w2.weight', 'model.layers.25.block_sparse_moe.experts.3.w3.weight', 'model.layers.25.block_sparse_moe.experts.4.w1.weight', 'model.layers.25.block_sparse_moe.experts.4.w2.weight', 'model.layers.25.block_sparse_moe.experts.4.w3.weight', 'model.layers.25.block_sparse_moe.experts.5.w1.weight', 'model.layers.25.block_sparse_moe.experts.5.w2.weight', 'model.layers.25.block_sparse_moe.experts.5.w3.weight', 'model.layers.25.block_sparse_moe.experts.8.w1.weight', 'model.layers.25.block_sparse_moe.experts.8.w2.weight', 'model.layers.25.block_sparse_moe.experts.8.w3.weight', 'model.layers.25.block_sparse_moe.experts.9.w1.weight', 'model.layers.25.block_sparse_moe.experts.9.w2.weight', 'model.layers.25.block_sparse_moe.experts.9.w3.weight', 'model.layers.26.block_sparse_moe.experts.1.w1.weight', 'model.layers.26.block_sparse_moe.experts.1.w2.weight', 'model.layers.26.block_sparse_moe.experts.1.w3.weight', 'model.layers.26.block_sparse_moe.experts.11.w1.weight', 'model.layers.26.block_sparse_moe.experts.11.w2.weight', 'model.layers.26.block_sparse_moe.experts.11.w3.weight', 'model.layers.26.block_sparse_moe.experts.12.w1.weight', 'model.layers.26.block_sparse_moe.experts.12.w2.weight', 'model.layers.26.block_sparse_moe.experts.12.w3.weight', 'model.layers.26.block_sparse_moe.experts.14.w1.weight', 'model.layers.26.block_sparse_moe.experts.14.w2.weight', 'model.layers.26.block_sparse_moe.experts.14.w3.weight', 'model.layers.26.block_sparse_moe.experts.15.w1.weight', 'model.layers.26.block_sparse_moe.experts.15.w2.weight', 'model.layers.26.block_sparse_moe.experts.15.w3.weight', 'model.layers.26.block_sparse_moe.experts.2.w1.weight', 'model.layers.26.block_sparse_moe.experts.2.w2.weight', 'model.layers.26.block_sparse_moe.experts.2.w3.weight', 'model.layers.26.block_sparse_moe.experts.3.w1.weight', 'model.layers.26.block_sparse_moe.experts.3.w2.weight', 'model.layers.26.block_sparse_moe.experts.3.w3.weight', 'model.layers.26.block_sparse_moe.experts.4.w1.weight', 'model.layers.26.block_sparse_moe.experts.4.w2.weight', 'model.layers.26.block_sparse_moe.experts.4.w3.weight', 'model.layers.26.block_sparse_moe.experts.5.w1.weight', 'model.layers.26.block_sparse_moe.experts.5.w2.weight', 'model.layers.26.block_sparse_moe.experts.5.w3.weight', 'model.layers.26.block_sparse_moe.experts.6.w1.weight', 'model.layers.26.block_sparse_moe.experts.6.w2.weight', 'model.layers.26.block_sparse_moe.experts.6.w3.weight', 'model.layers.26.block_sparse_moe.experts.7.w1.weight', 'model.layers.26.block_sparse_moe.experts.7.w2.weight', 'model.layers.26.block_sparse_moe.experts.7.w3.weight', 'model.layers.26.block_sparse_moe.experts.8.w1.weight', 'model.layers.26.block_sparse_moe.experts.8.w2.weight', 'model.layers.26.block_sparse_moe.experts.8.w3.weight', 'model.layers.27.block_sparse_moe.experts.1.w1.weight', 'model.layers.27.block_sparse_moe.experts.1.w2.weight', 'model.layers.27.block_sparse_moe.experts.1.w3.weight', 'model.layers.27.block_sparse_moe.experts.10.w1.weight', 'model.layers.27.block_sparse_moe.experts.10.w2.weight', 'model.layers.27.block_sparse_moe.experts.10.w3.weight', 'model.layers.27.block_sparse_moe.experts.11.w1.weight', 'model.layers.27.block_sparse_moe.experts.11.w2.weight', 'model.layers.27.block_sparse_moe.experts.11.w3.weight', 'model.layers.27.block_sparse_moe.experts.12.w1.weight', 'model.layers.27.block_sparse_moe.experts.12.w2.weight', 'model.layers.27.block_sparse_moe.experts.12.w3.weight', 'model.layers.27.block_sparse_moe.experts.14.w1.weight', 'model.layers.27.block_sparse_moe.experts.14.w2.weight', 'model.layers.27.block_sparse_moe.experts.14.w3.weight', 'model.layers.27.block_sparse_moe.experts.15.w1.weight', 'model.layers.27.block_sparse_moe.experts.15.w2.weight', 'model.layers.27.block_sparse_moe.experts.15.w3.weight', 'model.layers.27.block_sparse_moe.experts.2.w1.weight', 'model.layers.27.block_sparse_moe.experts.2.w2.weight', 'model.layers.27.block_sparse_moe.experts.2.w3.weight', 'model.layers.27.block_sparse_moe.experts.3.w1.weight', 'model.layers.27.block_sparse_moe.experts.3.w2.weight', 'model.layers.27.block_sparse_moe.experts.3.w3.weight', 'model.layers.27.block_sparse_moe.experts.4.w1.weight', 'model.layers.27.block_sparse_moe.experts.4.w2.weight', 'model.layers.27.block_sparse_moe.experts.4.w3.weight', 'model.layers.27.block_sparse_moe.experts.5.w1.weight', 'model.layers.27.block_sparse_moe.experts.5.w2.weight', 'model.layers.27.block_sparse_moe.experts.5.w3.weight', 'model.layers.27.block_sparse_moe.experts.7.w1.weight', 'model.layers.27.block_sparse_moe.experts.7.w2.weight', 'model.layers.27.block_sparse_moe.experts.7.w3.weight', 'model.layers.27.block_sparse_moe.experts.8.w1.weight', 'model.layers.27.block_sparse_moe.experts.8.w2.weight', 'model.layers.27.block_sparse_moe.experts.8.w3.weight', 'model.layers.28.block_sparse_moe.experts.1.w1.weight', 'model.layers.28.block_sparse_moe.experts.1.w2.weight', 'model.layers.28.block_sparse_moe.experts.1.w3.weight', 'model.layers.28.block_sparse_moe.experts.10.w1.weight', 'model.layers.28.block_sparse_moe.experts.10.w2.weight', 'model.layers.28.block_sparse_moe.experts.10.w3.weight', 'model.layers.28.block_sparse_moe.experts.11.w1.weight', 'model.layers.28.block_sparse_moe.experts.11.w2.weight', 'model.layers.28.block_sparse_moe.experts.11.w3.weight', 'model.layers.28.block_sparse_moe.experts.12.w1.weight', 'model.layers.28.block_sparse_moe.experts.12.w2.weight', 'model.layers.28.block_sparse_moe.experts.12.w3.weight', 'model.layers.28.block_sparse_moe.experts.13.w1.weight', 'model.layers.28.block_sparse_moe.experts.13.w2.weight', 'model.layers.28.block_sparse_moe.experts.13.w3.weight', 'model.layers.28.block_sparse_moe.experts.15.w1.weight', 'model.layers.28.block_sparse_moe.experts.15.w2.weight', 'model.layers.28.block_sparse_moe.experts.15.w3.weight', 'model.layers.28.block_sparse_moe.experts.3.w1.weight', 'model.layers.28.block_sparse_moe.experts.3.w2.weight', 'model.layers.28.block_sparse_moe.experts.3.w3.weight', 'model.layers.28.block_sparse_moe.experts.4.w1.weight', 'model.layers.28.block_sparse_moe.experts.4.w2.weight', 'model.layers.28.block_sparse_moe.experts.4.w3.weight', 'model.layers.28.block_sparse_moe.experts.5.w1.weight', 'model.layers.28.block_sparse_moe.experts.5.w2.weight', 'model.layers.28.block_sparse_moe.experts.5.w3.weight', 'model.layers.28.block_sparse_moe.experts.7.w1.weight', 'model.layers.28.block_sparse_moe.experts.7.w2.weight', 'model.layers.28.block_sparse_moe.experts.7.w3.weight', 'model.layers.28.block_sparse_moe.experts.8.w1.weight', 'model.layers.28.block_sparse_moe.experts.8.w2.weight', 'model.layers.28.block_sparse_moe.experts.8.w3.weight', 'model.layers.28.block_sparse_moe.experts.9.w1.weight', 'model.layers.28.block_sparse_moe.experts.9.w2.weight', 'model.layers.28.block_sparse_moe.experts.9.w3.weight', 'model.layers.29.block_sparse_moe.experts.10.w1.weight', 'model.layers.29.block_sparse_moe.experts.10.w2.weight', 'model.layers.29.block_sparse_moe.experts.10.w3.weight', 'model.layers.29.block_sparse_moe.experts.11.w1.weight', 'model.layers.29.block_sparse_moe.experts.11.w2.weight', 'model.layers.29.block_sparse_moe.experts.11.w3.weight', 'model.layers.29.block_sparse_moe.experts.12.w1.weight', 'model.layers.29.block_sparse_moe.experts.12.w2.weight', 'model.layers.29.block_sparse_moe.experts.12.w3.weight', 'model.layers.29.block_sparse_moe.experts.13.w1.weight', 'model.layers.29.block_sparse_moe.experts.13.w2.weight', 'model.layers.29.block_sparse_moe.experts.13.w3.weight', 'model.layers.29.block_sparse_moe.experts.14.w1.weight', 'model.layers.29.block_sparse_moe.experts.14.w2.weight', 'model.layers.29.block_sparse_moe.experts.14.w3.weight', 'model.layers.29.block_sparse_moe.experts.15.w1.weight', 'model.layers.29.block_sparse_moe.experts.15.w2.weight', 'model.layers.29.block_sparse_moe.experts.15.w3.weight', 'model.layers.29.block_sparse_moe.experts.2.w1.weight', 'model.layers.29.block_sparse_moe.experts.2.w2.weight', 'model.layers.29.block_sparse_moe.experts.2.w3.weight', 'model.layers.29.block_sparse_moe.experts.3.w1.weight', 'model.layers.29.block_sparse_moe.experts.3.w2.weight', 'model.layers.29.block_sparse_moe.experts.3.w3.weight', 'model.layers.29.block_sparse_moe.experts.4.w1.weight', 'model.layers.29.block_sparse_moe.experts.4.w2.weight', 'model.layers.29.block_sparse_moe.experts.4.w3.weight', 'model.layers.29.block_sparse_moe.experts.5.w1.weight', 'model.layers.29.block_sparse_moe.experts.5.w2.weight', 'model.layers.29.block_sparse_moe.experts.5.w3.weight', 'model.layers.29.block_sparse_moe.experts.6.w1.weight', 'model.layers.29.block_sparse_moe.experts.6.w2.weight', 'model.layers.29.block_sparse_moe.experts.6.w3.weight', 'model.layers.29.block_sparse_moe.experts.9.w1.weight', 'model.layers.29.block_sparse_moe.experts.9.w2.weight', 'model.layers.29.block_sparse_moe.experts.9.w3.weight', 'model.layers.3.block_sparse_moe.experts.1.w1.weight', 'model.layers.3.block_sparse_moe.experts.1.w2.weight', 'model.layers.3.block_sparse_moe.experts.1.w3.weight', 'model.layers.3.block_sparse_moe.experts.10.w1.weight', 'model.layers.3.block_sparse_moe.experts.10.w2.weight', 'model.layers.3.block_sparse_moe.experts.10.w3.weight', 'model.layers.3.block_sparse_moe.experts.11.w1.weight', 'model.layers.3.block_sparse_moe.experts.11.w2.weight', 'model.layers.3.block_sparse_moe.experts.11.w3.weight', 'model.layers.3.block_sparse_moe.experts.13.w1.weight', 'model.layers.3.block_sparse_moe.experts.13.w2.weight', 'model.layers.3.block_sparse_moe.experts.13.w3.weight', 'model.layers.3.block_sparse_moe.experts.14.w1.weight', 'model.layers.3.block_sparse_moe.experts.14.w2.weight', 'model.layers.3.block_sparse_moe.experts.14.w3.weight', 'model.layers.3.block_sparse_moe.experts.15.w1.weight', 'model.layers.3.block_sparse_moe.experts.15.w2.weight', 'model.layers.3.block_sparse_moe.experts.15.w3.weight', 'model.layers.3.block_sparse_moe.experts.2.w1.weight', 'model.layers.3.block_sparse_moe.experts.2.w2.weight', 'model.layers.3.block_sparse_moe.experts.2.w3.weight', 'model.layers.3.block_sparse_moe.experts.3.w1.weight', 'model.layers.3.block_sparse_moe.experts.3.w2.weight', 'model.layers.3.block_sparse_moe.experts.3.w3.weight', 'model.layers.3.block_sparse_moe.experts.4.w1.weight', 'model.layers.3.block_sparse_moe.experts.4.w2.weight', 'model.layers.3.block_sparse_moe.experts.4.w3.weight', 'model.layers.3.block_sparse_moe.experts.5.w1.weight', 'model.layers.3.block_sparse_moe.experts.5.w2.weight', 'model.layers.3.block_sparse_moe.experts.5.w3.weight', 'model.layers.3.block_sparse_moe.experts.6.w1.weight', 'model.layers.3.block_sparse_moe.experts.6.w2.weight', 'model.layers.3.block_sparse_moe.experts.6.w3.weight', 'model.layers.3.block_sparse_moe.experts.9.w1.weight', 'model.layers.3.block_sparse_moe.experts.9.w2.weight', 'model.layers.3.block_sparse_moe.experts.9.w3.weight', 'model.layers.30.block_sparse_moe.experts.10.w1.weight', 'model.layers.30.block_sparse_moe.experts.10.w2.weight', 'model.layers.30.block_sparse_moe.experts.10.w3.weight', 'model.layers.30.block_sparse_moe.experts.11.w1.weight', 'model.layers.30.block_sparse_moe.experts.11.w2.weight', 'model.layers.30.block_sparse_moe.experts.11.w3.weight', 'model.layers.30.block_sparse_moe.experts.12.w1.weight', 'model.layers.30.block_sparse_moe.experts.12.w2.weight', 'model.layers.30.block_sparse_moe.experts.12.w3.weight', 'model.layers.30.block_sparse_moe.experts.13.w1.weight', 'model.layers.30.block_sparse_moe.experts.13.w2.weight', 'model.layers.30.block_sparse_moe.experts.13.w3.weight', 'model.layers.30.block_sparse_moe.experts.14.w1.weight', 'model.layers.30.block_sparse_moe.experts.14.w2.weight', 'model.layers.30.block_sparse_moe.experts.14.w3.weight', 'model.layers.30.block_sparse_moe.experts.2.w1.weight', 'model.layers.30.block_sparse_moe.experts.2.w2.weight', 'model.layers.30.block_sparse_moe.experts.2.w3.weight', 'model.layers.30.block_sparse_moe.experts.3.w1.weight', 'model.layers.30.block_sparse_moe.experts.3.w2.weight', 'model.layers.30.block_sparse_moe.experts.3.w3.weight', 'model.layers.30.block_sparse_moe.experts.4.w1.weight', 'model.layers.30.block_sparse_moe.experts.4.w2.weight', 'model.layers.30.block_sparse_moe.experts.4.w3.weight', 'model.layers.30.block_sparse_moe.experts.5.w1.weight', 'model.layers.30.block_sparse_moe.experts.5.w2.weight', 'model.layers.30.block_sparse_moe.experts.5.w3.weight', 'model.layers.30.block_sparse_moe.experts.6.w1.weight', 'model.layers.30.block_sparse_moe.experts.6.w2.weight', 'model.layers.30.block_sparse_moe.experts.6.w3.weight', 'model.layers.30.block_sparse_moe.experts.7.w1.weight', 'model.layers.30.block_sparse_moe.experts.7.w2.weight', 'model.layers.30.block_sparse_moe.experts.7.w3.weight', 'model.layers.30.block_sparse_moe.experts.9.w1.weight', 'model.layers.30.block_sparse_moe.experts.9.w2.weight', 'model.layers.30.block_sparse_moe.experts.9.w3.weight', 'model.layers.31.block_sparse_moe.experts.11.w1.weight', 'model.layers.31.block_sparse_moe.experts.11.w2.weight', 'model.layers.31.block_sparse_moe.experts.11.w3.weight', 'model.layers.31.block_sparse_moe.experts.12.w1.weight', 'model.layers.31.block_sparse_moe.experts.12.w2.weight', 'model.layers.31.block_sparse_moe.experts.12.w3.weight', 'model.layers.31.block_sparse_moe.experts.13.w1.weight', 'model.layers.31.block_sparse_moe.experts.13.w2.weight', 'model.layers.31.block_sparse_moe.experts.13.w3.weight', 'model.layers.31.block_sparse_moe.experts.15.w1.weight', 'model.layers.31.block_sparse_moe.experts.15.w2.weight', 'model.layers.31.block_sparse_moe.experts.15.w3.weight', 'model.layers.31.block_sparse_moe.experts.2.w1.weight', 'model.layers.31.block_sparse_moe.experts.2.w2.weight', 'model.layers.31.block_sparse_moe.experts.2.w3.weight', 'model.layers.31.block_sparse_moe.experts.3.w1.weight', 'model.layers.31.block_sparse_moe.experts.3.w2.weight', 'model.layers.31.block_sparse_moe.experts.3.w3.weight', 'model.layers.31.block_sparse_moe.experts.4.w1.weight', 'model.layers.31.block_sparse_moe.experts.4.w2.weight', 'model.layers.31.block_sparse_moe.experts.4.w3.weight', 'model.layers.31.block_sparse_moe.experts.5.w1.weight', 'model.layers.31.block_sparse_moe.experts.5.w2.weight', 'model.layers.31.block_sparse_moe.experts.5.w3.weight', 'model.layers.31.block_sparse_moe.experts.6.w1.weight', 'model.layers.31.block_sparse_moe.experts.6.w2.weight', 'model.layers.31.block_sparse_moe.experts.6.w3.weight', 'model.layers.31.block_sparse_moe.experts.7.w1.weight', 'model.layers.31.block_sparse_moe.experts.7.w2.weight', 'model.layers.31.block_sparse_moe.experts.7.w3.weight', 'model.layers.31.block_sparse_moe.experts.8.w1.weight', 'model.layers.31.block_sparse_moe.experts.8.w2.weight', 'model.layers.31.block_sparse_moe.experts.8.w3.weight', 'model.layers.31.block_sparse_moe.experts.9.w1.weight', 'model.layers.31.block_sparse_moe.experts.9.w2.weight', 'model.layers.31.block_sparse_moe.experts.9.w3.weight', 'model.layers.4.block_sparse_moe.experts.1.w1.weight', 'model.layers.4.block_sparse_moe.experts.1.w2.weight', 'model.layers.4.block_sparse_moe.experts.1.w3.weight', 'model.layers.4.block_sparse_moe.experts.10.w1.weight', 'model.layers.4.block_sparse_moe.experts.10.w2.weight', 'model.layers.4.block_sparse_moe.experts.10.w3.weight', 'model.layers.4.block_sparse_moe.experts.12.w1.weight', 'model.layers.4.block_sparse_moe.experts.12.w2.weight', 'model.layers.4.block_sparse_moe.experts.12.w3.weight', 'model.layers.4.block_sparse_moe.experts.13.w1.weight', 'model.layers.4.block_sparse_moe.experts.13.w2.weight', 'model.layers.4.block_sparse_moe.experts.13.w3.weight', 'model.layers.4.block_sparse_moe.experts.15.w1.weight', 'model.layers.4.block_sparse_moe.experts.15.w2.weight', 'model.layers.4.block_sparse_moe.experts.15.w3.weight', 'model.layers.4.block_sparse_moe.experts.2.w1.weight', 'model.layers.4.block_sparse_moe.experts.2.w2.weight', 'model.layers.4.block_sparse_moe.experts.2.w3.weight', 'model.layers.4.block_sparse_moe.experts.4.w1.weight', 'model.layers.4.block_sparse_moe.experts.4.w2.weight', 'model.layers.4.block_sparse_moe.experts.4.w3.weight', 'model.layers.4.block_sparse_moe.experts.5.w1.weight', 'model.layers.4.block_sparse_moe.experts.5.w2.weight', 'model.layers.4.block_sparse_moe.experts.5.w3.weight', 'model.layers.4.block_sparse_moe.experts.6.w1.weight', 'model.layers.4.block_sparse_moe.experts.6.w2.weight', 'model.layers.4.block_sparse_moe.experts.6.w3.weight', 'model.layers.4.block_sparse_moe.experts.7.w1.weight', 'model.layers.4.block_sparse_moe.experts.7.w2.weight', 'model.layers.4.block_sparse_moe.experts.7.w3.weight', 'model.layers.4.block_sparse_moe.experts.8.w1.weight', 'model.layers.4.block_sparse_moe.experts.8.w2.weight', 'model.layers.4.block_sparse_moe.experts.8.w3.weight', 'model.layers.4.block_sparse_moe.experts.9.w1.weight', 'model.layers.4.block_sparse_moe.experts.9.w2.weight', 'model.layers.4.block_sparse_moe.experts.9.w3.weight', 'model.layers.5.block_sparse_moe.experts.10.w1.weight', 'model.layers.5.block_sparse_moe.experts.10.w2.weight', 'model.layers.5.block_sparse_moe.experts.10.w3.weight', 'model.layers.5.block_sparse_moe.experts.11.w1.weight', 'model.layers.5.block_sparse_moe.experts.11.w2.weight', 'model.layers.5.block_sparse_moe.experts.11.w3.weight', 'model.layers.5.block_sparse_moe.experts.12.w1.weight', 'model.layers.5.block_sparse_moe.experts.12.w2.weight', 'model.layers.5.block_sparse_moe.experts.12.w3.weight', 'model.layers.5.block_sparse_moe.experts.15.w1.weight', 'model.layers.5.block_sparse_moe.experts.15.w2.weight', 'model.layers.5.block_sparse_moe.experts.15.w3.weight', 'model.layers.5.block_sparse_moe.experts.2.w1.weight', 'model.layers.5.block_sparse_moe.experts.2.w2.weight', 'model.layers.5.block_sparse_moe.experts.2.w3.weight', 'model.layers.5.block_sparse_moe.experts.3.w1.weight', 'model.layers.5.block_sparse_moe.experts.3.w2.weight', 'model.layers.5.block_sparse_moe.experts.3.w3.weight', 'model.layers.5.block_sparse_moe.experts.4.w1.weight', 'model.layers.5.block_sparse_moe.experts.4.w2.weight', 'model.layers.5.block_sparse_moe.experts.4.w3.weight', 'model.layers.5.block_sparse_moe.experts.5.w1.weight', 'model.layers.5.block_sparse_moe.experts.5.w2.weight', 'model.layers.5.block_sparse_moe.experts.5.w3.weight', 'model.layers.5.block_sparse_moe.experts.6.w1.weight', 'model.layers.5.block_sparse_moe.experts.6.w2.weight', 'model.layers.5.block_sparse_moe.experts.6.w3.weight', 'model.layers.5.block_sparse_moe.experts.7.w1.weight', 'model.layers.5.block_sparse_moe.experts.7.w2.weight', 'model.layers.5.block_sparse_moe.experts.7.w3.weight', 'model.layers.5.block_sparse_moe.experts.8.w1.weight', 'model.layers.5.block_sparse_moe.experts.8.w2.weight', 'model.layers.5.block_sparse_moe.experts.8.w3.weight', 'model.layers.5.block_sparse_moe.experts.9.w1.weight', 'model.layers.5.block_sparse_moe.experts.9.w2.weight', 'model.layers.5.block_sparse_moe.experts.9.w3.weight', 'model.layers.6.block_sparse_moe.experts.1.w1.weight', 'model.layers.6.block_sparse_moe.experts.1.w2.weight', 'model.layers.6.block_sparse_moe.experts.1.w3.weight', 'model.layers.6.block_sparse_moe.experts.10.w1.weight', 'model.layers.6.block_sparse_moe.experts.10.w2.weight', 'model.layers.6.block_sparse_moe.experts.10.w3.weight', 'model.layers.6.block_sparse_moe.experts.12.w1.weight', 'model.layers.6.block_sparse_moe.experts.12.w2.weight', 'model.layers.6.block_sparse_moe.experts.12.w3.weight', 'model.layers.6.block_sparse_moe.experts.13.w1.weight', 'model.layers.6.block_sparse_moe.experts.13.w2.weight', 'model.layers.6.block_sparse_moe.experts.13.w3.weight', 'model.layers.6.block_sparse_moe.experts.14.w1.weight', 'model.layers.6.block_sparse_moe.experts.14.w2.weight', 'model.layers.6.block_sparse_moe.experts.14.w3.weight', 'model.layers.6.block_sparse_moe.experts.15.w1.weight', 'model.layers.6.block_sparse_moe.experts.15.w2.weight', 'model.layers.6.block_sparse_moe.experts.15.w3.weight', 'model.layers.6.block_sparse_moe.experts.2.w1.weight', 'model.layers.6.block_sparse_moe.experts.2.w2.weight', 'model.layers.6.block_sparse_moe.experts.2.w3.weight', 'model.layers.6.block_sparse_moe.experts.4.w1.weight', 'model.layers.6.block_sparse_moe.experts.4.w2.weight', 'model.layers.6.block_sparse_moe.experts.4.w3.weight', 'model.layers.6.block_sparse_moe.experts.5.w1.weight', 'model.layers.6.block_sparse_moe.experts.5.w2.weight', 'model.layers.6.block_sparse_moe.experts.5.w3.weight', 'model.layers.6.block_sparse_moe.experts.6.w1.weight', 'model.layers.6.block_sparse_moe.experts.6.w2.weight', 'model.layers.6.block_sparse_moe.experts.6.w3.weight', 'model.layers.6.block_sparse_moe.experts.7.w1.weight', 'model.layers.6.block_sparse_moe.experts.7.w2.weight', 'model.layers.6.block_sparse_moe.experts.7.w3.weight', 'model.layers.6.block_sparse_moe.experts.9.w1.weight', 'model.layers.6.block_sparse_moe.experts.9.w2.weight', 'model.layers.6.block_sparse_moe.experts.9.w3.weight', 'model.layers.7.block_sparse_moe.experts.10.w1.weight', 'model.layers.7.block_sparse_moe.experts.10.w2.weight', 'model.layers.7.block_sparse_moe.experts.10.w3.weight', 'model.layers.7.block_sparse_moe.experts.11.w1.weight', 'model.layers.7.block_sparse_moe.experts.11.w2.weight', 'model.layers.7.block_sparse_moe.experts.11.w3.weight', 'model.layers.7.block_sparse_moe.experts.12.w1.weight', 'model.layers.7.block_sparse_moe.experts.12.w2.weight', 'model.layers.7.block_sparse_moe.experts.12.w3.weight', 'model.layers.7.block_sparse_moe.experts.13.w1.weight', 'model.layers.7.block_sparse_moe.experts.13.w2.weight', 'model.layers.7.block_sparse_moe.experts.13.w3.weight', 'model.layers.7.block_sparse_moe.experts.14.w1.weight', 'model.layers.7.block_sparse_moe.experts.14.w2.weight', 'model.layers.7.block_sparse_moe.experts.14.w3.weight', 'model.layers.7.block_sparse_moe.experts.15.w1.weight', 'model.layers.7.block_sparse_moe.experts.15.w2.weight', 'model.layers.7.block_sparse_moe.experts.15.w3.weight', 'model.layers.7.block_sparse_moe.experts.2.w1.weight', 'model.layers.7.block_sparse_moe.experts.2.w2.weight', 'model.layers.7.block_sparse_moe.experts.2.w3.weight', 'model.layers.7.block_sparse_moe.experts.3.w1.weight', 'model.layers.7.block_sparse_moe.experts.3.w2.weight', 'model.layers.7.block_sparse_moe.experts.3.w3.weight', 'model.layers.7.block_sparse_moe.experts.4.w1.weight', 'model.layers.7.block_sparse_moe.experts.4.w2.weight', 'model.layers.7.block_sparse_moe.experts.4.w3.weight', 'model.layers.7.block_sparse_moe.experts.6.w1.weight', 'model.layers.7.block_sparse_moe.experts.6.w2.weight', 'model.layers.7.block_sparse_moe.experts.6.w3.weight', 'model.layers.7.block_sparse_moe.experts.7.w1.weight', 'model.layers.7.block_sparse_moe.experts.7.w2.weight', 'model.layers.7.block_sparse_moe.experts.7.w3.weight', 'model.layers.7.block_sparse_moe.experts.9.w1.weight', 'model.layers.7.block_sparse_moe.experts.9.w2.weight', 'model.layers.7.block_sparse_moe.experts.9.w3.weight', 'model.layers.8.block_sparse_moe.experts.1.w1.weight', 'model.layers.8.block_sparse_moe.experts.1.w2.weight', 'model.layers.8.block_sparse_moe.experts.1.w3.weight', 'model.layers.8.block_sparse_moe.experts.10.w1.weight', 'model.layers.8.block_sparse_moe.experts.10.w2.weight', 'model.layers.8.block_sparse_moe.experts.10.w3.weight', 'model.layers.8.block_sparse_moe.experts.11.w1.weight', 'model.layers.8.block_sparse_moe.experts.11.w2.weight', 'model.layers.8.block_sparse_moe.experts.11.w3.weight', 'model.layers.8.block_sparse_moe.experts.13.w1.weight', 'model.layers.8.block_sparse_moe.experts.13.w2.weight', 'model.layers.8.block_sparse_moe.experts.13.w3.weight', 'model.layers.8.block_sparse_moe.experts.14.w1.weight', 'model.layers.8.block_sparse_moe.experts.14.w2.weight', 'model.layers.8.block_sparse_moe.experts.14.w3.weight', 'model.layers.8.block_sparse_moe.experts.15.w1.weight', 'model.layers.8.block_sparse_moe.experts.15.w2.weight', 'model.layers.8.block_sparse_moe.experts.15.w3.weight', 'model.layers.8.block_sparse_moe.experts.2.w1.weight', 'model.layers.8.block_sparse_moe.experts.2.w2.weight', 'model.layers.8.block_sparse_moe.experts.2.w3.weight', 'model.layers.8.block_sparse_moe.experts.3.w1.weight', 'model.layers.8.block_sparse_moe.experts.3.w2.weight', 'model.layers.8.block_sparse_moe.experts.3.w3.weight', 'model.layers.8.block_sparse_moe.experts.5.w1.weight', 'model.layers.8.block_sparse_moe.experts.5.w2.weight', 'model.layers.8.block_sparse_moe.experts.5.w3.weight', 'model.layers.8.block_sparse_moe.experts.6.w1.weight', 'model.layers.8.block_sparse_moe.experts.6.w2.weight', 'model.layers.8.block_sparse_moe.experts.6.w3.weight', 'model.layers.8.block_sparse_moe.experts.7.w1.weight', 'model.layers.8.block_sparse_moe.experts.7.w2.weight', 'model.layers.8.block_sparse_moe.experts.7.w3.weight', 'model.layers.8.block_sparse_moe.experts.8.w1.weight', 'model.layers.8.block_sparse_moe.experts.8.w2.weight', 'model.layers.8.block_sparse_moe.experts.8.w3.weight', 'model.layers.9.block_sparse_moe.experts.1.w1.weight', 'model.layers.9.block_sparse_moe.experts.1.w2.weight', 'model.layers.9.block_sparse_moe.experts.1.w3.weight', 'model.layers.9.block_sparse_moe.experts.10.w1.weight', 'model.layers.9.block_sparse_moe.experts.10.w2.weight', 'model.layers.9.block_sparse_moe.experts.10.w3.weight', 'model.layers.9.block_sparse_moe.experts.11.w1.weight', 'model.layers.9.block_sparse_moe.experts.11.w2.weight', 'model.layers.9.block_sparse_moe.experts.11.w3.weight', 'model.layers.9.block_sparse_moe.experts.13.w1.weight', 'model.layers.9.block_sparse_moe.experts.13.w2.weight', 'model.layers.9.block_sparse_moe.experts.13.w3.weight', 'model.layers.9.block_sparse_moe.experts.15.w1.weight', 'model.layers.9.block_sparse_moe.experts.15.w2.weight', 'model.layers.9.block_sparse_moe.experts.15.w3.weight', 'model.layers.9.block_sparse_moe.experts.2.w1.weight', 'model.layers.9.block_sparse_moe.experts.2.w2.weight', 'model.layers.9.block_sparse_moe.experts.2.w3.weight', 'model.layers.9.block_sparse_moe.experts.4.w1.weight', 'model.layers.9.block_sparse_moe.experts.4.w2.weight', 'model.layers.9.block_sparse_moe.experts.4.w3.weight', 'model.layers.9.block_sparse_moe.experts.5.w1.weight', 'model.layers.9.block_sparse_moe.experts.5.w2.weight', 'model.layers.9.block_sparse_moe.experts.5.w3.weight', 'model.layers.9.block_sparse_moe.experts.6.w1.weight', 'model.layers.9.block_sparse_moe.experts.6.w2.weight', 'model.layers.9.block_sparse_moe.experts.6.w3.weight', 'model.layers.9.block_sparse_moe.experts.7.w1.weight', 'model.layers.9.block_sparse_moe.experts.7.w2.weight', 'model.layers.9.block_sparse_moe.experts.7.w3.weight', 'model.layers.9.block_sparse_moe.experts.8.w1.weight', 'model.layers.9.block_sparse_moe.experts.8.w2.weight', 'model.layers.9.block_sparse_moe.experts.8.w3.weight', 'model.layers.9.block_sparse_moe.experts.9.w1.weight', 'model.layers.9.block_sparse_moe.experts.9.w2.weight', 'model.layers.9.block_sparse_moe.experts.9.w3.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-01-08:11:27:09,732 WARNING  [huggingface.py:98] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
2025-01-08:11:27:09,732 INFO     [huggingface.py:496] Model type cannot be determined. Using default model type 'causal'
2025-01-08:11:27:09,745 WARNING  [huggingface.py:279] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
0
tensor(6)
tensor(15)
tensor(13)
tensor(0)
done!
1
tensor(0)
tensor(4)
tensor(7)
tensor(1)
done!
2
tensor(15)
tensor(4)
tensor(5)
tensor(0)
done!
3
tensor(12)
tensor(7)
tensor(8)
tensor(0)
done!
4
tensor(11)
tensor(14)
tensor(3)
tensor(0)
done!
5
tensor(13)
tensor(1)
tensor(14)
tensor(0)
done!
6
tensor(8)
tensor(3)
tensor(11)
tensor(0)
done!
7
tensor(8)
tensor(0)
tensor(5)
tensor(1)
done!
8
tensor(9)
tensor(4)
tensor(12)
tensor(0)
done!
9
tensor(3)
tensor(12)
tensor(14)
tensor(0)
done!
10
tensor(0)
tensor(4)
tensor(11)
tensor(1)
done!
11
tensor(9)
tensor(4)
tensor(3)
tensor(0)
done!
12
tensor(2)
tensor(7)
tensor(3)
tensor(0)
done!
13
tensor(7)
tensor(11)
tensor(10)
tensor(0)
done!
14
tensor(7)
tensor(14)
tensor(13)
tensor(0)
done!
15
tensor(15)
tensor(6)
tensor(1)
tensor(0)
done!
16
tensor(9)
tensor(13)
tensor(12)
tensor(0)
done!
17
tensor(10)
tensor(8)
tensor(11)
tensor(0)
done!
18
tensor(8)
tensor(6)
tensor(7)
tensor(0)
done!
19
tensor(12)
tensor(8)
tensor(10)
tensor(0)
done!
20
tensor(7)
tensor(13)
tensor(5)
tensor(0)
done!
21
tensor(11)
tensor(13)
tensor(10)
tensor(0)
done!
22
tensor(11)
tensor(2)
tensor(15)
tensor(0)
done!
23
tensor(0)
tensor(1)
tensor(4)
tensor(2)
done!
24
tensor(10)
tensor(4)
tensor(6)
tensor(0)
done!
25
tensor(6)
tensor(7)
tensor(14)
tensor(0)
done!
26
tensor(10)
tensor(9)
tensor(13)
tensor(0)
done!
27
tensor(13)
tensor(6)
tensor(9)
tensor(0)
done!
28
tensor(6)
tensor(2)
tensor(14)
tensor(0)
done!
29
tensor(1)
tensor(7)
tensor(8)
tensor(0)
done!
30
tensor(0)
tensor(15)
tensor(8)
tensor(1)
done!
31
tensor(14)
tensor(10)
tensor(1)
tensor(0)
done!
all done!
all_gate number:32

PhiMoEForCausalLM(
  (model): PhiMoEModel(
    (embed_tokens): Embedding(32064, 4096)
    (layers): ModuleList(
      (0-31): 32 x PhiMoEDecoderLayer(
        (self_attn): PhiMoESdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
        )
        (block_sparse_moe): PhiMoESparseMoeBlock(
          (gate): Linear(in_features=4096, out_features=16, bias=False)
          (experts): ModuleList(
            (0-15): 16 x PhiMoEBlockSparseTop2MLP(
              (w1): Linear(in_features=4096, out_features=6400, bias=False)
              (w2): Linear(in_features=6400, out_features=4096, bias=False)
              (w3): Linear(in_features=4096, out_features=6400, bias=False)
              (act_fn): SiLU()
            )
          )
        )
        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=4096, out_features=32064, bias=True)
)
model
PhiMoEModel(
  (embed_tokens): Embedding(32064, 4096)
  (layers): ModuleList(
    (0-31): 32 x PhiMoEDecoderLayer(
      (self_attn): PhiMoESdpaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
        (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
        (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
        (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
      )
      (block_sparse_moe): PhiMoESparseMoeBlock(
        (gate): Linear(in_features=4096, out_features=16, bias=False)
        (experts): ModuleList(
          (0-15): 16 x PhiMoEBlockSparseTop2MLP(
            (w1): Linear(in_features=4096, out_features=6400, bias=False)
            (w2): Linear(in_features=6400, out_features=4096, bias=False)
            (w3): Linear(in_features=4096, out_features=6400, bias=False)
            (act_fn): SiLU()
          )
        )
      )
      (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.embed_tokens
Embedding(32064, 4096)
model.layers
ModuleList(
  (0-31): 32 x PhiMoEDecoderLayer(
    (self_attn): PhiMoESdpaAttention(
      (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
      (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
      (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
      (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
      (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
    )
    (block_sparse_moe): PhiMoESparseMoeBlock(
      (gate): Linear(in_features=4096, out_features=16, bias=False)
      (experts): ModuleList(
        (0-15): 16 x PhiMoEBlockSparseTop2MLP(
          (w1): Linear(in_features=4096, out_features=6400, bias=False)
          (w2): Linear(in_features=6400, out_features=4096, bias=False)
          (w3): Linear(in_features=4096, out_features=6400, bias=False)
          (act_fn): SiLU()
        )
      )
    )
    (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  )
)
model.layers.0
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.0.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.0.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.0.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.0.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.0.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.0.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.0.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.0.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.0.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.0.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.0.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.0.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.0.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.0.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.0.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.0.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.1
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.1.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.1.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.1.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.1.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.1.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.1.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.1.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.1.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.1.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.1.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.1.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.1.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.1.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.1.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.1.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.1.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.2
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.2.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.2.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.2.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.2.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.2.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.2.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.2.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.2.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.2.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.2.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.2.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.2.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.2.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.2.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.2.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.2.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.3
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.3.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.3.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.3.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.3.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.3.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.3.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.3.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.3.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.3.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.3.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.3.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.3.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.3.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.3.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.3.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.3.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.4
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.4.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.4.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.4.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.4.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.4.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.4.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.4.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.4.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.4.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.4.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.4.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.4.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.4.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.4.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.4.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.4.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.5
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.5.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.5.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.5.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.5.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.5.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.5.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.5.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.5.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.5.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.5.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.5.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.5.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.5.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.5.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.5.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.5.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.6
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.6.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.6.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.6.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.6.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.6.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.6.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.6.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.6.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.6.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.6.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.6.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.6.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.6.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.6.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.6.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.6.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.7
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.7.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.7.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.7.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.7.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.7.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.7.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.7.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.7.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.7.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.7.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.7.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.7.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.7.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.7.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.7.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.7.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.8
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.8.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.8.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.8.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.8.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.8.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.8.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.8.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.8.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.8.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.8.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.8.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.8.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.8.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.8.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.8.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.8.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.9
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.9.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.9.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.9.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.9.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.9.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.9.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.9.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.9.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.9.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.9.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.9.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.9.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.9.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.9.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.9.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.9.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.10
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.10.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.10.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.10.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.10.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.10.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.10.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.10.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.10.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.10.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.10.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.10.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.10.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.10.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.10.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.10.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.10.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.11
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.11.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.11.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.11.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.11.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.11.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.11.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.11.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.11.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.11.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.11.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.11.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.11.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.11.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.11.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.11.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.11.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.12
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.12.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.12.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.12.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.12.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.12.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.12.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.12.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.12.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.12.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.12.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.3
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.3.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.3.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.3.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.3.act_fn
SiLU()
model.layers.12.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.12.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.12.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.12.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.12.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.12.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.13
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.13.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.13.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.13.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.13.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.13.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.13.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.13.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.13.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.13.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.13.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.13.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.13.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.13.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.13.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.13.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.13.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.14
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.14.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.14.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.14.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.14.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.14.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.14.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.14.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.14.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.14.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.14.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.14.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.14.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.14.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.14.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.14.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.14.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.15
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.15.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.15.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.15.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.15.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.15.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.15.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.15.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.15.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.15.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.15.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.15.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.15.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.15.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.15.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.15.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.15.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.16
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.16.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.16.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.16.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.16.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.16.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.16.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.16.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.16.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.16.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.16.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.16.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.16.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.16.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.16.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.16.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.16.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.17
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.17.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.17.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.17.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.17.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.17.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.17.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.17.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.17.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.17.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.17.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.17.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.17.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.17.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.17.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.17.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.17.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.18
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.18.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.18.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.18.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.18.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.18.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.18.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.18.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.18.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.18.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.18.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.18.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.18.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.18.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.18.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.18.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.18.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.19
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.19.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.19.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.19.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.19.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.19.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.19.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.19.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.19.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.19.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.19.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.19.block_sparse_moe.experts.12
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.19.block_sparse_moe.experts.12.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.12.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.19.block_sparse_moe.experts.12.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.19.block_sparse_moe.experts.12.act_fn
SiLU()
model.layers.19.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.19.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.20
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.20.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.20.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.20.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.20.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.20.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.20.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.20.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.20.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.20.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.20.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.5
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.5.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.5.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.5.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.5.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.20.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.20.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.20.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.20.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.20.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.20.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.21
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.21.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.21.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.21.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.21.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.21.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.21.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.21.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.21.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.21.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.21.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.21.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.21.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.21.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.21.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.21.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.21.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.22
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.22.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.22.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.22.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.22.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.22.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.22.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.22.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.22.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.22.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.22.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.11
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.11.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.11.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.11.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.11.act_fn
SiLU()
model.layers.22.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.22.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.22.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.22.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.22.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.22.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.23
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.23.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.23.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.23.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.23.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.23.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.23.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.23.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.23.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.23.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.23.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.23.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.23.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.23.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.23.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.23.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.23.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.24
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.24.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.24.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.24.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.24.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.24.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.24.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.24.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.24.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.24.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.24.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.4
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.4.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.4.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.4.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.4.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.24.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.24.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.24.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.24.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.24.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.24.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.25
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.25.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.25.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.25.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.25.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.25.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.25.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.25.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.25.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.25.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.25.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.25.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.25.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.25.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.25.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.25.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.25.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.26
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.26.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.26.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.26.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.26.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.26.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.26.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.26.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.26.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.26.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.26.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.26.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.26.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.26.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.26.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.26.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.26.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.27
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.27.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.27.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.27.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.27.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.27.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.27.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.27.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.27.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.27.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.27.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.9
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.9.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.9.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.9.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.9.act_fn
SiLU()
model.layers.27.block_sparse_moe.experts.13
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.27.block_sparse_moe.experts.13.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.13.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.27.block_sparse_moe.experts.13.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.27.block_sparse_moe.experts.13.act_fn
SiLU()
model.layers.27.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.27.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.28
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.28.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.28.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.28.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.28.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.28.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.28.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.28.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.28.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.28.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.28.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.2
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.2.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.2.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.2.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.2.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.6
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.6.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.6.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.6.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.6.act_fn
SiLU()
model.layers.28.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.28.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.28.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.28.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.28.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.28.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.29
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.29.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.29.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.29.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.29.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.29.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.29.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.29.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.29.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.29.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.29.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.7
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.7.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.7.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.7.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.7.act_fn
SiLU()
model.layers.29.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.29.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.29.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.29.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.29.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.29.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.30
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.30.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.30.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.30.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.30.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.30.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.30.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.30.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.30.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.30.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.30.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.8
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.8.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.8.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.8.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.8.act_fn
SiLU()
model.layers.30.block_sparse_moe.experts.15
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.30.block_sparse_moe.experts.15.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.15.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.30.block_sparse_moe.experts.15.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.30.block_sparse_moe.experts.15.act_fn
SiLU()
model.layers.30.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.30.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.31
PhiMoEDecoderLayer(
  (self_attn): PhiMoESdpaAttention(
    (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
    (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
    (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
  )
  (block_sparse_moe): PhiMoESparseMoeBlock(
    (gate): Linear(in_features=4096, out_features=16, bias=False)
    (experts): ModuleList(
      (0-15): 16 x PhiMoEBlockSparseTop2MLP(
        (w1): Linear(in_features=4096, out_features=6400, bias=False)
        (w2): Linear(in_features=6400, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=6400, bias=False)
        (act_fn): SiLU()
      )
    )
  )
  (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
model.layers.31.self_attn
PhiMoESdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()
)
model.layers.31.self_attn.q_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.31.self_attn.k_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.31.self_attn.v_proj
Linear(in_features=4096, out_features=1024, bias=True)
model.layers.31.self_attn.o_proj
Linear(in_features=4096, out_features=4096, bias=True)
model.layers.31.self_attn.rotary_emb
Phi3LongRoPEScaledRotaryEmbedding()
model.layers.31.block_sparse_moe
PhiMoESparseMoeBlock(
  (gate): Linear(in_features=4096, out_features=16, bias=False)
  (experts): ModuleList(
    (0-15): 16 x PhiMoEBlockSparseTop2MLP(
      (w1): Linear(in_features=4096, out_features=6400, bias=False)
      (w2): Linear(in_features=6400, out_features=4096, bias=False)
      (w3): Linear(in_features=4096, out_features=6400, bias=False)
      (act_fn): SiLU()
    )
  )
)
model.layers.31.block_sparse_moe.gate
Linear(in_features=4096, out_features=16, bias=False)
model.layers.31.block_sparse_moe.experts
ModuleList(
  (0-15): 16 x PhiMoEBlockSparseTop2MLP(
    (w1): Linear(in_features=4096, out_features=6400, bias=False)
    (w2): Linear(in_features=6400, out_features=4096, bias=False)
    (w3): Linear(in_features=4096, out_features=6400, bias=False)
    (act_fn): SiLU()
  )
)
model.layers.31.block_sparse_moe.experts.0
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.0.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.0.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.0.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.0.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.1
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.1.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.1.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.1.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.1.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.10
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.10.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.10.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.10.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.10.act_fn
SiLU()
model.layers.31.block_sparse_moe.experts.14
PhiMoEBlockSparseTop2MLP(
  (w1): Linear(in_features=4096, out_features=6400, bias=False)
  (w2): Linear(in_features=6400, out_features=4096, bias=False)
  (w3): Linear(in_features=4096, out_features=6400, bias=False)
  (act_fn): SiLU()
)
model.layers.31.block_sparse_moe.experts.14.w1
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.14.w2
Linear(in_features=6400, out_features=4096, bias=False)
model.layers.31.block_sparse_moe.experts.14.w3
Linear(in_features=4096, out_features=6400, bias=False)
model.layers.31.block_sparse_moe.experts.14.act_fn
SiLU()
model.layers.31.input_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.layers.31.post_attention_layernorm
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
model.norm
2025-01-08:11:27:24,381 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-01-08:11:27:24,381 INFO     [evaluator.py:217] Using pre-initialized model
Using the latest cached version of the module from /root/autodl-tmp/huggingface/modules/datasets_modules/datasets/hails--mmlu_no_train/6e75f77a579ee4ce0a66d822aa8a2660c81c6c5b49d1a3489f5a787e37886aa9 (last modified on Tue Jan  7 01:08:17 2025) since it couldn't be found locally at hails/mmlu_no_train, or remotely on the Hugging Face Hub.
2025-01-08:11:29:04,470 WARNING  [load.py:1569] Using the latest cached version of the module from /root/autodl-tmp/huggingface/modules/datasets_modules/datasets/hails--mmlu_no_train/6e75f77a579ee4ce0a66d822aa8a2660c81c6c5b49d1a3489f5a787e37886aa9 (last modified on Tue Jan  7 01:08:17 2025) since it couldn't be found locally at hails/mmlu_no_train, or remotely on the Hugging Face Hub.
2025-01-08:11:29:04,541 WARNING  [evaluator.py:270] Overwriting default num_fewshot of mmlu_formal_logic from None to 0
2025-01-08:11:29:04,541 INFO     [task.py:415] Building contexts for mmlu_formal_logic on rank 0...
LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
lm_head
Linear(in_features=4096, out_features=32064, bias=True)
  0%|                                                                                                                                                                                   | 0/50 [00:00<?, ?it/s] 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 49/50 [00:00<00:00, 482.56it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 480.80it/s]
2025-01-08:11:29:04,649 INFO     [evaluator.py:496] Running loglikelihood requests
Running loglikelihood requests:   0%|                                                                                                                                                  | 0/200 [00:00<?, ?it/s]Layer: gate_0 - Captured router_logits: [0.12767553329467773, 0.1476399302482605, 0.13753584027290344, -0.25120314955711365, -0.21148747205734253, -0.18403714895248413, 0.14871038496494293, -0.24392928183078766, 0.09702540189027786, 0.11798734962940216, 0.12694254517555237, 0.12567636370658875, 0.10696733742952347, 0.1322627067565918, -1.1314178705215454, 0.15078505873680115]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.08978470414876938, 0.05750463157892227, 0.040560148656368256, 0.06561991572380066, 0.10415653884410858, 0.04580356925725937, 0.05951523780822754, 0.0644366517663002, -0.0039342595264315605, 0.058632999658584595, -0.1699102520942688, 0.04519868642091751, -0.008448905311524868, -0.01793059892952442, 0.003936289809644222, 0.019832797348499298]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07598169893026352, 0.06151936203241348, 0.11214672029018402, 0.043500278145074844, 0.11861619353294373, 0.12227878719568253, 0.05920039862394333, -0.09910643845796585, 0.08013185858726501, 0.06962702423334122, 0.00023031080490909517, 0.10235805064439774, -0.1855447143316269, 0.018693940714001656, -0.06488310545682907, 0.10044530034065247]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12931092083454132, 0.18419961631298065, 0.13358497619628906, 0.15437644720077515, 0.14768895506858826, 0.12030887603759766, 0.08798834681510925, 0.13807611167430878, 0.1450391262769699, -0.581216037273407, -0.055002227425575256, 0.10270878672599792, 0.1261269599199295, -0.35234907269477844, -0.1054408997297287, -0.10840219259262085]
Layer: gate_3 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07804818451404572, 0.07286505401134491, 0.07438411563634872, 0.07197578996419907, 0.06273876875638962, -0.10942259430885315, 0.07429317384958267, 0.04158996418118477, -0.07947081327438354, 0.009588387794792652, -0.18437543511390686, 0.12003745883703232, -0.10090851038694382, -0.16671381890773773, 0.08708175271749496, -0.04961004480719566]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.1440601497888565, 0.13649362325668335, 0.124404177069664, 0.07988423854112625, -0.03981626033782959, -0.018657390028238297, -0.019991744309663773, 0.06795496493577957, 0.07211150228977203, -0.12697762250900269, -0.3118111193180084, 0.15479004383087158, -0.2056029587984085, 0.1497587263584137, 0.14079253375530243, 0.07855742424726486]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.19399070739746094, 0.10645444691181183, 0.08379344642162323, 0.23210835456848145, 0.16166481375694275, 0.11766134947538376, 0.01305991318076849, -0.04945458099246025, 0.05121656134724617, 0.18106098473072052, -0.5444676876068115, 0.13507375121116638, 0.3328782618045807, -0.20834068953990936, 0.10662403702735901, 0.16971167922019958]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.18317125737667084, 0.1296117603778839, 0.17057636380195618, 0.06301021575927734, -0.8332035541534424, -0.2110755294561386, -0.08605409413576126, -0.16431009769439697, -0.17455334961414337, -0.21003615856170654, -0.24704548716545105, -0.019578853622078896, 0.06361506879329681, -0.014125716872513294, -0.5624502897262573, -0.2783736288547516]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21543745696544647, 0.2744598388671875, -0.08891700208187103, 0.42017847299575806, 0.2967885136604309, 0.3568415343761444, 0.16688452661037445, 0.14880171418190002, 0.009570751339197159, 0.26484641432762146, 0.23825468122959137, -0.043316129595041275, 0.45612290501594543, -0.2514611780643463, 0.034310027956962585, 0.18764173984527588]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3175925612449646, 0.2612662613391876, 0.1383744329214096, 0.6133768558502197, 0.2691015899181366, 0.3018757700920105, 0.6926265954971313, 0.632737398147583, 0.19612690806388855, -0.04569209739565849, 0.21201854944229126, 0.48309940099716187, 0.5785840749740601, 0.2887697219848633, 0.7315404415130615, 0.5601456165313721]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.6528536677360535, 0.7027493715286255, 0.17916782200336456, 0.5678756237030029, 0.6337207555770874, -0.18497280776500702, 0.31425705552101135, 0.5974061489105225, -0.06575467437505722, -0.30152857303619385, 0.4003514349460602, 0.5770179033279419, -0.008411997929215431, 0.22413001954555511, 0.23540237545967102, 0.3481735289096832]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.25637054443359375, 0.48059818148612976, 0.5923231840133667, 0.4304204285144806, 0.6496397256851196, 0.23096846044063568, 0.6081163883209229, 0.3033171594142914, 0.48877865076065063, 0.8461237549781799, 0.4875023066997528, 0.3323495388031006, 0.24358244240283966, 0.029696397483348846, -0.3386029899120331, 0.5155919194221497]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.1304209679365158, 1.1006722450256348, 0.8943297863006592, 0.557088315486908, 0.1943986415863037, 0.5387824177742004, 0.4064200818538666, 0.5799064040184021, 0.5792465209960938, 0.47833967208862305, 1.4148458242416382, 0.7320885062217712, 0.859069287776947, 0.7458518743515015, 0.6022964715957642, 0.6476866602897644]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.1878221035003662, 1.1319998502731323, 1.3466120958328247, 0.6474205851554871, 0.9751774072647095, 0.476222425699234, 1.0691746473312378, 1.188184142112732, 1.002297282218933, 1.0178903341293335, 1.1020675897598267, 1.4050325155258179, 0.4046957492828369, 1.2908384799957275, 1.0969774723052979, 0.8748371005058289]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.4716799557209015, 0.7307737469673157, 1.2573273181915283, 1.1534137725830078, 0.9967984557151794, 1.3227097988128662, -0.20150332152843475, 0.44135043025016785, 0.6364058256149292, 0.46848276257514954, -0.25850754976272583, 0.5521000027656555, 0.833758533000946, 0.9029015898704529, 1.1312510967254639, 0.37387457489967346]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.7764430642127991, 1.334385633468628, 1.2619132995605469, 0.9718907475471497, 1.1246347427368164, 1.2801563739776611, 1.3828012943267822, 0.8188561797142029, 0.7907272577285767, 1.0900298357009888, 1.051146388053894, 0.17399713397026062, 1.748824119567871, 0.9499109983444214, 0.9130396842956543, 1.0293761491775513]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9455837607383728, 0.6483842134475708, 1.0805115699768066, 1.0431386232376099, 0.9707240462303162, 0.8179517388343811, -0.4502953290939331, 1.7164545059204102, -0.4831746816635132, 1.7731105089187622, -0.5191048979759216, 1.1004160642623901, 1.434503197669983, 1.337206244468689, 1.241030216217041, -0.09725390374660492]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:   0%|▋                                                                                                                                         | 1/200 [00:07<23:21,  7.04s/it]Layer: gate_17 - Captured router_logits: [0.8849290013313293, 1.4051343202590942, 1.6888240575790405, 1.9669691324234009, 1.9536426067352295, 1.7350873947143555, 1.6146609783172607, 1.9057029485702515, 1.5746484994888306, 1.7077699899673462, 1.980463981628418, 1.4822067022323608, 1.5804436206817627, 1.6545370817184448, 1.412491798400879, 1.885416030883789]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5107533931732178, 1.1361968517303467, 1.6691237688064575, 1.8682539463043213, 1.310271978378296, 1.6792770624160767, 1.3419824838638306, 2.1849448680877686, 2.2857797145843506, 1.5907474756240845, 1.5064178705215454, 1.3777605295181274, 1.6386905908584595, 1.5251721143722534, 1.6795011758804321, 1.4989113807678223]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.167064905166626, 1.939396858215332, 1.6534392833709717, 1.8686223030090332, 1.966253638267517, 1.8596408367156982, 1.9954322576522827, 1.9552868604660034, 1.9911861419677734, 1.7664353847503662, 1.983203649520874, 1.939442753791809, 2.611112356185913, 1.9894636869430542, 2.0405924320220947, 1.8511179685592651]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.7631223201751709, 1.3744319677352905, 1.330230951309204, 0.8811479806900024, 0.5036574006080627, 1.4473813772201538, 1.3179398775100708, 1.6255433559417725, 0.9599548578262329, 1.227527141571045, 1.2676137685775757, 1.0749058723449707, 1.0972011089324951, 1.2502334117889404, 1.3811185359954834, 0.9538727402687073]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.502678394317627, 2.612231731414795, 2.4894840717315674, 2.6231963634490967, 2.619567632675171, 2.7344281673431396, 2.791475296020508, 2.7449700832366943, 2.528228521347046, 2.8206653594970703, 2.761050224304199, 2.795396089553833, 2.6177523136138916, 2.7220704555511475, 2.9874331951141357, 2.780536413192749]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [2.088294744491577, 1.760785698890686, 1.9675720930099487, 1.7097684144973755, 2.0968761444091797, 1.921286940574646, 2.0310118198394775, 1.7619781494140625, 2.0305323600769043, 2.1282689571380615, 1.6864079236984253, 2.0461878776550293, 1.957607388496399, 1.9432373046875, 1.9653897285461426, 2.2339611053466797]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.352285623550415, 2.047205686569214, 2.0433478355407715, 1.9844497442245483, 2.1037251949310303, 2.024897336959839, 1.9330188035964966, 2.156586170196533, 1.9916857481002808, 1.912054419517517, 2.095304012298584, 1.9215072393417358, 2.124762535095215, 1.4557242393493652, 2.1477103233337402, 2.175640821456909]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.9190239906311035, 3.1336755752563477, 2.84049129486084, 2.832486391067505, 3.1339404582977295, 3.0647990703582764, 3.1131420135498047, 2.90396785736084, 3.0631234645843506, 3.1015186309814453, 2.983630418777466, 3.0355868339538574, 2.8499813079833984, 3.1032192707061768, 3.0339243412017822, 3.1403098106384277]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_25 - Captured router_logits: [2.1745965480804443, 2.0864148139953613, 2.011172294616699, 2.2278409004211426, 1.7526189088821411, 2.05598783493042, 2.071596384048462, 2.0798521041870117, 2.1182608604431152, 2.0912938117980957, 2.1389026641845703, 2.0896668434143066, 2.1209568977355957, 2.124443292617798, 1.9252204895019531, 2.3420615196228027]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.1890079975128174, 2.0311522483825684, 2.0505287647247314, 2.215559244155884, 2.060997486114502, 2.0448787212371826, 2.0689425468444824, 2.0528018474578857, 1.9944257736206055, 2.2161009311676025, 2.359872341156006, 2.076876163482666, 2.1776492595672607, 2.0922486782073975, 2.0017173290252686, 2.1014275550842285]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.9154976606369019, 1.8626595735549927, 1.8916808366775513, 1.994696021080017, 1.8797731399536133, 1.9030121564865112, 1.9529286623001099, 1.9927748441696167, 1.9597352743148804, 2.0420966148376465, 1.6543078422546387, 1.914805293083191, 1.973589301109314, 1.8589801788330078, 1.8944783210754395, 1.887992024421692]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.3467891216278076, 3.3895480632781982, 3.6285476684570312, 3.4375524520874023, 3.4983737468719482, 3.360567331314087, 3.451611280441284, 3.5266692638397217, 3.3498241901397705, 3.462623357772827, 3.383244752883911, 3.215498447418213, 3.3766603469848633, 3.2132112979888916, 3.476649522781372, 3.327946186065674]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.249533176422119, 7.663609504699707, 7.422752380371094, 7.291304111480713, 7.1400251388549805, 6.961308002471924, 7.510744094848633, 7.426399230957031, 7.427495002746582, 7.219820976257324, 7.271252632141113, 7.118618965148926, 7.286895751953125, 7.401111125946045, 7.607102870941162, 7.28544282913208]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.479706764221191, 4.623009204864502, 4.50350284576416, 4.247082710266113, 4.758090019226074, 4.500610828399658, 4.3823933601379395, 4.437306880950928, 4.560248851776123, 4.605187892913818, 4.549882888793945, 4.060942649841309, 4.4730544090271, 4.740981578826904, 4.5779032707214355, 4.32875394821167]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.208721876144409, 3.0221500396728516, 2.970585346221924, 2.7648684978485107, 2.945425271987915, 3.121356964111328, 2.9449574947357178, 2.902712106704712, 2.9544835090637207, 3.1492605209350586, 3.1084840297698975, 2.593315839767456, 3.0321104526519775, 3.037400484085083, 3.154315233230591, 3.152085304260254]
Layer: gate_31 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.10039185732603073, 0.10681203007698059, 0.1076117679476738, -0.21450397372245789, -0.17500008642673492, -0.0854950100183487, 0.13039667904376984, -0.20843228697776794, 0.07344506680965424, 0.09952949732542038, 0.09530476480722427, 0.08827060461044312, 0.09381060302257538, 0.1104678139090538, -0.9591925144195557, 0.1227227970957756]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07809445261955261, 0.04377082362771034, 0.023487476631999016, 0.0648828074336052, 0.06854798644781113, 0.0519026480615139, 0.03062637709081173, 0.06155800819396973, 0.001407455070875585, 0.05396290123462677, -0.12448574602603912, 0.051320627331733704, 0.004571665544062853, 0.008026053197681904, 0.01269544381648302, 0.009673708118498325]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.054603274911642075, 0.0620490126311779, 0.09799166768789291, 0.020344970747828484, 0.07734937220811844, 0.12505683302879333, 0.050258491188287735, -0.0831543579697609, 0.08543488383293152, 0.07297830283641815, -0.0025883375201374292, 0.09608016908168793, -0.1298125833272934, 0.02122744731605053, -0.04035408794879913, 0.09920996427536011]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11559862643480301, 0.1355680376291275, 0.12864463031291962, 0.11828748881816864, 0.12394485622644424, 0.12814539670944214, 0.010676957666873932, 0.13355103135108948, 0.13547398149967194, -0.5534183979034424, 0.040256403386592865, 0.09054139256477356, 0.09238558262586594, -0.3124924302101135, -0.07549479603767395, -0.06085806339979172]
Layer: gate_3 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.05486889183521271, 0.07952354848384857, 0.06824636459350586, 0.03996090963482857, 0.11508398503065109, -0.09105732291936874, 0.02391275018453598, 0.002939669182524085, -0.06277994811534882, -0.00585787370800972, -0.1771843582391739, 0.10680937021970749, -0.11068432778120041, -0.18591582775115967, 0.10075800120830536, -0.10350542515516281]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07071341574192047, 0.10433819890022278, 0.03789543733000755, 0.028867527842521667, -0.02287481538951397, -0.06816717237234116, -0.004008303862065077, 0.06483878940343857, 0.04261801391839981, -0.12196151912212372, -0.2047104835510254, 0.15146183967590332, -0.17005354166030884, 0.14756286144256592, 0.11894036829471588, 0.06673669070005417]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.17212042212486267, 0.03218341991305351, 0.05289933830499649, 0.2440582513809204, 0.13799194991588593, 0.11894185096025467, -0.12490949779748917, -0.08371232450008392, -0.016428356990218163, 0.2171543836593628, -0.5141769647598267, 0.164153590798378, 0.361253559589386, -0.24946308135986328, 0.11165950447320938, 0.1582997739315033]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.2882098853588104, 0.02193772792816162, 0.07642596960067749, -0.009227292612195015, -0.9262765049934387, -0.29485929012298584, -0.10632496327161789, -0.42869555950164795, -0.1896795630455017, -0.2651502788066864, -0.16790558397769928, -0.1196213811635971, 0.027063017711043358, -0.06310010701417923, -0.44074469804763794, -0.02346199005842209]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.1802099049091339, 0.2348172962665558, -0.04567684605717659, 0.4348042905330658, 0.26927798986434937, 0.36417508125305176, 0.18162289261817932, 0.16694830358028412, -0.010589473880827427, 0.1637182980775833, 0.27882298827171326, 0.010960941202938557, 0.5012204647064209, -0.12797561287879944, 0.016141613945364952, 0.13836848735809326]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.330352246761322, 0.2019014209508896, 0.12370549142360687, 0.6627125144004822, 0.37857723236083984, 0.4295639991760254, 0.7230567336082458, 0.7706378698348999, 0.20012061297893524, 0.12131960690021515, 0.32162150740623474, 0.5385565161705017, 0.6324317455291748, 0.3267253339290619, 0.838866114616394, 0.6453428268432617]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7245215177536011, 0.7782675623893738, 0.25277701020240784, 0.6020082831382751, 0.6309712529182434, -0.17236724495887756, 0.37896236777305603, 0.730873167514801, -0.08245749026536942, -0.38676130771636963, 0.4690608084201813, 0.5418228507041931, 0.2084730714559555, 0.23768876492977142, 0.2093956619501114, 0.36624690890312195]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.35124772787094116, 0.5695926547050476, 0.7837507724761963, 0.4973577857017517, 0.6318735480308533, 0.17483724653720856, 0.6577112078666687, 0.29366323351860046, 0.5689787864685059, 0.9113967418670654, 0.5677134990692139, 0.36775949597358704, 0.20030425488948822, 0.16583320498466492, -0.3045811653137207, 0.6165542602539062]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.16192586719989777, 1.000118613243103, 0.8560755252838135, 0.5282065272331238, 0.1820043921470642, 0.4618513286113739, 0.3445199728012085, 0.47516828775405884, 0.43480178713798523, 0.4359632432460785, 1.5368549823760986, 0.772150993347168, 0.7664821743965149, 0.7539283037185669, 0.6557750105857849, 0.6718502044677734]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.1663587093353271, 1.1642991304397583, 1.5192527770996094, 0.6443796157836914, 1.1557973623275757, 0.2671692371368408, 1.0979855060577393, 0.9464607238769531, 1.0084760189056396, 1.0016216039657593, 1.079710602760315, 1.465728521347046, 0.4237842559814453, 1.2952853441238403, 1.2033700942993164, 0.8456964492797852]
Layer: gate_13 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.50575852394104, 0.7449721693992615, 1.2201281785964966, 1.1488791704177856, 0.8618514537811279, 0.9164677858352661, -0.174862802028656, 0.4531939923763275, 0.5493483543395996, 0.5779764652252197, -0.3743817210197449, 0.34190866351127625, 0.8916486501693726, 0.9031396508216858, 1.1022180318832397, 0.253404438495636]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6338335275650024, 1.2769767045974731, 1.1458302736282349, 0.9482612013816833, 0.9359260201454163, 1.2217410802841187, 1.3560384511947632, 0.7865707874298096, 0.7887244820594788, 1.008832335472107, 0.8724429607391357, 0.1537727266550064, 1.6477954387664795, 0.9197039604187012, 0.8794704675674438, 0.8163765668869019]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9601998329162598, 0.5970605611801147, 1.0565476417541504, 1.150130271911621, 0.8352857828140259, 0.7361980676651001, -0.4061215817928314, 1.551916480064392, -0.38460204005241394, 1.5842305421829224, -0.6215771436691284, 0.9432468414306641, 1.3133646249771118, 1.2926690578460693, 1.170603632926941, -0.28263941407203674]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.8997415900230408, 1.659786581993103, 1.7660108804702759, 2.0381863117218018, 2.0674824714660645, 1.8048310279846191, 1.675768494606018, 1.9902949333190918, 1.685211181640625, 1.8006209135055542, 1.9915133714675903, 1.5268900394439697, 1.4973820447921753, 1.6386113166809082, 1.459210753440857, 2.01328706741333]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.679477572441101, 1.2035075426101685, 1.8535335063934326, 2.193368911743164, 1.5374866724014282, 1.8806151151657104, 1.4731979370117188, 2.2394490242004395, 2.4564218521118164, 1.7298623323440552, 1.600512981414795, 1.428537130355835, 1.7172582149505615, 1.703791856765747, 1.7481086254119873, 1.6849548816680908]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.219653606414795, 1.9869499206542969, 1.7134853601455688, 1.9196292161941528, 2.1048574447631836, 1.8512521982192993, 2.035041093826294, 1.9824856519699097, 1.8259844779968262, 1.7894877195358276, 2.0908548831939697, 1.9224900007247925, 2.758653163909912, 2.1039297580718994, 2.0736305713653564, 2.0172488689422607]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.7442054748535156, 1.3763632774353027, 1.29945969581604, 0.8070727586746216, 0.32596907019615173, 1.4480574131011963, 1.3096259832382202, 1.1644405126571655, 1.0562520027160645, 1.1960828304290771, 1.2433576583862305, 1.069686770439148, 1.1848615407943726, 1.335508108139038, 1.2151182889938354, 0.9924684762954712]
Running loglikelihood requests:   2%|███▍                                                                                                                                      | 5/200 [00:13<08:07,  2.50s/it]Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.359553337097168, 2.4606635570526123, 2.355412721633911, 2.438707113265991, 2.3914566040039062, 2.5853943824768066, 2.582691192626953, 2.559340000152588, 2.373093366622925, 2.569258689880371, 2.588555097579956, 2.3407833576202393, 2.3597350120544434, 2.5114541053771973, 2.7141010761260986, 2.6517598628997803]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.7941088676452637, 1.5055979490280151, 1.65768301486969, 1.558349370956421, 1.7985950708389282, 1.7129570245742798, 1.7336870431900024, 1.4263594150543213, 1.7764251232147217, 1.900709867477417, 1.4428187608718872, 1.558266282081604, 1.7144348621368408, 1.6376923322677612, 1.7243151664733887, 2.0093557834625244]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.352252721786499, 1.8895865678787231, 1.7242878675460815, 1.752877116203308, 1.8795958757400513, 1.813452124595642, 1.7656596899032593, 1.9378997087478638, 1.8127115964889526, 1.618734359741211, 1.8861836194992065, 1.7569540739059448, 1.7845542430877686, 1.208168387413025, 1.7706042528152466, 1.883132815361023]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.624497652053833, 2.8717501163482666, 2.6520133018493652, 2.6231460571289062, 2.754167079925537, 2.813960075378418, 2.826380968093872, 2.6261889934539795, 2.71523118019104, 2.7905149459838867, 2.740229845046997, 2.702733278274536, 2.6360340118408203, 2.88839054107666, 2.7859885692596436, 2.81284236907959]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_25 - Captured router_logits: [2.065974712371826, 1.9698381423950195, 1.9750752449035645, 2.214449167251587, 1.6776186227798462, 1.9682400226593018, 1.992220163345337, 2.0760021209716797, 2.0487122535705566, 2.0432329177856445, 2.0773775577545166, 2.012544631958008, 2.009371042251587, 2.0576372146606445, 1.7947032451629639, 2.2406439781188965]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.088585615158081, 1.9608217477798462, 1.9385530948638916, 2.147693395614624, 1.9779809713363647, 2.0066778659820557, 2.03483247756958, 1.9260785579681396, 1.9856702089309692, 2.2026472091674805, 2.308570623397827, 2.083314895629883, 2.0725796222686768, 2.1116557121276855, 1.9545239210128784, 2.0679709911346436]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.9765918254852295, 1.8984875679016113, 1.9689606428146362, 2.0519003868103027, 1.8861167430877686, 1.9058853387832642, 2.0428500175476074, 2.0358970165252686, 1.9785510301589966, 2.0602591037750244, 1.6692739725112915, 1.9548574686050415, 2.0441250801086426, 1.9362927675247192, 1.9766286611557007, 1.9524133205413818]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.254493236541748, 3.235224485397339, 3.5389859676361084, 3.3127260208129883, 3.418728828430176, 3.2708351612091064, 3.236912965774536, 3.3869388103485107, 3.1944150924682617, 3.3289873600006104, 3.3066089153289795, 3.1504745483398438, 3.377873659133911, 3.098410129547119, 3.369638442993164, 3.22115421295166]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [6.899745464324951, 7.195647239685059, 6.991649627685547, 6.8656158447265625, 6.818194389343262, 6.54033088684082, 7.257194995880127, 7.020468711853027, 7.08929443359375, 6.853488922119141, 6.916121006011963, 6.778170585632324, 6.942714691162109, 7.046378135681152, 7.215932846069336, 6.894220352172852]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.132546901702881, 4.231204986572266, 4.077531337738037, 3.7619247436523438, 4.337630748748779, 4.226814270019531, 4.019654273986816, 4.058825492858887, 4.146944046020508, 4.167148590087891, 4.2138848304748535, 3.6764345169067383, 4.012148380279541, 4.233938694000244, 4.201613426208496, 4.03494930267334]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2224810123443604, 3.091912269592285, 3.0437066555023193, 2.7409615516662598, 3.016083240509033, 3.161930561065674, 2.9684855937957764, 3.030407428741455, 3.015291690826416, 3.1399617195129395, 3.2113211154937744, 2.6127285957336426, 3.089456558227539, 3.080278158187866, 3.2438321113586426, 3.170954942703247]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.14017972350120544, 0.14938943088054657, 0.13847525417804718, -0.22215810418128967, -0.1828320026397705, -0.19466347992420197, 0.15967459976673126, -0.2588910460472107, 0.10701028257608414, 0.1271018534898758, 0.13291506469249725, 0.08815135806798935, 0.11717390269041061, 0.13361714780330658, -1.0801955461502075, 0.1525995284318924]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09247147291898727, 0.06621703505516052, 0.03163977712392807, 0.06398074328899384, 0.10450206696987152, 0.04862283170223236, 0.05784009397029877, 0.05585723742842674, 0.01526265311986208, 0.07078850269317627, -0.1593002825975418, 0.05333157628774643, -0.0005892766057513654, -0.003613261040300131, -0.007907290942966938, -0.0009735870989970863]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.052338194102048874, 0.09287840127944946, 0.09865453839302063, 0.046573247760534286, 0.08945254981517792, 0.1235603392124176, 0.05405987426638603, -0.07790887355804443, 0.07137619704008102, 0.04829920828342438, -0.01284538023173809, 0.09963558614253998, -0.13899509608745575, 0.046549733728170395, -0.027839042246341705, 0.11396157741546631]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12202736735343933, 0.16362395882606506, 0.13410060107707977, 0.1493874341249466, 0.112349733710289, 0.12645025551319122, 0.06763293594121933, 0.1825713962316513, 0.14855335652828217, -0.5603960156440735, -0.06669606268405914, 0.09452179074287415, 0.04999905452132225, -0.2646600306034088, -0.18426162004470825, -0.09099166840314865]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.059025172144174576, 0.09001506119966507, 0.06691696494817734, -0.016667449846863747, 0.036848150193691254, -0.155878484249115, 0.07198170572519302, 0.028425069525837898, 0.060415688902139664, -0.017407972365617752, -0.10064466297626495, 0.14808978140354156, -0.20199726521968842, -0.14263342320919037, 0.07663402706384659, -0.00013438246969599277]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.11028379201889038, 0.14178518950939178, 0.07257212698459625, -0.025668377056717873, 0.0017986168386414647, 0.0454920269548893, 0.009869136847555637, 0.1359313577413559, 0.022960690781474113, -0.16212689876556396, -0.29246753454208374, 0.1664045751094818, -0.13251245021820068, 0.11074449121952057, 0.19716991484165192, 0.06669081002473831]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.1308719366788864, 0.13946667313575745, -0.026806505396962166, 0.24927426874637604, 0.14679910242557526, 0.04468962922692299, 0.051719531416893005, 0.05658235400915146, 0.05470934882760048, 0.1453857123851776, -0.46943745017051697, 0.14814843237400055, 0.3640882074832916, -0.13691234588623047, 0.07117896527051926, 0.1487564891576767]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.3668276369571686, 0.1378050148487091, 0.1795409768819809, 0.056306470185518265, -0.8752568364143372, -0.2839709222316742, -0.16946354508399963, -0.12197423726320267, -0.2988802492618561, -0.3325175344944, -0.045316632837057114, -0.0834733322262764, 0.188765287399292, -0.1006663367152214, -0.31086844205856323, -0.08905524760484695]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.1549765020608902, 0.25026941299438477, 0.06787729263305664, 0.4059554934501648, 0.21683402359485626, 0.384247362613678, 0.164831280708313, 0.3458315432071686, -0.05123275890946388, -0.09082178771495819, 0.2918510437011719, -0.06499017775058746, 0.3992442190647125, -0.05009409412741661, 0.07939518988132477, 0.13787305355072021]
Layer: gate_8 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.5147561430931091, 0.5358676910400391, 0.2774292230606079, 0.5458826422691345, 0.43859201669692993, 0.35597583651542664, 0.9636749029159546, 0.9329400658607483, 0.19217093288898468, 0.06373988091945648, 0.6018808484077454, 0.6601145267486572, 0.8481245040893555, 0.5092326998710632, 1.0700945854187012, 0.7981150150299072]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7985978722572327, 0.9273937940597534, 0.3764435648918152, 0.7423467636108398, 0.6845442652702332, -0.15964989364147186, 0.5356338024139404, 0.8894486427307129, 0.32681289315223694, -0.2743469476699829, 0.5908220410346985, 0.6736409664154053, 0.5582774877548218, 0.3310551643371582, 0.08358597010374069, 0.6868413090705872]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.43912744522094727, 0.7126736044883728, 0.9468153715133667, 0.5288843512535095, 0.7271431684494019, 0.3558696508407593, 0.8684424757957458, 0.5476213097572327, 0.6509416103363037, 0.9932294487953186, 0.7294791340827942, 0.7284122705459595, 0.5929322838783264, 0.5094946622848511, 0.13183632493019104, 0.9183123707771301]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.37106138467788696, 1.1176173686981201, 0.8132131695747375, 0.605099081993103, 0.5661242604255676, 0.6694470643997192, 0.3579532206058502, 0.3939392566680908, 0.6461813449859619, 0.612335205078125, 1.5152125358581543, 0.7965558171272278, 0.9542718529701233, 0.8741811513900757, 0.7400240302085876, 0.743101954460144]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.2737810611724854, 1.1896661520004272, 1.3689857721328735, 0.6731526255607605, 1.0155764818191528, 0.3493867516517639, 1.110003113746643, 0.4962611794471741, 1.1161237955093384, 1.049797534942627, 0.9981263875961304, 1.467624306678772, 0.6028063297271729, 1.328547716140747, 1.1648526191711426, 0.9895725250244141]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5688893795013428, 0.9984206557273865, 1.2250789403915405, 1.191918134689331, 0.9565553069114685, 0.8356269001960754, -0.08850281685590744, 0.1762741357088089, 0.6548425555229187, 0.6081909537315369, -0.4654276669025421, 0.5349222421646118, 0.9987443089485168, 0.9405020475387573, 1.1234182119369507, 0.4760804772377014]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [1.058298945426941, 1.3856064081192017, 1.3437625169754028, 1.108791470527649, 1.2711926698684692, 1.4217627048492432, 1.3516451120376587, 0.9623439311981201, 0.9691088795661926, 1.0801934003829956, 1.210469126701355, 0.6556766033172607, 1.6923574209213257, 1.2073901891708374, 1.219382643699646, 0.815652072429657]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.2468537092208862, 0.839912474155426, 1.0020464658737183, 0.991980254650116, 0.8011205792427063, 0.9253655076026917, -0.4702069163322449, 1.6555064916610718, 0.1948966532945633, 1.5060540437698364, 0.15866480767726898, 1.0713708400726318, 1.3773046731948853, 1.2449074983596802, 1.021539330482483, 0.113584503531456]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [2.041630268096924, 2.3313257694244385, 2.0910000801086426, 2.8031232357025146, 2.3127763271331787, 2.0746281147003174, 2.056140184402466, 2.464733362197876, 1.9648405313491821, 2.2813172340393066, 2.153563976287842, 1.7964072227478027, 1.6243716478347778, 1.8782095909118652, 2.4172072410583496, 2.6904029846191406]
Layer: gate_17 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6870800256729126, 1.328464388847351, 1.8849513530731201, 2.0811593532562256, 1.867326021194458, 2.0259552001953125, 1.549161434173584, 2.270542621612549, 2.3463408946990967, 1.7828062772750854, 2.0677740573883057, 1.9624489545822144, 2.1369121074676514, 2.1527063846588135, 2.0079851150512695, 1.9053319692611694]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.5420567989349365, 2.210749864578247, 1.92471444606781, 2.289987802505493, 2.374028444290161, 2.2443885803222656, 2.42203426361084, 2.3628337383270264, 2.0110697746276855, 2.037402629852295, 2.369033098220825, 2.2114145755767822, 2.761672019958496, 2.4612948894500732, 2.3996078968048096, 2.293440580368042]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.1408307552337646, 1.7621605396270752, 1.6636055707931519, 1.308562994003296, 0.7139426469802856, 1.8410756587982178, 1.690661072731018, 1.485878586769104, 1.3303340673446655, 1.621964931488037, 1.504312515258789, 1.4652726650238037, 1.527123212814331, 1.7315213680267334, 1.6870102882385254, 1.3881077766418457]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.9803082942962646, 3.040565013885498, 2.9464545249938965, 3.1681206226348877, 2.944805145263672, 3.152374744415283, 3.170413017272949, 3.154026985168457, 3.0505294799804688, 3.1900582313537598, 3.2718985080718994, 2.889816999435425, 2.911073923110962, 3.173065423965454, 3.289665699005127, 3.1282927989959717]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [2.170799493789673, 1.8254785537719727, 1.9232655763626099, 1.8272827863693237, 2.1121139526367188, 2.087951898574829, 2.100529670715332, 1.7721221446990967, 2.169081449508667, 2.167093515396118, 1.8171029090881348, 1.8294103145599365, 2.1353065967559814, 2.1011910438537598, 2.0847561359405518, 2.3502261638641357]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.2417922019958496, 2.11602783203125, 1.991448998451233, 1.9692412614822388, 2.06097674369812, 2.05981183052063, 1.8450827598571777, 2.2041423320770264, 2.053558826446533, 1.8081953525543213, 2.1234610080718994, 1.9169840812683105, 2.051701545715332, 1.384468913078308, 1.930652141571045, 2.0878610610961914]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.8523192405700684, 3.066195249557495, 2.865264415740967, 2.9256227016448975, 2.977168321609497, 3.0264132022857666, 3.0455896854400635, 2.8252811431884766, 2.9326374530792236, 3.0292887687683105, 2.7389354705810547, 3.00950288772583, 2.918687582015991, 3.077651262283325, 3.0682241916656494, 3.0500082969665527]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:   4%|██████▏                                                                                                                                   | 9/200 [00:20<06:22,  2.00s/it]Layer: gate_25 - Captured router_logits: [2.122591018676758, 1.9994513988494873, 2.030686855316162, 2.247319459915161, 1.807390570640564, 1.9562572240829468, 2.0573740005493164, 2.0526010990142822, 2.117569923400879, 2.0265347957611084, 2.0868940353393555, 2.0987346172332764, 2.045290231704712, 2.077535390853882, 1.8188341856002808, 2.300973653793335]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.0988285541534424, 1.9596736431121826, 1.9763904809951782, 2.1881816387176514, 1.9803749322891235, 2.0542027950286865, 2.0005505084991455, 2.003695011138916, 1.9907516241073608, 2.2393009662628174, 2.295307159423828, 2.076643943786621, 2.12381911277771, 2.173173189163208, 1.9678585529327393, 2.0730390548706055]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.9721325635910034, 1.9439338445663452, 1.9800938367843628, 2.0033090114593506, 1.88465416431427, 1.9784128665924072, 2.054718494415283, 2.0457916259765625, 1.955099105834961, 2.090085983276367, 1.704528570175171, 1.9712897539138794, 1.9731956720352173, 1.7955647706985474, 1.9552518129348755, 1.9235193729400635]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.2604739665985107, 3.2452566623687744, 3.4416089057922363, 3.347269296646118, 3.3777570724487305, 3.230713367462158, 3.1552951335906982, 3.3730218410491943, 3.248563051223755, 3.317492961883545, 3.2710776329040527, 3.134756565093994, 3.3206188678741455, 3.082329511642456, 3.4166386127471924, 3.3201966285705566]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [6.7978739738464355, 7.014571666717529, 6.762570858001709, 6.727412700653076, 6.607375144958496, 6.503835678100586, 7.097171783447266, 6.905320644378662, 6.9113545417785645, 6.720850944519043, 6.771373271942139, 6.568992614746094, 6.700143814086914, 6.883418560028076, 7.116291046142578, 6.71912956237793]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_30 - Captured router_logits: [4.118648529052734, 4.245823383331299, 4.126997470855713, 3.828296661376953, 4.471730709075928, 4.310829162597656, 3.9985814094543457, 4.137115478515625, 4.22528076171875, 4.181268215179443, 4.193539142608643, 3.758118152618408, 4.1139817237854, 4.342112064361572, 4.242898941040039, 4.031677722930908]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.9603099822998047, 2.8471269607543945, 2.9080405235290527, 2.6777334213256836, 2.8689045906066895, 2.9271395206451416, 2.8157284259796143, 2.71748423576355, 2.725118637084961, 2.8926644325256348, 2.9901065826416016, 2.2068302631378174, 2.801136016845703, 2.88606333732605, 3.0194919109344482, 2.8596346378326416]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.13294516503810883, 0.15411634743213654, 0.1420370191335678, -0.24319443106651306, -0.18659766018390656, -0.19986595213413239, 0.16103392839431763, -0.26562628149986267, 0.10299097001552582, 0.12911325693130493, 0.13210366666316986, 0.11079297214746475, 0.11352012306451797, 0.1301373392343521, -1.1266766786575317, 0.15811748802661896]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.09516291320323944, 0.05058453232049942, 0.035582538694143295, 0.06104007735848427, 0.10162670910358429, 0.045725803822278976, 0.042725421488285065, 0.05413399636745453, 0.0027469329070299864, 0.05437954515218735, -0.16201649606227875, 0.04766545444726944, 0.009315595030784607, -0.021205803379416466, -0.024975169450044632, 0.007426880300045013]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05333522707223892, 0.05701630562543869, 0.0870165079832077, 0.042438432574272156, 0.0893261507153511, 0.11812714487314224, 0.07836081832647324, -0.09994378685951233, 0.06436751782894135, 0.026652678847312927, -0.0022080272901803255, 0.10011200606822968, -0.17546702921390533, 0.030524685978889465, -0.024753456935286522, 0.1153019517660141]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12361003458499908, 0.15991908311843872, 0.11849691718816757, 0.14301195740699768, 0.09504891186952591, 0.12387802451848984, 0.10993931442499161, 0.18237346410751343, 0.12476709485054016, -0.5138475894927979, -0.0843053013086319, 0.08812350779771805, 0.04471863806247711, -0.2374744564294815, -0.1791594922542572, -0.10479791462421417]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.070348359644413, 0.07655145972967148, 0.07963041961193085, 0.0027518905699253082, 0.035879701375961304, -0.1690894365310669, 0.0949668437242508, 0.008335054852068424, 0.024255432188510895, 0.03142468258738518, -0.13035108149051666, 0.13993220031261444, -0.21052125096321106, -0.20487327873706818, 0.07829578965902328, 0.05252181366086006]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.12996090948581696, 0.1558675915002823, 0.030268235132098198, -0.032172005623579025, 0.026974130421876907, 0.0508744940161705, 0.021623758599162102, 0.1596015840768814, 0.02187170460820198, -0.12678751349449158, -0.2032967209815979, 0.15871348977088928, -0.1668388396501541, 0.11088526993989944, 0.17956103384494781, 0.06731907278299332]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.09401102364063263, 0.14242389798164368, -0.006071858108043671, 0.24664756655693054, 0.1638798713684082, 0.07953379303216934, 0.07944729924201965, 0.07966805249452591, -0.008542949333786964, 0.15786974132061005, -0.44210943579673767, 0.19939643144607544, 0.3619988262653351, -0.0956592783331871, 0.07707347720861435, 0.16656100749969482]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.3303225636482239, 0.10568734258413315, 0.1841137707233429, -0.023264016956090927, -0.8498362898826599, -0.21720491349697113, -0.1459282785654068, -0.13368387520313263, -0.2844791114330292, -0.3030520975589752, -0.03379597142338753, -0.07701637595891953, 0.2538394331932068, -0.0728088915348053, -0.3267393708229065, 0.024160148575901985]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.22897842526435852, 0.25524139404296875, 0.03069763444364071, 0.4233601689338684, 0.2699519693851471, 0.3647213876247406, 0.15905477106571198, 0.2988792359828949, -0.08025488257408142, -0.02490651234984398, 0.32662224769592285, -0.11795740574598312, 0.43321388959884644, -0.07116914540529251, 0.1198766902089119, 0.10190363973379135]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.39815592765808105, 0.4221232533454895, 0.1308716982603073, 0.5257687568664551, 0.3559820353984833, 0.4015543460845947, 0.8121476173400879, 0.8079355955123901, 0.11971469968557358, 0.11580284684896469, 0.5385195016860962, 0.5976104736328125, 0.7350721955299377, 0.5104758143424988, 0.9366364479064941, 0.6697455048561096]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7003095149993896, 0.8487809896469116, 0.3175756335258484, 0.68178790807724, 0.6386780738830566, -0.2517501413822174, 0.4505646824836731, 0.7913662791252136, 0.28216752409935, -0.15795624256134033, 0.47627606987953186, 0.5908998250961304, 0.49853256344795227, 0.30205976963043213, 0.042142897844314575, 0.6153700947761536]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.4661492109298706, 0.6891984939575195, 0.8972998857498169, 0.5464338660240173, 0.789843738079071, 0.5537292957305908, 0.8249088525772095, 0.5149462819099426, 0.6081588268280029, 0.9345532655715942, 0.7190899848937988, 0.6748104095458984, 0.5829586982727051, 0.6050154566764832, 0.14736154675483704, 0.9204161763191223]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.46249908208847046, 1.144736409187317, 0.8063766956329346, 0.5782713890075684, 0.6772997975349426, 0.6132175922393799, 0.38253313302993774, 0.42105531692504883, 0.6820457577705383, 0.7423262000083923, 1.5582879781723022, 0.806867778301239, 0.9899023771286011, 0.8826928734779358, 0.7589837908744812, 0.7594627737998962]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.3006813526153564, 1.1762646436691284, 1.3704639673233032, 0.7684025764465332, 0.9200688600540161, 0.537144124507904, 1.03473699092865, 0.5542064905166626, 0.9947569370269775, 1.0275052785873413, 0.8709781169891357, 1.4531850814819336, 0.6878958344459534, 1.4038504362106323, 1.0495566129684448, 0.9882023930549622]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.511275589466095, 0.810327410697937, 1.1409778594970703, 1.1805126667022705, 0.8667168021202087, 0.5168221592903137, -0.11627253890037537, 0.03360993415117264, 0.4479801058769226, 0.5649526119232178, -0.451786607503891, 0.43922159075737, 0.9767278432846069, 0.8899745345115662, 1.094635009765625, 0.34126144647598267]
Layer: gate_14 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.8497101664543152, 1.4527112245559692, 1.2993465662002563, 1.0741428136825562, 1.2585550546646118, 1.4117295742034912, 1.3489339351654053, 0.9306533932685852, 0.8582820892333984, 1.0861570835113525, 1.1581913232803345, 0.48478269577026367, 1.7162708044052124, 1.015892505645752, 1.0468355417251587, 0.716427206993103]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.3872359991073608, 0.8256790041923523, 1.1693363189697266, 1.1561074256896973, 0.9263971447944641, 0.949974000453949, -0.52603679895401, 1.9536465406417847, -0.08865830302238464, 1.7053370475769043, -0.31025436520576477, 1.1682971715927124, 1.5090066194534302, 1.4181406497955322, 1.2482638359069824, -0.15371555089950562]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.3986009359359741, 1.9314548969268799, 1.8440903425216675, 2.409776210784912, 2.1149070262908936, 1.8167192935943604, 1.737526297569275, 2.1336569786071777, 1.7189489603042603, 2.0847678184509277, 1.9413111209869385, 1.5346522331237793, 1.6050752401351929, 1.8225125074386597, 1.8384121656417847, 2.3830206394195557]
Layer: gate_17 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4447381496429443, 1.0538880825042725, 1.5667186975479126, 1.8456645011901855, 1.722453236579895, 1.8664501905441284, 1.2459732294082642, 2.1429214477539062, 2.1537420749664307, 1.494705319404602, 1.6799479722976685, 1.6804653406143188, 1.8053268194198608, 1.9207158088684082, 1.7105276584625244, 1.5757884979248047]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.257216215133667, 1.9314298629760742, 1.6212729215621948, 1.916711449623108, 2.0188241004943848, 1.952452540397644, 2.1261820793151855, 2.049175500869751, 1.7789053916931152, 1.6760882139205933, 2.009868860244751, 1.8499395847320557, 2.9404456615448, 2.160693883895874, 2.049574375152588, 1.9900875091552734]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6548545956611633, 1.2964903116226196, 1.317928671836853, 0.8458368182182312, 0.6689420938491821, 1.30520498752594, 1.3369033336639404, 1.078813910484314, 0.9996865391731262, 1.0772734880447388, 1.2010207176208496, 0.9500646591186523, 1.2186049222946167, 1.4056133031845093, 1.278466820716858, 1.0426746606826782]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_21 - Captured router_logits: [2.675732374191284, 2.719670295715332, 2.5985004901885986, 2.8944218158721924, 2.7951080799102783, 2.924530029296875, 3.0159859657287598, 2.8296914100646973, 2.6765429973602295, 3.0397825241088867, 2.910822868347168, 2.689342498779297, 2.858907461166382, 2.7902348041534424, 3.10980486869812, 2.935396432876587]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.854292631149292, 1.486892819404602, 1.6139960289001465, 1.613459825515747, 1.8297230005264282, 1.7072064876556396, 1.8229243755340576, 1.4186904430389404, 1.741476058959961, 2.039299964904785, 1.3346610069274902, 1.5497550964355469, 1.747660756111145, 1.6318024396896362, 1.8797218799591064, 2.022218942642212]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4056038856506348, 2.042738437652588, 2.027726650238037, 1.8328077793121338, 1.956026554107666, 1.983256220817566, 1.8467464447021484, 2.1282434463500977, 1.9341177940368652, 1.7740519046783447, 2.0725603103637695, 1.884846568107605, 2.0252685546875, 1.4218015670776367, 1.9427951574325562, 2.06207013130188]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.6391491889953613, 2.889097213745117, 2.6786680221557617, 2.66884446144104, 2.7588307857513428, 2.834230422973633, 2.8466639518737793, 2.6541295051574707, 2.7937960624694824, 2.9047951698303223, 2.636624574661255, 2.8375027179718018, 2.636165142059326, 2.9066379070281982, 2.919506788253784, 2.871098279953003]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_25 - Captured router_logits: [2.227027177810669, 2.104567527770996, 2.1088666915893555, 2.2866220474243164, 1.8533767461776733, 2.0795671939849854, 2.0977180004119873, 2.1592257022857666, 2.1611335277557373, 2.2241625785827637, 2.2234609127044678, 2.1958274841308594, 2.1731584072113037, 2.232588768005371, 1.9140353202819824, 2.412234306335449]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.1648247241973877, 2.062059164047241, 2.08990740776062, 2.292870044708252, 2.0521576404571533, 2.0974042415618896, 2.1216225624084473, 2.0228021144866943, 2.074862003326416, 2.3585875034332275, 2.3757474422454834, 2.206317186355591, 2.20751690864563, 2.2758262157440186, 2.0098562240600586, 2.1442999839782715]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.126875162124634, 2.0678110122680664, 2.1084706783294678, 2.1607720851898193, 2.039278507232666, 2.0880978107452393, 2.178677558898926, 2.1878907680511475, 2.119777202606201, 2.2037413120269775, 1.832290768623352, 2.112208366394043, 2.171658515930176, 2.0976765155792236, 2.104058027267456, 2.1559410095214844]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.5199501514434814, 3.5397000312805176, 3.839066505432129, 3.660538673400879, 3.700824737548828, 3.572474718093872, 3.4925408363342285, 3.707218647003174, 3.4860541820526123, 3.631519317626953, 3.5949180126190186, 3.3883018493652344, 3.7003681659698486, 3.3809947967529297, 3.6875574588775635, 3.5962748527526855]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:   6%|████████▉                                                                                                                                | 13/200 [00:25<05:25,  1.74s/it]Layer: gate_29 - Captured router_logits: [7.243575096130371, 7.52654504776001, 7.366933822631836, 7.2038679122924805, 7.143710613250732, 6.872066020965576, 7.6517815589904785, 7.39826774597168, 7.440789222717285, 7.195044040679932, 7.228034496307373, 7.123086929321289, 7.338191509246826, 7.430891513824463, 7.618277072906494, 7.2147393226623535]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.662542343139648, 4.801654815673828, 4.681551933288574, 4.330930233001709, 4.935622692108154, 4.703036785125732, 4.59335994720459, 4.626153945922852, 4.782347679138184, 4.738773822784424, 4.715874671936035, 4.196902275085449, 4.604719638824463, 4.82314920425415, 4.774168014526367, 4.564185619354248]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.290654182434082, 3.1599984169006348, 3.1416916847229004, 2.848410129547119, 3.1333799362182617, 3.267584800720215, 3.04642391204834, 3.0613203048706055, 3.045213460922241, 3.1779465675354004, 3.211796522140503, 2.5197134017944336, 3.156508684158325, 3.2307162284851074, 3.414654493331909, 3.2601773738861084]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.06722424179315567, 0.08201584219932556, 0.09251657873392105, -0.2742089033126831, -0.2584388256072998, -0.06919004768133163, 0.08995427936315536, -0.05660875141620636, 0.05680199712514877, 0.06630845367908478, 0.06528542190790176, 0.031324584037065506, 0.07089140266180038, 0.10204671323299408, -1.031093955039978, 0.10105161368846893]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_1 - Captured router_logits: [0.06809686869382858, 0.03388732671737671, 0.02342628315091133, 0.05265281721949577, 0.06796500086784363, 0.017320385202765465, 0.04876009002327919, 0.10120606422424316, 0.059617429971694946, 0.056544914841651917, -0.19292780756950378, 0.06010379269719124, 0.005964809097349644, -0.025077814236283302, 0.043998751789331436, 0.03752828389406204]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06898950785398483, 0.02873588725924492, 0.07769577950239182, 0.052174557000398636, 0.12708261609077454, 0.100943423807621, 0.07589078694581985, -0.10944735258817673, 0.09614798426628113, 0.1162080392241478, 0.021944211795926094, 0.08236152678728104, -0.21670934557914734, 0.011651493608951569, -0.042165033519268036, 0.10699134320020676]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11647052317857742, 0.11917886883020401, 0.08407474309206009, 0.09843435138463974, 0.11902768909931183, 0.08003199845552444, -0.0994846448302269, 0.12329205870628357, 0.133977010846138, -0.6263397336006165, 0.006556987296789885, 0.06178377941250801, 0.23429925739765167, -0.38125646114349365, 0.03178448975086212, -0.1551567018032074]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.01109345629811287, 0.07286030054092407, 0.03114861622452736, 0.21422341465950012, 0.08143071085214615, -0.07191195338964462, 0.0009550886461511254, -0.012153598479926586, -0.06691531836986542, -0.008872312493622303, -0.2799164652824402, 0.08218315243721008, 0.06688818335533142, -0.2566395401954651, 0.11953150480985641, -0.10329671949148178]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07035961002111435, 0.13165447115898132, 0.030873624607920647, 0.1753343641757965, -0.1582348495721817, -0.14011602103710175, 0.045788321644067764, 0.018006091937422752, 0.22115088999271393, -0.12408366799354553, -0.2141847461462021, 0.1573628932237625, -0.2837829291820526, 0.20832109451293945, 0.13979868590831757, 0.0926346406340599]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.17274883389472961, -0.0443333275616169, 0.08093365281820297, 0.23162221908569336, 0.17066743969917297, 0.12862704694271088, -0.18663638830184937, -0.10677219182252884, 0.040246546268463135, 0.20122703909873962, -0.6605744361877441, 0.17252732813358307, 0.41056257486343384, -0.43784165382385254, 0.30146342515945435, 0.233967125415802]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.025918466970324516, -0.03411799669265747, 0.05191582813858986, 0.019711125642061234, -1.0795633792877197, -0.21206754446029663, -0.07264932245016098, -0.6873651742935181, 0.1978842169046402, -0.16736328601837158, -0.3929535448551178, -0.00046467353240586817, -0.19512872397899628, 0.14295488595962524, -0.8109086155891418, -0.34280019998550415]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2343224585056305, 0.28063303232192993, -0.22412580251693726, 0.4298792779445648, 0.32881391048431396, 0.35696858167648315, 0.17450650036334991, 0.045133549720048904, 0.032015904784202576, 0.5684492588043213, 0.30866116285324097, 0.03706232085824013, 0.5852570533752441, -0.30755072832107544, -0.35011589527130127, 0.2983982563018799]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3085744082927704, 0.20581528544425964, 0.17274683713912964, 1.0301055908203125, 0.28546789288520813, 0.14572107791900635, 0.5215023756027222, 0.716214120388031, 0.4239647388458252, -0.0014220824232324958, 0.0885261818766594, 0.4877251386642456, 0.3924241364002228, 0.036012131720781326, 0.8113186359405518, 0.6136904358863831]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.8441926836967468, 0.8458722829818726, 0.33459362387657166, 0.5281696319580078, 0.5419954061508179, -0.23775677382946014, 0.42880889773368835, 0.6984703540802002, -0.17600297927856445, -0.3808657228946686, 0.520863950252533, 0.6491530537605286, -0.006545168347656727, 0.11727052181959152, 0.5286317467689514, 0.3209058344364166]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.38180306553840637, 0.7923946380615234, 0.6689971685409546, 0.3763943016529083, 0.7444906234741211, 0.012020974420011044, 0.6350683569908142, 0.17680960893630981, 0.5570825934410095, 1.218912959098816, 0.4784565567970276, 0.376953661441803, -0.04166293889284134, 0.03680495172739029, -0.399717777967453, 0.4861625134944916]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.022649329155683517, 0.7922343015670776, 1.0548313856124878, 0.5735365748405457, 0.09935959428548813, 0.43963614106178284, 0.5743347406387329, 0.93625408411026, 0.32310426235198975, 0.36344510316848755, 1.5802809000015259, 0.7327480316162109, 0.7494292259216309, 0.6664848923683167, 0.6144103407859802, 0.761300802230835]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.977264940738678, 1.0985084772109985, 1.2809282541275024, 0.31460875272750854, 1.0419151782989502, 0.01153973676264286, 0.9833691716194153, 1.3309520483016968, 0.9749405980110168, 0.924813449382782, 1.2411525249481201, 1.343239665031433, 0.1077626422047615, 0.984796941280365, 0.8700794577598572, 0.6214507818222046]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5617055296897888, 0.717281699180603, 1.3586041927337646, 1.1746584177017212, 1.1008974313735962, 1.7270324230194092, -0.07720812410116196, 1.333503007888794, 0.6694529056549072, 0.6268270611763, -0.6252040863037109, 0.6316473484039307, 0.9845470190048218, 1.1391435861587524, 1.3005186319351196, 0.33929839730262756]
Running loglikelihood requests:   8%|███████████▋                                                                                                                             | 17/200 [00:32<05:08,  1.68s/it]Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.4335463345050812, 1.3397520780563354, 0.9699100852012634, 0.9215432405471802, 0.8326388597488403, 1.0858087539672852, 1.7200698852539062, 0.6927756071090698, 0.617938756942749, 0.8926287293434143, 0.7098154425621033, -0.014949744567275047, 1.7408125400543213, 0.7460843324661255, 0.7123932838439941, 1.1498171091079712]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9749090075492859, 0.35472461581230164, 0.9602559208869934, 1.1476916074752808, 0.8701591491699219, 0.6838971972465515, -0.13629662990570068, 1.6470948457717896, -0.47099030017852783, 2.32831072807312, -0.5693431496620178, 0.9976823925971985, 1.128820538520813, 1.1480966806411743, 1.0806469917297363, 0.0726260170340538]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.5113109350204468, 1.2724730968475342, 1.5851255655288696, 1.6714447736740112, 1.8170632123947144, 1.4699838161468506, 1.5193785429000854, 1.5933356285095215, 1.8689922094345093, 1.506384253501892, 2.2457594871520996, 1.4556635618209839, 1.4842783212661743, 1.4669854640960693, 0.9581260681152344, 1.662308931350708]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5377871990203857, 1.0752136707305908, 1.681725025177002, 2.0697903633117676, 1.096631407737732, 1.359405517578125, 1.687152624130249, 1.8921858072280884, 2.703619956970215, 1.3958873748779297, 1.3154996633529663, 1.1090031862258911, 1.3052438497543335, 1.1982115507125854, 1.4905695915222168, 1.4571202993392944]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.7863603830337524, 1.7127290964126587, 1.6107463836669922, 1.5409871339797974, 1.8446580171585083, 1.443833827972412, 1.7130706310272217, 1.6181721687316895, 2.1161415576934814, 1.5678656101226807, 1.912511944770813, 1.5706138610839844, 2.3390145301818848, 1.8175947666168213, 1.8461683988571167, 1.7213571071624756]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.899771511554718, 1.1995269060134888, 1.1093146800994873, 0.8459364771842957, 0.2650426924228668, 1.6501470804214478, 1.175333857536316, 1.6405730247497559, 0.722579300403595, 1.2956281900405884, 1.1151771545410156, 1.1755579710006714, 1.1006971597671509, 1.1850966215133667, 1.1054551601409912, 0.8083850741386414]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.354992389678955, 2.4973573684692383, 2.422818899154663, 2.380446195602417, 2.2928707599639893, 2.572420835494995, 2.4949839115142822, 2.5321695804595947, 2.334299087524414, 2.524447441101074, 2.6107375621795654, 2.693068265914917, 2.176997661590576, 2.544520616531372, 2.5629100799560547, 2.54548716545105]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.6591871976852417, 1.5883432626724243, 2.025256633758545, 1.388007640838623, 1.6833659410476685, 1.7351658344268799, 1.577447772026062, 1.5221447944641113, 1.7021127939224243, 1.831503987312317, 1.6523301601409912, 1.8977307081222534, 1.6379315853118896, 1.6608357429504395, 1.5408918857574463, 1.9435361623764038]
Layer: gate_22 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4507980346679688, 2.0591163635253906, 1.5797920227050781, 1.7502917051315308, 2.061131477355957, 1.798053503036499, 1.7510682344436646, 1.803512454032898, 1.8433938026428223, 1.7625970840454102, 1.835039496421814, 1.8840502500534058, 1.6662988662719727, 0.9695221185684204, 1.7061142921447754, 1.8399871587753296]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.895014524459839, 2.9952261447906494, 2.755305767059326, 2.589492082595825, 2.9470372200012207, 2.6901395320892334, 2.8637804985046387, 2.6657397747039795, 2.6982030868530273, 2.8004074096679688, 3.1361446380615234, 2.6713063716888428, 2.731621265411377, 2.95995831489563, 2.7739920616149902, 2.8559999465942383]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.0387122631073, 1.9680427312850952, 1.9599928855895996, 2.2195894718170166, 1.5981866121292114, 2.0138401985168457, 2.0633022785186768, 2.2192065715789795, 2.0824081897735596, 2.0050811767578125, 2.0621414184570312, 1.9577817916870117, 2.041837453842163, 2.0368776321411133, 2.2812042236328125, 2.255948066711426]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_26 - Captured router_logits: [2.0504093170166016, 2.124281883239746, 2.0437605381011963, 2.1287331581115723, 2.010185718536377, 2.1755218505859375, 2.0450682640075684, 1.9878920316696167, 2.0225307941436768, 2.203583002090454, 2.6540393829345703, 2.0285022258758545, 2.0845415592193604, 2.134608507156372, 2.0990285873413086, 2.150941848754883]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.058398485183716, 1.9398633241653442, 1.9468950033187866, 2.025275230407715, 1.8595250844955444, 1.9489612579345703, 2.098451852798462, 2.0845866203308105, 1.954169511795044, 2.0390641689300537, 1.5653992891311646, 1.9249762296676636, 2.1954803466796875, 2.2852370738983154, 1.9304715394973755, 1.9535467624664307]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [3.2447879314422607, 3.3307735919952393, 3.565152645111084, 3.255467414855957, 3.536764144897461, 3.2497928142547607, 3.575165271759033, 3.4257938861846924, 3.2131307125091553, 3.2911927700042725, 3.2763562202453613, 3.005265712738037, 3.2219386100769043, 3.093564748764038, 3.3492531776428223, 3.2155449390411377]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [6.6904296875, 7.301728248596191, 6.814541339874268, 6.6328911781311035, 6.579308032989502, 6.393921375274658, 6.8433051109313965, 6.9016499519348145, 6.9304704666137695, 6.593564033508301, 6.6838860511779785, 6.603580951690674, 6.754487037658691, 6.884365558624268, 7.0899858474731445, 6.734387397766113]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.112899303436279, 4.176647186279297, 4.015456199645996, 3.7048730850219727, 4.361242771148682, 4.155868053436279, 3.7712771892547607, 4.073515892028809, 4.32355260848999, 4.029508590698242, 3.95405650138855, 3.3654353618621826, 3.7608084678649902, 4.325221061706543, 4.083859443664551, 3.9473605155944824]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.7223782539367676, 2.6712028980255127, 2.7101268768310547, 2.6306192874908447, 2.62219500541687, 2.689955234527588, 2.674046039581299, 2.424304246902466, 2.605107307434082, 2.5035879611968994, 3.1612439155578613, 1.8857526779174805, 2.5943186283111572, 2.5396358966827393, 3.005486011505127, 2.6815295219421387]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09841688722372055, 0.10245658457279205, 0.10205406695604324, -0.21149112284183502, -0.15848186612129211, -0.09630230814218521, 0.12827864289283752, -0.17929363250732422, 0.07480030506849289, 0.09513857215642929, 0.09804806113243103, 0.08005999773740768, 0.09430886059999466, 0.10559996217489243, -0.9149430394172668, 0.12106576561927795]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07516231387853622, 0.03445587307214737, 0.025231512263417244, 0.05949917808175087, 0.06272440403699875, 0.04700589179992676, 0.028112441301345825, 0.05541222542524338, -0.004988572094589472, 0.05569121614098549, -0.14012473821640015, 0.0429081954061985, 0.013440252281725407, -0.006215761415660381, 0.005356457084417343, 0.008944256231188774]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.04622984677553177, 0.05847996845841408, 0.0941205769777298, 0.02940015122294426, 0.07481154054403305, 0.11433091014623642, 0.04081014543771744, -0.09377916902303696, 0.07427479326725006, 0.06793420016765594, -0.003973084501922131, 0.09417787194252014, -0.13176076114177704, 0.0065230559557676315, -0.04154209792613983, 0.09811341017484665]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11422563344240189, 0.1313367635011673, 0.1241409108042717, 0.11789309978485107, 0.10390278697013855, 0.12036721408367157, -0.006580982357263565, 0.13687650859355927, 0.1344086229801178, -0.5402726531028748, 0.012471429072320461, 0.0849727913737297, 0.09114617854356766, -0.28700128197669983, -0.09750232845544815, -0.07807787507772446]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.04742925241589546, 0.08158407360315323, 0.07420188933610916, 0.029600581154227257, 0.09815610200166702, -0.09357698261737823, 0.024582700803875923, 0.005678677465766668, -0.0339353121817112, 0.011008954606950283, -0.18752743303775787, 0.10686980187892914, -0.096247099339962, -0.19675371050834656, 0.09573733061552048, -0.07535865902900696]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.06206580251455307, 0.11160340160131454, 0.04773440212011337, 0.028532028198242188, -0.004855923354625702, -0.06399908661842346, 0.014742601662874222, 0.07099033892154694, 0.04133792221546173, -0.11140068620443344, -0.18008293211460114, 0.159095898270607, -0.17036651074886322, 0.14506252110004425, 0.13206331431865692, 0.06276413053274155]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.16128288209438324, 0.009777153842151165, 0.048599839210510254, 0.2579655051231384, 0.13792607188224792, 0.092860147356987, -0.13106440007686615, -0.08256309479475021, -0.006385812535881996, 0.20592328906059265, -0.5115807056427002, 0.17643260955810547, 0.34194517135620117, -0.27108755707740784, 0.08413367718458176, 0.17364419996738434]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.2743767499923706, -0.017293516546487808, 0.05037541687488556, -0.01957150548696518, -0.9138806462287903, -0.28359612822532654, -0.10082117468118668, -0.3953489363193512, -0.19817538559436798, -0.22208712995052338, -0.1951947659254074, -0.09205108135938644, 0.06036438047885895, -0.05361665040254593, -0.4764477014541626, -0.03488738089799881]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.15425854921340942, 0.21886146068572998, -0.0888875350356102, 0.432062029838562, 0.27074453234672546, 0.3727230429649353, 0.15215399861335754, 0.19516974687576294, -0.006023864261806011, 0.19320984184741974, 0.3212875425815582, 0.03226244077086449, 0.5024911165237427, -0.17112517356872559, -0.06266596913337708, 0.1657928079366684]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.34265434741973877, 0.26498037576675415, 0.11663492023944855, 0.6269208192825317, 0.33915773034095764, 0.4052571654319763, 0.6571242809295654, 0.7416014671325684, 0.1800636500120163, 0.13103733956813812, 0.29773828387260437, 0.512031614780426, 0.6203488707542419, 0.25517886877059937, 0.803466796875, 0.6714430451393127]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7061076760292053, 0.7519993185997009, 0.22570064663887024, 0.5674420595169067, 0.556201696395874, -0.17868287861347198, 0.392269104719162, 0.7155548334121704, -0.09207843244075775, -0.37340760231018066, 0.4258413314819336, 0.5213531255722046, 0.30146336555480957, 0.20853374898433685, 0.19437333941459656, 0.3389858603477478]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.35962897539138794, 0.6001219153404236, 0.7800819873809814, 0.5302257537841797, 0.6898770928382874, 0.21995611488819122, 0.6747106909751892, 0.3486243486404419, 0.5637891292572021, 0.8937222361564636, 0.6047033667564392, 0.4105483293533325, 0.2222852110862732, 0.2565150260925293, -0.19042417407035828, 0.5887653231620789]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.14413194358348846, 0.9596699476242065, 0.8431696891784668, 0.5278127789497375, 0.24999254941940308, 0.4425745904445648, 0.37915635108947754, 0.486695796251297, 0.4410059154033661, 0.5489329695701599, 1.5991694927215576, 0.7775719165802002, 0.7595494389533997, 0.7465856075286865, 0.6851567029953003, 0.7587058544158936]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.1085866689682007, 1.0721094608306885, 1.5323230028152466, 0.5390164256095886, 1.0458978414535522, 0.27931201457977295, 1.0124918222427368, 0.9075590372085571, 0.9517866373062134, 0.9564903974533081, 1.0071247816085815, 1.464001178741455, 0.4445815682411194, 1.239769458770752, 1.0370789766311646, 0.7609756588935852]
Layer: gate_13 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5240052938461304, 0.7295799851417542, 1.136391282081604, 1.1177449226379395, 0.8491336107254028, 0.8820254802703857, -0.10101408511400223, 0.4420629143714905, 0.525197446346283, 0.5977538824081421, -0.37860020995140076, 0.36737459897994995, 0.9474766850471497, 0.9109669923782349, 1.085874319076538, 0.26255175471305847]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6111339926719666, 1.3487780094146729, 1.1625677347183228, 0.9039788842201233, 0.954289436340332, 1.232432246208191, 1.3459632396697998, 0.7643346786499023, 0.8252863883972168, 1.0203179121017456, 0.860870897769928, 0.29049643874168396, 1.689573049545288, 0.9039868712425232, 0.9113178253173828, 0.8623084425926208]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.0770090818405151, 0.7293834090232849, 1.148466944694519, 1.2129695415496826, 0.9252994656562805, 0.8063969612121582, -0.26476046442985535, 1.6936917304992676, -0.30944719910621643, 1.7926814556121826, -0.514788031578064, 1.0168668031692505, 1.366884708404541, 1.4007527828216553, 1.2843036651611328, -0.14655587077140808]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.0666002035140991, 1.8798564672470093, 1.943015694618225, 2.2482223510742188, 2.2488832473754883, 1.9736053943634033, 1.821278691291809, 2.141679286956787, 1.8591783046722412, 1.9581563472747803, 2.2288339138031006, 1.6995527744293213, 1.5109351873397827, 1.746604561805725, 1.5953519344329834, 2.2590110301971436]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Running loglikelihood requests:  10%|██████████████▍                                                                                                                          | 21/200 [00:38<04:46,  1.60s/it]Layer: gate_18 - Captured router_logits: [1.6564806699752808, 1.1599196195602417, 1.8770750761032104, 2.1846671104431152, 1.5768964290618896, 1.8577526807785034, 1.4654877185821533, 2.190425157546997, 2.510145664215088, 1.6995445489883423, 1.6526230573654175, 1.5487792491912842, 1.7653257846832275, 1.860853672027588, 1.7652698755264282, 1.6976280212402344]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.1372766494750977, 1.8953161239624023, 1.6294962167739868, 1.845802664756775, 2.024850845336914, 1.7565313577651978, 1.9721626043319702, 1.9087105989456177, 1.7476427555084229, 1.69678795337677, 2.031151294708252, 1.8519138097763062, 2.7874276638031006, 2.0644099712371826, 1.9950045347213745, 1.919560194015503]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6824049949645996, 1.2982921600341797, 1.2056306600570679, 0.7694444060325623, 0.3288859724998474, 1.431138277053833, 1.2319551706314087, 1.100010871887207, 0.986690878868103, 1.1527904272079468, 1.283562421798706, 0.9710718393325806, 1.1912875175476074, 1.3248491287231445, 1.2153278589248657, 1.024177074432373]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.474261999130249, 2.5470309257507324, 2.4869611263275146, 2.568881034851074, 2.5016512870788574, 2.71024489402771, 2.7172253131866455, 2.667832374572754, 2.478851079940796, 2.703134298324585, 2.693755626678467, 2.4112279415130615, 2.4103636741638184, 2.624539613723755, 2.8139588832855225, 2.7742807865142822]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.8479923009872437, 1.5574491024017334, 1.699307918548584, 1.6056727170944214, 1.8298499584197998, 1.7664029598236084, 1.7910785675048828, 1.4765303134918213, 1.8313614130020142, 1.9803802967071533, 1.480310320854187, 1.6125447750091553, 1.7649506330490112, 1.7021065950393677, 1.7838013172149658, 2.084789991378784]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.4020628929138184, 1.9205455780029297, 1.7688112258911133, 1.7745022773742676, 1.913710355758667, 1.8662222623825073, 1.826286792755127, 1.986863374710083, 1.8594194650650024, 1.6495496034622192, 1.9326666593551636, 1.8035719394683838, 1.8287293910980225, 1.1762083768844604, 1.8360282182693481, 1.9570852518081665]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.6438095569610596, 2.9406495094299316, 2.730029821395874, 2.6729495525360107, 2.8554913997650146, 2.833056688308716, 2.8943240642547607, 2.7192256450653076, 2.7687318325042725, 2.864170551300049, 2.80082631111145, 2.7727627754211426, 2.727240562438965, 2.9492738246917725, 2.868767499923706, 2.869370937347412]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_25 - Captured router_logits: [2.1096248626708984, 2.0472073554992676, 2.0312936305999756, 2.2891204357147217, 1.6796735525131226, 2.031663179397583, 2.032015323638916, 2.15519380569458, 2.100940465927124, 2.092087745666504, 2.1382033824920654, 2.0741493701934814, 2.083000421524048, 2.1378564834594727, 1.8345478773117065, 2.3151772022247314]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.124950647354126, 1.9730308055877686, 1.970819354057312, 2.171908140182495, 2.00423526763916, 2.027188777923584, 2.065514087677002, 1.9755362272262573, 1.998327374458313, 2.2464170455932617, 2.337214231491089, 2.0790443420410156, 2.0923774242401123, 2.1990854740142822, 1.9533315896987915, 2.1048271656036377]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.0301921367645264, 1.926093339920044, 2.0132830142974854, 2.10595965385437, 1.9189984798431396, 1.9569627046585083, 2.0831186771392822, 2.076474189758301, 2.0164403915405273, 2.0992588996887207, 1.6862614154815674, 1.9859453439712524, 2.1154847145080566, 1.9892969131469727, 2.0039937496185303, 1.9827303886413574]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.398796558380127, 3.4250361919403076, 3.735482931137085, 3.475830078125, 3.5885653495788574, 3.422091484069824, 3.4149603843688965, 3.551351308822632, 3.357912063598633, 3.498110055923462, 3.457746982574463, 3.2886805534362793, 3.55481219291687, 3.258798122406006, 3.537710666656494, 3.378286600112915]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.24260950088501, 7.574682712554932, 7.350049018859863, 7.235698223114014, 7.12037992477417, 6.905503273010254, 7.615478992462158, 7.418022632598877, 7.452908992767334, 7.232639789581299, 7.29487419128418, 7.133362293243408, 7.297256946563721, 7.389788627624512, 7.587298393249512, 7.311426639556885]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.409331321716309, 4.497593402862549, 4.341160774230957, 3.967839241027832, 4.593199253082275, 4.47193717956543, 4.236677646636963, 4.3280863761901855, 4.451746463775635, 4.4025444984436035, 4.466985702514648, 3.8791823387145996, 4.2714104652404785, 4.5192975997924805, 4.44445276260376, 4.267375469207764]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.302800178527832, 3.1315484046936035, 3.098870038986206, 2.8321125507354736, 3.0852479934692383, 3.2020294666290283, 3.0435922145843506, 3.0807762145996094, 3.07361102104187, 3.1937286853790283, 3.299921751022339, 2.5864503383636475, 3.1543195247650146, 3.1499581336975098, 3.35373854637146, 3.2543537616729736]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.08653120696544647, 0.102500319480896, 0.10534143447875977, -0.2758120894432068, -0.2517474293708801, -0.11185016483068466, 0.1228547915816307, -0.1397392451763153, 0.0607987605035305, 0.09025689214468002, 0.09863409399986267, 0.07979398220777512, 0.08679787069559097, 0.10110954195261002, -1.1384620666503906, 0.1149502545595169]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0730108991265297, 0.0410018153488636, 0.03553372994065285, 0.034034889191389084, 0.07222376018762589, 0.008964819833636284, 0.03973788395524025, 0.07088033109903336, 0.019937854260206223, 0.056257039308547974, -0.1948554366827011, 0.05813836678862572, 0.0007701943395659328, -0.043365225195884705, 0.017971854656934738, 0.036172203719615936]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08021248877048492, 0.03235682100057602, 0.08150900900363922, 0.055300284177064896, 0.09422440081834793, 0.10617566108703613, 0.06057894229888916, -0.12535229325294495, 0.06175389513373375, 0.09645331650972366, 0.018185485154390335, 0.08966369926929474, -0.21673178672790527, 0.017958851531147957, -0.056359246373176575, 0.08871887624263763]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1353866010904312, 0.13031283020973206, 0.07117819041013718, 0.09836996346712112, 0.1070641428232193, 0.08783186972141266, -0.04032381251454353, 0.12178867310285568, 0.15269675850868225, -0.6035639643669128, -0.038117699325084686, 0.058220770210027695, 0.17107097804546356, -0.3289963901042938, -0.03246745094656944, -0.14428983628749847]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.02362249046564102, 0.06656937301158905, 0.07982967048883438, 0.13293476402759552, 0.05756574496626854, -0.08424180001020432, 0.011821816675364971, -0.02115093544125557, -0.04270017892122269, 0.03433467820286751, -0.24187403917312622, 0.1107732504606247, -0.02105497010052204, -0.25589507818222046, 0.09063460677862167, -0.004404369276016951]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.10494910180568695, 0.12179756164550781, 0.021066205576062202, 0.11472415179014206, -0.013555810786783695, -0.08079371601343155, 0.05044334754347801, 0.041001249104738235, 0.16139113903045654, -0.13217924535274506, -0.12838684022426605, 0.13329727947711945, -0.21428725123405457, 0.15931755304336548, 0.13621245324611664, 0.06607385724782944]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11988367140293121, -0.05730421096086502, 0.10476525872945786, 0.2760705351829529, 0.208669051527977, 0.11783605068922043, -0.1327531784772873, -0.08742498606443405, 0.11232376098632812, 0.15927810966968536, -0.6007282733917236, 0.19147959351539612, 0.3258284330368042, -0.28329405188560486, 0.21387411653995514, 0.227683424949646]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.06565803289413452, -0.06375711411237717, 0.07330828160047531, 0.05612647160887718, -1.0954937934875488, -0.19332288205623627, -0.14969433844089508, -0.4938027262687683, 0.007673365529626608, -0.16434237360954285, -0.23901201784610748, -0.025562234222888947, 0.05980568379163742, -0.003937953617423773, -0.5706830620765686, -0.04283219948410988]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.14791172742843628, 0.26773199439048767, -0.162713423371315, 0.4148758053779602, 0.36030712723731995, 0.28907510638237, 0.19088344275951385, 0.19913573563098907, 0.0463104285299778, 0.346841037273407, 0.21207410097122192, 0.08399592339992523, 0.45120108127593994, -0.20322082936763763, -0.13648942112922668, 0.23915500938892365]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3023861050605774, 0.2537679374217987, 0.08126062899827957, 0.8069496750831604, 0.2685629427433014, 0.335382878780365, 0.5606125593185425, 0.6364879012107849, 0.3796177804470062, 0.025361964479088783, 0.24747709929943085, 0.5012279152870178, 0.6206143498420715, 0.3853013217449188, 0.7502027153968811, 0.6302218437194824]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.8173129558563232, 0.7716286778450012, 0.30444931983947754, 0.6260014772415161, 0.6794676184654236, 0.03896697238087654, 0.35039442777633667, 0.6660926342010498, 0.03418303653597832, -0.1929900348186493, 0.4193094074726105, 0.6455607414245605, 0.11900081485509872, 0.1556113362312317, 0.37875092029571533, 0.3312825858592987]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.39309102296829224, 0.6857025623321533, 0.7093685269355774, 0.6340104341506958, 0.8042851090431213, 0.46326208114624023, 0.6253843903541565, 0.41503843665122986, 0.6588437557220459, 1.0519659519195557, 0.5920565128326416, 0.45267191529273987, 0.21557626128196716, 0.12239158153533936, -0.3124733567237854, 0.5905926823616028]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.1441885083913803, 0.9123718738555908, 0.920687735080719, 0.5842928886413574, 0.16214588284492493, 0.6068562865257263, 0.5399492979049683, 0.7353368997573853, 0.5591909885406494, 0.6047340631484985, 1.5807613134384155, 0.7026622295379639, 0.841698944568634, 0.8011974096298218, 0.5334227085113525, 0.700142502784729]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.2306017875671387, 1.245036005973816, 1.4339474439620972, 0.49346086382865906, 1.0568758249282837, 0.41030487418174744, 1.1529067754745483, 1.3154536485671997, 1.0775779485702515, 1.1742860078811646, 1.177316427230835, 1.4999043941497803, 0.3489930331707001, 1.3345712423324585, 1.2546908855438232, 0.8549705743789673]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6219218373298645, 0.8084242939949036, 1.3528265953063965, 1.2896397113800049, 1.0587060451507568, 1.258310317993164, -0.11143410205841064, 0.7451292872428894, 0.7546842098236084, 0.48850956559181213, -0.30455437302589417, 0.5276060700416565, 1.0488755702972412, 1.0556244850158691, 1.3100515604019165, 0.3715275228023529]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6150238513946533, 1.4346064329147339, 1.246769905090332, 1.062678575515747, 1.071768879890442, 1.267397165298462, 1.421552062034607, 0.7852024435997009, 0.9026803970336914, 1.0257139205932617, 0.8809818029403687, 0.17780491709709167, 1.7345062494277954, 0.90306556224823, 0.8483073711395264, 1.0772933959960938]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.0282448530197144, 0.5888294577598572, 1.1475183963775635, 1.2488597631454468, 0.951379656791687, 0.7002997398376465, -0.13496249914169312, 1.9243565797805786, -0.4965299963951111, 2.148096799850464, -0.49351316690444946, 1.148590087890625, 1.3303569555282593, 1.2774574756622314, 1.2036688327789307, 0.03169550746679306]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.7492401599884033, 1.4128527641296387, 1.6086807250976562, 1.9110456705093384, 1.8968181610107422, 1.6025567054748535, 1.515892744064331, 1.7714046239852905, 1.6503331661224365, 1.790934443473816, 1.9728407859802246, 1.4968653917312622, 1.4173105955123901, 1.5789011716842651, 1.3105753660202026, 2.0080292224884033]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.4987050294876099, 1.1318693161010742, 1.6241867542266846, 2.026930332183838, 1.2617384195327759, 1.60686457157135, 1.5441828966140747, 2.0124504566192627, 2.323474407196045, 1.499306559562683, 1.4395047426223755, 1.3395543098449707, 1.4372347593307495, 1.6979483366012573, 1.5833650827407837, 1.5285975933074951]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.95125412940979, 1.7923123836517334, 1.520225167274475, 1.603495717048645, 1.8195093870162964, 1.5290284156799316, 1.650282859802246, 1.7233328819274902, 1.989479422569275, 1.5651510953903198, 1.887672781944275, 1.7135214805603027, 2.5729970932006836, 1.834047555923462, 1.7858152389526367, 1.6939579248428345]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.7091613411903381, 1.2702926397323608, 1.261720895767212, 0.8242437243461609, 0.4376561939716339, 1.4917535781860352, 1.2116572856903076, 1.2969332933425903, 0.8574682474136353, 1.1486490964889526, 1.197824478149414, 0.9812241196632385, 1.0343241691589355, 1.2433748245239258, 1.301617980003357, 0.8410722017288208]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.46034574508667, 2.5425784587860107, 2.469221591949463, 2.542536497116089, 2.4503495693206787, 2.6457433700561523, 2.643582344055176, 2.7251968383789062, 2.406712055206299, 2.640470504760742, 2.700288772583008, 2.750148057937622, 2.5207252502441406, 2.550424098968506, 2.926163911819458, 2.694092035293579]
Running loglikelihood requests:  12%|█████████████████▏                                                                                                                       | 25/200 [00:43<04:23,  1.51s/it]Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.8172801733016968, 1.6499191522598267, 1.8996225595474243, 1.6217725276947021, 1.8733471632003784, 1.8127267360687256, 1.8666197061538696, 1.66144597530365, 1.8567321300506592, 2.057386875152588, 1.569016695022583, 1.9668035507202148, 1.7576290369033813, 1.7487574815750122, 1.747463345527649, 2.093186140060425]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.5725913047790527, 2.2254672050476074, 1.9774746894836426, 1.9221627712249756, 2.2576422691345215, 2.0804967880249023, 1.9616316556930542, 2.110307455062866, 2.075615882873535, 1.8589061498641968, 2.1354875564575195, 2.0263078212738037, 2.1543970108032227, 1.4509061574935913, 2.297039747238159, 2.111774444580078]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.8837196826934814, 3.1386144161224365, 2.871209144592285, 2.829137086868286, 3.2384068965911865, 3.0491607189178467, 3.0545594692230225, 2.894115924835205, 3.0012242794036865, 2.9568119049072266, 3.1247329711914062, 3.0164222717285156, 2.9360904693603516, 3.1387276649475098, 3.0539772510528564, 3.0728132724761963]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.2413313388824463, 2.160979986190796, 2.0186922550201416, 2.2021234035491943, 1.7894021272659302, 2.0612919330596924, 2.0742454528808594, 2.1823842525482178, 2.098052740097046, 2.0681514739990234, 2.1425507068634033, 2.0591487884521484, 2.081446647644043, 2.1080102920532227, 2.0713109970092773, 2.3365726470947266]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.2544188499450684, 2.0783796310424805, 2.0146358013153076, 2.2644827365875244, 2.067997694015503, 2.135488986968994, 2.0829076766967773, 2.060621976852417, 2.046146869659424, 2.257840633392334, 2.5167741775512695, 2.122631549835205, 2.132631540298462, 2.2330682277679443, 2.01786208152771, 2.168957233428955]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.1212189197540283, 2.0198636054992676, 2.0752484798431396, 2.1883552074432373, 2.0924949645996094, 2.088832378387451, 2.137838125228882, 2.1802573204040527, 2.07771897315979, 2.1950783729553223, 1.8038924932479858, 2.0793867111206055, 2.228590726852417, 2.1544015407562256, 2.0381314754486084, 2.032404899597168]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.4386892318725586, 3.4521963596343994, 3.655691146850586, 3.471602439880371, 3.6011948585510254, 3.4270429611206055, 3.5592288970947266, 3.570133686065674, 3.3376986980438232, 3.525928020477295, 3.410416603088379, 3.243260622024536, 3.4906630516052246, 3.2253241539001465, 3.546639919281006, 3.3444972038269043]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.279968738555908, 7.747708797454834, 7.4604363441467285, 7.28682804107666, 7.145003795623779, 6.954448699951172, 7.574659824371338, 7.53542423248291, 7.578518867492676, 7.300429821014404, 7.3522186279296875, 7.112335205078125, 7.3674774169921875, 7.498816013336182, 7.69349479675293, 7.285031318664551]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.22382116317749, 4.41236686706543, 4.200655460357666, 3.9205617904663086, 4.433716773986816, 4.267429351806641, 4.039754867553711, 4.166175842285156, 4.254873275756836, 4.245288848876953, 4.236231327056885, 3.856355667114258, 4.171299457550049, 4.4861531257629395, 4.350263595581055, 4.0992960929870605]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1059682369232178, 2.873464345932007, 2.9007978439331055, 2.714844226837158, 2.876683235168457, 2.9686145782470703, 2.8843255043029785, 2.786808490753174, 2.863848924636841, 2.9511146545410156, 3.130582809448242, 2.3374664783477783, 2.844648838043213, 2.894331693649292, 3.101076364517212, 3.0795488357543945]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.10156241059303284, 0.11880460381507874, 0.11462479829788208, -0.26120316982269287, -0.22324223816394806, -0.12457121163606644, 0.1323382407426834, -0.18182218074798584, 0.06970258057117462, 0.10331645607948303, 0.11437535285949707, 0.0907420814037323, 0.10030167549848557, 0.11380739510059357, -1.1767696142196655, 0.1282806545495987]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08246573060750961, 0.053570035845041275, 0.042384494096040726, 0.051872894167900085, 0.0782473161816597, 0.01877431385219097, 0.04471602290868759, 0.07119110226631165, 0.017840305343270302, 0.06413125991821289, -0.17105931043624878, 0.04280470684170723, -0.008672050200402737, -0.01979483850300312, 0.007912850938737392, 0.02448386885225773]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07405440509319305, 0.0423847921192646, 0.09364140033721924, 0.05652633681893349, 0.09972134977579117, 0.1222156435251236, 0.06832488626241684, -0.11952292919158936, 0.06778749078512192, 0.0839671790599823, -0.002775193890556693, 0.08995860069990158, -0.18672139942646027, 0.021716110408306122, -0.03615603595972061, 0.09901511669158936]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13278670608997345, 0.13596481084823608, 0.08815643936395645, 0.14815421402454376, 0.12339028716087341, 0.09848102927207947, -0.004586478229612112, 0.15106244385242462, 0.14872027933597565, -0.5460725426673889, -0.04289371520280838, 0.060767896473407745, 0.14469322562217712, -0.25974369049072266, -0.08181516081094742, -0.14507614076137543]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.030646512284874916, 0.06092114374041557, 0.07715567201375961, 0.10747774690389633, 0.06519881635904312, -0.08172164112329483, 0.01097898930311203, 0.010677245445549488, -0.03549505025148392, 0.021395159885287285, -0.19192487001419067, 0.10074164718389511, -0.07196732610464096, -0.20557862520217896, 0.08761432766914368, -0.000767178600654006]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.11794412136077881, 0.14448988437652588, 0.04579731822013855, 0.10282977670431137, -0.006605839356780052, -0.03784260153770447, 0.0267085749655962, 0.03989429399371147, 0.11499007046222687, -0.11081790179014206, -0.1559116244316101, 0.14651808142662048, -0.18380814790725708, 0.14631980657577515, 0.13052694499492645, 0.07895214110612869]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.15129126608371735, -0.06925908476114273, 0.13370436429977417, 0.28266385197639465, 0.20507873594760895, 0.10749713331460953, -0.11090186983346939, -0.059191539883613586, 0.1627267450094223, 0.1650903820991516, -0.5550285577774048, 0.18149623274803162, 0.34191983938217163, -0.2896535098552704, 0.1928228884935379, 0.20699948072433472]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.09191744774580002, -0.0766935646533966, 0.07988745719194412, 0.08400347828865051, -1.0562405586242676, -0.17893880605697632, -0.1721186637878418, -0.43075451254844666, -0.043043866753578186, -0.10239730030298233, -0.27701815962791443, -0.061000607907772064, 0.003626279765740037, -0.026990177109837532, -0.6199726462364197, -0.1722670942544937]
Layer: gate_7 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20483918488025665, 0.24517972767353058, -0.15215611457824707, 0.36799928545951843, 0.386408269405365, 0.2736935019493103, 0.2116948813199997, 0.15829510986804962, 0.15693819522857666, 0.469022274017334, 0.19508372247219086, 0.04936537891626358, 0.4029653072357178, -0.2359902709722519, -0.17950348556041718, 0.23809032142162323]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3050112724304199, 0.33085647225379944, 0.15147069096565247, 0.9084275364875793, 0.34890836477279663, 0.3938414752483368, 0.6345337629318237, 0.7046380639076233, 0.4378413259983063, 0.1372634470462799, 0.24181559681892395, 0.5691763758659363, 0.6531965732574463, 0.45162221789360046, 0.7916166186332703, 0.6647346615791321]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.8580309152603149, 0.7637493014335632, 0.2981582283973694, 0.6216784119606018, 0.6472253799438477, 0.0949530154466629, 0.3585553765296936, 0.6850326657295227, -0.05737270414829254, -0.08938132226467133, 0.40303391218185425, 0.6351172924041748, 0.04401148110628128, 0.22096285223960876, 0.43317484855651855, 0.3196471333503723]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.29640570282936096, 0.5160584449768066, 0.576560914516449, 0.5384891033172607, 0.6888084411621094, 0.2621760964393616, 0.5711660385131836, 0.268390417098999, 0.48982784152030945, 0.9166424870491028, 0.45753994584083557, 0.29329797625541687, 0.047316886484622955, -0.04579643905162811, -0.43828773498535156, 0.4919276535511017]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.14505532383918762, 0.8661121726036072, 0.8753916025161743, 0.5396062731742859, 0.08258955925703049, 0.5063855648040771, 0.5432318449020386, 0.689249575138092, 0.4469546675682068, 0.5304045677185059, 1.5317206382751465, 0.6416734457015991, 0.8307898044586182, 0.655678927898407, 0.548028290271759, 0.7217354774475098]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.2046821117401123, 1.2411338090896606, 1.426491141319275, 0.5297812819480896, 1.0419903993606567, 0.3517540693283081, 1.148085355758667, 1.4128862619400024, 1.0639631748199463, 1.1642447710037231, 1.1920808553695679, 1.5236937999725342, 0.35345458984375, 1.3891888856887817, 1.0765674114227295, 0.8889654874801636]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6253938674926758, 0.8708915710449219, 1.3258938789367676, 1.303760290145874, 1.1033287048339844, 1.4947171211242676, -0.08751371502876282, 0.9210321307182312, 0.6381765604019165, 0.5251152515411377, -0.36123126745224, 0.584626317024231, 1.0381038188934326, 1.1367915868759155, 1.3112715482711792, 0.4520241618156433]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.5968540906906128, 1.4278204441070557, 1.2197461128234863, 0.9799093008041382, 1.0065561532974243, 1.186057448387146, 1.5101487636566162, 0.736047089099884, 0.8742437362670898, 1.0414217710494995, 0.8395122289657593, 0.18788665533065796, 1.8076488971710205, 0.8570709824562073, 0.8310495018959045, 1.1806179285049438]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9500339031219482, 0.5723581910133362, 1.102654218673706, 1.0953991413116455, 0.8410576581954956, 0.6713770031929016, -0.1699582189321518, 1.7731753587722778, -0.5149754285812378, 2.1725311279296875, -0.6047306060791016, 1.0880509614944458, 1.2376093864440918, 1.200047254562378, 1.195168375968933, -0.03304570913314819]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.7146337032318115, 1.4308627843856812, 1.5527777671813965, 1.8018468618392944, 1.9258673191070557, 1.5028483867645264, 1.4489080905914307, 1.711495280265808, 1.616784930229187, 1.7375959157943726, 1.9476583003997803, 1.4292603731155396, 1.297870397567749, 1.5233854055404663, 1.2315810918807983, 1.8548433780670166]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.40946364402771, 1.0224132537841797, 1.5433058738708496, 1.953797459602356, 1.205744981765747, 1.5681241750717163, 1.4289096593856812, 1.9420603513717651, 2.251927614212036, 1.4188224077224731, 1.3283390998840332, 1.2782427072525024, 1.377718448638916, 1.4609112739562988, 1.5496898889541626, 1.42774498462677]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.8360085487365723, 1.7671879529953003, 1.4335076808929443, 1.5391418933868408, 1.7339190244674683, 1.5107601881027222, 1.5841811895370483, 1.655678153038025, 1.9154140949249268, 1.5201611518859863, 1.7885987758636475, 1.6103549003601074, 2.47263765335083, 1.7591822147369385, 1.7362706661224365, 1.5989041328430176]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.683932900428772, 1.290511131286621, 1.2131587266921997, 0.77104651927948, 0.5445541739463806, 1.4841021299362183, 1.2083301544189453, 1.4087756872177124, 0.8067507147789001, 1.1131317615509033, 1.2301583290100098, 1.0372034311294556, 1.040963888168335, 1.1838994026184082, 1.3223670721054077, 0.9386508464813232]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.2135074138641357, 2.3426578044891357, 2.2296407222747803, 2.335176467895508, 2.234941244125366, 2.446357250213623, 2.4066779613494873, 2.493823528289795, 2.2220981121063232, 2.4196712970733643, 2.472524404525757, 2.546713352203369, 2.2929129600524902, 2.336359739303589, 2.690089225769043, 2.474998950958252]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.7420721054077148, 1.5589077472686768, 1.8338159322738647, 1.5644115209579468, 1.7914354801177979, 1.7360296249389648, 1.7687498331069946, 1.5694507360458374, 1.8204394578933716, 1.9455794095993042, 1.5328714847564697, 1.885249376296997, 1.6680032014846802, 1.6554847955703735, 1.6749740839004517, 1.9559001922607422]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.566589117050171, 2.1219418048858643, 1.903977394104004, 1.8406985998153687, 2.129877805709839, 1.949103593826294, 1.8852001428604126, 2.013701915740967, 1.979093313217163, 1.7489933967590332, 2.023049831390381, 1.9214191436767578, 2.041524887084961, 1.3757472038269043, 2.1578712463378906, 1.9859695434570312]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.7867062091827393, 3.0235543251037598, 2.763150453567505, 2.6705121994018555, 3.058112621307373, 2.9249939918518066, 2.982821226119995, 2.7827208042144775, 2.869657516479492, 2.8472421169281006, 3.0155131816864014, 2.8524935245513916, 2.7845871448516846, 3.0093331336975098, 2.912590503692627, 2.9438419342041016]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  14%|███████████████████▊                                                                                                                     | 29/200 [00:48<04:06,  1.44s/it]Layer: gate_25 - Captured router_logits: [2.160752058029175, 2.0576138496398926, 1.9554572105407715, 2.1591379642486572, 1.7103022336959839, 2.032026767730713, 2.007218599319458, 2.122969150543213, 2.05727219581604, 2.013094425201416, 2.1159799098968506, 1.9797980785369873, 2.0134122371673584, 2.0523386001586914, 2.016190528869629, 2.2232086658477783]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.181537628173828, 1.9909799098968506, 1.9230564832687378, 2.1649084091186523, 1.9991995096206665, 2.0705740451812744, 2.0429439544677734, 2.0150794982910156, 1.978959083557129, 2.1608726978302, 2.4438934326171875, 2.046645402908325, 2.0760583877563477, 2.1484453678131104, 1.966023325920105, 2.110901117324829]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.0654032230377197, 1.962490200996399, 2.0078887939453125, 2.1293323040008545, 2.0159518718719482, 2.006913185119629, 2.0704407691955566, 2.101832866668701, 2.0260815620422363, 2.1258013248443604, 1.7496223449707031, 2.0281217098236084, 2.181413173675537, 2.0989911556243896, 1.9833282232284546, 1.9655003547668457]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.3134214878082275, 3.3269996643066406, 3.586508274078369, 3.359208583831787, 3.4838218688964844, 3.3074047565460205, 3.4785680770874023, 3.45698881149292, 3.230586528778076, 3.4171531200408936, 3.2905452251434326, 3.140774965286255, 3.358854293823242, 3.115537166595459, 3.438356876373291, 3.2847092151641846]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.097620487213135, 7.5882673263549805, 7.242928504943848, 7.10042667388916, 6.96424674987793, 6.779904365539551, 7.408694267272949, 7.339768886566162, 7.451125144958496, 7.086165904998779, 7.192355155944824, 6.961154937744141, 7.12631368637085, 7.300694465637207, 7.543046474456787, 7.17914342880249]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.1339640617370605, 4.288153171539307, 4.095709323883057, 3.8206913471221924, 4.420438289642334, 4.240085601806641, 3.9568357467651367, 4.13481330871582, 4.231293201446533, 4.20366907119751, 4.19177770614624, 3.7175285816192627, 4.056363105773926, 4.401424884796143, 4.268396377563477, 3.9885032176971436]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.0571694374084473, 2.840730667114258, 2.8498246669769287, 2.7192745208740234, 2.8160009384155273, 2.902742862701416, 2.907876968383789, 2.763019323348999, 2.8128018379211426, 2.900205135345459, 3.1367688179016113, 2.2890799045562744, 2.8239567279815674, 2.8688466548919678, 3.0693538188934326, 3.0017099380493164]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.05857619643211365, 0.07488658279180527, 0.08709412068128586, -0.2817274034023285, -0.2691067159175873, -0.08392923325300217, 0.08688326179981232, -0.06338296085596085, 0.058127354830503464, 0.05920109525322914, 0.06766127794981003, 0.0410020649433136, 0.06882592290639877, 0.09298576414585114, -1.0518254041671753, 0.10000760108232498]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.0675005093216896, 0.014296690933406353, 0.02580653503537178, 0.044756125658750534, 0.059042416512966156, 0.006609232164919376, 0.03295024111866951, 0.08955440670251846, 0.05710196867585182, 0.04756860062479973, -0.19440551102161407, 0.05252479389309883, -0.0007624754216521978, -0.028113704174757004, 0.031788039952516556, 0.03555949032306671]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06684721261262894, 0.02124028466641903, 0.07874377816915512, 0.05106952413916588, 0.1281459480524063, 0.08512478321790695, 0.06525573134422302, -0.1210302785038948, 0.07409075647592545, 0.1079612448811531, 0.03246915712952614, 0.09118364751338959, -0.22455567121505737, 0.0058165439404547215, -0.036376576870679855, 0.09701507538557053]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13930121064186096, 0.10710060596466064, 0.08186742663383484, 0.11776244640350342, 0.101468525826931, 0.10175440460443497, -0.036235738545656204, 0.13761888444423676, 0.14857695996761322, -0.6729704737663269, 0.007916195318102837, 0.04082116484642029, 0.26151415705680847, -0.39438101649284363, 0.02363271266222, -0.13787570595741272]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.015194334089756012, 0.0732133537530899, 0.05187840387225151, 0.1877976804971695, 0.08792313933372498, -0.07830417901277542, -0.002727087587118149, -0.030533211305737495, -0.06205479055643082, 0.01832752302289009, -0.2872864305973053, 0.09096372872591019, 0.0669930949807167, -0.2565932869911194, 0.11382104456424713, -0.09246863424777985]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.0859326422214508, 0.13308928906917572, 0.04445458948612213, 0.14207303524017334, -0.10563360154628754, -0.09797461330890656, 0.06025754287838936, 0.025266876444220543, 0.22560320794582367, -0.14686113595962524, -0.13381224870681763, 0.15278410911560059, -0.2897551953792572, 0.196201890707016, 0.1389993131160736, 0.09209097921848297]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.18465271592140198, -0.03896154463291168, 0.043608617037534714, 0.2545143663883209, 0.1730155199766159, 0.1250411421060562, -0.1559780091047287, -0.10346588492393494, 0.03311175853013992, 0.19622783362865448, -0.7016775608062744, 0.17872245609760284, 0.37410616874694824, -0.4180893301963806, 0.2918166518211365, 0.23195114731788635]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.02132246643304825, -0.04264090955257416, 0.05083990842103958, 0.008966978639364243, -1.1669602394104004, -0.17000924050807953, -0.06646016985177994, -0.6200783848762512, 0.15317827463150024, -0.19307498633861542, -0.3940794765949249, 0.03790796920657158, -0.14539210498332977, 0.11439894139766693, -0.8293606638908386, -0.27063822746276855]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.22006365656852722, 0.2622131109237671, -0.1994442194700241, 0.4608323276042938, 0.29856231808662415, 0.35518109798431396, 0.12686574459075928, 0.057107165455818176, 0.014536275528371334, 0.5136101841926575, 0.24675250053405762, 0.09589036554098129, 0.5284999012947083, -0.29006144404411316, -0.25307366251945496, 0.2880295515060425]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.2712360620498657, 0.17623305320739746, 0.17455238103866577, 1.0068602561950684, 0.24943062663078308, 0.1577107459306717, 0.531704843044281, 0.6846969127655029, 0.40904176235198975, -0.05945679545402527, 0.09925364702939987, 0.4828152656555176, 0.4746134579181671, 0.09319300949573517, 0.8633235692977905, 0.6082277894020081]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9122227430343628, 0.9130546450614929, 0.3154122233390808, 0.550892174243927, 0.5919833183288574, -0.1592986136674881, 0.45558321475982666, 0.7119525671005249, -0.1451835185289383, -0.36626482009887695, 0.5023545622825623, 0.6920039653778076, 0.015137897804379463, 0.14470620453357697, 0.5224334597587585, 0.32755565643310547]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.3582804501056671, 0.8048518896102905, 0.6893476247787476, 0.3744722306728363, 0.7978513836860657, 0.0020553553476929665, 0.6765920519828796, 0.20281843841075897, 0.5475724935531616, 1.2178326845169067, 0.5246469378471375, 0.3264436721801758, -0.09971233457326889, -0.02477685734629631, -0.4554626941680908, 0.4468510150909424]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.012125164270401001, 0.8368701338768005, 1.0424574613571167, 0.6194950342178345, 0.07954868674278259, 0.40401598811149597, 0.6629994511604309, 0.9841271042823792, 0.3561580181121826, 0.3902339041233063, 1.5998092889785767, 0.7176355123519897, 0.7536371350288391, 0.721100389957428, 0.5989106297492981, 0.8143810629844666]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7613228559494019, 0.9522104263305664, 1.1382575035095215, 0.15258803963661194, 0.9844006299972534, -0.1127951443195343, 0.8892495632171631, 1.3534510135650635, 0.9096471667289734, 0.8422362208366394, 1.093142032623291, 1.2520203590393066, -0.0579741895198822, 0.8922667503356934, 0.7662166953086853, 0.4796932339668274]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5618825554847717, 0.6343075037002563, 1.2994033098220825, 1.1119016408920288, 1.071690559387207, 1.5225224494934082, -0.0394018329679966, 1.4024657011032104, 0.7829671502113342, 0.5874642133712769, -0.6131600141525269, 0.5989326238632202, 0.999878466129303, 1.111283540725708, 1.3020274639129639, 0.2732101082801819]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.34764987230300903, 1.3055068254470825, 0.9340716600418091, 0.9011358618736267, 0.7925163507461548, 0.9865407347679138, 1.6301329135894775, 0.7010252475738525, 0.6929311156272888, 0.9164044260978699, 0.6038550138473511, -0.178871288895607, 1.6302707195281982, 0.743823766708374, 0.6891121864318848, 1.024794578552246]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9627876281738281, 0.3270311653614044, 1.014446496963501, 1.2069449424743652, 0.981045663356781, 0.5944929122924805, 0.044467415660619736, 1.6912015676498413, -0.6937206387519836, 2.1018402576446533, -0.6836662888526917, 0.973930299282074, 1.1752454042434692, 1.1610404253005981, 1.111452579498291, 0.07934301346540451]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.41108664870262146, 1.2921370267868042, 1.7395219802856445, 1.760384202003479, 1.928576111793518, 1.6129984855651855, 1.585587739944458, 1.6945325136184692, 1.981388807296753, 1.569333553314209, 2.4361395835876465, 1.6901054382324219, 1.536831021308899, 1.6640888452529907, 0.9148125052452087, 1.7822074890136719]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6575541496276855, 1.2187920808792114, 1.8144800662994385, 2.1855993270874023, 1.1508320569992065, 1.3942970037460327, 1.8378151655197144, 2.0338528156280518, 2.807873249053955, 1.5319799184799194, 1.3742538690567017, 1.173598051071167, 1.3866541385650635, 1.2701207399368286, 1.599535584449768, 1.6022568941116333]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.7508529424667358, 1.6259223222732544, 1.579764485359192, 1.4988597631454468, 1.8064807653427124, 1.3206573724746704, 1.5756175518035889, 1.5146552324295044, 2.0306036472320557, 1.5477550029754639, 1.8403711318969727, 1.5409986972808838, 2.2316558361053467, 1.7553805112838745, 1.7648531198501587, 1.6453582048416138]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.0748209953308105, 1.2354453802108765, 1.1818153858184814, 0.8500377535820007, 0.16319403052330017, 1.6979501247406006, 1.221906304359436, 1.6781989336013794, 0.8296215534210205, 1.3455541133880615, 1.1314438581466675, 1.2104557752609253, 1.2038030624389648, 1.2894268035888672, 1.175581455230713, 0.793519914150238]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.3111329078674316, 2.431631326675415, 2.3569767475128174, 2.287721872329712, 2.2544987201690674, 2.5241644382476807, 2.407318592071533, 2.498494863510132, 2.25641131401062, 2.4958674907684326, 2.5560059547424316, 2.641998052597046, 2.1030147075653076, 2.5353643894195557, 2.5263242721557617, 2.4743237495422363]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.6471153497695923, 1.559090495109558, 2.0260472297668457, 1.3430590629577637, 1.6907469034194946, 1.7410274744033813, 1.590710997581482, 1.5490574836730957, 1.6950807571411133, 1.8236701488494873, 1.6068118810653687, 1.8569154739379883, 1.6176321506500244, 1.678832769393921, 1.5162914991378784, 1.9378291368484497]
Layer: gate_22 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4455206394195557, 2.0967369079589844, 1.5883289575576782, 1.791540265083313, 2.0743823051452637, 1.8120839595794678, 1.7654908895492554, 1.7831989526748657, 1.861518383026123, 1.7304238080978394, 1.8397855758666992, 1.875146746635437, 1.693739652633667, 0.9376583099365234, 1.7406971454620361, 1.8593257665634155]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.8253862857818604, 2.99643874168396, 2.7256200313568115, 2.5373375415802, 2.968369960784912, 2.661378860473633, 2.820672035217285, 2.59533429145813, 2.6136105060577393, 2.7283785343170166, 3.0519444942474365, 2.677908182144165, 2.710667133331299, 2.9561352729797363, 2.7395336627960205, 2.8200724124908447]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.016927719116211, 1.941890001296997, 1.9272189140319824, 2.162555456161499, 1.563236117362976, 1.979356288909912, 2.0454792976379395, 2.2127809524536133, 2.063664436340332, 1.9515068531036377, 2.043962001800537, 1.9178260564804077, 1.984411358833313, 1.9644893407821655, 2.2518258094787598, 2.2260091304779053]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_26 - Captured router_logits: [2.0949647426605225, 2.1509015560150146, 2.0402870178222656, 2.1459357738494873, 2.0017764568328857, 2.1787831783294678, 2.0544285774230957, 1.9970732927322388, 2.067547082901001, 2.197258472442627, 2.673859119415283, 2.0218048095703125, 2.100097179412842, 2.136368751525879, 2.0766170024871826, 2.1795215606689453]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.0570716857910156, 1.9567415714263916, 1.9336131811141968, 2.039015531539917, 1.8657554388046265, 1.9354010820388794, 2.1048498153686523, 2.0994796752929688, 1.9573419094085693, 2.049326181411743, 1.5711355209350586, 1.9387229681015015, 2.1947121620178223, 2.255472183227539, 1.9352229833602905, 1.9407916069030762]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [3.190945625305176, 3.2960872650146484, 3.4905171394348145, 3.180049180984497, 3.413153648376465, 3.1891767978668213, 3.448876142501831, 3.3665733337402344, 3.1521172523498535, 3.2216620445251465, 3.1926066875457764, 2.952345371246338, 3.1887903213500977, 3.0220069885253906, 3.2645676136016846, 3.132833957672119]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  16%|██████████████████████▌                                                                                                                  | 33/200 [00:53<03:51,  1.39s/it]Layer: gate_29 - Captured router_logits: [6.811258316040039, 7.407660961151123, 6.974457263946533, 6.73105001449585, 6.673335552215576, 6.479629039764404, 6.991392135620117, 6.938485622406006, 7.042576313018799, 6.7368388175964355, 6.80420446395874, 6.709794521331787, 6.908565044403076, 6.970026969909668, 7.188825607299805, 6.826189994812012]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.307438373565674, 4.352323055267334, 4.142719268798828, 3.8433055877685547, 4.4820427894592285, 4.279825687408447, 3.9058384895324707, 4.2609453201293945, 4.405702114105225, 4.167119026184082, 4.131325721740723, 3.507279634475708, 3.8981664180755615, 4.499524116516113, 4.251501560211182, 4.149444103240967]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [2.726654529571533, 2.6954429149627686, 2.7226314544677734, 2.6589348316192627, 2.643109083175659, 2.7052414417266846, 2.6965131759643555, 2.4247703552246094, 2.6075057983398438, 2.5423836708068848, 3.121899127960205, 1.9678764343261719, 2.5899906158447266, 2.533343553543091, 2.969393253326416, 2.647418975830078]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06430117785930634, 0.07858980447053909, 0.0913909301161766, -0.28663554787635803, -0.26763439178466797, -0.08327951282262802, 0.09301234781742096, -0.07169993966817856, 0.06184539198875427, 0.06608229875564575, 0.07190035283565521, 0.04008324444293976, 0.07222787290811539, 0.10201507061719894, -1.057660460472107, 0.10073772072792053]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07001612335443497, 0.027560049667954445, 0.02716374211013317, 0.045772239565849304, 0.06387610733509064, 0.011166379787027836, 0.045082565397024155, 0.09147914499044418, 0.0618610717356205, 0.05152945592999458, -0.18651534616947174, 0.050390779972076416, -0.002009871182963252, -0.023339247331023216, 0.03626032918691635, 0.033565450459718704]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0627637654542923, 0.017360733821988106, 0.08756715059280396, 0.05330515280365944, 0.13741031289100647, 0.09772402793169022, 0.06627307832241058, -0.11400550603866577, 0.0967685729265213, 0.11467521637678146, 0.029146933928132057, 0.08442019671201706, -0.21712049841880798, 1.6050013073254377e-05, -0.03123682737350464, 0.0938636064529419]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13565926253795624, 0.11093083769083023, 0.08642527461051941, 0.11860916763544083, 0.11497560888528824, 0.0998421385884285, -0.038045864552259445, 0.1316748708486557, 0.15723678469657898, -0.6703624725341797, 0.008936924859881401, 0.048050545156002045, 0.23949123919010162, -0.37077346444129944, 0.032337505370378494, -0.13332821428775787]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.01655258610844612, 0.06957827508449554, 0.05367061495780945, 0.19478031992912292, 0.07914585620164871, -0.0711672231554985, 0.008633802644908428, -0.016446974128484726, -0.06825767457485199, 0.005954931024461985, -0.27844083309173584, 0.08672277629375458, 0.05232543498277664, -0.2435556948184967, 0.10524100065231323, -0.08328128606081009]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.07457345724105835, 0.14405317604541779, 0.05854159593582153, 0.15949535369873047, -0.12740908563137054, -0.10115694999694824, 0.05447709932923317, 0.02345229871571064, 0.21490301191806793, -0.13351239264011383, -0.1658327579498291, 0.1574411392211914, -0.27948346734046936, 0.20770755410194397, 0.136390820145607, 0.10011346638202667]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.20358997583389282, -0.02063118666410446, 0.05237181484699249, 0.26847484707832336, 0.1719510853290558, 0.1431441754102707, -0.17488059401512146, -0.11455663293600082, 0.052845411002635956, 0.20470573008060455, -0.7004761695861816, 0.19254857301712036, 0.36330994963645935, -0.4579043388366699, 0.2699929475784302, 0.2301386296749115]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.0139697240665555, -0.05087542533874512, 0.07049417495727539, 0.011637733317911625, -1.1320616006851196, -0.17990262806415558, -0.06864012032747269, -0.6339951753616333, 0.17053216695785522, -0.16620007157325745, -0.429739773273468, 0.03382984548807144, -0.15351080894470215, 0.1234482005238533, -0.860717236995697, -0.33996933698654175]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20739881694316864, 0.2724507451057434, -0.1996905505657196, 0.44706490635871887, 0.2931886911392212, 0.34807252883911133, 0.12401928007602692, 0.08277307450771332, 0.024741675704717636, 0.5185683965682983, 0.265973836183548, 0.0667458027601242, 0.5282002091407776, -0.29297104477882385, -0.2637137174606323, 0.28516700863838196]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.28360992670059204, 0.15761733055114746, 0.1931774914264679, 1.0030001401901245, 0.26655879616737366, 0.16534316539764404, 0.567771852016449, 0.6997672319412231, 0.40432247519493103, -0.04000673070549965, 0.09397304058074951, 0.48741352558135986, 0.43976476788520813, 0.05426599830389023, 0.8706756830215454, 0.6083558201789856]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.88862544298172, 0.9179655909538269, 0.30637627840042114, 0.5670190453529358, 0.6122820973396301, -0.1552259922027588, 0.4550928771495819, 0.7183766961097717, -0.16145923733711243, -0.36823272705078125, 0.5236790180206299, 0.6643264889717102, 0.035019759088754654, 0.15454204380512238, 0.5410242080688477, 0.38010355830192566]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.3472346365451813, 0.735022246837616, 0.6571298837661743, 0.32614201307296753, 0.7316758036613464, -0.04388882964849472, 0.6695352792739868, 0.16385026276111603, 0.498489648103714, 1.1882681846618652, 0.48460233211517334, 0.28490400314331055, -0.11878467351198196, -0.054520074278116226, -0.5076707005500793, 0.4390329122543335]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [-0.011394988745450974, 0.825439453125, 0.9639673233032227, 0.5833134651184082, 0.07613875716924667, 0.3630736470222473, 0.6384045481681824, 0.9155002236366272, 0.34111660718917847, 0.3920900821685791, 1.5514723062515259, 0.695689857006073, 0.7232785224914551, 0.6564372777938843, 0.5791286826133728, 0.7565166354179382]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6946184635162354, 0.8980076313018799, 1.0897252559661865, 0.17588087916374207, 0.8902198672294617, -0.18146900832653046, 0.8465433716773987, 1.4291589260101318, 0.8980641961097717, 0.6782389283180237, 1.0223687887191772, 1.1971362829208374, -0.043878406286239624, 0.7760906219482422, 0.7170060873031616, 0.37036368250846863]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  18%|█████████████████████████▎                                                                                                               | 37/200 [00:58<03:38,  1.34s/it]Layer: gate_14 - Captured router_logits: [0.5133063197135925, 0.49427148699760437, 1.182621717453003, 0.9681544303894043, 0.9012544751167297, 1.4019906520843506, 0.04764283820986748, 1.3284454345703125, 0.7600460648536682, 0.5464029312133789, -0.5667819380760193, 0.5015791654586792, 0.8759727478027344, 0.9605697989463806, 1.1656296253204346, 0.28170084953308105]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.3453082740306854, 1.0980900526046753, 0.8066165447235107, 0.7715631127357483, 0.6592335104942322, 0.8309675455093384, 1.4573783874511719, 0.6898552179336548, 0.548429548740387, 0.7347972989082336, 0.5930913090705872, 0.15235473215579987, 1.454620361328125, 0.638679563999176, 0.608363687992096, 1.0694776773452759]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.8025217056274414, 0.39711031317710876, 0.7917974591255188, 0.9834434390068054, 0.6594658493995667, 0.6633362770080566, 0.0746878907084465, 1.2666682004928589, -0.16320061683654785, 1.9837321043014526, -0.26423507928848267, 0.9397575259208679, 0.9218856692314148, 0.9799689650535583, 0.8551647067070007, 0.3971831798553467]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.6721193194389343, 1.2924948930740356, 1.482058048248291, 1.5107895135879517, 1.6262518167495728, 1.3720920085906982, 1.5049957036972046, 1.4360326528549194, 1.6328282356262207, 1.402366042137146, 2.024357795715332, 1.2675459384918213, 1.282492995262146, 1.2179404497146606, 0.9597672820091248, 1.509980320930481]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5806673765182495, 1.1772619485855103, 1.6243754625320435, 2.118619441986084, 1.3037899732589722, 1.4485095739364624, 1.9959038496017456, 1.9388270378112793, 2.639648914337158, 1.5289031267166138, 1.4650356769561768, 1.3547723293304443, 1.5122077465057373, 1.283375859260559, 1.7026770114898682, 1.627922534942627]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.9345918893814087, 1.817129135131836, 1.8190081119537354, 1.773163914680481, 2.0097732543945312, 1.6061811447143555, 1.8929001092910767, 1.7672175168991089, 2.2504093647003174, 1.6273130178451538, 2.054417371749878, 1.677703619003296, 2.204728126525879, 1.9776571989059448, 1.9541386365890503, 1.9284473657608032]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [1.070158839225769, 1.2988423109054565, 1.227457880973816, 1.0022789239883423, 0.30908000469207764, 1.6908831596374512, 1.2277073860168457, 1.5874475240707397, 0.9621506929397583, 1.2583215236663818, 1.1068096160888672, 1.178013563156128, 1.19921875, 1.2739064693450928, 1.1722673177719116, 0.9237920641899109]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.432966709136963, 2.526695966720581, 2.5400280952453613, 2.4167239665985107, 2.3910012245178223, 2.6149256229400635, 2.5652449131011963, 2.4993038177490234, 2.4196364879608154, 2.605501413345337, 2.6533756256103516, 2.7726125717163086, 2.2143971920013428, 2.486793279647827, 2.573120594024658, 2.5625240802764893]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.6933108568191528, 1.5844500064849854, 1.9384732246398926, 1.4450629949569702, 1.7376095056533813, 1.7646898031234741, 1.6327635049819946, 1.5563488006591797, 1.6870927810668945, 1.7938176393508911, 1.6488063335418701, 1.9468997716903687, 1.6938070058822632, 1.6757389307022095, 1.5971728563308716, 1.871869444847107]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.352536678314209, 2.0298783779144287, 1.626700758934021, 1.7687381505966187, 1.9656548500061035, 1.8323627710342407, 1.7997186183929443, 1.8428730964660645, 1.8255372047424316, 1.7683504819869995, 1.870581865310669, 1.8344112634658813, 1.7200886011123657, 1.076621174812317, 1.6960939168930054, 1.94891357421875]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.892841339111328, 2.961592674255371, 2.696119546890259, 2.6701576709747314, 2.9567208290100098, 2.811368703842163, 2.821485757827759, 2.7449004650115967, 2.7196924686431885, 2.822232723236084, 3.062788248062134, 2.7100472450256348, 2.8177871704101562, 2.9594271183013916, 2.7938570976257324, 2.8633153438568115]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.9748111963272095, 1.9696625471115112, 1.948591947555542, 2.1731040477752686, 1.607548475265503, 1.9730664491653442, 1.9859737157821655, 2.1021604537963867, 2.050809144973755, 1.97555410861969, 2.0507686138153076, 1.945350170135498, 2.0532100200653076, 1.9919148683547974, 2.2337119579315186, 2.219787836074829]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_26 - Captured router_logits: [1.9681485891342163, 2.0399134159088135, 1.9755643606185913, 2.028721809387207, 1.9663913249969482, 2.028425455093384, 1.987642765045166, 1.984800934791565, 1.9573959112167358, 2.1251888275146484, 2.454369306564331, 1.9770983457565308, 2.0381901264190674, 1.9253464937210083, 1.9519810676574707, 2.0429582595825195]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.9875377416610718, 1.8923736810684204, 1.943724513053894, 2.0012266635894775, 1.831189513206482, 1.8826634883880615, 2.0297298431396484, 1.992101788520813, 1.9219162464141846, 2.0132336616516113, 1.5734210014343262, 1.8874396085739136, 2.1237382888793945, 2.2051424980163574, 1.945804476737976, 1.915043592453003]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [3.2108888626098633, 3.2874066829681396, 3.5007612705230713, 3.248404026031494, 3.338961601257324, 3.198451042175293, 3.543667793273926, 3.3561344146728516, 3.164219856262207, 3.24419903755188, 3.2132320404052734, 2.987993001937866, 3.219510078430176, 3.0834691524505615, 3.3478505611419678, 3.2129456996917725]
Layer: gate_28 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [6.705777645111084, 7.271534442901611, 6.789436340332031, 6.624109745025635, 6.6135993003845215, 6.449380397796631, 6.855441570281982, 6.857756614685059, 6.950654983520508, 6.644968509674072, 6.656067371368408, 6.617760181427002, 6.80679178237915, 6.893208026885986, 7.061105728149414, 6.816835880279541]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.060618877410889, 4.097259998321533, 3.916633129119873, 3.667943000793457, 4.272488117218018, 4.1139960289001465, 3.685821771621704, 4.066728591918945, 4.205066204071045, 3.9730255603790283, 3.9342029094696045, 3.364164113998413, 3.705118179321289, 4.261074542999268, 4.0009260177612305, 3.9505341053009033]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.6786983013153076, 2.630077600479126, 2.64522647857666, 2.6041147708892822, 2.574456214904785, 2.618898630142212, 2.649144172668457, 2.3865787982940674, 2.5243964195251465, 2.471346378326416, 3.088094711303711, 1.902278184890747, 2.550630807876587, 2.526331663131714, 2.9417197704315186, 2.599818706512451]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.08780478686094284, 0.10611553490161896, 0.10715106874704361, -0.26115429401397705, -0.240205779671669, -0.1289127916097641, 0.12298258394002914, -0.13714583218097687, 0.06928267329931259, 0.09055767953395844, 0.10154687613248825, 0.07981759309768677, 0.08882537484169006, 0.10295938700437546, -1.1361550092697144, 0.11846160888671875]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07413486391305923, 0.04870160296559334, 0.0391889363527298, 0.0357171855866909, 0.07650301605463028, 0.015210136771202087, 0.045624203979969025, 0.07331424206495285, 0.014318661764264107, 0.05262630060315132, -0.19307589530944824, 0.05244515836238861, 0.001313122222200036, -0.04402343928813934, 0.017583472654223442, 0.040944647043943405]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08041714131832123, 0.03346407040953636, 0.08633673936128616, 0.061024174094200134, 0.0962393656373024, 0.11132253706455231, 0.07026367634534836, -0.12141378223896027, 0.06856442987918854, 0.0887988954782486, 0.006335166748613119, 0.0920005515217781, -0.20672735571861267, 0.0039038355462253094, -0.05413014814257622, 0.0960266962647438]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13409987092018127, 0.13454294204711914, 0.07357743382453918, 0.10384407639503479, 0.1064552292227745, 0.08761780709028244, -0.03386586904525757, 0.134085550904274, 0.14900556206703186, -0.57146817445755, -0.04848840832710266, 0.05763784050941467, 0.16260111331939697, -0.29707303643226624, -0.049130432307720184, -0.1440679281949997]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.024450577795505524, 0.0737321600317955, 0.07417736202478409, 0.11322171241044998, 0.05346127972006798, -0.08213857561349869, 0.010247564874589443, -0.018284743651747704, -0.04071201756596565, 0.04403722286224365, -0.22717662155628204, 0.12223634868860245, -0.03535129129886627, -0.24381598830223083, 0.09711083024740219, -0.008910009637475014]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.08942407369613647, 0.12266021966934204, 0.030856335535645485, 0.08393954485654831, -0.02393212355673313, -0.06139494106173515, 0.04065568372607231, 0.04228035360574722, 0.14747679233551025, -0.11959565430879593, -0.09275460243225098, 0.13216164708137512, -0.2016618847846985, 0.15005086362361908, 0.14684456586837769, 0.07003328949213028]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.1290779560804367, -0.07095124572515488, 0.0797070562839508, 0.2897454500198364, 0.19340184330940247, 0.0992288738489151, -0.12836214900016785, -0.062048885971307755, 0.12532036006450653, 0.1490861475467682, -0.5852399468421936, 0.1912236362695694, 0.32636556029319763, -0.27702370285987854, 0.20192214846611023, 0.22710205614566803]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.07719305902719498, -0.08332870900630951, 0.08570284396409988, 0.05215394124388695, -1.0729187726974487, -0.21971869468688965, -0.15435175597667694, -0.47053325176239014, -0.005357302725315094, -0.15153226256370544, -0.2497483342885971, -0.017556875944137573, 0.07376805692911148, -0.013600416481494904, -0.573380172252655, -0.02923746407032013]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.16604292392730713, 0.23718033730983734, -0.1318809539079666, 0.42031019926071167, 0.34991255402565, 0.2979390323162079, 0.19753985106945038, 0.1631985306739807, 0.07193414866924286, 0.37402278184890747, 0.21255828440189362, 0.0942845344543457, 0.4373703598976135, -0.1768999546766281, -0.12584209442138672, 0.22159191966056824]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.35421836376190186, 0.27230408787727356, 0.13122598826885223, 0.8647704124450684, 0.33914798498153687, 0.37704986333847046, 0.6075935959815979, 0.7048539519309998, 0.37813952565193176, 0.11572472006082535, 0.27670881152153015, 0.5466321110725403, 0.6758567690849304, 0.39790862798690796, 0.7955546379089355, 0.6618502140045166]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.8622605800628662, 0.8031736612319946, 0.3286629617214203, 0.6370711922645569, 0.6503040194511414, 0.08588970452547073, 0.4038209021091461, 0.7304830551147461, 0.012300356291234493, -0.13849453628063202, 0.43763741850852966, 0.6679180860519409, 0.12324317544698715, 0.22474412620067596, 0.45684343576431274, 0.3673850893974304]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.3996078670024872, 0.6745783090591431, 0.663988471031189, 0.6744396686553955, 0.7845606207847595, 0.43461287021636963, 0.6433472037315369, 0.4023861885070801, 0.6219306588172913, 1.0861557722091675, 0.5446287989616394, 0.4384121298789978, 0.15594135224819183, 0.16250541806221008, -0.2840697765350342, 0.5877442955970764]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.1835450381040573, 0.936358630657196, 0.9022662043571472, 0.602696418762207, 0.23508977890014648, 0.5788801908493042, 0.6055738925933838, 0.7532650232315063, 0.5402228236198425, 0.6323027610778809, 1.615807056427002, 0.7231477499008179, 0.8652971386909485, 0.7606045007705688, 0.6108372807502747, 0.72923743724823]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.2630099058151245, 1.2507083415985107, 1.522587537765503, 0.5689319968223572, 1.1014833450317383, 0.46323272585868835, 1.1835612058639526, 1.4591138362884521, 1.153324007987976, 1.2046732902526855, 1.210225224494934, 1.525509238243103, 0.490504652261734, 1.4111241102218628, 1.2141951322555542, 0.9287289977073669]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7021779417991638, 0.8762811422348022, 1.3884737491607666, 1.3499104976654053, 1.1085405349731445, 1.322040319442749, 0.06181112676858902, 0.8422950506210327, 0.792864203453064, 0.5492765307426453, -0.2903900146484375, 0.5524285435676575, 1.1064653396606445, 1.1106579303741455, 1.359592318534851, 0.43189123272895813]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6699123978614807, 1.4988632202148438, 1.2678120136260986, 1.0333144664764404, 1.0888652801513672, 1.3163553476333618, 1.4912163019180298, 0.7906607389450073, 0.9394747018814087, 1.0645921230316162, 0.9087353348731995, 0.32158488035202026, 1.8037513494491577, 0.9308477640151978, 0.9040576219558716, 1.0710084438323975]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.0930308103561401, 0.6303786039352417, 1.1922670602798462, 1.3100906610488892, 0.9045829772949219, 0.7496621608734131, -0.11505693942308426, 1.9808303117752075, -0.3830101490020752, 2.136444330215454, -0.4271380305290222, 1.2118114233016968, 1.3248111009597778, 1.3276785612106323, 1.2455226182937622, 0.04074333608150482]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  20%|████████████████████████████                                                                                                             | 41/200 [01:03<03:28,  1.31s/it]Layer: gate_17 - Captured router_logits: [0.8864721655845642, 1.6260589361190796, 1.7384260892868042, 2.025860548019409, 2.0516719818115234, 1.7042932510375977, 1.6284312009811401, 1.909826636314392, 1.793911099433899, 1.9148075580596924, 2.0826919078826904, 1.6274137496948242, 1.5323933362960815, 1.667380928993225, 1.4312210083007812, 2.1653246879577637]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.4848047494888306, 1.1586726903915405, 1.655320644378662, 2.0375282764434814, 1.3660171031951904, 1.6657291650772095, 1.4742779731750488, 2.020768404006958, 2.3037092685699463, 1.5221810340881348, 1.4644489288330078, 1.357154130935669, 1.4844763278961182, 1.688127040863037, 1.6266950368881226, 1.543599247932434]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.9196885824203491, 1.7943778038024902, 1.4962520599365234, 1.610601782798767, 1.82271409034729, 1.5413275957107544, 1.661520004272461, 1.6935611963272095, 1.8883522748947144, 1.544048547744751, 1.8834576606750488, 1.662925362586975, 2.6399993896484375, 1.8434761762619019, 1.7930269241333008, 1.6890017986297607]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6675652861595154, 1.279369592666626, 1.2245701551437378, 0.7829647660255432, 0.521777868270874, 1.4953364133834839, 1.2134623527526855, 1.2296175956726074, 0.8140659332275391, 1.1126459836959839, 1.203374981880188, 0.9390336275100708, 1.0506221055984497, 1.2615573406219482, 1.2832118272781372, 0.8776417970657349]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.4628424644470215, 2.5466082096099854, 2.486769199371338, 2.5599896907806396, 2.4732203483581543, 2.645449161529541, 2.6506595611572266, 2.7096686363220215, 2.4450924396514893, 2.661818027496338, 2.7044363021850586, 2.7117247581481934, 2.5281240940093994, 2.558722496032715, 2.896632432937622, 2.7199747562408447]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.7959825992584229, 1.635998010635376, 1.865400791168213, 1.6538861989974976, 1.8547258377075195, 1.7818650007247925, 1.804588794708252, 1.6143872737884521, 1.840242862701416, 2.0378293991088867, 1.5392745733261108, 1.898210048675537, 1.7363592386245728, 1.7270231246948242, 1.7331080436706543, 2.048161268234253]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.5678579807281494, 2.145576238632202, 1.9183623790740967, 1.8325364589691162, 2.1828036308288574, 2.0123519897460938, 1.9093451499938965, 2.0308756828308105, 2.00413179397583, 1.8148328065872192, 2.0661251544952393, 1.9325823783874512, 2.0307741165161133, 1.4003711938858032, 2.1702401638031006, 2.013916015625]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.7561862468719482, 3.0083563327789307, 2.779682159423828, 2.6915335655212402, 3.036436080932617, 2.9035325050354004, 2.958693504333496, 2.7676095962524414, 2.859426498413086, 2.8696932792663574, 2.964461326599121, 2.881460428237915, 2.7791383266448975, 3.0236852169036865, 2.908656597137451, 2.9229846000671387]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.2007229328155518, 2.098397731781006, 1.9968485832214355, 2.20064115524292, 1.7613781690597534, 2.056419849395752, 2.049400568008423, 2.1450748443603516, 2.084256887435913, 2.066002607345581, 2.1076836585998535, 2.047476291656494, 2.066594123840332, 2.0784752368927, 2.0301575660705566, 2.297049045562744]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.1770267486572266, 2.041290760040283, 1.9714642763137817, 2.2130045890808105, 2.030017375946045, 2.091726541519165, 2.0587172508239746, 2.0320043563842773, 2.0107192993164062, 2.2501635551452637, 2.472780704498291, 2.0874617099761963, 2.1127817630767822, 2.1892757415771484, 1.9832649230957031, 2.1088662147521973]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.0798680782318115, 2.003655195236206, 2.044550895690918, 2.146148681640625, 2.0345559120178223, 2.061624050140381, 2.1121044158935547, 2.1413111686706543, 2.0396013259887695, 2.15766978263855, 1.7844609022140503, 2.049109697341919, 2.198765516281128, 2.1624953746795654, 2.004302740097046, 2.0141501426696777]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.4553987979888916, 3.464951753616333, 3.707737922668457, 3.488306999206543, 3.6117029190063477, 3.441305160522461, 3.569855213165283, 3.5867156982421875, 3.36079740524292, 3.5440690517425537, 3.4356937408447266, 3.256357431411743, 3.5084726810455322, 3.2612416744232178, 3.6061227321624756, 3.392005443572998]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.417123794555664, 7.919231414794922, 7.610506057739258, 7.435888290405273, 7.3216986656188965, 7.145523548126221, 7.728680610656738, 7.736799240112305, 7.7243123054504395, 7.467393398284912, 7.47975492477417, 7.321072101593018, 7.535289287567139, 7.666422367095947, 7.868663787841797, 7.495645999908447]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.295173168182373, 4.525740146636963, 4.307305335998535, 4.019458293914795, 4.5336384773254395, 4.372835159301758, 4.15827751159668, 4.285778045654297, 4.398091793060303, 4.350202560424805, 4.333729267120361, 3.9329094886779785, 4.251850605010986, 4.553555011749268, 4.4572649002075195, 4.194089412689209]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1406495571136475, 2.903512477874756, 2.943570375442505, 2.7437245845794678, 2.9107470512390137, 3.008824348449707, 2.9015159606933594, 2.8305208683013916, 2.848646879196167, 2.9854049682617188, 3.177995204925537, 2.4331247806549072, 2.9078664779663086, 2.9218335151672363, 3.1688852310180664, 3.1217048168182373]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09955191612243652, 0.11900019645690918, 0.11737913638353348, -0.2682783901691437, -0.243808314204216, -0.12694139778614044, 0.13201536238193512, -0.17551518976688385, 0.07664768397808075, 0.10335052758455276, 0.10967087000608444, 0.09353259950876236, 0.09814544767141342, 0.11101695895195007, -1.1779325008392334, 0.129022017121315]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08209507912397385, 0.04670368880033493, 0.044850751757621765, 0.043960824608802795, 0.08233754336833954, 0.01766403205692768, 0.04516828432679176, 0.07115329802036285, 0.018306691199541092, 0.06010391190648079, -0.1957899034023285, 0.057938411831855774, -0.003249481553211808, -0.038395222276449203, 0.015339448116719723, 0.03908485546708107]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08702362328767776, 0.03884515166282654, 0.08434536308050156, 0.05639654025435448, 0.09810872375965118, 0.11666112393140793, 0.06738072633743286, -0.1276276856660843, 0.06660322844982147, 0.09081964194774628, 0.006996882613748312, 0.08870254456996918, -0.20086215436458588, 0.01617431640625, -0.046919774264097214, 0.1012485921382904]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1296948939561844, 0.12988783419132233, 0.07937248051166534, 0.11494552344083786, 0.10608606040477753, 0.0844024047255516, -0.010370560921728611, 0.14177440106868744, 0.15047091245651245, -0.5708113312721252, -0.05695450305938721, 0.06202678009867668, 0.13692019879817963, -0.29283082485198975, -0.058786336332559586, -0.14670413732528687]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.029517168179154396, 0.0868646651506424, 0.07638239860534668, 0.10167686641216278, 0.0547754280269146, -0.09141336381435394, 0.00884096696972847, -0.01388980820775032, -0.03706209734082222, 0.04683894291520119, -0.22544799745082855, 0.11758127808570862, -0.04828926548361778, -0.24849167466163635, 0.09789352118968964, 0.0050228433683514595]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.11385057866573334, 0.12593166530132294, 0.033726226538419724, 0.08536747097969055, 0.001063976320438087, -0.06703594326972961, 0.04368811473250389, 0.059285227209329605, 0.1306995153427124, -0.11555448174476624, -0.12284669280052185, 0.1379648596048355, -0.20998597145080566, 0.14717911183834076, 0.136691614985466, 0.06949197500944138]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.12835706770420074, -0.06260964274406433, 0.10595433413982391, 0.29101210832595825, 0.20133289694786072, 0.1002410277724266, -0.13495047390460968, -0.06728588044643402, 0.1075202152132988, 0.1464781016111374, -0.5928750038146973, 0.19635070860385895, 0.3410792052745819, -0.2649209499359131, 0.17672426998615265, 0.2214181274175644]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.07004330307245255, -0.06104816496372223, 0.08275138586759567, 0.06265294551849365, -1.0928770303726196, -0.19101689755916595, -0.15532530844211578, -0.46119949221611023, -0.018082985654473305, -0.14095653593540192, -0.23303711414337158, -0.024761974811553955, 0.07197020947933197, -0.017399581149220467, -0.5481662750244141, -0.05055911839008331]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.14641308784484863, 0.2322504222393036, -0.16255825757980347, 0.3934890329837799, 0.37726324796676636, 0.280251145362854, 0.19239084422588348, 0.17539937794208527, 0.06122380495071411, 0.33455905318260193, 0.19887588918209076, 0.08836933225393295, 0.4310404062271118, -0.21627342700958252, -0.13518096506595612, 0.22785435616970062]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.32014888525009155, 0.24457959830760956, 0.10428215563297272, 0.8259000778198242, 0.30034080147743225, 0.3745291531085968, 0.6055063605308533, 0.6694972515106201, 0.3777907192707062, 0.07391177117824554, 0.2760985195636749, 0.518638551235199, 0.6805629134178162, 0.3948674499988556, 0.7733821868896484, 0.6449564695358276]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.8029513359069824, 0.7579929232597351, 0.3158731162548065, 0.6378650665283203, 0.6664246916770935, 0.026539715006947517, 0.367730051279068, 0.6861760020256042, 0.030080992728471756, -0.17788571119308472, 0.44196686148643494, 0.6519839763641357, 0.09968621283769608, 0.17837418615818024, 0.36593812704086304, 0.3491952121257782]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.3998490869998932, 0.6541481614112854, 0.6836616396903992, 0.634438693523407, 0.8039067983627319, 0.4650822579860687, 0.6518484354019165, 0.42939814925193787, 0.6756980419158936, 0.9980944991111755, 0.5924518704414368, 0.4599410593509674, 0.18283605575561523, 0.12060204893350601, -0.3038286566734314, 0.6142358183860779]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.1743805855512619, 0.9619731903076172, 0.8647993206977844, 0.5431697368621826, 0.1796962171792984, 0.5948324799537659, 0.5004550218582153, 0.6591123938560486, 0.5852121710777283, 0.6547929644584656, 1.5684089660644531, 0.6741931438446045, 0.8804609179496765, 0.7821254730224609, 0.5892531275749207, 0.7071573138237]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.2733224630355835, 1.1815162897109985, 1.4274539947509766, 0.5297443866729736, 0.983980119228363, 0.5722793340682983, 1.1139334440231323, 1.2372896671295166, 1.0653339624404907, 1.1667144298553467, 1.1029144525527954, 1.465112328529358, 0.4601571559906006, 1.4345500469207764, 1.2008839845657349, 0.9608224630355835]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7124545574188232, 0.8876993656158447, 1.4089078903198242, 1.3824518918991089, 1.1400731801986694, 1.237882137298584, -0.04990912601351738, 0.7279139161109924, 0.790844202041626, 0.48938679695129395, -0.17338386178016663, 0.6356769800186157, 1.0995815992355347, 1.1177469491958618, 1.340675711631775, 0.48603618144989014]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6980330944061279, 1.4747511148452759, 1.3037225008010864, 1.0387146472930908, 1.0959408283233643, 1.290052056312561, 1.4004451036453247, 0.7791165113449097, 0.9262425899505615, 1.0511940717697144, 0.9047706127166748, 0.20711995661258698, 1.7742507457733154, 0.9253394603729248, 0.8954391479492188, 1.0232312679290771]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.0323275327682495, 0.6014288067817688, 1.165425181388855, 1.2538849115371704, 0.9647467136383057, 0.7251782417297363, -0.22480648756027222, 2.042515754699707, -0.45673537254333496, 2.1224136352539062, -0.4536946713924408, 1.1842869520187378, 1.401279091835022, 1.3122987747192383, 1.2503201961517334, -0.093083955347538]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.8543105125427246, 1.5104213953018188, 1.6403274536132812, 2.013362169265747, 2.013272285461426, 1.629042387008667, 1.518729329109192, 1.8410217761993408, 1.6564456224441528, 1.8221536874771118, 1.9330317974090576, 1.5135568380355835, 1.4759255647659302, 1.689476728439331, 1.4694370031356812, 2.11578106880188]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.447816014289856, 1.10835599899292, 1.5798602104187012, 1.9762449264526367, 1.2834686040878296, 1.6463569402694702, 1.3282456398010254, 1.9939621686935425, 2.232290267944336, 1.4508756399154663, 1.470505952835083, 1.3289731740951538, 1.4255173206329346, 1.7228670120239258, 1.6153234243392944, 1.511915922164917]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.961050033569336, 1.8302030563354492, 1.4917726516723633, 1.6136064529418945, 1.8180328607559204, 1.588566541671753, 1.7073755264282227, 1.7409720420837402, 1.8160611391067505, 1.578672170639038, 1.8775458335876465, 1.7241443395614624, 2.6725096702575684, 1.8476999998092651, 1.7864017486572266, 1.6730983257293701]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6533869504928589, 1.315126895904541, 1.2545174360275269, 0.8078770637512207, 0.4721062183380127, 1.523980975151062, 1.2221033573150635, 1.1490967273712158, 0.8611297607421875, 1.0953068733215332, 1.2186026573181152, 0.9531398415565491, 1.0279372930526733, 1.2832443714141846, 1.2738033533096313, 0.9592266082763672]
Running loglikelihood requests:  22%|██████████████████████████████▊                                                                                                          | 45/200 [01:08<03:21,  1.30s/it]Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.423308849334717, 2.499030828475952, 2.448939561843872, 2.5571506023406982, 2.440155506134033, 2.6508383750915527, 2.61505389213562, 2.6658260822296143, 2.4015398025512695, 2.6195385456085205, 2.657223701477051, 2.5426597595214844, 2.4629034996032715, 2.501607656478882, 2.887997627258301, 2.674499988555908]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.8316247463226318, 1.6408329010009766, 1.838767647743225, 1.6335984468460083, 1.8778767585754395, 1.7944221496582031, 1.86347234249115, 1.6289856433868408, 1.848589301109314, 2.058124303817749, 1.5046379566192627, 1.78355872631073, 1.7191888093948364, 1.713466763496399, 1.7564198970794678, 2.071981191635132]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.572594404220581, 2.1296517848968506, 1.9411773681640625, 1.8851560354232788, 2.1447396278381348, 2.0237128734588623, 1.9084783792495728, 2.0464894771575928, 2.0228216648101807, 1.7684745788574219, 2.074629783630371, 1.948102593421936, 2.0910484790802, 1.3726979494094849, 2.184699058532715, 2.0519590377807617]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.7696855068206787, 3.031006336212158, 2.740398406982422, 2.7378334999084473, 3.0342183113098145, 2.931602954864502, 2.947392225265503, 2.8000333309173584, 2.8898072242736816, 2.8808281421661377, 2.9092485904693604, 2.8954052925109863, 2.8019328117370605, 3.032288074493408, 2.9383628368377686, 2.946821451187134]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.1805624961853027, 2.075160026550293, 1.9649658203125, 2.184095859527588, 1.7169972658157349, 2.0140323638916016, 2.029207944869995, 2.1011853218078613, 2.0578086376190186, 2.062168598175049, 2.110595464706421, 2.029813528060913, 2.055659055709839, 2.0686144828796387, 1.9344013929367065, 2.264129638671875]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.178832530975342, 2.015704393386841, 1.965032935142517, 2.2077832221984863, 2.0345354080200195, 2.102109670639038, 2.063711643218994, 2.0356802940368652, 1.99126136302948, 2.2637381553649902, 2.4449524879455566, 2.0834481716156006, 2.1141059398651123, 2.2227158546447754, 2.0016496181488037, 2.1258912086486816]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.0919349193573, 1.9923443794250488, 2.0459887981414795, 2.15956449508667, 2.0489578247070312, 2.073367118835449, 2.097421169281006, 2.1371009349823, 2.0599796772003174, 2.162539482116699, 1.7716093063354492, 2.0542151927948, 2.226562738418579, 2.1159911155700684, 2.001204252243042, 2.0164384841918945]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.4162349700927734, 3.4124767780303955, 3.663356065750122, 3.457084894180298, 3.556513786315918, 3.407817840576172, 3.445631504058838, 3.547471523284912, 3.3299858570098877, 3.509018659591675, 3.395721435546875, 3.2093517780303955, 3.4827985763549805, 3.227365016937256, 3.5396780967712402, 3.3448925018310547]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.2386474609375, 7.679581642150879, 7.3939738273620605, 7.2794904708862305, 7.1429948806762695, 6.968385219573975, 7.561572074890137, 7.518950939178467, 7.576188087463379, 7.2877116203308105, 7.351596832275391, 7.094508647918701, 7.3224005699157715, 7.473384380340576, 7.690417289733887, 7.283595561981201]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_30 - Captured router_logits: [4.260699272155762, 4.470015048980713, 4.274217128753662, 3.9652318954467773, 4.5252485275268555, 4.319464683532715, 4.1206231117248535, 4.209267616271973, 4.321535587310791, 4.332912445068359, 4.296170711517334, 3.85749888420105, 4.222077369689941, 4.537986755371094, 4.403926372528076, 4.125711917877197]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.258706569671631, 3.0102601051330566, 3.0519542694091797, 2.835925817489624, 3.0316479206085205, 3.1116719245910645, 3.0303938388824463, 2.9394314289093018, 2.9851224422454834, 3.098801374435425, 3.208435297012329, 2.441908121109009, 3.005173444747925, 3.055053234100342, 3.2538585662841797, 3.228116750717163]
Layer: gate_31 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.08891661465167999, 0.1067325621843338, 0.10467655956745148, -0.256132572889328, -0.23837780952453613, -0.12178590893745422, 0.12317546457052231, -0.12789906561374664, 0.07352863997220993, 0.09028683602809906, 0.09723276644945145, 0.08204784989356995, 0.09305591881275177, 0.10406230390071869, -1.112913727760315, 0.11565840989351273]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07324343919754028, 0.04550455883145332, 0.03565039858222008, 0.03795582428574562, 0.07509710639715195, 0.016566166654229164, 0.04777172952890396, 0.07389691472053528, 0.011628791689872742, 0.05376870185136795, -0.19187094271183014, 0.046181268990039825, 0.0065576364286243916, -0.04088262468576431, 0.017311731353402138, 0.04211745038628578]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07631690800189972, 0.0316518172621727, 0.08586669713258743, 0.06559690833091736, 0.09496881812810898, 0.10713351517915726, 0.06438913941383362, -0.11977671086788177, 0.06100056692957878, 0.08548644185066223, 0.007550105918198824, 0.08560927957296371, -0.1964714676141739, 0.0052660792134702206, -0.043263133615255356, 0.10433967411518097]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13111448287963867, 0.13556984066963196, 0.07497572898864746, 0.11077120155096054, 0.10895213484764099, 0.08500444889068604, -0.010989801958203316, 0.1437767893075943, 0.14624786376953125, -0.5696489214897156, -0.05640646442770958, 0.06343396008014679, 0.15685570240020752, -0.2938847541809082, -0.05765561759471893, -0.15074126422405243]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.02122076228260994, 0.0754014328122139, 0.07126457989215851, 0.10961011797189713, 0.04801429435610771, -0.08535253256559372, 0.0045487890020012856, -0.014593628235161304, -0.036851316690444946, 0.05646039918065071, -0.22001256048679352, 0.1235172301530838, -0.034822795540094376, -0.24036036431789398, 0.09161686897277832, -0.013615796342492104]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.1004713922739029, 0.12108447402715683, 0.02923004701733589, 0.08680155128240585, -0.003847908228635788, -0.05656803026795387, 0.03586810827255249, 0.04930974543094635, 0.1386343538761139, -0.1254098266363144, -0.08926551789045334, 0.13361696898937225, -0.20997615158557892, 0.14399905502796173, 0.15125539898872375, 0.0690479427576065]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.1303253173828125, -0.06293697655200958, 0.08306409418582916, 0.2927814722061157, 0.1906421184539795, 0.10092009603977203, -0.12465884536504745, -0.05931127443909645, 0.13093796372413635, 0.1472785770893097, -0.5806548595428467, 0.1998123675584793, 0.3330532908439636, -0.26466646790504456, 0.19967322051525116, 0.23062540590763092]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.061960618942976, -0.07790342718362808, 0.09061901271343231, 0.05362612009048462, -1.0693278312683105, -0.18618933856487274, -0.14820562303066254, -0.4717848002910614, 0.0043229772709310055, -0.13511471450328827, -0.25214338302612305, -0.019467990845441818, 0.08600884675979614, -0.0068570575676858425, -0.5588365793228149, -0.03761233389377594]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.1791263222694397, 0.2420717179775238, -0.14539535343647003, 0.4333643913269043, 0.36603325605392456, 0.2910386621952057, 0.18851633369922638, 0.15463998913764954, 0.08628132939338684, 0.3758373260498047, 0.2226976454257965, 0.07504685968160629, 0.44642531871795654, -0.20333045721054077, -0.16043031215667725, 0.24083055555820465]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.33435410261154175, 0.2709178328514099, 0.1278558373451233, 0.8581974506378174, 0.3195498585700989, 0.3391728401184082, 0.5829358100891113, 0.685239315032959, 0.42336970567703247, 0.08694159984588623, 0.2443341165781021, 0.5139738917350769, 0.667968213558197, 0.36196956038475037, 0.7842305898666382, 0.65346360206604]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.8185005187988281, 0.7827064990997314, 0.31138506531715393, 0.6278737783432007, 0.6388814449310303, 0.07282556593418121, 0.38410383462905884, 0.6982667446136475, 0.006539277732372284, -0.15046809613704681, 0.42701855301856995, 0.668169379234314, 0.11591823399066925, 0.21015945076942444, 0.4473727345466614, 0.3252352774143219]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.4247649908065796, 0.6968839168548584, 0.6760827302932739, 0.6842166185379028, 0.7766103148460388, 0.4200011193752289, 0.6603193879127502, 0.4048869013786316, 0.641017735004425, 1.0576550960540771, 0.5722702741622925, 0.4408775568008423, 0.1656073033809662, 0.15286724269390106, -0.2626539170742035, 0.5879725813865662]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.1801728755235672, 0.9075067639350891, 0.9021771550178528, 0.585030734539032, 0.20926852524280548, 0.6050602197647095, 0.5823850035667419, 0.6952508687973022, 0.51349937915802, 0.622686505317688, 1.5985386371612549, 0.7072045207023621, 0.842340886592865, 0.757411777973175, 0.600995659828186, 0.7104640007019043]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.2055649757385254, 1.2052255868911743, 1.4828635454177856, 0.5229417681694031, 1.0331509113311768, 0.43978413939476013, 1.1355520486831665, 1.3414067029953003, 1.1006792783737183, 1.1660659313201904, 1.1295719146728516, 1.472378134727478, 0.4769166111946106, 1.3488620519638062, 1.2015705108642578, 0.90008544921875]
Layer: gate_13 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6932957172393799, 0.8597642779350281, 1.3615790605545044, 1.329410195350647, 1.1083132028579712, 1.3379647731781006, 0.03012814372777939, 0.7874624133110046, 0.7778299450874329, 0.5236954092979431, -0.28759869933128357, 0.5830705761909485, 1.082919716835022, 1.1128027439117432, 1.3248366117477417, 0.4383736252784729]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6639639139175415, 1.5141102075576782, 1.2617156505584717, 1.024791955947876, 1.0985124111175537, 1.2835534811019897, 1.4881938695907593, 0.7908449769020081, 0.9259814620018005, 1.0631916522979736, 0.886942982673645, 0.2911849319934845, 1.8027271032333374, 0.9057325124740601, 0.900192141532898, 1.0299137830734253]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.0928325653076172, 0.6316461563110352, 1.2078139781951904, 1.317746639251709, 0.9529365301132202, 0.7091206908226013, -0.14823861420154572, 2.023104190826416, -0.473995566368103, 2.1814627647399902, -0.45523133873939514, 1.16245436668396, 1.360978364944458, 1.3284146785736084, 1.2567769289016724, -0.03957439213991165]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.185299277305603, 1.6928340196609497, 1.8115307092666626, 2.2122631072998047, 2.0824339389801025, 1.8137683868408203, 1.6839900016784668, 2.0305018424987793, 1.803330659866333, 1.977033019065857, 2.1033363342285156, 1.6541855335235596, 1.6477504968643188, 1.791237235069275, 1.6290252208709717, 2.38370418548584]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.422494649887085, 1.1325130462646484, 1.5792659521102905, 1.968990683555603, 1.2547916173934937, 1.6260944604873657, 1.4260400533676147, 1.9713736772537231, 2.2301437854766846, 1.5176339149475098, 1.5289216041564941, 1.4386427402496338, 1.5052238702774048, 1.8624473810195923, 1.5215946435928345, 1.5262863636016846]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.9520548582077026, 1.8526456356048584, 1.5335198640823364, 1.6533477306365967, 1.8613210916519165, 1.5540010929107666, 1.748457670211792, 1.78080153465271, 1.8118783235549927, 1.6054340600967407, 1.9333652257919312, 1.702776551246643, 2.7676475048065186, 1.8890970945358276, 1.808332920074463, 1.7277029752731323]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6810200810432434, 1.2970225811004639, 1.1969298124313354, 0.8704206347465515, 0.3833264410495758, 1.5071831941604614, 1.2226935625076294, 1.1407862901687622, 0.8593857288360596, 1.1354637145996094, 1.2483258247375488, 0.928281843662262, 1.0689982175827026, 1.303769826889038, 1.2673084735870361, 0.9432187676429749]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.4792346954345703, 2.540254831314087, 2.5102312564849854, 2.6264472007751465, 2.5226504802703857, 2.70149302482605, 2.6550936698913574, 2.716323137283325, 2.4655144214630127, 2.680842876434326, 2.7317914962768555, 2.673126220703125, 2.4229178428649902, 2.5905048847198486, 2.8552873134613037, 2.714508533477783]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.8530561923980713, 1.6813600063323975, 1.8739734888076782, 1.6476118564605713, 1.8913809061050415, 1.845167875289917, 1.8512781858444214, 1.6645429134368896, 1.8693517446517944, 2.0958940982818604, 1.5566991567611694, 1.8562192916870117, 1.77701735496521, 1.757059097290039, 1.7925785779953003, 2.125413179397583]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.554064989089966, 2.155532121658325, 1.8815785646438599, 1.874612808227539, 2.1491799354553223, 2.0023183822631836, 1.9268786907196045, 2.037590265274048, 1.9974907636642456, 1.7661052942276, 2.0780954360961914, 1.9201292991638184, 2.006343126296997, 1.2518259286880493, 2.079538583755493, 2.0195720195770264]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.708527088165283, 3.0035433769226074, 2.7328784465789795, 2.6843504905700684, 3.0141379833221436, 2.8891284465789795, 2.957526445388794, 2.794999361038208, 2.826634407043457, 2.8727593421936035, 2.924873113632202, 2.8268988132476807, 2.8171639442443848, 3.0093791484832764, 2.905143976211548, 2.9295217990875244]
Running loglikelihood requests:  24%|█████████████████████████████████▌                                                                                                       | 49/200 [01:13<03:10,  1.26s/it]Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.1419894695281982, 2.0773844718933105, 1.988368272781372, 2.206481456756592, 1.6675392389297485, 2.0239250659942627, 2.015092611312866, 2.1652491092681885, 2.0635342597961426, 2.0576698780059814, 2.1111581325531006, 2.024365186691284, 2.064521074295044, 2.093549966812134, 1.984317421913147, 2.2700016498565674]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.181396961212158, 2.055015802383423, 2.001049280166626, 2.221982717514038, 2.0565478801727295, 2.121567726135254, 2.10436749458313, 2.0729739665985107, 2.013413667678833, 2.2731757164001465, 2.5054900646209717, 2.117614269256592, 2.1187262535095215, 2.2489547729492188, 1.9953972101211548, 2.138742446899414]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.083373546600342, 1.9893711805343628, 2.0373752117156982, 2.151052951812744, 2.0180487632751465, 2.0641708374023438, 2.1247401237487793, 2.1444358825683594, 2.04548716545105, 2.150636911392212, 1.7190881967544556, 2.059368133544922, 2.2169742584228516, 2.142650842666626, 2.004849433898926, 2.0199248790740967]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.4105477333068848, 3.419774293899536, 3.686697006225586, 3.4333198070526123, 3.546088457107544, 3.394909381866455, 3.496140241622925, 3.5344526767730713, 3.3448452949523926, 3.47456431388855, 3.3844728469848633, 3.170060634613037, 3.470480442047119, 3.210705518722534, 3.5570108890533447, 3.3324074745178223]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.2740397453308105, 7.750227451324463, 7.436734676361084, 7.280661106109619, 7.197526454925537, 7.027983665466309, 7.58348274230957, 7.563529968261719, 7.606310844421387, 7.316867351531982, 7.337141513824463, 7.187701225280762, 7.380742073059082, 7.507231712341309, 7.7344441413879395, 7.3383989334106445]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.285408020019531, 4.486886024475098, 4.301793098449707, 3.960165500640869, 4.5272746086120605, 4.384504795074463, 4.140477180480957, 4.2556047439575195, 4.369361877441406, 4.339044570922852, 4.32762336730957, 3.8111565113067627, 4.1866774559021, 4.564470291137695, 4.41427755355835, 4.152442932128906]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.148369789123535, 2.885310411453247, 2.964285373687744, 2.7569735050201416, 2.9370851516723633, 2.9949283599853516, 2.9402780532836914, 2.803039789199829, 2.848031520843506, 2.9567649364471436, 3.1995112895965576, 2.396177053451538, 2.927473783493042, 2.923198699951172, 3.2047526836395264, 3.109287977218628]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09451030194759369, 0.11402463912963867, 0.10959984362125397, -0.23857061564922333, -0.2185034453868866, -0.13021795451641083, 0.12575310468673706, -0.19214142858982086, 0.07531322538852692, 0.09826971590518951, 0.10384675115346909, 0.0899825394153595, 0.09572826325893402, 0.10615413635969162, -1.090158462524414, 0.12120147049427032]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07615922391414642, 0.04826422780752182, 0.0336005762219429, 0.03939228132367134, 0.07853559404611588, 0.019067682325839996, 0.048370663076639175, 0.05840672552585602, 0.001057696295902133, 0.05423903465270996, -0.18034756183624268, 0.057023536413908005, 0.004408961161971092, -0.036121439188718796, 0.012053265236318111, 0.04273517057299614]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0850885733962059, 0.040334153920412064, 0.08872669190168381, 0.060653336346149445, 0.08872425556182861, 0.11237984895706177, 0.060719698667526245, -0.11542177945375443, 0.07368682324886322, 0.07393676042556763, 0.0012057085987180471, 0.08787830919027328, -0.17934860289096832, 0.0205597672611475, -0.05339839309453964, 0.10426754504442215]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1344694346189499, 0.13677182793617249, 0.07979676127433777, 0.11479329317808151, 0.09406935423612595, 0.09431891143321991, 0.014647863805294037, 0.13823705911636353, 0.1292327344417572, -0.5170972347259521, -0.06279824674129486, 0.07292730361223221, 0.1076422929763794, -0.24630658328533173, -0.08212976902723312, -0.13515803217887878]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.04144406318664551, 0.0759609043598175, 0.08138551563024521, 0.06215408444404602, 0.05225842073559761, -0.08155196905136108, 0.019804367795586586, -0.015964385122060776, -0.005304037127643824, 0.04550834000110626, -0.202797994017601, 0.11893483996391296, -0.06431932747364044, -0.23676134645938873, 0.0874229371547699, 0.008745857514441013]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09226380288600922, 0.12379009276628494, 0.02569984830915928, 0.06378904730081558, 0.006273042876273394, -0.031247414648532867, 0.0312189981341362, 0.08243221044540405, 0.08441409468650818, -0.1063283160328865, -0.12045932561159134, 0.1444365531206131, -0.18095149099826813, 0.13359913229942322, 0.14246609807014465, 0.0701947882771492]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11024010926485062, -0.04955337196588516, 0.06897827982902527, 0.2819977402687073, 0.16631482541561127, 0.06832239776849747, -0.11169801652431488, -0.08293110877275467, 0.085573710501194, 0.14108596742153168, -0.5536274909973145, 0.19553060829639435, 0.3309834897518158, -0.22572174668312073, 0.10363811254501343, 0.20171263813972473]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.1651829183101654, -0.03948816657066345, 0.10502954572439194, 0.027106627821922302, -1.0439425706863403, -0.1972043812274933, -0.1671815812587738, -0.38589826226234436, -0.12573805451393127, -0.17801125347614288, -0.19124171137809753, -0.04428962618112564, 0.16882364451885223, -0.07909766584634781, -0.4725040793418884, 0.009950749576091766]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.16494576632976532, 0.24118760228157043, -0.06674543023109436, 0.4344245195388794, 0.3627815842628479, 0.3083181083202362, 0.1838211566209793, 0.25770822167396545, -0.02981209009885788, 0.184493750333786, 0.2581973373889923, 0.04050822928547859, 0.4443274140357971, -0.13648903369903564, -0.055910125374794006, 0.18353556096553802]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3720616400241852, 0.32748550176620483, 0.09148956835269928, 0.6787397265434265, 0.3082573711872101, 0.4414660334587097, 0.6516073346138, 0.7306019067764282, 0.30675965547561646, 0.09298767149448395, 0.39325687289237976, 0.5321305990219116, 0.6983510255813599, 0.4640772342681885, 0.8460946679115295, 0.6656143665313721]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7322148680686951, 0.7432267665863037, 0.3167837858200073, 0.6030688881874084, 0.6546701192855835, -0.07723766565322876, 0.38494962453842163, 0.7050138115882874, 0.13402996957302094, -0.12115801870822906, 0.3956468105316162, 0.5794142484664917, 0.24841371178627014, 0.1821250021457672, 0.22448904812335968, 0.38278788328170776]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.4723112881183624, 0.6574627161026001, 0.7916562557220459, 0.6595951318740845, 0.7985049486160278, 0.5816046595573425, 0.6815338730812073, 0.5309845209121704, 0.6029818654060364, 1.0402971506118774, 0.6459968090057373, 0.5740565657615662, 0.3919990360736847, 0.31497666239738464, -0.06428835541009903, 0.702942430973053]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.2928813099861145, 1.1061493158340454, 0.8760017156600952, 0.6012812852859497, 0.4021740257740021, 0.6595409512519836, 0.5534750819206238, 0.6428444981575012, 0.6625673770904541, 0.727599024772644, 1.730061650276184, 0.7977768182754517, 0.9506286382675171, 0.8369876146316528, 0.7083197236061096, 0.7987090349197388]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.593082070350647, 1.3979963064193726, 1.669047236442566, 0.7547614574432373, 1.1853184700012207, 0.8097569942474365, 1.3001139163970947, 1.3224788904190063, 1.2674760818481445, 1.3443917036056519, 1.2668979167938232, 1.6704760789871216, 0.7524317502975464, 1.6759226322174072, 1.3886224031448364, 1.1944366693496704]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [0.9080916047096252, 1.081477165222168, 1.5940271615982056, 1.5491126775741577, 1.2941248416900635, 1.3753687143325806, 0.13232778012752533, 0.6056511998176575, 0.9519877433776855, 0.5938882827758789, 0.01686767302453518, 0.8006449341773987, 1.2242937088012695, 1.2348179817199707, 1.4940894842147827, 0.5520110726356506]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.8178772926330566, 1.5254117250442505, 1.4092215299606323, 1.0999609231948853, 1.2621870040893555, 1.4079642295837402, 1.491823673248291, 0.8622629046440125, 0.9444005489349365, 1.1483032703399658, 1.0183926820755005, 0.3798075318336487, 1.8315314054489136, 0.9969449639320374, 1.0567364692687988, 1.0885905027389526]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.1844412088394165, 0.7349528074264526, 1.255935549736023, 1.222052812576294, 0.9068719148635864, 0.7908440828323364, -0.32345113158226013, 2.108936071395874, -0.36608418822288513, 2.0669949054718018, -0.310245156288147, 1.252014398574829, 1.4787741899490356, 1.4797440767288208, 1.3275671005249023, -0.05122668296098709]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.1306802034378052, 1.8387442827224731, 1.895754098892212, 2.366621732711792, 2.1760897636413574, 1.8765398263931274, 1.7830582857131958, 2.0734169483184814, 1.8660324811935425, 2.1552529335021973, 2.0832247734069824, 1.6869347095489502, 1.727095127105713, 1.8931161165237427, 1.828230619430542, 2.4000744819641113]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.576682209968567, 1.2790002822875977, 1.8243424892425537, 1.987289547920227, 1.528930425643921, 1.8311188220977783, 1.4960217475891113, 2.172921895980835, 2.3543601036071777, 1.6257283687591553, 1.6881744861602783, 1.564786672592163, 1.6368948221206665, 1.8956626653671265, 1.7828137874603271, 1.652742862701416]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.314722776412964, 2.0779881477355957, 1.7190459966659546, 1.9294092655181885, 2.0994014739990234, 1.8984960317611694, 2.0543456077575684, 2.0641555786132812, 2.028820753097534, 1.8247053623199463, 2.1484742164611816, 1.9417251348495483, 2.9869906902313232, 2.1932966709136963, 2.1127429008483887, 1.964387059211731]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6418814659118652, 1.3778727054595947, 1.3730919361114502, 0.8933567404747009, 0.6413893103599548, 1.4823561906814575, 1.349924921989441, 1.3109211921691895, 1.021070122718811, 1.1473366022109985, 1.344399094581604, 1.027053952217102, 1.1246904134750366, 1.3476109504699707, 1.453489065170288, 1.0619529485702515]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.7037041187286377, 2.7538838386535645, 2.694211721420288, 2.843594789505005, 2.7366714477539062, 2.901038408279419, 2.959719657897949, 2.8558475971221924, 2.6561737060546875, 2.9169254302978516, 2.922882318496704, 2.8097035884857178, 2.7621724605560303, 2.7571096420288086, 3.1646370887756348, 2.9652256965637207]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.9563746452331543, 1.7000694274902344, 1.806480050086975, 1.7621434926986694, 1.9859713315963745, 1.8919795751571655, 1.963551640510559, 1.6636228561401367, 1.9489240646362305, 2.184656858444214, 1.591802954673767, 1.86495041847229, 1.8400706052780151, 1.8444290161132812, 1.9141677618026733, 2.190807342529297]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.567866325378418, 2.1936776638031006, 2.124979257583618, 1.9787492752075195, 2.1508402824401855, 2.1281886100769043, 2.0117979049682617, 2.18849778175354, 2.093204975128174, 1.8676122426986694, 2.21504282951355, 2.0414083003997803, 2.1939525604248047, 1.5321820974349976, 2.3031809329986572, 2.1562159061431885]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.8609251976013184, 3.1520514488220215, 2.8701090812683105, 2.842454671859741, 3.0691823959350586, 3.0830814838409424, 3.0429189205169678, 2.9033169746398926, 3.057244300842285, 3.058455228805542, 2.953312635421753, 3.0275323390960693, 2.8860156536102295, 3.1586451530456543, 3.082699775695801, 3.1416940689086914]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_25 - Captured router_logits: [2.3233463764190674, 2.2013862133026123, 2.1335713863372803, 2.3082778453826904, 1.9186019897460938, 2.1450960636138916, 2.1734375953674316, 2.1707923412323, 2.2116189002990723, 2.2026374340057373, 2.229017496109009, 2.2404210567474365, 2.203680992126465, 2.2781081199645996, 2.0350747108459473, 2.4399216175079346]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.289151430130005, 2.1041197776794434, 2.1076793670654297, 2.3434882164001465, 2.118295907974243, 2.170267343521118, 2.169159412384033, 2.123739719390869, 2.098930835723877, 2.3857533931732178, 2.4480948448181152, 2.199843406677246, 2.24029278755188, 2.28182053565979, 2.067561626434326, 2.211918592453003]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.1402764320373535, 2.0377964973449707, 2.0891551971435547, 2.199737548828125, 2.075503349304199, 2.105210304260254, 2.1410303115844727, 2.222069025039673, 2.1413068771362305, 2.2168099880218506, 1.8346439599990845, 2.0968666076660156, 2.2263927459716797, 2.0873990058898926, 2.081639051437378, 2.0887699127197266]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Running loglikelihood requests:  26%|████████████████████████████████████▎                                                                                                    | 53/200 [01:17<03:00,  1.23s/it]Layer: gate_28 - Captured router_logits: [3.6097264289855957, 3.6128182411193848, 3.874558448791504, 3.7003328800201416, 3.8406224250793457, 3.6453561782836914, 3.5837082862854004, 3.778611660003662, 3.561075210571289, 3.7257227897644043, 3.630486488342285, 3.44073748588562, 3.7199931144714355, 3.442746162414551, 3.6996114253997803, 3.5961012840270996]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.774351596832275, 8.20777702331543, 7.885531902313232, 7.810335636138916, 7.6179351806640625, 7.396039009094238, 8.120137214660645, 8.014692306518555, 8.064953804016113, 7.782369613647461, 7.782467365264893, 7.628618240356445, 7.873055458068848, 7.957806587219238, 8.186946868896484, 7.779375076293945]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.8562703132629395, 5.059698104858398, 4.912065505981445, 4.497857093811035, 5.152528762817383, 4.934601306915283, 4.752087116241455, 4.839635372161865, 4.9667253494262695, 4.94840145111084, 4.872994899749756, 4.412361145019531, 4.815450668334961, 5.177173614501953, 5.020524024963379, 4.708616256713867]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.454927921295166, 3.214501142501831, 3.2816085815429688, 3.1504034996032715, 3.249276876449585, 3.345273494720459, 3.25354266166687, 3.158207654953003, 3.1901097297668457, 3.3121964931488037, 3.4002339839935303, 2.62341046333313, 3.2045984268188477, 3.2483882904052734, 3.470618724822998, 3.414806604385376]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.0880495011806488, 0.10141963511705399, 0.10299603641033173, -0.2574212849140167, -0.21020694077014923, -0.15209384262561798, 0.11389057338237762, -0.15165585279464722, 0.09241964668035507, 0.085063636302948, 0.09433016180992126, 0.10357660800218582, 0.09130942076444626, 0.10589331388473511, -1.0496333837509155, 0.11977212876081467]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.0803864449262619, 0.041469309478998184, 0.031366512179374695, 0.04558918997645378, 0.0758303552865982, 0.02690347097814083, 0.03304606303572655, 0.06674898415803909, 0.022741587832570076, 0.045918058604002, -0.19287195801734924, 0.05103759095072746, 0.012890854850411415, -0.028766486793756485, 0.0038506516721099615, 0.01233252789825201]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.04984905198216438, 0.027879036962985992, 0.0914469063282013, 0.05385689437389374, 0.11013393104076385, 0.08090229332447052, 0.06445375829935074, -0.1161259114742279, 0.07401976734399796, 0.06653749197721481, 0.0028688579332083464, 0.08486659079790115, -0.17513595521450043, 0.01531351637095213, -0.03030329756438732, 0.10736538469791412]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11055496335029602, 0.11696559190750122, 0.08012745529413223, 0.11597837507724762, 0.09433206915855408, 0.12988688051700592, 0.007946767844259739, 0.1362045854330063, 0.11743348091840744, -0.5550230145454407, -0.07590555399656296, 0.09875772148370743, 0.16677747666835785, -0.2903067171573639, -0.08341051638126373, -0.11860679090023041]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.03562850132584572, 0.047139935195446014, 0.07474770396947861, 0.11647887527942657, 0.05980400741100311, -0.10038143396377563, 0.027588892728090286, -0.014045391231775284, -0.04517940804362297, 0.03181076422333717, -0.1933264434337616, 0.08917934447526932, -0.08598772436380386, -0.1911976933479309, 0.09644317626953125, 0.01636350527405739]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.10394266247749329, 0.1250683218240738, 0.039631690829992294, 0.0608300045132637, -0.027006948366761208, -0.021373288705945015, 0.037794046103954315, 0.08379349857568741, 0.11711328476667404, -0.11599515378475189, -0.13800595700740814, 0.13355356454849243, -0.21276021003723145, 0.13811689615249634, 0.1569196581840515, 0.07023924589157104]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.14101138710975647, 0.025273745879530907, 0.01706511527299881, 0.2597067058086395, 0.15398269891738892, 0.09667064994573593, -0.06295386701822281, -0.042068351060152054, 0.03622809424996376, 0.15082985162734985, -0.5407353043556213, 0.2172277718782425, 0.35244297981262207, -0.23999813199043274, 0.17820145189762115, 0.22936080396175385]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.10173927992582321, 0.016912754625082016, 0.11808974295854568, -0.02265690267086029, -0.9807330965995789, -0.17810219526290894, -0.12450888007879257, -0.28822946548461914, -0.0019042706117033958, -0.19685977697372437, -0.19376833736896515, -0.0107332244515419, 0.022824184969067574, 0.019744856283068657, -0.5389229655265808, -0.12057419866323471]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.1291813999414444, 0.24806073307991028, -0.051811400800943375, 0.3946594297885895, 0.24295733869075775, 0.3479982316493988, 0.14156974852085114, 0.22579161822795868, -0.03533323481678963, 0.35501277446746826, 0.27005988359451294, 0.060485634952783585, 0.4914085566997528, -0.21410445868968964, -0.06429093331098557, 0.186510369181633]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.39911574125289917, 0.22534963488578796, 0.13911086320877075, 0.7617946267127991, 0.2757631540298462, 0.3585163950920105, 0.6168273687362671, 0.6698220372200012, 0.20642037689685822, 0.0710020363330841, 0.21302689611911774, 0.4791404902935028, 0.5928008556365967, 0.3842088282108307, 0.8310683965682983, 0.6253527402877808]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7811492085456848, 0.7849873304367065, 0.30389198660850525, 0.5939316153526306, 0.6025871634483337, -0.1746450811624527, 0.39948058128356934, 0.6722444891929626, -0.01848716475069523, -0.1991085410118103, 0.49352291226387024, 0.539199948310852, 0.20206418633460999, 0.25369250774383545, 0.27778178453445435, 0.3928395211696625]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.380315363407135, 0.5929606556892395, 0.7166809439659119, 0.4236653447151184, 0.7475042343139648, 0.33958715200424194, 0.770987868309021, 0.35748639702796936, 0.5628100633621216, 0.9396674036979675, 0.5561336874961853, 0.3387535810470581, 0.27210381627082825, 0.2438058853149414, -0.2306172400712967, 0.5872748494148254]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.19586382806301117, 0.964759349822998, 0.8446941375732422, 0.5854842662811279, 0.2246682196855545, 0.4934718608856201, 0.4889174997806549, 0.6481419205665588, 0.5005025267601013, 0.6587750911712646, 1.5633734464645386, 0.7850801944732666, 0.9206437468528748, 0.711358368396759, 0.7509721517562866, 0.7412682771682739]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.1314488649368286, 1.0964244604110718, 1.2734205722808838, 0.4314688444137573, 0.9951614141464233, 0.3438030779361725, 0.9963534474372864, 1.030576229095459, 1.010878324508667, 0.9895959496498108, 1.0264590978622437, 1.440545678138733, 0.26602423191070557, 1.2493194341659546, 1.0695819854736328, 0.7124186158180237]
Running loglikelihood requests:  28%|███████████████████████████████████████                                                                                                  | 57/200 [01:22<02:53,  1.22s/it]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5805345177650452, 0.7761057019233704, 1.261541724205017, 1.1960538625717163, 1.0967246294021606, 0.9852017164230347, 0.016794105991721153, 0.9010933637619019, 0.6948899626731873, 0.6155564785003662, -0.32130610942840576, 0.5573892593383789, 1.126466155052185, 1.0460171699523926, 1.2436798810958862, 0.4170739948749542]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6425793170928955, 1.4036684036254883, 1.1718217134475708, 0.9996066093444824, 1.0147900581359863, 1.268151879310608, 1.3752697706222534, 0.7280119061470032, 0.8762175440788269, 0.9499263167381287, 0.9004795551300049, 0.35119199752807617, 1.6665188074111938, 0.9121293425559998, 0.8840751051902771, 1.0586743354797363]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.147197961807251, 0.6371918320655823, 0.9751185774803162, 1.033107876777649, 0.8898425102233887, 0.7985101342201233, -0.22388915717601776, 1.7029684782028198, -0.08971470594406128, 2.0930426120758057, -0.3263716995716095, 1.1393691301345825, 1.331795573234558, 1.2126319408416748, 1.1422646045684814, 0.11409146338701248]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.063439130783081, 1.768434762954712, 1.9082542657852173, 2.2468199729919434, 2.1528892517089844, 1.8663839101791382, 1.7759037017822266, 2.0019114017486572, 2.0417582988739014, 2.0600287914276123, 2.3806796073913574, 1.6390732526779175, 1.4514554738998413, 1.714560627937317, 1.6166601181030273, 2.3128597736358643]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6269991397857666, 1.2073558568954468, 1.733275294303894, 2.086608409881592, 1.494218349456787, 1.698136568069458, 1.6504020690917969, 2.15470552444458, 2.7239575386047363, 1.6177892684936523, 1.6255159378051758, 1.5024597644805908, 1.697106957435608, 1.7374982833862305, 1.7821903228759766, 1.6546940803527832]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.2184722423553467, 1.9975080490112305, 1.7839924097061157, 1.9238464832305908, 2.090437173843384, 1.8122797012329102, 2.026580572128296, 2.0113768577575684, 2.1371757984161377, 1.7051849365234375, 2.146407127380371, 1.933316946029663, 2.7776389122009277, 2.134793519973755, 2.0759949684143066, 1.9661705493927002]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.7945188283920288, 1.282731294631958, 1.2842817306518555, 0.8117548823356628, 0.414520800113678, 1.6136260032653809, 1.2569080591201782, 1.3794293403625488, 0.8236179351806641, 1.1817559003829956, 1.148103952407837, 1.053367018699646, 1.1892396211624146, 1.3109854459762573, 1.3032863140106201, 1.052901268005371]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.566707134246826, 2.6916913986206055, 2.6590192317962646, 2.6687912940979004, 2.6004862785339355, 2.7853448390960693, 2.8447105884552, 2.70454740524292, 2.615657091140747, 2.8356494903564453, 2.8088018894195557, 2.7623448371887207, 2.5860562324523926, 2.7261178493499756, 2.887813091278076, 2.79407000541687]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.8641618490219116, 1.5647393465042114, 1.8472504615783691, 1.499970555305481, 1.8834704160690308, 1.7545500993728638, 1.7736976146697998, 1.6047537326812744, 1.7886806726455688, 1.9721044301986694, 1.6112027168273926, 1.8807343244552612, 1.7814594507217407, 1.7263834476470947, 1.7507702112197876, 2.0574874877929688]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.415038585662842, 2.0217247009277344, 1.7879552841186523, 1.8347241878509521, 1.9519001245498657, 1.8170616626739502, 1.8471006155014038, 1.9530328512191772, 1.8339073657989502, 1.7982594966888428, 1.9085495471954346, 1.839316725730896, 1.8695327043533325, 1.1577211618423462, 1.8380038738250732, 2.0062549114227295]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.8443517684936523, 3.028899908065796, 2.7508187294006348, 2.6917738914489746, 3.0521440505981445, 2.87211275100708, 2.9516077041625977, 2.7785277366638184, 2.871952533721924, 2.9911279678344727, 2.9635539054870605, 2.907341241836548, 2.7543654441833496, 3.0129306316375732, 2.937333345413208, 2.966641664505005]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.0202090740203857, 1.9926412105560303, 1.9441393613815308, 2.215148448944092, 1.6707996129989624, 2.0057249069213867, 2.010551691055298, 2.1575732231140137, 2.0774006843566895, 2.007930040359497, 2.0803170204162598, 2.025534152984619, 2.0905895233154297, 2.0729072093963623, 2.0054941177368164, 2.2310240268707275]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.1751577854156494, 2.0322532653808594, 2.041135549545288, 2.2130463123321533, 2.036189317703247, 2.108225107192993, 2.116015911102295, 2.055285692214966, 2.001254081726074, 2.2491304874420166, 2.454925537109375, 2.0766661167144775, 2.1264405250549316, 2.2603952884674072, 2.0537495613098145, 2.128937005996704]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.057992458343506, 1.9643956422805786, 2.0115294456481934, 2.1041316986083984, 1.9368627071380615, 2.0405783653259277, 2.0882208347320557, 2.085374116897583, 2.0492820739746094, 2.0893421173095703, 1.7090970277786255, 1.9484341144561768, 2.163581609725952, 2.095928430557251, 1.9771286249160767, 2.015589952468872]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.3368959426879883, 3.40777325630188, 3.664234161376953, 3.4165639877319336, 3.4905645847320557, 3.3330230712890625, 3.485349416732788, 3.505326271057129, 3.309764862060547, 3.43169903755188, 3.3938517570495605, 3.1704776287078857, 3.412116765975952, 3.2128829956054688, 3.404163122177124, 3.3235597610473633]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [6.980104446411133, 7.528354167938232, 7.155953884124756, 7.007148265838623, 6.897312164306641, 6.726686954498291, 7.261775970458984, 7.167453765869141, 7.235137939453125, 6.949312686920166, 7.035378932952881, 6.880251407623291, 7.095566749572754, 7.151648998260498, 7.4128217697143555, 7.103125095367432]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.432315826416016, 4.529743671417236, 4.355218887329102, 4.084096431732178, 4.672880172729492, 4.510452747344971, 4.215086936950684, 4.416855812072754, 4.498030662536621, 4.436810493469238, 4.373666286468506, 3.8657214641571045, 4.244724750518799, 4.641776084899902, 4.437121391296387, 4.260643005371094]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.9403085708618164, 2.7933900356292725, 2.8053441047668457, 2.7086033821105957, 2.754755735397339, 2.8377513885498047, 2.80853533744812, 2.6734986305236816, 2.7950735092163086, 2.7716829776763916, 3.1049015522003174, 2.2539985179901123, 2.770791530609131, 2.776958465576172, 3.03658127784729, 2.8329365253448486]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09073841571807861, 0.10947201400995255, 0.10663081705570221, -0.23216687142848969, -0.20863819122314453, -0.13887034356594086, 0.12390001863241196, -0.17968185245990753, 0.07172568142414093, 0.09536856412887573, 0.1040305495262146, 0.0885588601231575, 0.09128281474113464, 0.10133453458547592, -1.0703387260437012, 0.11954622715711594]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07384704053401947, 0.05144122242927551, 0.0337655283510685, 0.03704262897372246, 0.07766195386648178, 0.017018847167491913, 0.04593735560774803, 0.06057151034474373, 0.008696179836988449, 0.05082417279481888, -0.18217569589614868, 0.05450674518942833, 0.0037835463881492615, -0.04210854321718216, 0.005196727812290192, 0.04305547848343849]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08311477303504944, 0.031244449317455292, 0.0833883211016655, 0.05936012044548988, 0.09083520621061325, 0.11292990297079086, 0.06636171042919159, -0.11596223711967468, 0.06845631450414658, 0.08315324783325195, -0.007184871472418308, 0.09046081453561783, -0.18073076009750366, 0.0166676864027977, -0.05207758769392967, 0.10096466541290283]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13125357031822205, 0.13623382151126862, 0.07574251294136047, 0.10678602010011673, 0.09063988924026489, 0.08170118927955627, -0.011971047148108482, 0.14044363796710968, 0.13097280263900757, -0.51374351978302, -0.06380563974380493, 0.073496013879776, 0.10862597078084946, -0.26023080945014954, -0.0700974315404892, -0.13073013722896576]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.028031131252646446, 0.07227769494056702, 0.07639745622873306, 0.057479891926050186, 0.05159847065806389, -0.07300630211830139, 0.011487916111946106, -0.01627623662352562, -0.016590656712651253, 0.04057483747601509, -0.20735779404640198, 0.11453939974308014, -0.05503306910395622, -0.2305479347705841, 0.09331457316875458, 0.007166581694036722]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.08237271755933762, 0.11882367730140686, 0.019495122134685516, 0.07328633219003677, -0.0009379792609252036, -0.04140407592058182, 0.035380229353904724, 0.08139388263225555, 0.08913486450910568, -0.09852331131696701, -0.10544785857200623, 0.14200930297374725, -0.17843197286128998, 0.1359277069568634, 0.14326784014701843, 0.07364136725664139]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.10489899665117264, -0.048660758882761, 0.0748533084988594, 0.28045719861984253, 0.1719893366098404, 0.06554628908634186, -0.1055002510547638, -0.06311105191707611, 0.08989979326725006, 0.1378186196088791, -0.5295963883399963, 0.21722102165222168, 0.32844123244285583, -0.22518883645534515, 0.11698831617832184, 0.19706319272518158]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.17196987569332123, -0.04801159352064133, 0.09389787167310715, 0.021077902987599373, -1.0345587730407715, -0.2201322466135025, -0.17434731125831604, -0.39326539635658264, -0.10572916269302368, -0.17827844619750977, -0.17276951670646667, -0.04205012321472168, 0.16922993957996368, -0.06564164161682129, -0.44777488708496094, 0.023234060034155846]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.14539259672164917, 0.2396063208580017, -0.06665206700563431, 0.453442245721817, 0.35332760214805603, 0.3191026449203491, 0.1940108686685562, 0.25002968311309814, -0.04376164823770523, 0.21236315369606018, 0.2584143579006195, 0.0699259340763092, 0.4481099545955658, -0.12612710893154144, -0.0449492521584034, 0.1957685649394989]
Layer: gate_8 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.37345069646835327, 0.31074485182762146, 0.07383084297180176, 0.6770923137664795, 0.3111251890659332, 0.4100426137447357, 0.6618326902389526, 0.7235360741615295, 0.2766239643096924, 0.11560951173305511, 0.4325924515724182, 0.5362114310264587, 0.7210502624511719, 0.4617123603820801, 0.8503983020782471, 0.6703359484672546]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7270330786705017, 0.7777782082557678, 0.34290874004364014, 0.61170494556427, 0.6241404414176941, -0.07780566811561584, 0.38508379459381104, 0.700840413570404, 0.17759738862514496, -0.09591946750879288, 0.4057338237762451, 0.5691351890563965, 0.3177109956741333, 0.1852668970823288, 0.2221459001302719, 0.4253532588481903]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.44217193126678467, 0.6576417088508606, 0.7678459286689758, 0.6643211841583252, 0.7714755535125732, 0.5748559236526489, 0.7080234289169312, 0.5666167736053467, 0.5986453294754028, 1.0390956401824951, 0.6479933857917786, 0.5716255903244019, 0.4322090148925781, 0.3503628969192505, -0.0312759093940258, 0.723979115486145]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.3230021297931671, 1.1087900400161743, 0.8582686185836792, 0.610465943813324, 0.460159569978714, 0.6512330174446106, 0.5618694424629211, 0.653662383556366, 0.6484635472297668, 0.7753493189811707, 1.7259984016418457, 0.7874807715415955, 0.9911850690841675, 0.8448923826217651, 0.7463824152946472, 0.8130514621734619]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.6253553628921509, 1.4203205108642578, 1.664543628692627, 0.7899729609489441, 1.2030986547470093, 0.8473986387252808, 1.3083206415176392, 1.2857904434204102, 1.2708497047424316, 1.3579944372177124, 1.275736927986145, 1.6982159614562988, 0.8316795229911804, 1.731454610824585, 1.397930383682251, 1.2743111848831177]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8691710233688354, 1.06766676902771, 1.568419337272644, 1.5395640134811401, 1.2453203201293945, 1.262595534324646, 0.15991388261318207, 0.5441020727157593, 0.9233847856521606, 0.5975801944732666, -0.013979428447782993, 0.807738721370697, 1.1996990442276, 1.2064149379730225, 1.498474359512329, 0.5604158639907837]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.8071187734603882, 1.5185120105743408, 1.38075852394104, 1.1029443740844727, 1.2500041723251343, 1.4127297401428223, 1.457942008972168, 0.8714430928230286, 0.9238662719726562, 1.1278653144836426, 1.0088000297546387, 0.4084213078022003, 1.820902943611145, 1.0124726295471191, 1.0637362003326416, 0.9994380474090576]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.2536976337432861, 0.7934244871139526, 1.2687021493911743, 1.2774378061294556, 0.9282566905021667, 0.8355067372322083, -0.3633734881877899, 2.1352858543395996, -0.2753215730190277, 2.0792551040649414, -0.27823251485824585, 1.247741937637329, 1.500894546508789, 1.4664766788482666, 1.3489378690719604, -0.017346931621432304]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  30%|█████████████████████████████████████████▊                                                                                               | 61/200 [01:27<02:46,  1.20s/it]Layer: gate_17 - Captured router_logits: [1.1494927406311035, 1.9140573740005493, 1.8767645359039307, 2.368974208831787, 2.1751744747161865, 1.8696343898773193, 1.7663568258285522, 2.0939199924468994, 1.84347665309906, 2.224029064178467, 2.0812556743621826, 1.6377791166305542, 1.719035029411316, 1.8430562019348145, 1.8335492610931396, 2.4555928707122803]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.5546120405197144, 1.2113418579101562, 1.7669707536697388, 1.9901779890060425, 1.5504393577575684, 1.8407723903656006, 1.426790714263916, 2.116377592086792, 2.3395159244537354, 1.599005937576294, 1.6483025550842285, 1.5532360076904297, 1.6106553077697754, 1.9595603942871094, 1.7571730613708496, 1.6149502992630005]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.249739170074463, 2.0191731452941895, 1.6242146492004395, 1.8379836082458496, 2.041936159133911, 1.8358664512634277, 1.96903395652771, 1.9992942810058594, 1.9326945543289185, 1.778301477432251, 2.066776990890503, 1.8538284301757812, 2.9819815158843994, 2.1155407428741455, 2.056819200515747, 1.8900580406188965]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.5686555504798889, 1.3304152488708496, 1.3220782279968262, 0.804887056350708, 0.6367242932319641, 1.4751853942871094, 1.3091667890548706, 1.1426923274993896, 0.9240939021110535, 1.0657591819763184, 1.2926827669143677, 0.9570597410202026, 1.0753147602081299, 1.3369755744934082, 1.393284559249878, 1.03964364528656]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.6141562461853027, 2.668544292449951, 2.619414806365967, 2.762688159942627, 2.644819498062134, 2.8218345642089844, 2.87131404876709, 2.7690320014953613, 2.5737082958221436, 2.8346595764160156, 2.832831621170044, 2.6898751258850098, 2.6668052673339844, 2.6638457775115967, 3.0600688457489014, 2.8951239585876465]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.8893814086914062, 1.641012191772461, 1.7819790840148926, 1.7408101558685303, 1.9252781867980957, 1.8486065864562988, 1.9059804677963257, 1.6469719409942627, 1.9142496585845947, 2.1558732986450195, 1.5598375797271729, 1.8194730281829834, 1.7786593437194824, 1.7939817905426025, 1.8551610708236694, 2.121148109436035]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5858359336853027, 2.15478515625, 2.079164505004883, 1.962698221206665, 2.1502065658569336, 2.1111745834350586, 1.987998366355896, 2.1334095001220703, 2.0566301345825195, 1.8590068817138672, 2.187992572784424, 2.029491901397705, 2.1513445377349854, 1.4877161979675293, 2.246084451675415, 2.1177902221679688]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.7860379219055176, 3.0736207962036133, 2.7946817874908447, 2.7564454078674316, 2.971950054168701, 2.993041753768921, 2.9498813152313232, 2.8184750080108643, 2.982999324798584, 2.974567413330078, 2.880857467651367, 2.9279823303222656, 2.7875523567199707, 3.09689998626709, 2.9943456649780273, 3.032931089401245]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_25 - Captured router_logits: [2.270132541656494, 2.156785011291504, 2.089683771133423, 2.289386510848999, 1.8616949319839478, 2.1126270294189453, 2.120689868927002, 2.142479658126831, 2.1722772121429443, 2.1670777797698975, 2.1861352920532227, 2.192345380783081, 2.1495234966278076, 2.237529754638672, 2.001302480697632, 2.388840913772583]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.2471261024475098, 2.0794825553894043, 2.0584335327148438, 2.306757926940918, 2.0792245864868164, 2.146658182144165, 2.1419029235839844, 2.078892469406128, 2.0733988285064697, 2.373577117919922, 2.4242095947265625, 2.16133451461792, 2.196722984313965, 2.296891689300537, 2.046076774597168, 2.194301128387451]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.140404462814331, 2.0246081352233887, 2.073819637298584, 2.185441255569458, 2.0583128929138184, 2.099437952041626, 2.1353235244750977, 2.1929731369018555, 2.1206374168395996, 2.2074203491210938, 1.811431884765625, 2.0784499645233154, 2.239081382751465, 2.0893607139587402, 2.064141273498535, 2.083195447921753]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.5798966884613037, 3.5813026428222656, 3.8730571269989014, 3.6563732624053955, 3.809908866882324, 3.612962484359741, 3.573312997817993, 3.7467498779296875, 3.527462959289551, 3.6931796073913574, 3.595309019088745, 3.393501043319702, 3.7007150650024414, 3.419201135635376, 3.689488410949707, 3.562488555908203]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.65589714050293, 8.114737510681152, 7.751420974731445, 7.675728797912598, 7.5112128257751465, 7.286714553833008, 8.026366233825684, 7.929262638092041, 7.966043472290039, 7.653833866119385, 7.67115592956543, 7.536954879760742, 7.776638507843018, 7.842978477478027, 8.091020584106445, 7.70621919631958]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.833890438079834, 5.040155410766602, 4.873248100280762, 4.4723968505859375, 5.141133785247803, 4.939092636108398, 4.737996578216553, 4.798447608947754, 4.937870025634766, 4.910835266113281, 4.85139274597168, 4.371396541595459, 4.770733833312988, 5.160639762878418, 4.983743667602539, 4.692258834838867]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.467459201812744, 3.241236448287964, 3.282139778137207, 3.1671535968780518, 3.25234055519104, 3.342411518096924, 3.27535080909729, 3.171924591064453, 3.205972671508789, 3.3006863594055176, 3.4317376613616943, 2.6187026500701904, 3.2191739082336426, 3.238194465637207, 3.472712278366089, 3.409672737121582]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.07000678032636642, 0.0839604064822197, 0.0833052545785904, -0.1948465257883072, -0.18515917658805847, -0.12725681066513062, 0.10291831940412521, -0.10097865760326385, 0.08265645056962967, 0.07037802785634995, 0.07885244488716125, 0.0680527314543724, 0.08057709783315659, 0.09130752086639404, -0.9566117525100708, 0.09695526957511902]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.05475232005119324, 0.05254868417978287, 0.03200054168701172, 0.036204744130373, 0.07057534158229828, 0.01368018053472042, 0.04311437904834747, 0.07348296046257019, 0.018690338358283043, 0.0472538061439991, -0.15537883341312408, 0.0372626967728138, 0.008505604229867458, -0.016947327181696892, -0.0019238872919231653, 0.03397367522120476]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05958983674645424, 0.03551558032631874, 0.08569756150245667, 0.06543806195259094, 0.11663594841957092, 0.09629465639591217, 0.05823414772748947, -0.09999853372573853, 0.06257372349500656, 0.06550231575965881, -0.0034204360563308, 0.10407289862632751, -0.14636091887950897, 0.02797289378941059, -0.03473415970802307, 0.09460347145795822]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11612267047166824, 0.126216858625412, 0.08162600547075272, 0.11591044068336487, 0.111836276948452, 0.09777888655662537, -0.010112877003848553, 0.14842194318771362, 0.13545656204223633, -0.501119077205658, -0.02940228208899498, 0.04519472271203995, 0.16696254909038544, -0.2415534108877182, -0.083047054708004, -0.1182895377278328]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.025737587362527847, 0.06982436031103134, 0.08315680921077728, 0.09178664535284042, 0.04317621886730194, -0.081560879945755, 0.02517065405845642, -0.010574000887572765, -0.03931879997253418, 0.03165149688720703, -0.17334145307540894, 0.12070723623037338, -0.06389649957418442, -0.15187737345695496, 0.07528812438249588, -0.013861943036317825]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09656516462564468, 0.14528602361679077, 0.030341871082782745, 0.06150941178202629, -0.042672865092754364, 0.018589645624160767, 0.04889355227351189, 0.041917234659194946, 0.1286490112543106, -0.10654433816671371, -0.10910153388977051, 0.15746869146823883, -0.15113741159439087, 0.1275850385427475, 0.15167547762393951, 0.11089957505464554]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.12327905744314194, -0.01925789564847946, 0.02897738479077816, 0.2952759265899658, 0.1516684740781784, 0.056806743144989014, -0.05390206724405289, -0.07595130801200867, 0.09106513857841492, 0.15135718882083893, -0.5123432278633118, 0.244785338640213, 0.3508858382701874, -0.23337633907794952, 0.1876685619354248, 0.22454054653644562]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.14263810217380524, -0.01869065873324871, 0.08817258477210999, 0.012825765646994114, -1.0061836242675781, -0.1963941752910614, -0.16867925226688385, -0.31320133805274963, -0.07704116404056549, -0.15365798771381378, -0.21008014678955078, -0.043756671249866486, 0.12348067760467529, -0.05291275680065155, -0.5122014880180359, -0.008623970672488213]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.17302250862121582, 0.26956474781036377, -0.04668045416474342, 0.43446841835975647, 0.3540551960468292, 0.3392312526702881, 0.20281513035297394, 0.24678543210029602, 0.016145609319210052, 0.300126850605011, 0.31418928503990173, 0.08709043264389038, 0.4640404284000397, -0.055093348026275635, -0.014601836912333965, 0.20379312336444855]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.37696704268455505, 0.4155023396015167, 0.1547732800245285, 0.8144327998161316, 0.3259046673774719, 0.41554439067840576, 0.6810855865478516, 0.7834975123405457, 0.2390647977590561, 0.1502740979194641, 0.42986488342285156, 0.5702745914459229, 0.7456610798835754, 0.5033133625984192, 0.9404201507568359, 0.6965763568878174]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7789705991744995, 0.8080557584762573, 0.29435989260673523, 0.6089023351669312, 0.6661046147346497, -0.0017147755715996027, 0.38247963786125183, 0.7319660782814026, 0.06527289003133774, -0.06122814118862152, 0.4374978244304657, 0.5642009377479553, 0.25650718808174133, 0.1774246245622635, 0.20921076834201813, 0.3856887221336365]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.3573094606399536, 0.5981829762458801, 0.6162830591201782, 0.5881557464599609, 0.6849846839904785, 0.3233588635921478, 0.6651419401168823, 0.43388816714286804, 0.5115506649017334, 0.9634881615638733, 0.47314146161079407, 0.35906219482421875, 0.21329112350940704, 0.15869253873825073, -0.26670151948928833, 0.550989031791687]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.19083555042743683, 0.8907480239868164, 0.7975736260414124, 0.5574756264686584, 0.2591008245944977, 0.5245811939239502, 0.6021892428398132, 0.6434199810028076, 0.4950517416000366, 0.5601990818977356, 1.6055903434753418, 0.6831936836242676, 0.8972642421722412, 0.7003458738327026, 0.575325071811676, 0.7147390842437744]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.2172572612762451, 1.2839601039886475, 1.392802119255066, 0.5621646046638489, 1.0395902395248413, 0.37450510263442993, 1.0922271013259888, 1.3211675882339478, 1.0912659168243408, 1.139817476272583, 1.1381741762161255, 1.460788369178772, 0.4260457456111908, 1.4348628520965576, 1.2308834791183472, 0.946936309337616]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7006161212921143, 0.8601797819137573, 1.3725730180740356, 1.3644704818725586, 1.0705236196517944, 1.05020272731781, 0.0607660748064518, 0.710061252117157, 0.7461135983467102, 0.5349204540252686, -0.33305203914642334, 0.5910995602607727, 1.0474828481674194, 1.0819863080978394, 1.3270246982574463, 0.40754640102386475]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.5731178522109985, 1.3743566274642944, 1.1668442487716675, 1.0006451606750488, 1.015533208847046, 1.1798254251480103, 1.3910585641860962, 0.7539805173873901, 0.8154628276824951, 0.9808864593505859, 0.8122813105583191, 0.2095016986131668, 1.664904236793518, 0.871311604976654, 0.8394009470939636, 0.9374547600746155]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.0525569915771484, 0.7276061177253723, 1.1046150922775269, 1.1312925815582275, 0.9328263401985168, 0.7338753938674927, -0.28702035546302795, 1.9301332235336304, -0.4090462327003479, 2.1607046127319336, -0.5160045623779297, 1.0804800987243652, 1.305372714996338, 1.328519344329834, 1.2534525394439697, -0.06919587403535843]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.0163077116012573, 1.7248510122299194, 1.7850260734558105, 2.111109495162964, 2.0422205924987793, 1.734097957611084, 1.6791856288909912, 1.926777958869934, 1.8666250705718994, 2.11995267868042, 2.1092987060546875, 1.5089197158813477, 1.6125816106796265, 1.7394384145736694, 1.5778247117996216, 2.133178949356079]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.4101487398147583, 1.0918389558792114, 1.6751466989517212, 1.828142762184143, 1.3909612894058228, 1.6833622455596924, 1.4544050693511963, 1.9948616027832031, 2.3392837047576904, 1.5034303665161133, 1.4406509399414062, 1.3617615699768066, 1.5221060514450073, 1.6362314224243164, 1.6629682779312134, 1.5396946668624878]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.97574782371521, 1.7606931924819946, 1.4115283489227295, 1.6027300357818604, 1.7652937173843384, 1.5401320457458496, 1.7028403282165527, 1.7112388610839844, 1.803297519683838, 1.49565589427948, 1.7917554378509521, 1.5410016775131226, 2.6627068519592285, 1.8550877571105957, 1.752152919769287, 1.6146472692489624]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6045518517494202, 1.2585749626159668, 1.220757007598877, 0.7995172739028931, 0.5691401958465576, 1.3585505485534668, 1.2237958908081055, 1.218000054359436, 0.7569655179977417, 1.072700023651123, 1.2297494411468506, 0.99625563621521, 1.0580612421035767, 1.263256311416626, 1.285028338432312, 0.9456642866134644]
Running loglikelihood requests:  32%|████████████████████████████████████████████▌                                                                                            | 65/200 [01:31<02:39,  1.18s/it]Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.5692219734191895, 2.6404690742492676, 2.5822877883911133, 2.701707124710083, 2.5731265544891357, 2.768343687057495, 2.8018383979797363, 2.810798168182373, 2.5235824584960938, 2.7900211811065674, 2.823091983795166, 2.801680564880371, 2.608314275741577, 2.6244924068450928, 2.9695825576782227, 2.8069067001342773]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.7960352897644043, 1.4939560890197754, 1.7801603078842163, 1.6210435628890991, 1.8329012393951416, 1.773956537246704, 1.7864265441894531, 1.5315266847610474, 1.8517565727233887, 2.0281593799591064, 1.5431846380233765, 1.8418314456939697, 1.7089096307754517, 1.715108036994934, 1.7551161050796509, 2.014632225036621]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.428544282913208, 2.121232032775879, 1.9118200540542603, 1.8460030555725098, 2.032888174057007, 1.9718939065933228, 1.8658900260925293, 2.025193452835083, 1.9529081583023071, 1.783251404762268, 2.0621719360351562, 1.9460309743881226, 1.9817423820495605, 1.3088958263397217, 2.053485155105591, 1.9885504245758057]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.701140880584717, 2.9314751625061035, 2.708193778991699, 2.6011548042297363, 2.9158644676208496, 2.803562641143799, 2.837437629699707, 2.7002670764923096, 2.7810287475585938, 2.842576265335083, 2.856754779815674, 2.784109354019165, 2.7016713619232178, 2.933684825897217, 2.853264331817627, 2.8889424800872803]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_25 - Captured router_logits: [2.1750547885894775, 2.098849058151245, 2.048664093017578, 2.2102723121643066, 1.779726266860962, 2.102017402648926, 2.062096357345581, 2.165188789367676, 2.1044869422912598, 2.0691447257995605, 2.1298668384552, 2.083498954772949, 2.0958199501037598, 2.159151315689087, 2.0863795280456543, 2.342135190963745]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.156916618347168, 2.0408968925476074, 2.032986879348755, 2.220163583755493, 1.9975463151931763, 2.0681989192962646, 2.0900285243988037, 2.0285215377807617, 2.018564462661743, 2.2701449394226074, 2.4485487937927246, 2.093431234359741, 2.106053113937378, 2.179964303970337, 1.9904100894927979, 2.127351999282837]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.05181884765625, 1.971250057220459, 1.992769718170166, 2.1038522720336914, 1.9580236673355103, 2.005446434020996, 2.092684268951416, 2.1323914527893066, 2.032362222671509, 2.1176788806915283, 1.6811509132385254, 2.000993490219116, 2.1400904655456543, 2.1174240112304688, 2.0193357467651367, 2.020932197570801]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.433023691177368, 3.4315202236175537, 3.7106077671051025, 3.488185405731201, 3.635100841522217, 3.476102113723755, 3.5486068725585938, 3.604334831237793, 3.375086545944214, 3.500087022781372, 3.448479175567627, 3.2188825607299805, 3.494194746017456, 3.2696807384490967, 3.5079619884490967, 3.4090757369995117]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.376771926879883, 7.875367164611816, 7.507726192474365, 7.351563453674316, 7.246774673461914, 7.035641670227051, 7.720927715301514, 7.619383811950684, 7.64559268951416, 7.364744186401367, 7.371888160705566, 7.2341156005859375, 7.470114707946777, 7.54801607131958, 7.79862642288208, 7.42480993270874]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.661893367767334, 4.858187198638916, 4.670656204223633, 4.299189567565918, 4.9281792640686035, 4.749940395355225, 4.514969348907471, 4.663658142089844, 4.858620643615723, 4.751031875610352, 4.690019607543945, 4.134854316711426, 4.543527603149414, 4.970657825469971, 4.82441520690918, 4.513084888458252]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2872860431671143, 3.0658819675445557, 3.175590753555298, 3.0166261196136475, 3.1385657787323, 3.17592716217041, 3.1314280033111572, 2.983635425567627, 3.0313222408294678, 3.1064326763153076, 3.385592460632324, 2.4348552227020264, 3.0974321365356445, 3.0791516304016113, 3.4098281860351562, 3.2265024185180664]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.0970127061009407, 0.11579760909080505, 0.11004596203565598, -0.23164629936218262, -0.21459004282951355, -0.13625270128250122, 0.12787753343582153, -0.19815847277641296, 0.0794437900185585, 0.10046364367008209, 0.10667145997285843, 0.09118741005659103, 0.0975390076637268, 0.10885526984930038, -1.0986294746398926, 0.12452396005392075]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07801792025566101, 0.05412745103240013, 0.03539033234119415, 0.03998902067542076, 0.08002576977014542, 0.019313804805278778, 0.043587420135736465, 0.056528348475694656, 0.005588354077190161, 0.05392257869243622, -0.17541664838790894, 0.055713552981615067, 0.003843559417873621, -0.0332401879131794, 0.004985126666724682, 0.04060670733451843]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08085920661687851, 0.0389915369451046, 0.08494328707456589, 0.05652856081724167, 0.09102004766464233, 0.11281447112560272, 0.06024637818336487, -0.11794944107532501, 0.06855346262454987, 0.0710970014333725, -0.0005881287506781518, 0.08451922237873077, -0.1728758066892624, 0.027222588658332825, -0.04627850279211998, 0.10227958112955093]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.133889839053154, 0.12895794212818146, 0.08112228661775589, 0.10653872787952423, 0.09567166119813919, 0.0839984193444252, 0.01786566711962223, 0.1386619508266449, 0.13195227086544037, -0.5187498927116394, -0.06442452222108841, 0.07264305651187897, 0.10127884149551392, -0.2392013967037201, -0.08876648545265198, -0.13749051094055176]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.03862152621150017, 0.07301180809736252, 0.07857939600944519, 0.05328647792339325, 0.05422446131706238, -0.08390278369188309, 0.02461087517440319, -0.013397193513810635, -0.004760111682116985, 0.0440056249499321, -0.19771015644073486, 0.11157984286546707, -0.07182768732309341, -0.22547954320907593, 0.08371147513389587, 0.023783212527632713]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.08920170366764069, 0.11986939609050751, 0.02928529866039753, 0.06776740401983261, 0.016446705907583237, -0.030169934034347534, 0.033314358443021774, 0.08858341723680496, 0.08054974675178528, -0.1159447729587555, -0.12615975737571716, 0.14624206721782684, -0.1901138871908188, 0.12956038117408752, 0.14989922940731049, 0.06731844693422318]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.10695932060480118, -0.03084632195532322, 0.07911422848701477, 0.2785983979701996, 0.17267896234989166, 0.06383303552865982, -0.11688335239887238, -0.07431618869304657, 0.10211195051670074, 0.1375570297241211, -0.5509465932846069, 0.20645849406719208, 0.32233384251594543, -0.21441465616226196, 0.1037302166223526, 0.20201392471790314]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.1487424522638321, -0.03175963833928108, 0.11455587297677994, 0.02178376540541649, -1.0736727714538574, -0.19962505996227264, -0.16765762865543365, -0.3653542399406433, -0.11986976116895676, -0.16934306919574738, -0.17353449761867523, -0.03220110386610031, 0.16945064067840576, -0.0737355574965477, -0.458087682723999, 0.011305180378258228]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.1405452936887741, 0.23736386001110077, -0.07406327873468399, 0.4445571303367615, 0.3388713598251343, 0.29551342129707336, 0.18766453862190247, 0.22134289145469666, -0.04058317095041275, 0.21632488071918488, 0.25038859248161316, 0.036643289029598236, 0.4328534007072449, -0.1376744508743286, -0.06886459141969681, 0.20087510347366333]
Layer: gate_8 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3893921375274658, 0.3287907838821411, 0.07882602512836456, 0.6918807029724121, 0.3233189582824707, 0.44132059812545776, 0.6626601219177246, 0.7347356081008911, 0.3268895149230957, 0.10311123728752136, 0.39652949571609497, 0.5252729654312134, 0.6662424802780151, 0.47154030203819275, 0.8503473997116089, 0.6487345695495605]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7305449843406677, 0.7694525718688965, 0.36250782012939453, 0.6107029318809509, 0.6864439845085144, -0.036268919706344604, 0.3848823010921478, 0.7081530690193176, 0.185893714427948, -0.10597619414329529, 0.41164007782936096, 0.5729435086250305, 0.2606436014175415, 0.19496896862983704, 0.2617386281490326, 0.4086519181728363]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.43956950306892395, 0.6418797373771667, 0.7753989696502686, 0.6673246026039124, 0.7815772294998169, 0.5572612285614014, 0.684617280960083, 0.5275855660438538, 0.59532630443573, 1.0315827131271362, 0.6611342430114746, 0.5705510377883911, 0.39668652415275574, 0.31240153312683105, -0.03493563085794449, 0.7184483408927917]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.3248347342014313, 1.101603388786316, 0.8537457585334778, 0.5917149782180786, 0.4213525652885437, 0.6653515100479126, 0.5347795486450195, 0.651772677898407, 0.66897052526474, 0.7263275980949402, 1.7219525575637817, 0.7693408727645874, 0.9640427231788635, 0.8255591988563538, 0.7075422406196594, 0.7868595719337463]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.5965855121612549, 1.3853704929351807, 1.64710533618927, 0.8200305700302124, 1.1922097206115723, 0.8062388300895691, 1.294701099395752, 1.288643717765808, 1.29257333278656, 1.3582640886306763, 1.2675001621246338, 1.6977670192718506, 0.7729353904724121, 1.7054308652877808, 1.4067941904067993, 1.2376600503921509]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [0.893486738204956, 1.0720479488372803, 1.5797348022460938, 1.5548336505889893, 1.2785407304763794, 1.3528822660446167, 0.13762083649635315, 0.6207304000854492, 0.964214563369751, 0.6053925156593323, -0.02086891047656536, 0.8111657500267029, 1.2111247777938843, 1.2283912897109985, 1.4802203178405762, 0.5679818391799927]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.8171305656433105, 1.5129032135009766, 1.395795226097107, 1.0978889465332031, 1.2632410526275635, 1.3923046588897705, 1.4927661418914795, 0.886677086353302, 0.9588303565979004, 1.126983404159546, 1.0238386392593384, 0.40804240107536316, 1.81548011302948, 1.012007236480713, 1.049389123916626, 1.1119054555892944]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.217370629310608, 0.7778154611587524, 1.2486467361450195, 1.2334725856781006, 0.9033536911010742, 0.8290064334869385, -0.30305394530296326, 2.0834550857543945, -0.27568912506103516, 2.1125032901763916, -0.30798235535621643, 1.2742226123809814, 1.4798156023025513, 1.4719082117080688, 1.335593581199646, -0.03777129948139191]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.1682239770889282, 1.89341139793396, 1.8950779438018799, 2.3433306217193604, 2.1701245307922363, 1.8709841966629028, 1.7790073156356812, 2.0906994342803955, 1.8573192358016968, 2.209819793701172, 2.107151508331299, 1.6581131219863892, 1.707313060760498, 1.8222095966339111, 1.8705388307571411, 2.3968825340270996]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.538563847541809, 1.2199548482894897, 1.7910698652267456, 1.966058373451233, 1.5802935361862183, 1.8711670637130737, 1.4968516826629639, 2.144564390182495, 2.30998158454895, 1.600948452949524, 1.6709136962890625, 1.6141349077224731, 1.6545518636703491, 1.9252173900604248, 1.784090518951416, 1.6462032794952393]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.2656025886535645, 2.0385944843292236, 1.6797291040420532, 1.8739924430847168, 2.046754837036133, 1.900814414024353, 2.0155184268951416, 2.0308914184570312, 2.0046801567077637, 1.779525637626648, 2.076341152191162, 1.851547360420227, 2.9813778400421143, 2.1405956745147705, 2.060793399810791, 1.9027900695800781]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6127427816390991, 1.3700133562088013, 1.3735953569412231, 0.8998441696166992, 0.7522993087768555, 1.4886023998260498, 1.3574713468551636, 1.31657874584198, 0.976935625076294, 1.1286330223083496, 1.3054885864257812, 0.9961521625518799, 1.1343448162078857, 1.3591338396072388, 1.4582419395446777, 1.0617835521697998]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.6815948486328125, 2.7339649200439453, 2.6703691482543945, 2.8424413204193115, 2.7363269329071045, 2.874577283859253, 2.949042558670044, 2.8548388481140137, 2.6421024799346924, 2.9068338871002197, 2.9039595127105713, 2.7798712253570557, 2.768533706665039, 2.757641553878784, 3.151732921600342, 2.965346336364746]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.964106798171997, 1.6700594425201416, 1.8201196193695068, 1.806127905845642, 1.993498682975769, 1.906378984451294, 1.9800105094909668, 1.6938072443008423, 1.967228651046753, 2.184224843978882, 1.6104121208190918, 1.8748822212219238, 1.849791407585144, 1.8486120700836182, 1.9344216585159302, 2.1787991523742676]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5453288555145264, 2.1869213581085205, 2.1454288959503174, 1.9906576871871948, 2.195155620574951, 2.1434762477874756, 2.0189080238342285, 2.2013280391693115, 2.100116729736328, 1.8745993375778198, 2.239403247833252, 2.057187795639038, 2.2202882766723633, 1.5713444948196411, 2.293276309967041, 2.161449432373047]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.854247808456421, 3.1389901638031006, 2.8416924476623535, 2.851438045501709, 3.0546209812164307, 3.0855700969696045, 3.024901866912842, 2.902663230895996, 3.0732858180999756, 3.0513691902160645, 2.95228910446167, 3.0052123069763184, 2.879465103149414, 3.157879114151001, 3.079479217529297, 3.1147444248199463]
Running loglikelihood requests:  34%|███████████████████████████████████████████████▎                                                                                         | 69/200 [01:36<02:33,  1.17s/it]Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_25 - Captured router_logits: [2.3126866817474365, 2.1843647956848145, 2.1217153072357178, 2.3021533489227295, 1.935034990310669, 2.1488094329833984, 2.15669846534729, 2.137601613998413, 2.2002763748168945, 2.200247049331665, 2.2190184593200684, 2.2117555141448975, 2.185451030731201, 2.2734973430633545, 2.0360095500946045, 2.425976514816284]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.2775721549987793, 2.092830181121826, 2.1024112701416016, 2.3434200286865234, 2.1107707023620605, 2.1631863117218018, 2.1615705490112305, 2.114734411239624, 2.101055145263672, 2.371518850326538, 2.4285919666290283, 2.200303554534912, 2.220369815826416, 2.2514171600341797, 2.0801730155944824, 2.2090985774993896]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.136727809906006, 2.035752296447754, 2.083449363708496, 2.1957743167877197, 2.0753397941589355, 2.1112637519836426, 2.142916202545166, 2.219041347503662, 2.1381654739379883, 2.21053147315979, 1.8304064273834229, 2.0977678298950195, 2.2274956703186035, 2.0921027660369873, 2.0687224864959717, 2.0946080684661865]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.6171109676361084, 3.618278980255127, 3.873021364212036, 3.7075116634368896, 3.8368847370147705, 3.6602094173431396, 3.60782790184021, 3.7936906814575195, 3.5746068954467773, 3.731159210205078, 3.63663911819458, 3.4420628547668457, 3.7306714057922363, 3.4522624015808105, 3.6934173107147217, 3.60524582862854]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.80932092666626, 8.257883071899414, 7.922401428222656, 7.850157260894775, 7.647372722625732, 7.425502300262451, 8.136588096618652, 8.049957275390625, 8.070063591003418, 7.794442176818848, 7.792023181915283, 7.655372142791748, 7.884230613708496, 7.984310150146484, 8.222088813781738, 7.82111930847168]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.902164936065674, 5.113243103027344, 4.947145938873291, 4.53924560546875, 5.20397424697876, 4.991827011108398, 4.7892255783081055, 4.870631694793701, 4.999631881713867, 4.983339786529541, 4.908853054046631, 4.4392781257629395, 4.845750331878662, 5.226942539215088, 5.065403938293457, 4.739950180053711]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.530242681503296, 3.2832963466644287, 3.3477630615234375, 3.2236242294311523, 3.3230884075164795, 3.4191808700561523, 3.321134090423584, 3.219111919403076, 3.2690508365631104, 3.3732974529266357, 3.4678595066070557, 2.6786489486694336, 3.2741408348083496, 3.3161277770996094, 3.5242156982421875, 3.4637138843536377]
Layer: gate_31 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.08806035667657852, 0.10895327478647232, 0.1022200733423233, -0.22922702133655548, -0.20817603170871735, -0.14010876417160034, 0.11848568916320801, -0.1679384559392929, 0.07568570971488953, 0.09238992631435394, 0.0961960107088089, 0.08485445380210876, 0.09054824709892273, 0.10284904390573502, -1.067980408668518, 0.11761408299207687]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07348446547985077, 0.0462082177400589, 0.03213726729154587, 0.03723808005452156, 0.07786136865615845, 0.01973654329776764, 0.04512409493327141, 0.06079249829053879, 0.002564753172919154, 0.04722430929541588, -0.17375047504901886, 0.057029418647289276, 0.004895078483968973, -0.03369012847542763, 0.006415031384676695, 0.04410318285226822]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08530666679143906, 0.036127641797065735, 0.08813267201185226, 0.06412261724472046, 0.0903935432434082, 0.10271266847848892, 0.06531599164009094, -0.11393625289201736, 0.06271949410438538, 0.07285605370998383, 0.0003666239499580115, 0.09152296185493469, -0.17734394967556, 0.013397098518908024, -0.05158224701881409, 0.0990862250328064]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12883147597312927, 0.13441835343837738, 0.07688157260417938, 0.11759644001722336, 0.09040678292512894, 0.0911659374833107, 0.006281916983425617, 0.14462749660015106, 0.12487774342298508, -0.5010185241699219, -0.0702429935336113, 0.06986097246408463, 0.128379687666893, -0.25590139627456665, -0.08377477526664734, -0.1400967240333557]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.03741662576794624, 0.06926137208938599, 0.07424970716238022, 0.07703525573015213, 0.04275752976536751, -0.07050777971744537, 0.013630775734782219, -0.016274891793727875, -0.0014270446263253689, 0.05086586996912956, -0.20471344888210297, 0.11790938675403595, -0.05699783191084862, -0.23303285241127014, 0.08761695772409439, -0.002114789793267846]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09041787683963776, 0.12165609002113342, 0.016678104177117348, 0.07179319113492966, -0.0033733760938048363, -0.03008672036230564, 0.03645458072423935, 0.07726112008094788, 0.09117210656404495, -0.1015743613243103, -0.10107654333114624, 0.1390036940574646, -0.1833488494157791, 0.1318470984697342, 0.14370673894882202, 0.0720110759139061]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.11443362385034561, -0.04633122310042381, 0.04998623579740524, 0.2823253273963928, 0.15398362278938293, 0.06759794056415558, -0.10591716319322586, -0.07811080664396286, 0.10374175012111664, 0.1400560438632965, -0.5557524561882019, 0.21866852045059204, 0.33212727308273315, -0.2287260890007019, 0.12801779806613922, 0.20000243186950684]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.14560584723949432, -0.05010893568396568, 0.0801197811961174, 0.02284378372132778, -1.0657761096954346, -0.20383702218532562, -0.14798538386821747, -0.4001519978046417, -0.09136071801185608, -0.16181308031082153, -0.1886751502752304, -0.046122197061777115, 0.16238656640052795, -0.0663587674498558, -0.47509220242500305, -0.01592874899506569]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.16329611837863922, 0.23778706789016724, -0.08464108407497406, 0.4396616518497467, 0.3569689989089966, 0.3001137375831604, 0.17985181510448456, 0.24371746182441711, -0.051239874213933945, 0.23354588449001312, 0.25728458166122437, 0.07101327925920486, 0.44646817445755005, -0.14083829522132874, -0.07521483302116394, 0.21804502606391907]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.36400794982910156, 0.31751853227615356, 0.08941330760717392, 0.722477912902832, 0.2934534549713135, 0.39368465542793274, 0.6392328143119812, 0.7310690879821777, 0.32784169912338257, 0.10351750999689102, 0.3975941836833954, 0.5321047902107239, 0.6979632377624512, 0.4239765405654907, 0.8579142689704895, 0.6629517674446106]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7339568734169006, 0.7584968209266663, 0.35070255398750305, 0.5936065912246704, 0.6324003338813782, -0.028583580628037453, 0.3743862509727478, 0.7115694284439087, 0.11486680805683136, -0.10374730825424194, 0.40549978613853455, 0.5772346258163452, 0.22245821356773376, 0.16019566357135773, 0.269982248544693, 0.34878313541412354]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.4301588833332062, 0.6547739505767822, 0.7505272626876831, 0.6589998006820679, 0.7574640512466431, 0.461220383644104, 0.6405181884765625, 0.49966809153556824, 0.5860549807548523, 1.0411748886108398, 0.603230357170105, 0.48976314067840576, 0.27191483974456787, 0.23780906200408936, -0.13212983310222626, 0.6175276041030884]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.25305262207984924, 1.0452839136123657, 0.8514505624771118, 0.5996226072311401, 0.3619668185710907, 0.6573197245597839, 0.5938785672187805, 0.6864257454872131, 0.5970219969749451, 0.6927392482757568, 1.6759529113769531, 0.7813878655433655, 0.9268901348114014, 0.8269479274749756, 0.674352765083313, 0.7679862380027771]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.541176676750183, 1.396607518196106, 1.606377124786377, 0.7138930559158325, 1.2059803009033203, 0.7102565765380859, 1.3079801797866821, 1.3850339651107788, 1.289287805557251, 1.3248642683029175, 1.2887016534805298, 1.6236578226089478, 0.6725070476531982, 1.6065760850906372, 1.4411989450454712, 1.1846047639846802]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8322461843490601, 1.049540638923645, 1.5244945287704468, 1.4725264310836792, 1.2587467432022095, 1.1181138753890991, 0.09861783683300018, 0.6448654532432556, 1.0365371704101562, 0.5464929938316345, -0.10588791966438293, 0.7517754435539246, 1.1544864177703857, 1.1952942609786987, 1.451446533203125, 0.5019729137420654]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.7562326192855835, 1.4762380123138428, 1.3464261293411255, 1.1060280799865723, 1.2121896743774414, 1.3626290559768677, 1.465878963470459, 0.8482518792152405, 0.9503456354141235, 1.1216685771942139, 0.957615852355957, 0.3292149305343628, 1.7465770244598389, 0.9883251786231995, 1.02217698097229, 1.0020452737808228]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.113666296005249, 0.6665208339691162, 1.1709126234054565, 1.180432677268982, 0.8623661398887634, 0.7435224652290344, -0.20636986196041107, 1.9767529964447021, -0.3759579062461853, 1.998971939086914, -0.3759138584136963, 1.1201272010803223, 1.4048513174057007, 1.3657907247543335, 1.2483283281326294, -0.0034575327299535275]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.1345044374465942, 1.816562533378601, 1.925003170967102, 2.3687870502471924, 2.1716156005859375, 1.916537880897522, 1.8028829097747803, 2.0990240573883057, 1.9193816184997559, 2.2298316955566406, 2.1212875843048096, 1.6961525678634644, 1.7773115634918213, 1.879157543182373, 1.8294343948364258, 2.3961520195007324]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.5043784379959106, 1.2161942720413208, 1.7291109561920166, 1.913169503211975, 1.5216403007507324, 1.8085390329360962, 1.5362801551818848, 2.117506504058838, 2.2642221450805664, 1.580156922340393, 1.6364706754684448, 1.5555541515350342, 1.6260135173797607, 1.9110170602798462, 1.7048767805099487, 1.6275064945220947]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.2394907474517822, 2.032417058944702, 1.6824958324432373, 1.8461472988128662, 2.020603656768799, 1.8382433652877808, 1.951310396194458, 1.9863649606704712, 2.013270854949951, 1.7600197792053223, 2.0680091381073, 1.846689224243164, 2.916825532913208, 2.108242988586426, 2.026627779006958, 1.8721469640731812]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6652934551239014, 1.370235800743103, 1.340803861618042, 0.8897489905357361, 0.7037419080734253, 1.461065411567688, 1.3196995258331299, 1.328948974609375, 0.9817953109741211, 1.1358857154846191, 1.284872055053711, 0.9920748472213745, 1.1470037698745728, 1.3537266254425049, 1.4131219387054443, 1.012448787689209]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.6685802936553955, 2.727186679840088, 2.6835644245147705, 2.8094866275787354, 2.698922872543335, 2.8799855709075928, 2.9341888427734375, 2.8596110343933105, 2.6053884029388428, 2.895808219909668, 2.908923864364624, 2.870634078979492, 2.7515947818756104, 2.7360005378723145, 3.120840549468994, 2.943284511566162]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.889707326889038, 1.6386340856552124, 1.796020269393921, 1.7076259851455688, 1.921892762184143, 1.8410848379135132, 1.8967009782791138, 1.6263816356658936, 1.896909475326538, 2.146407127380371, 1.5572171211242676, 1.8694401979446411, 1.8011000156402588, 1.8025786876678467, 1.8663849830627441, 2.099883794784546]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5230507850646973, 2.1727893352508545, 2.0732996463775635, 1.951003074645996, 2.1594927310943604, 2.092956304550171, 1.985074758529663, 2.1495115756988525, 2.0556740760803223, 1.8658688068389893, 2.177619695663452, 2.015178918838501, 2.1645941734313965, 1.518843650817871, 2.2379143238067627, 2.1132519245147705]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.813418388366699, 3.0955779552459717, 2.809870719909668, 2.794844388961792, 3.076810359954834, 3.031205654144287, 2.955521821975708, 2.8413069248199463, 2.9988739490509033, 2.9970834255218506, 2.9526708126068115, 2.971388339996338, 2.8333072662353516, 3.105982542037964, 3.0343759059906006, 3.065304756164551]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_25 - Captured router_logits: [2.3097736835479736, 2.2273619174957275, 2.1434762477874756, 2.2988641262054443, 1.9347220659255981, 2.158345937728882, 2.181086778640747, 2.1807823181152344, 2.205662488937378, 2.1953799724578857, 2.2236454486846924, 2.2156593799591064, 2.2061197757720947, 2.271007537841797, 2.10347056388855, 2.445286512374878]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.2729709148406982, 2.0964276790618896, 2.0958633422851562, 2.3215420246124268, 2.1083569526672363, 2.1459438800811768, 2.1646270751953125, 2.1141703128814697, 2.0884807109832764, 2.356776237487793, 2.45934796333313, 2.182744026184082, 2.1939914226531982, 2.2677664756774902, 2.0356485843658447, 2.1968190670013428]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.11337947845459, 2.026054620742798, 2.0611915588378906, 2.184739589691162, 2.048903703689575, 2.0727264881134033, 2.122746467590332, 2.192084312438965, 2.1041646003723145, 2.1957530975341797, 1.788958191871643, 2.071612596511841, 2.199857234954834, 2.1075637340545654, 2.061400890350342, 2.0655016899108887]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Running loglikelihood requests:  36%|██████████████████████████████████████████████████                                                                                       | 73/200 [01:41<02:27,  1.16s/it]Layer: gate_28 - Captured router_logits: [3.5252292156219482, 3.526580572128296, 3.7783637046813965, 3.596498489379883, 3.727881908416748, 3.559569835662842, 3.5673089027404785, 3.6876320838928223, 3.4791200160980225, 3.625286817550659, 3.535179853439331, 3.3391151428222656, 3.612091302871704, 3.3463804721832275, 3.6092891693115234, 3.4930312633514404]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.643740177154541, 8.090386390686035, 7.770670413970947, 7.643840312957764, 7.475189208984375, 7.279298782348633, 7.971584320068359, 7.902929782867432, 7.92554235458374, 7.640486717224121, 7.6426215171813965, 7.486719608306885, 7.730679988861084, 7.805939197540283, 8.041123390197754, 7.658984661102295]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.714834213256836, 4.914658069610596, 4.753126621246338, 4.364976406097412, 4.987592697143555, 4.790107727050781, 4.590091228485107, 4.674396514892578, 4.848514080047607, 4.805609226226807, 4.7267022132873535, 4.268664360046387, 4.65133810043335, 5.032541275024414, 4.866851806640625, 4.580550193786621]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.378450393676758, 3.1477296352386475, 3.2083048820495605, 3.092838764190674, 3.1774234771728516, 3.2482283115386963, 3.1965625286102295, 3.085510730743408, 3.0972793102264404, 3.20636248588562, 3.3575363159179688, 2.540863275527954, 3.1361186504364014, 3.1536128520965576, 3.4140706062316895, 3.3123056888580322]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09707454591989517, 0.11740774661302567, 0.11220917105674744, -0.23531828820705414, -0.20728164911270142, -0.1329323798418045, 0.12981857359409332, -0.20295152068138123, 0.0770815834403038, 0.1031661406159401, 0.10933160036802292, 0.0977000817656517, 0.09853869676589966, 0.11011680960655212, -1.0902314186096191, 0.12581588327884674]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07845395058393478, 0.05222471058368683, 0.03599950298666954, 0.04184266924858093, 0.08038976043462753, 0.018171578645706177, 0.049979016184806824, 0.05617344006896019, 0.004467187449336052, 0.05775948986411095, -0.17928941547870636, 0.06250043213367462, 0.0028728211764246225, -0.03608235344290733, 0.004988894332200289, 0.04095878079533577]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0829942524433136, 0.03390900045633316, 0.08735020458698273, 0.05619015917181969, 0.09222856909036636, 0.11388217657804489, 0.06729637086391449, -0.11604573577642441, 0.07155925780534744, 0.0736568346619606, -0.00030258469632826746, 0.08719845116138458, -0.17625921964645386, 0.029404204338788986, -0.047523170709609985, 0.09994958341121674]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13794779777526855, 0.1350201815366745, 0.07833272218704224, 0.10842853039503098, 0.08966832607984543, 0.08968546986579895, 0.010861348360776901, 0.13950954377651215, 0.13831965625286102, -0.5135841369628906, -0.06543781608343124, 0.07856300473213196, 0.09807463735342026, -0.2604278028011322, -0.08628213405609131, -0.1312376707792282]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.04121847823262215, 0.07085032761096954, 0.08259885758161545, 0.05400990694761276, 0.05359666794538498, -0.07847464829683304, 0.02204403467476368, -0.006395786069333553, -0.005849622655659914, 0.042249422520399094, -0.20464394986629486, 0.12078847736120224, -0.0741102322936058, -0.23761416971683502, 0.08735564351081848, 0.012032470665872097]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09271282702684402, 0.1210511177778244, 0.02180599234998226, 0.06835348904132843, 0.014107149094343185, -0.03649354353547096, 0.035342030227184296, 0.09018447995185852, 0.07746882736682892, -0.1039450541138649, -0.13811908662319183, 0.1460447907447815, -0.18280097842216492, 0.13253585994243622, 0.14130769670009613, 0.06960520148277283]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.11349592357873917, -0.043561287224292755, 0.06960904598236084, 0.2823525369167328, 0.1811068058013916, 0.06867336481809616, -0.1222022995352745, -0.0787101611495018, 0.08853664249181747, 0.13539256155490875, -0.5606618523597717, 0.2065764218568802, 0.32566705346107483, -0.22756804525852203, 0.09723548591136932, 0.1933843344449997]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.1609257161617279, -0.038749292492866516, 0.1009703055024147, 0.03693515062332153, -1.068869948387146, -0.1978304237127304, -0.1679311990737915, -0.40296512842178345, -0.13663779199123383, -0.1781829297542572, -0.1852087527513504, -0.030455321073532104, 0.16516482830047607, -0.07750125974416733, -0.4821593761444092, -0.0019130128202959895]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.15163694322109222, 0.23571836948394775, -0.061621855944395065, 0.44524794816970825, 0.3313872218132019, 0.28478798270225525, 0.17850171029567719, 0.24568451941013336, -0.053454115986824036, 0.17363744974136353, 0.2522013485431671, 0.03386358544230461, 0.4340354800224304, -0.14343613386154175, -0.06454384326934814, 0.1867782175540924]
Layer: gate_8 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.37736210227012634, 0.32146134972572327, 0.08197229355573654, 0.6428765654563904, 0.2945559322834015, 0.4203380346298218, 0.6474063396453857, 0.7275596857070923, 0.30767205357551575, 0.10957789421081543, 0.39118489623069763, 0.5165130496025085, 0.6827614307403564, 0.46007078886032104, 0.8389099836349487, 0.6415319442749023]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7038522362709045, 0.7399797439575195, 0.3334936797618866, 0.5883700847625732, 0.6421358585357666, -0.04509051516652107, 0.3619491457939148, 0.690136194229126, 0.15820428729057312, -0.12277009338140488, 0.3976001441478729, 0.5415879487991333, 0.2642532289028168, 0.1759830266237259, 0.2129320502281189, 0.39230871200561523]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.4395090341567993, 0.6438372731208801, 0.777298629283905, 0.6408573985099792, 0.7726876735687256, 0.5308344960212708, 0.6899999380111694, 0.515223503112793, 0.5740661025047302, 1.007477879524231, 0.6416304111480713, 0.5498902201652527, 0.3865123689174652, 0.3109140396118164, -0.05685808137059212, 0.7119885087013245]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.30999428033828735, 1.106581449508667, 0.8247010111808777, 0.5788455009460449, 0.3983975052833557, 0.6563599705696106, 0.5240896344184875, 0.6260923147201538, 0.6455063223838806, 0.7303681969642639, 1.7449384927749634, 0.7750605940818787, 0.9632996916770935, 0.8115785717964172, 0.7227708101272583, 0.7840208411216736]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.5998002290725708, 1.3742941617965698, 1.6427489519119263, 0.7795553207397461, 1.1647833585739136, 0.821942925453186, 1.2896223068237305, 1.2043254375457764, 1.2789909839630127, 1.3534451723098755, 1.225968360900879, 1.6606171131134033, 0.7621869444847107, 1.7137351036071777, 1.3884276151657104, 1.2193493843078613]
Running loglikelihood requests:  38%|████████████████████████████████████████████████████▋                                                                                    | 77/200 [01:45<02:19,  1.13s/it]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [0.9224981069564819, 1.0764967203140259, 1.5908515453338623, 1.572275161743164, 1.2818008661270142, 1.2779453992843628, 0.09090009331703186, 0.5364511609077454, 0.9593570232391357, 0.591063916683197, -0.006199588067829609, 0.7952539920806885, 1.211832880973816, 1.227501392364502, 1.506504774093628, 0.5712405443191528]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.7886088490486145, 1.5194424390792847, 1.4108736515045166, 1.1071245670318604, 1.2632118463516235, 1.404366135597229, 1.4443403482437134, 0.8510173559188843, 0.9476690292358398, 1.1217169761657715, 1.016237735748291, 0.3668642044067383, 1.8246947526931763, 1.023543119430542, 1.0534632205963135, 1.0424370765686035]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.2485857009887695, 0.7562054991722107, 1.2853550910949707, 1.2709376811981201, 0.9643746614456177, 0.8481246829032898, -0.3601800501346588, 2.174095630645752, -0.33035707473754883, 2.10321044921875, -0.31040406227111816, 1.3065332174301147, 1.552992343902588, 1.5249598026275635, 1.3737766742706299, -0.07178179174661636]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.219539999961853, 1.9610927104949951, 1.906467318534851, 2.431828498840332, 2.236748218536377, 1.9315028190612793, 1.808456301689148, 2.139922618865967, 1.916559100151062, 2.2444095611572266, 2.1396584510803223, 1.7074099779129028, 1.771798014640808, 1.9134190082550049, 1.9618167877197266, 2.488642930984497]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.5504484176635742, 1.2718110084533691, 1.801350474357605, 1.9642584323883057, 1.5946863889694214, 1.8934625387191772, 1.457511067390442, 2.1758501529693604, 2.356968641281128, 1.622707724571228, 1.71249258518219, 1.6438018083572388, 1.6625511646270752, 1.968170404434204, 1.788254737854004, 1.6311079263687134]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.2980144023895264, 2.0490481853485107, 1.6753422021865845, 1.8787933588027954, 2.0597147941589355, 1.9013937711715698, 2.025794506072998, 2.0555014610290527, 1.9812896251678467, 1.8186885118484497, 2.097759962081909, 1.8993327617645264, 3.006598949432373, 2.1558849811553955, 2.0807721614837646, 1.9395983219146729]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6114051938056946, 1.3710802793502808, 1.380957841873169, 0.8691123723983765, 0.7010945081710815, 1.4745736122131348, 1.334806203842163, 1.229769229888916, 0.9681878089904785, 1.1004146337509155, 1.3040015697479248, 0.9862322211265564, 1.0833616256713867, 1.3921658992767334, 1.4724112749099731, 1.0724934339523315]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.659553289413452, 2.719346046447754, 2.6740477085113525, 2.826974391937256, 2.7121591567993164, 2.875736951828003, 2.9336867332458496, 2.826798915863037, 2.646693229675293, 2.90324330329895, 2.884082078933716, 2.7297065258026123, 2.7360281944274902, 2.7443132400512695, 3.1398816108703613, 2.9430651664733887]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.9474256038665771, 1.6672875881195068, 1.8035449981689453, 1.7937504053115845, 1.9901469945907593, 1.896295428276062, 1.9717791080474854, 1.6748212575912476, 1.947514533996582, 2.1815598011016846, 1.5936871767044067, 1.816506028175354, 1.8392260074615479, 1.8281261920928955, 1.9243133068084717, 2.195849657058716]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.5754458904266357, 2.1762237548828125, 2.131829261779785, 1.9842802286148071, 2.1736373901367188, 2.125089406967163, 2.0144004821777344, 2.184713363647461, 2.0798871517181396, 1.865383267402649, 2.225198268890381, 2.0643978118896484, 2.204460620880127, 1.5375735759735107, 2.281822681427002, 2.1651101112365723]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.8427157402038574, 3.1264970302581787, 2.856901168823242, 2.835219383239746, 3.0400679111480713, 3.073634624481201, 3.0122129917144775, 2.887092351913452, 3.0719473361968994, 3.0402214527130127, 2.9171009063720703, 2.995704412460327, 2.866748571395874, 3.1423513889312744, 3.083704710006714, 3.115548610687256]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_25 - Captured router_logits: [2.306119203567505, 2.178679943084717, 2.1184654235839844, 2.301748037338257, 1.9092912673950195, 2.141590118408203, 2.1450371742248535, 2.156935214996338, 2.195591449737549, 2.20332670211792, 2.2177038192749023, 2.2118122577667236, 2.1768758296966553, 2.280024766921997, 2.011080026626587, 2.4171416759490967]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.259981393814087, 2.0846056938171387, 2.0855302810668945, 2.331439733505249, 2.0922176837921143, 2.1477339267730713, 2.1555545330047607, 2.0953080654144287, 2.077998161315918, 2.387173891067505, 2.418846607208252, 2.186217784881592, 2.2039899826049805, 2.263747215270996, 2.0559186935424805, 2.205792188644409]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.149869918823242, 2.0399727821350098, 2.08689284324646, 2.2012927532196045, 2.077681541442871, 2.118216037750244, 2.1556894779205322, 2.22453236579895, 2.147724151611328, 2.218393087387085, 1.826423168182373, 2.10567569732666, 2.241774559020996, 2.0957541465759277, 2.0792346000671387, 2.0945186614990234]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.6402335166931152, 3.64257550239563, 3.8973476886749268, 3.7286996841430664, 3.859261989593506, 3.6742465496063232, 3.601419448852539, 3.811551332473755, 3.585028648376465, 3.744971513748169, 3.6524953842163086, 3.4525763988494873, 3.756171464920044, 3.4725213050842285, 3.718994617462158, 3.6054017543792725]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.7489118576049805, 8.188777923583984, 7.875757694244385, 7.784526824951172, 7.581564903259277, 7.372835159301758, 8.107550621032715, 7.988860130310059, 8.014753341674805, 7.733502388000488, 7.760658264160156, 7.609657287597656, 7.848023414611816, 7.924857139587402, 8.177721977233887, 7.769184112548828]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.937675952911377, 5.155935287475586, 5.004729270935059, 4.580583095550537, 5.253821849822998, 5.0374755859375, 4.840419292449951, 4.913229942321777, 5.057938098907471, 5.030210971832275, 4.963187217712402, 4.475342273712158, 4.8857855796813965, 5.282508850097656, 5.10250997543335, 4.772763252258301]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.5495965480804443, 3.2957699298858643, 3.3565447330474854, 3.2465832233428955, 3.3324270248413086, 3.4312033653259277, 3.344965696334839, 3.239370346069336, 3.274723529815674, 3.389321804046631, 3.462597608566284, 2.6889617443084717, 3.2925097942352295, 3.3273301124572754, 3.5321056842803955, 3.4911317825317383]
Layer: gate_31 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.0942746251821518, 0.11341911554336548, 0.10575281083583832, -0.22262686491012573, -0.18525616824626923, -0.12922143936157227, 0.1302236169576645, -0.2033662497997284, 0.06619590520858765, 0.09548801183700562, 0.10518646240234375, 0.09471495449542999, 0.09754090011119843, 0.10814497619867325, -1.0967390537261963, 0.1196523979306221]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08020243048667908, 0.06283598393201828, 0.03620559722185135, 0.0486137680709362, 0.07379692792892456, 0.02061217837035656, 0.05055251717567444, 0.055655717849731445, 0.003316091140732169, 0.05532673001289368, -0.15553347766399384, 0.03268757835030556, -0.004630727227777243, -0.02082497626543045, -0.004339857958257198, 0.024397393688559532]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06310461461544037, 0.028019843623042107, 0.08797180652618408, 0.05331263691186905, 0.09165971726179123, 0.12085531651973724, 0.06824620813131332, -0.10511618852615356, 0.05862598866224289, 0.06721596419811249, -0.022471318021416664, 0.08786965906620026, -0.15483534336090088, 0.020758459344506264, -0.024917639791965485, 0.10064728558063507]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1221722662448883, 0.1426023542881012, 0.085105761885643, 0.1426512748003006, 0.10855402797460556, 0.09061066061258316, 0.016570746898651123, 0.15302380919456482, 0.13088469207286835, -0.4697782099246979, -0.054130300879478455, 0.06252606958150864, 0.11150925606489182, -0.20424434542655945, -0.10567369312047958, -0.13238082826137543]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.03375359997153282, 0.04619177430868149, 0.07498173415660858, 0.0638279989361763, 0.06311576068401337, -0.06412739306688309, 0.010802572593092918, 0.0237558763474226, -0.016678135842084885, 0.006939074955880642, -0.14078263938426971, 0.0959160178899765, -0.10474306344985962, -0.16620585322380066, 0.07271455228328705, -0.0010310325305908918]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.0989006906747818, 0.16433002054691315, 0.05451289936900139, 0.08133146911859512, 0.0031755228992551565, 0.0058935475535690784, 0.020440537482500076, 0.050630342215299606, 0.07874779403209686, -0.08577165007591248, -0.15797558426856995, 0.15927548706531525, -0.1258651614189148, 0.13816799223423004, 0.1384621560573578, 0.09387107938528061]
Layer: gate_5 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.15289492905139923, -0.04899035766720772, 0.11527514457702637, 0.290440171957016, 0.18275581300258636, 0.07839762419462204, -0.07047254592180252, -0.044734131544828415, 0.18070454895496368, 0.15217383205890656, -0.4384434223175049, 0.1708122044801712, 0.3351585268974304, -0.23014016449451447, 0.11970679461956024, 0.1786479949951172]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.14255470037460327, -0.04812880605459213, 0.10786374658346176, 0.09268214553594589, -0.8840253353118896, -0.15749222040176392, -0.1687583029270172, -0.30696460604667664, -0.12311270087957382, -0.0851101353764534, -0.19378691911697388, -0.04552384465932846, 0.09373858571052551, -0.05091320723295212, -0.45221593976020813, -0.15560883283615112]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.18398143351078033, 0.22143137454986572, -0.1468457132577896, 0.3612963855266571, 0.3520578145980835, 0.2629469633102417, 0.19272086024284363, 0.15531592071056366, 0.1646304577589035, 0.390319287776947, 0.22481441497802734, 0.006758376024663448, 0.4119264483451843, -0.2000083029270172, -0.1995791345834732, 0.22410622239112854]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.33110812306404114, 0.3739158809185028, 0.14274479448795319, 0.7991142272949219, 0.3593345582485199, 0.40946096181869507, 0.610647976398468, 0.6932670474052429, 0.43174123764038086, 0.24306869506835938, 0.2787146270275116, 0.5380460023880005, 0.662147581577301, 0.497412770986557, 0.7681028842926025, 0.6539772748947144]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.7503567337989807, 0.7281049489974976, 0.2945820689201355, 0.6093392968177795, 0.6244368553161621, 0.09487766772508621, 0.3624657690525055, 0.6706068515777588, 0.03282702714204788, -0.008940107189118862, 0.40052834153175354, 0.5926607251167297, 0.19252151250839233, 0.23365451395511627, 0.3383089601993561, 0.32445040345191956]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.31236663460731506, 0.5111005902290344, 0.563758134841919, 0.6260098218917847, 0.6532899737358093, 0.3793122470378876, 0.5580866932868958, 0.3549480736255646, 0.4574795365333557, 0.8550935387611389, 0.4698047637939453, 0.37983235716819763, 0.18096551299095154, 0.1254635900259018, -0.26431921124458313, 0.5468906164169312]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.24299462139606476, 0.9412280917167664, 0.8297256827354431, 0.5539339184761047, 0.2774920165538788, 0.5819414258003235, 0.5414187908172607, 0.6266342401504517, 0.5391202569007874, 0.6242956519126892, 1.5441679954528809, 0.6576405167579651, 0.89041668176651, 0.6743718385696411, 0.5692815780639648, 0.7342423796653748]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.3787569999694824, 1.206566572189331, 1.5342662334442139, 0.7202403545379639, 1.068400263786316, 0.6044884324073792, 1.139177918434143, 1.422163724899292, 1.0603034496307373, 1.2255403995513916, 1.1951353549957275, 1.527457594871521, 0.6063975095748901, 1.5636787414550781, 1.1353684663772583, 1.0625112056732178]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [0.711632251739502, 0.9052126407623291, 1.3495880365371704, 1.3801159858703613, 1.1823134422302246, 1.6159772872924805, 0.23751825094223022, 0.8138442039489746, 0.7348150610923767, 0.6420259475708008, -0.17917628586292267, 0.6953328847885132, 1.141656756401062, 1.1436718702316284, 1.331349492073059, 0.5395424962043762]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6521032452583313, 1.4200061559677124, 1.265774130821228, 0.9214966893196106, 1.0939463376998901, 1.2330870628356934, 1.4457801580429077, 0.7732031345367432, 0.9130498766899109, 1.014009952545166, 0.891286313533783, 0.3256795108318329, 1.765331506729126, 0.8293961882591248, 0.8590038418769836, 1.1972509622573853]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.0481785535812378, 0.6947901844978333, 1.1262701749801636, 1.0845088958740234, 0.8105337023735046, 0.7234837412834167, -0.12914332747459412, 1.832684874534607, -0.3912792503833771, 2.196669340133667, -0.5192450881004333, 1.1499825716018677, 1.2938365936279297, 1.2562777996063232, 1.2135264873504639, 0.03645969182252884]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  40%|███████████████████████████████████████████████████████▍                                                                                 | 81/200 [01:49<02:12,  1.11s/it]Layer: gate_17 - Captured router_logits: [0.9762869477272034, 1.6095235347747803, 1.6562843322753906, 2.0912885665893555, 1.999129295349121, 1.6306484937667847, 1.545100212097168, 1.874419093132019, 1.6550767421722412, 1.8942803144454956, 2.045474052429199, 1.4757565259933472, 1.3901724815368652, 1.5876318216323853, 1.5097007751464844, 2.1292948722839355]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.3752423524856567, 1.0585113763809204, 1.51750910282135, 1.911025881767273, 1.3144395351409912, 1.6779570579528809, 1.4035552740097046, 2.005769968032837, 2.231248378753662, 1.4631990194320679, 1.4516335725784302, 1.4564989805221558, 1.4669489860534668, 1.7328546047210693, 1.5839056968688965, 1.496002197265625]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.955013394355774, 1.8556758165359497, 1.4907926321029663, 1.6220803260803223, 1.8152637481689453, 1.573040246963501, 1.695403814315796, 1.7620761394500732, 1.8715702295303345, 1.548950433731079, 1.8811625242233276, 1.6868966817855835, 2.702915668487549, 1.8696333169937134, 1.8075501918792725, 1.6627633571624756]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6603775024414062, 1.3467193841934204, 1.2168015241622925, 0.7659406065940857, 0.543857216835022, 1.4503483772277832, 1.294142484664917, 1.358868956565857, 0.8080045580863953, 1.122864007949829, 1.3009696006774902, 0.9546555876731873, 1.0856859683990479, 1.2262003421783447, 1.3883390426635742, 0.9885856509208679]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.432704210281372, 2.5581164360046387, 2.4684712886810303, 2.600350856781006, 2.4823615550994873, 2.655973434448242, 2.6622838973999023, 2.7296390533447266, 2.4334771633148193, 2.695903778076172, 2.7116990089416504, 2.7720489501953125, 2.535140037536621, 2.588646650314331, 2.9319748878479004, 2.7428205013275146]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.8790847063064575, 1.6519309282302856, 1.8792880773544312, 1.6810600757598877, 1.8931796550750732, 1.8339850902557373, 1.8722195625305176, 1.648026466369629, 1.9238132238388062, 2.1086721420288086, 1.5671920776367188, 1.9195442199707031, 1.7799005508422852, 1.7754489183425903, 1.821677327156067, 2.104790449142456]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.581362724304199, 2.137644052505493, 2.03558349609375, 1.9038503170013428, 2.182429313659668, 2.0339319705963135, 1.9408621788024902, 2.1352193355560303, 2.0140230655670166, 1.778657078742981, 2.1180384159088135, 1.972547173500061, 2.0972275733947754, 1.4440401792526245, 2.2200100421905518, 2.0602786540985107]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.7759265899658203, 3.058905839920044, 2.8243420124053955, 2.706369638442993, 3.093376874923706, 3.0184521675109863, 3.0378894805908203, 2.8345847129821777, 2.935378313064575, 2.949573278427124, 2.9596073627471924, 2.9019205570220947, 2.8419482707977295, 3.0530588626861572, 2.9786362648010254, 3.011657238006592]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.2209572792053223, 2.1332497596740723, 2.0418038368225098, 2.2510311603546143, 1.8061790466308594, 2.1129941940307617, 2.0601422786712646, 2.1684768199920654, 2.113374710083008, 2.1291346549987793, 2.1801445484161377, 2.0935847759246826, 2.1157028675079346, 2.1465909481048584, 2.019132614135742, 2.332944631576538]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.228595495223999, 2.0262956619262695, 2.0031912326812744, 2.2138824462890625, 2.0428290367126465, 2.070697784423828, 2.0971169471740723, 2.068329095840454, 2.0068318843841553, 2.224362850189209, 2.443105936050415, 2.1016438007354736, 2.1509761810302734, 2.213456153869629, 1.9978504180908203, 2.120218515396118]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.084102153778076, 2.001180410385132, 2.053090810775757, 2.1642825603485107, 2.045072555541992, 2.050391435623169, 2.1001858711242676, 2.142958402633667, 2.0784292221069336, 2.1655824184417725, 1.8102755546569824, 2.0556812286376953, 2.191298484802246, 2.0610411167144775, 2.022573471069336, 2.0144951343536377]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.5307908058166504, 3.551093816757202, 3.8029372692108154, 3.610002040863037, 3.735870122909546, 3.5483689308166504, 3.6596972942352295, 3.702040910720825, 3.4871644973754883, 3.6636722087860107, 3.539175271987915, 3.374239921569824, 3.6264846324920654, 3.3658666610717773, 3.6963601112365723, 3.5104012489318848]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.694735527038574, 8.154240608215332, 7.823367118835449, 7.712218761444092, 7.557501316070557, 7.382282257080078, 7.967158794403076, 7.967293739318848, 7.989230632781982, 7.719067096710205, 7.729412078857422, 7.491066932678223, 7.709397315979004, 7.876709938049316, 8.105704307556152, 7.765205383300781]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.669191360473633, 4.831686973571777, 4.666296482086182, 4.371983051300049, 4.913193225860596, 4.711426734924316, 4.547722339630127, 4.649267673492432, 4.7674760818481445, 4.756698131561279, 4.758817195892334, 4.295482635498047, 4.67128324508667, 4.929264545440674, 4.805089473724365, 4.517937660217285]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.381903886795044, 3.0944387912750244, 3.1134591102600098, 2.929670810699463, 3.1020760536193848, 3.2351040840148926, 3.1054506301879883, 3.078878402709961, 3.0762064456939697, 3.252095937728882, 3.3478844165802, 2.609581232070923, 3.1369588375091553, 3.1863856315612793, 3.375932216644287, 3.330889940261841]
Layer: gate_31 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.0972939133644104, 0.10736550390720367, 0.10745745152235031, -0.24869289994239807, -0.18879054486751556, -0.15032321214675903, 0.1212613582611084, -0.1646323949098587, 0.09814777225255966, 0.09227170050144196, 0.09636958688497543, 0.10882935672998428, 0.09494240581989288, 0.11724869906902313, -1.0162545442581177, 0.12560801208019257]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.08352954685688019, 0.05009366571903229, 0.03324558958411217, 0.049484044313430786, 0.07238776236772537, 0.031305547803640366, 0.04349832609295845, 0.0653630793094635, 0.03235243633389473, 0.0454707108438015, -0.190704807639122, 0.05255934223532677, 0.019252493977546692, -0.026173030957579613, 0.005951089784502983, 0.014412284828722477]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05274667590856552, 0.03291728347539902, 0.0908413827419281, 0.06848566234111786, 0.11075808107852936, 0.08203337341547012, 0.07821357250213623, -0.1149364560842514, 0.08454933762550354, 0.06135593727231026, 0.005194876343011856, 0.08376339077949524, -0.17215366661548615, 0.02159304730594158, -0.016788708046078682, 0.1122656986117363]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.1109149158000946, 0.13376948237419128, 0.08888715505599976, 0.11342454701662064, 0.0934114158153534, 0.12767750024795532, 0.02968643419444561, 0.14518429338932037, 0.1167338564991951, -0.5364445447921753, -0.09199389815330505, 0.10701139271259308, 0.15796424448490143, -0.2717229425907135, -0.09884607791900635, -0.13340242207050323]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0348859578371048, 0.04414164647459984, 0.0702161192893982, 0.11338221281766891, 0.055434394627809525, -0.08809374272823334, 0.03502620384097099, -0.008122413419187069, -0.04408571869134903, 0.03899573162198067, -0.17823460698127747, 0.07607446610927582, -0.07585972547531128, -0.1919613629579544, 0.09519895911216736, 0.01598154939711094]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09865672141313553, 0.14178591966629028, 0.038216907531023026, 0.08797778934240341, -0.025837846100330353, -0.03495536372065544, 0.03823846951127052, 0.08149801194667816, 0.09577237069606781, -0.10611149668693542, -0.1611909121274948, 0.14001509547233582, -0.21181640028953552, 0.13387547433376312, 0.15389256179332733, 0.06470716744661331]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.15072189271450043, 0.025116464123129845, 0.03861869126558304, 0.25949224829673767, 0.14833903312683105, 0.09504111111164093, -0.0846981480717659, -0.05636805295944214, 0.05316745489835739, 0.14270247519016266, -0.5271835327148438, 0.21168683469295502, 0.352613627910614, -0.241941437125206, 0.15585589408874512, 0.22039709985256195]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.12191961705684662, 0.015046139247715473, 0.14243045449256897, 0.007257750257849693, -0.9004088640213013, -0.17793893814086914, -0.10083839297294617, -0.27392205595970154, 0.029925910755991936, -0.1686333268880844, -0.2114924043416977, 0.007463767658919096, 0.03215968236327171, 0.033686745911836624, -0.5634920597076416, -0.11403226852416992]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.1259518414735794, 0.2576483190059662, -0.10689470171928406, 0.3949660360813141, 0.22643697261810303, 0.3339236378669739, 0.14364242553710938, 0.17319989204406738, 0.031147291883826256, 0.44157323241233826, 0.27532362937927246, 0.027224207296967506, 0.5049505829811096, -0.29283612966537476, -0.1588701605796814, 0.20732784271240234]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3698870837688446, 0.22638776898384094, 0.12859749794006348, 0.7467824816703796, 0.29277345538139343, 0.3298637270927429, 0.5749167203903198, 0.6633610129356384, 0.28682923316955566, 0.08811471611261368, 0.16180871427059174, 0.455381840467453, 0.5125337243080139, 0.2585141658782959, 0.7945443391799927, 0.5863664150238037]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7422006726264954, 0.7517016530036926, 0.2789638638496399, 0.5660445094108582, 0.6240640878677368, -0.15404200553894043, 0.4050668179988861, 0.6628903746604919, -0.0722588375210762, -0.16010570526123047, 0.44603028893470764, 0.5064112544059753, 0.1456518918275833, 0.26842013001441956, 0.31638938188552856, 0.39274924993515015]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.38788503408432007, 0.5931075811386108, 0.6860510110855103, 0.48251989483833313, 0.7190782427787781, 0.40042027831077576, 0.7647836208343506, 0.3709210157394409, 0.5372740626335144, 0.9210599064826965, 0.5487774610519409, 0.3636605441570282, 0.2822667360305786, 0.2841290235519409, -0.1681804209947586, 0.6190776228904724]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.23615403473377228, 0.9726729393005371, 0.8484210968017578, 0.5944744944572449, 0.33442479372024536, 0.49388352036476135, 0.5497474670410156, 0.6624667644500732, 0.5139262080192566, 0.6784485578536987, 1.5718564987182617, 0.7835570573806763, 0.9368678331375122, 0.6775050759315491, 0.7552787661552429, 0.7444706559181213]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.1575822830200195, 1.056305170059204, 1.3418701887130737, 0.5589584708213806, 1.0087072849273682, 0.4377565085887909, 0.9729375839233398, 1.0825086832046509, 1.0067499876022339, 0.967509388923645, 1.0915168523788452, 1.438415765762329, 0.41749680042266846, 1.2828879356384277, 0.9035553336143494, 0.7857322096824646]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5665938854217529, 0.6858037710189819, 1.177001714706421, 1.105223298072815, 1.0243538618087769, 0.9964541792869568, 0.12602202594280243, 0.9038020372390747, 0.7166645526885986, 0.613533616065979, -0.3239928185939789, 0.5248429179191589, 1.064050555229187, 1.0078537464141846, 1.136025309562683, 0.37274041771888733]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6349679231643677, 1.3738229274749756, 1.1165138483047485, 0.9402943849563599, 1.005789875984192, 1.2252683639526367, 1.4324007034301758, 0.7582798004150391, 0.9229336380958557, 0.9378632307052612, 0.8824512958526611, 0.4418690800666809, 1.654794692993164, 0.8843054175376892, 0.8770502209663391, 1.1122088432312012]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.1998096704483032, 0.731992244720459, 1.0804585218429565, 1.135676622390747, 1.030592441558838, 0.8713854551315308, 0.028400391340255737, 1.6851524114608765, 0.010791261680424213, 2.164153814315796, -0.24366751313209534, 1.2037756443023682, 1.376755714416504, 1.2902131080627441, 1.2448619604110718, 0.2777121365070343]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.180505633354187, 1.88716721534729, 1.9469255208969116, 2.2754905223846436, 2.152266263961792, 1.8881314992904663, 1.8123961687088013, 2.0295190811157227, 2.046509265899658, 2.1086020469665527, 2.435704469680786, 1.6919565200805664, 1.5260547399520874, 1.7155358791351318, 1.6760025024414062, 2.3248374462127686]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.608811378479004, 1.2491644620895386, 1.680222988128662, 2.074425458908081, 1.5572681427001953, 1.7352949380874634, 1.7126474380493164, 2.1412086486816406, 2.6779401302337646, 1.5985689163208008, 1.6279021501541138, 1.5172830820083618, 1.7098592519760132, 1.77509343624115, 1.7658499479293823, 1.6524150371551514]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.324378490447998, 2.0775561332702637, 1.8793563842773438, 1.9963138103485107, 2.169367551803589, 1.95636785030365, 2.14359188079834, 2.112412452697754, 2.2597343921661377, 1.8037201166152954, 2.2300806045532227, 2.0252223014831543, 2.822049856185913, 2.2200300693511963, 2.1589159965515137, 2.069969654083252]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.831721842288971, 1.2532083988189697, 1.297110676765442, 0.8459461331367493, 0.5688677430152893, 1.6031893491744995, 1.2887499332427979, 1.4771720170974731, 0.8375303149223328, 1.1938369274139404, 1.1705482006072998, 1.0181975364685059, 1.2123912572860718, 1.3480327129364014, 1.3412854671478271, 1.1172330379486084]
Running loglikelihood requests:  42%|██████████████████████████████████████████████████████████▏                                                                              | 85/200 [01:54<02:07,  1.11s/it]Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.5754809379577637, 2.676553964614868, 2.670048713684082, 2.7002294063568115, 2.6161489486694336, 2.742919683456421, 2.86605167388916, 2.7262470722198486, 2.6129868030548096, 2.851156234741211, 2.821877956390381, 2.8166563510894775, 2.6534476280212402, 2.7646982669830322, 2.926765203475952, 2.81185245513916]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.9851640462875366, 1.6762723922729492, 1.9342085123062134, 1.6384295225143433, 1.9983848333358765, 1.8407986164093018, 1.9100829362869263, 1.7290925979614258, 1.854622483253479, 2.0809876918792725, 1.6853713989257812, 1.94140625, 1.8691281080245972, 1.8327536582946777, 1.8589822053909302, 2.107839822769165]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.417436122894287, 2.0291147232055664, 1.826405644416809, 1.8822451829910278, 1.9791183471679688, 1.8680545091629028, 1.8498152494430542, 1.998452067375183, 1.8535730838775635, 1.8196858167648315, 1.9205090999603271, 1.8545070886611938, 1.9350862503051758, 1.2799404859542847, 1.9117813110351562, 2.0265073776245117]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.8823702335357666, 3.0874009132385254, 2.8281667232513428, 2.7441329956054688, 3.1201016902923584, 2.9523186683654785, 3.011075496673584, 2.889508008956909, 2.9919958114624023, 3.080744504928589, 3.0435023307800293, 2.9920501708984375, 2.844268560409546, 3.0825791358947754, 2.997737169265747, 3.0742244720458984]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.078594207763672, 2.0225532054901123, 1.991249680519104, 2.243640661239624, 1.7374516725540161, 2.0493359565734863, 2.0588343143463135, 2.1588401794433594, 2.1123600006103516, 2.0634751319885254, 2.09234619140625, 2.06952166557312, 2.120774984359741, 2.1407768726348877, 2.04087495803833, 2.268702507019043]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.1849007606506348, 2.0502846240997314, 2.068819046020508, 2.214536428451538, 2.0384039878845215, 2.0927295684814453, 2.1109347343444824, 2.0593314170837402, 2.003335952758789, 2.241657257080078, 2.4419102668762207, 2.088014602661133, 2.1533524990081787, 2.2464046478271484, 2.0310068130493164, 2.1081271171569824]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.033832550048828, 1.9507675170898438, 2.0063986778259277, 2.083534002304077, 1.9394556283950806, 2.0111825466156006, 2.088913917541504, 2.0678157806396484, 2.045048236846924, 2.0692224502563477, 1.7168240547180176, 1.925642490386963, 2.131664991378784, 2.0609607696533203, 1.9783499240875244, 2.0003674030303955]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.408463478088379, 3.4826838970184326, 3.7072277069091797, 3.4935004711151123, 3.560971736907959, 3.41483736038208, 3.551837921142578, 3.588899612426758, 3.3936421871185303, 3.5237934589385986, 3.4688050746917725, 3.2716150283813477, 3.455073356628418, 3.298341989517212, 3.477013111114502, 3.4009149074554443]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.123845100402832, 7.678109645843506, 7.26678991317749, 7.181674480438232, 7.068089485168457, 6.890094757080078, 7.405543327331543, 7.342041969299316, 7.371170997619629, 7.121669769287109, 7.177313327789307, 7.033562183380127, 7.233912467956543, 7.304009437561035, 7.566858768463135, 7.239527702331543]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.501095294952393, 4.596501350402832, 4.424411296844482, 4.155046463012695, 4.725437164306641, 4.578801155090332, 4.294442176818848, 4.502131462097168, 4.555518627166748, 4.503546237945557, 4.440315246582031, 3.944310188293457, 4.302947044372559, 4.680992603302002, 4.503692626953125, 4.329168796539307]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.0611581802368164, 2.905673027038574, 2.8978118896484375, 2.7680540084838867, 2.8427038192749023, 2.9583771228790283, 2.8535757064819336, 2.8013086318969727, 2.8736491203308105, 2.8824994564056396, 3.1818082332611084, 2.3529558181762695, 2.8970625400543213, 2.888031005859375, 3.1250741481781006, 2.953472375869751]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09046143293380737, 0.10929781943559647, 0.10666584968566895, -0.24278748035430908, -0.22287936508655548, -0.11917891353368759, 0.12378158420324326, -0.1614067405462265, 0.07287292182445526, 0.0917908325791359, 0.09867949783802032, 0.09102898836135864, 0.0950438380241394, 0.10392018407583237, -1.0880717039108276, 0.1162656843662262]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07738187164068222, 0.04961705207824707, 0.037422165274620056, 0.03906133025884628, 0.07739978283643723, 0.019757581874728203, 0.047870054841041565, 0.067484050989151, 0.006267808377742767, 0.05388973653316498, -0.18618687987327576, 0.05067873373627663, 0.0053665065206587315, -0.04232791066169739, 0.014018453657627106, 0.0419585295021534]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.07863464206457138, 0.02929365262389183, 0.08326668292284012, 0.06077517569065094, 0.09060858190059662, 0.10709770768880844, 0.06348282843828201, -0.11662018299102783, 0.06245698779821396, 0.08552702516317368, 0.0008857267675921321, 0.084055595099926, -0.18817494809627533, 0.01114312931895256, -0.04357798397541046, 0.10489092767238617]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12866948544979095, 0.12854959070682526, 0.07600873708724976, 0.1041082814335823, 0.10015510022640228, 0.08077864348888397, 0.0036514902021735907, 0.14080917835235596, 0.13792771100997925, -0.5382881760597229, -0.055917393416166306, 0.06574701517820358, 0.12947677075862885, -0.27137166261672974, -0.06093849241733551, -0.13582007586956024]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.025637909770011902, 0.07416260987520218, 0.07474526017904282, 0.08244853466749191, 0.05436374619603157, -0.08144144713878632, 0.003960144240409136, -0.013563960790634155, -0.03098265640437603, 0.042099323123693466, -0.2004614621400833, 0.12062308192253113, -0.05317385867238045, -0.22777779400348663, 0.09143602102994919, 0.0010941379005089402]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.1008370891213417, 0.12449067831039429, 0.03426596522331238, 0.07130755484104156, 0.006410051602870226, -0.043143875896930695, 0.03422268480062485, 0.06777062267065048, 0.12137969583272934, -0.10926830023527145, -0.10203740000724792, 0.14401310682296753, -0.1824525147676468, 0.14347673952579498, 0.1529010385274887, 0.07668762654066086]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.12550592422485352, -0.053869571536779404, 0.09675434976816177, 0.2904380261898041, 0.1903184950351715, 0.09253940731287003, -0.11322536319494247, -0.053910184651613235, 0.12737487256526947, 0.1425860971212387, -0.5409468412399292, 0.19455067813396454, 0.3352588415145874, -0.2330254465341568, 0.15680274367332458, 0.2141827493906021]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.1009853333234787, -0.06152775138616562, 0.09629052877426147, 0.044032737612724304, -1.0134515762329102, -0.184453085064888, -0.16054882109165192, -0.41153883934020996, -0.03958822414278984, -0.13546930253505707, -0.19429367780685425, -0.021663084626197815, 0.12119471281766891, -0.02094557136297226, -0.4578251242637634, -0.024348115548491478]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.16270913183689117, 0.24483022093772888, -0.12021581083536148, 0.44572168588638306, 0.36969414353370667, 0.29958418011665344, 0.19711408019065857, 0.16042084991931915, 0.07496903836727142, 0.32552555203437805, 0.22229629755020142, 0.08528149873018265, 0.4464752674102783, -0.1644766479730606, -0.10828127712011337, 0.2225780189037323]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.35007232427597046, 0.278994083404541, 0.12357104569673538, 0.7460097074508667, 0.3292091190814972, 0.3595120310783386, 0.5974220037460327, 0.6781179904937744, 0.3793731927871704, 0.10347118228673935, 0.30679574608802795, 0.4977985918521881, 0.7214134931564331, 0.40178531408309937, 0.7810550928115845, 0.6696954369544983]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.794907808303833, 0.7595314979553223, 0.3228912651538849, 0.6459001898765564, 0.6190738677978516, 0.07297204434871674, 0.39326217770576477, 0.6940560936927795, 0.06252175569534302, -0.1365596503019333, 0.42539507150650024, 0.6398078799247742, 0.20532965660095215, 0.20593895018100739, 0.3626619279384613, 0.3550487458705902]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.4250704050064087, 0.6930702924728394, 0.7023901343345642, 0.722676157951355, 0.8160817623138428, 0.5166150331497192, 0.665558397769928, 0.4778175354003906, 0.6568533182144165, 1.011536717414856, 0.6211127042770386, 0.5190796852111816, 0.29503345489501953, 0.22760120034217834, -0.15905120968818665, 0.6363075375556946]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.21438881754875183, 0.9930238127708435, 0.8764738440513611, 0.5787212252616882, 0.2729916274547577, 0.6143648624420166, 0.5539032816886902, 0.643089771270752, 0.5602294206619263, 0.6914185881614685, 1.586692452430725, 0.7067865133285522, 0.8892577886581421, 0.7787417769432068, 0.6147935390472412, 0.7328357696533203]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.2793413400650024, 1.1790255308151245, 1.479845643043518, 0.5662396550178528, 0.9812920689582825, 0.5750572085380554, 1.1040308475494385, 1.2108910083770752, 1.0557626485824585, 1.1675314903259277, 1.0663790702819824, 1.4715322256088257, 0.5731520652770996, 1.425429105758667, 1.186710238456726, 0.9741945862770081]
Layer: gate_13 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7434545159339905, 0.8817984461784363, 1.3937256336212158, 1.3678349256515503, 1.1201494932174683, 1.2674089670181274, 0.13642561435699463, 0.6355563998222351, 0.7784850001335144, 0.5291937589645386, -0.11324001103639603, 0.6387866735458374, 1.1155036687850952, 1.1110613346099854, 1.3388956785202026, 0.49373799562454224]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.7381414175033569, 1.5326579809188843, 1.3081107139587402, 1.0239830017089844, 1.1629427671432495, 1.336137056350708, 1.4196882247924805, 0.7981237173080444, 0.9706678986549377, 1.0809682607650757, 0.943856418132782, 0.3484095335006714, 1.8108304738998413, 0.9447504878044128, 0.9374645352363586, 1.0433053970336914]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.1063380241394043, 0.6695214509963989, 1.2035224437713623, 1.2776134014129639, 0.9364823698997498, 0.7411994934082031, -0.16519270837306976, 2.0952179431915283, -0.37703749537467957, 2.0860488414764404, -0.35552239418029785, 1.1989041566848755, 1.4131286144256592, 1.3369026184082031, 1.2727051973342896, -0.010794439353048801]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.0937700271606445, 1.6950888633728027, 1.7510762214660645, 2.1739232540130615, 2.12967586517334, 1.7609257698059082, 1.6224216222763062, 1.987041711807251, 1.7572860717773438, 1.9878239631652832, 2.0260798931121826, 1.6037073135375977, 1.539046287536621, 1.7822881937026978, 1.6212401390075684, 2.279690980911255]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.5495848655700684, 1.2312136888504028, 1.6583893299102783, 2.0843677520751953, 1.4147272109985352, 1.8118215799331665, 1.4333292245864868, 2.110682249069214, 2.299375295639038, 1.562920331954956, 1.6013816595077515, 1.5023552179336548, 1.5865139961242676, 1.9187090396881104, 1.6902532577514648, 1.6103413105010986]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.1201725006103516, 1.9675883054733276, 1.6390197277069092, 1.7548664808273315, 1.977491021156311, 1.730004906654358, 1.8752973079681396, 1.908373236656189, 1.9077702760696411, 1.697558879852295, 2.040804624557495, 1.8478453159332275, 2.825186252593994, 1.9866368770599365, 1.9460265636444092, 1.814446210861206]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.684110164642334, 1.389738917350769, 1.2968239784240723, 0.858523964881897, 0.5188997983932495, 1.5612248182296753, 1.2905097007751465, 1.1879571676254272, 0.9275310635566711, 1.1396331787109375, 1.276591181755066, 0.988869845867157, 1.0841842889785767, 1.3005695343017578, 1.3491891622543335, 0.99629145860672]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.5712125301361084, 2.651266098022461, 2.6159329414367676, 2.72798228263855, 2.62646484375, 2.833749771118164, 2.797922134399414, 2.8200669288635254, 2.560664415359497, 2.778989791870117, 2.8181800842285156, 2.7422001361846924, 2.6101181507110596, 2.6890485286712646, 3.04072904586792, 2.8444058895111084]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.8934489488601685, 1.6744405031204224, 1.8315932750701904, 1.6810152530670166, 1.920740008354187, 1.8393405675888062, 1.904828667640686, 1.6584558486938477, 1.9110773801803589, 2.11381196975708, 1.5445994138717651, 1.8272101879119873, 1.7710182666778564, 1.7545957565307617, 1.8170796632766724, 2.098644733428955]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4981272220611572, 2.1499714851379395, 1.971012830734253, 1.8967448472976685, 2.1627895832061768, 2.053347110748291, 1.949320673942566, 2.0833775997161865, 2.0112197399139404, 1.7834185361862183, 2.108968734741211, 1.938251256942749, 2.109940528869629, 1.3915765285491943, 2.209792137145996, 2.0708625316619873]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.733255386352539, 3.0193135738372803, 2.737786293029785, 2.720776319503784, 3.0299031734466553, 2.9330265522003174, 2.9374561309814453, 2.7996983528137207, 2.8988232612609863, 2.892228126525879, 2.882246971130371, 2.88096022605896, 2.832300901412964, 3.02181339263916, 2.916107654571533, 2.958303689956665]
Running loglikelihood requests:  44%|████████████████████████████████████████████████████████████▉                                                                            | 89/200 [01:58<02:01,  1.10s/it]Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.2113425731658936, 2.110872268676758, 2.008418321609497, 2.220287561416626, 1.7444028854370117, 2.0428876876831055, 2.035207509994507, 2.134796619415283, 2.08449649810791, 2.0895681381225586, 2.1444692611694336, 2.07612943649292, 2.089926242828369, 2.1226541996002197, 1.9504423141479492, 2.3001396656036377]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.210458278656006, 2.0562474727630615, 2.0110907554626465, 2.237711191177368, 2.0668818950653076, 2.1225216388702393, 2.0928821563720703, 2.057656764984131, 2.0192809104919434, 2.2831716537475586, 2.431312084197998, 2.118088483810425, 2.137767791748047, 2.229947805404663, 2.003854990005493, 2.148845911026001]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.101412773132324, 2.0181353092193604, 2.060215950012207, 2.164926290512085, 2.0668578147888184, 2.0935232639312744, 2.116337776184082, 2.144606828689575, 2.0781612396240234, 2.17832088470459, 1.7817306518554688, 2.061845302581787, 2.2264368534088135, 2.092496633529663, 2.020393133163452, 2.0328543186187744]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.520458459854126, 3.512594223022461, 3.764864683151245, 3.5731143951416016, 3.6628198623657227, 3.518232583999634, 3.5481269359588623, 3.6523380279541016, 3.4291749000549316, 3.621519088745117, 3.5014209747314453, 3.315673828125, 3.590230703353882, 3.3352062702178955, 3.6468279361724854, 3.4466400146484375]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.576205253601074, 8.012201309204102, 7.7399821281433105, 7.6261372566223145, 7.498971939086914, 7.310577392578125, 7.906503200531006, 7.864550590515137, 7.906703472137451, 7.636864185333252, 7.670740604400635, 7.446232318878174, 7.66453742980957, 7.812916278839111, 8.043892860412598, 7.621913433074951]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_30 - Captured router_logits: [4.470895290374756, 4.701533317565918, 4.512734889984131, 4.171936511993408, 4.7050065994262695, 4.511160850524902, 4.35704231262207, 4.448856830596924, 4.539585590362549, 4.52154016494751, 4.51176643371582, 4.087341785430908, 4.459799766540527, 4.707026481628418, 4.608561038970947, 4.3299102783203125]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.3053138256073, 3.0607051849365234, 3.133073329925537, 2.8453125953674316, 3.110457420349121, 3.1877827644348145, 3.0431995391845703, 2.9993042945861816, 3.0288350582122803, 3.177669048309326, 3.280088186264038, 2.575359582901001, 3.0721020698547363, 3.0945489406585693, 3.3191184997558594, 3.2799136638641357]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.08993123471736908, 0.11038811504840851, 0.10468897968530655, -0.22217035293579102, -0.19883058965206146, -0.13590644299983978, 0.12197659909725189, -0.1811370998620987, 0.07899432629346848, 0.09535806626081467, 0.0982922911643982, 0.08887342363595963, 0.09469853341579437, 0.10158511996269226, -1.040274977684021, 0.1187080591917038]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07639400660991669, 0.05033599212765694, 0.03482269495725632, 0.03613324463367462, 0.0784631073474884, 0.019724756479263306, 0.04821910709142685, 0.05867380276322365, -0.00047711003571748734, 0.051834575831890106, -0.17169873416423798, 0.05115482956171036, 0.006516837049275637, -0.03768689185380936, 0.0016042812494561076, 0.04175476357340813]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.08368842303752899, 0.02981376461684704, 0.0842384397983551, 0.06284366548061371, 0.0850038081407547, 0.10472933948040009, 0.06184964254498482, -0.11520566791296005, 0.06548246741294861, 0.07973697036504745, -0.0013454398140311241, 0.08758357167243958, -0.17162081599235535, 0.014560648240149021, -0.04864351823925972, 0.09789849817752838]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13310696184635162, 0.13075223565101624, 0.07823292165994644, 0.11612726747989655, 0.09324520826339722, 0.07990100979804993, -0.0021707869600504637, 0.1444995105266571, 0.12014351785182953, -0.5059337019920349, -0.07178107649087906, 0.08271951228380203, 0.10027388483285904, -0.2655249834060669, -0.08445226401090622, -0.13363511860370636]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.03379129245877266, 0.07844919711351395, 0.0812298059463501, 0.043698750436306, 0.04484930261969566, -0.07291720062494278, 0.008234570734202862, -0.013523565605282784, -0.017717916518449783, 0.03401143103837967, -0.1943209320306778, 0.11795882135629654, -0.05603201687335968, -0.22665171325206757, 0.08753111213445663, -0.0025708714965730906]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.08545742183923721, 0.11956888437271118, 0.024045249447226524, 0.07025199383497238, 0.0035093652550131083, -0.03257107362151146, 0.04948173463344574, 0.08855201303958893, 0.08252464979887009, -0.10431180894374847, -0.10967026650905609, 0.14069870114326477, -0.17689739167690277, 0.12659770250320435, 0.14500388503074646, 0.0702003613114357]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.10735123604536057, -0.04709186032414436, 0.061457742005586624, 0.2908024489879608, 0.1545514315366745, 0.06208903715014458, -0.11521286517381668, -0.06925332546234131, 0.06683863699436188, 0.13125328719615936, -0.5394270420074463, 0.2187632918357849, 0.32894283533096313, -0.2154807448387146, 0.10530537366867065, 0.20048142969608307]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.17700982093811035, -0.042598988860845566, 0.09157834947109222, 0.01358570996671915, -1.0356297492980957, -0.19472867250442505, -0.1758793443441391, -0.3884222209453583, -0.13645318150520325, -0.17520497739315033, -0.18161486089229584, -0.05137215554714203, 0.19086408615112305, -0.08060573786497116, -0.4273567795753479, -0.0014813302550464869]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.14168789982795715, 0.22022199630737305, -0.04971504583954811, 0.4488684833049774, 0.3493392765522003, 0.2996070086956024, 0.17624559998512268, 0.27472084760665894, -0.06336664408445358, 0.15201422572135925, 0.2598668038845062, 0.05444326251745224, 0.4420437216758728, -0.1317095160484314, -0.04574429988861084, 0.18022802472114563]
Layer: gate_8 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.38261428475379944, 0.3068861663341522, 0.06004636734724045, 0.6051648855209351, 0.2891939878463745, 0.4224916994571686, 0.6408153772354126, 0.7157241106033325, 0.23938779532909393, 0.09243107587099075, 0.4151017665863037, 0.5117535591125488, 0.7045041918754578, 0.4346332252025604, 0.8653570413589478, 0.6596484780311584]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.682245135307312, 0.7742964625358582, 0.35577911138534546, 0.5991933345794678, 0.6051957607269287, -0.09071896225214005, 0.39173853397369385, 0.6831456422805786, 0.1471756249666214, -0.14054614305496216, 0.40084248781204224, 0.553075909614563, 0.30342355370521545, 0.17462657392024994, 0.1211620420217514, 0.4016338884830475]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.4899531602859497, 0.6948868036270142, 0.8152198195457458, 0.6703598499298096, 0.8357672095298767, 0.5875779986381531, 0.7203316688537598, 0.5922629237174988, 0.6146824955940247, 1.011613368988037, 0.6970405578613281, 0.6123054623603821, 0.4545632600784302, 0.37696224451065063, 0.003627379424870014, 0.7371799349784851]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.33174750208854675, 1.1399060487747192, 0.8480426669120789, 0.6016355752944946, 0.4214884042739868, 0.6818407773971558, 0.5310445427894592, 0.6090468764305115, 0.6578572988510132, 0.7694737315177917, 1.7454856634140015, 0.8035869598388672, 0.9878683686256409, 0.8470847606658936, 0.7568424344062805, 0.8199120163917542]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.6744357347488403, 1.4697340726852417, 1.6880725622177124, 0.7992000579833984, 1.189705729484558, 0.8182925581932068, 1.3534780740737915, 1.2001880407333374, 1.3346446752548218, 1.4261871576309204, 1.2748267650604248, 1.712620496749878, 0.7821707725524902, 1.7311457395553589, 1.4756640195846558, 1.2555029392242432]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [0.8720570206642151, 1.1095706224441528, 1.585187554359436, 1.5490262508392334, 1.2978461980819702, 1.1115992069244385, 0.04592204838991165, 0.4720726013183594, 0.9225818514823914, 0.5983210206031799, -0.07271578162908554, 0.801440417766571, 1.1928609609603882, 1.2318525314331055, 1.5087754726409912, 0.5409997701644897]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.816990852355957, 1.537607192993164, 1.398482084274292, 1.1433733701705933, 1.2781798839569092, 1.4742008447647095, 1.451064944267273, 0.8823363184928894, 0.9633678793907166, 1.1603246927261353, 1.0384601354599, 0.3877928555011749, 1.8185162544250488, 1.0614919662475586, 1.1004002094268799, 0.9890270829200745]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.2700996398925781, 0.7804747223854065, 1.2636713981628418, 1.249353051185608, 0.9591801166534424, 0.824225902557373, -0.3526341915130615, 2.1552884578704834, -0.32335400581359863, 2.0160536766052246, -0.3481322228908539, 1.2302842140197754, 1.5236217975616455, 1.5201056003570557, 1.3503130674362183, -0.06878364831209183]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.116132378578186, 1.8825085163116455, 1.875805139541626, 2.3597357273101807, 2.1747586727142334, 1.846814513206482, 1.754856824874878, 2.088488817214966, 1.8048166036605835, 2.2213973999023438, 2.0455527305603027, 1.6229223012924194, 1.720266342163086, 1.8203113079071045, 1.813212275505066, 2.4333910942077637]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.5153138637542725, 1.2043670415878296, 1.776014804840088, 1.926439642906189, 1.5818006992340088, 1.8679107427597046, 1.4276273250579834, 2.1145334243774414, 2.312614679336548, 1.5835926532745361, 1.6509312391281128, 1.5788347721099854, 1.641708493232727, 1.9854499101638794, 1.775467038154602, 1.6361889839172363]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.3592488765716553, 2.105644941329956, 1.7127354145050049, 1.9388337135314941, 2.1199936866760254, 1.9443496465682983, 2.0809991359710693, 2.09344220161438, 1.9688280820846558, 1.8554434776306152, 2.145277738571167, 1.930668592453003, 3.045541286468506, 2.204740047454834, 2.131364583969116, 1.9610823392868042]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6119709014892578, 1.3877500295639038, 1.3707127571105957, 0.8736653327941895, 0.7356944680213928, 1.4646906852722168, 1.3596559762954712, 1.1968326568603516, 1.0031250715255737, 1.0926733016967773, 1.2747795581817627, 0.9980568885803223, 1.1322413682937622, 1.3618698120117188, 1.4274704456329346, 1.1289923191070557]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.72021484375, 2.7566123008728027, 2.72649884223938, 2.869429349899292, 2.7527623176574707, 2.923285484313965, 2.991126537322998, 2.884944200515747, 2.678434371948242, 2.94256854057312, 2.9258663654327393, 2.769807815551758, 2.77862548828125, 2.756559371948242, 3.1725077629089355, 2.989781618118286]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.9361437559127808, 1.6515742540359497, 1.746093988418579, 1.7596561908721924, 1.9526599645614624, 1.8617089986801147, 1.9503734111785889, 1.6420913934707642, 1.9351481199264526, 2.1764461994171143, 1.5533217191696167, 1.7897167205810547, 1.822632074356079, 1.7930850982666016, 1.9057081937789917, 2.15411376953125]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.495382070541382, 2.1841533184051514, 2.1368913650512695, 1.9953868389129639, 2.1536600589752197, 2.129544496536255, 2.0044116973876953, 2.1924028396606445, 2.090341091156006, 1.8739094734191895, 2.220911741256714, 2.069761037826538, 2.2077035903930664, 1.5331496000289917, 2.30424165725708, 2.171552896499634]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.8099358081817627, 3.10518217086792, 2.8129777908325195, 2.802339553833008, 3.029768466949463, 3.0515542030334473, 2.979670286178589, 2.874311923980713, 3.029071092605591, 3.0345218181610107, 2.8721251487731934, 2.9792096614837646, 2.8388867378234863, 3.1128950119018555, 3.043841600418091, 3.084392786026001]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_25 - Captured router_logits: [2.3140084743499756, 2.207629680633545, 2.1391866207122803, 2.318214178085327, 1.9069234132766724, 2.147130012512207, 2.1589720249176025, 2.165311098098755, 2.2090625762939453, 2.2074966430664062, 2.2273623943328857, 2.242783308029175, 2.2029542922973633, 2.288785219192505, 1.9951732158660889, 2.425595998764038]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.277371644973755, 2.1019928455352783, 2.104248046875, 2.33931565284729, 2.114781141281128, 2.155161142349243, 2.1676900386810303, 2.1102426052093506, 2.0974113941192627, 2.40238618850708, 2.407514810562134, 2.1958236694335938, 2.2144129276275635, 2.2999086380004883, 2.0525360107421875, 2.215320110321045]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.1431455612182617, 2.0361688137054443, 2.0831120014190674, 2.1963043212890625, 2.072732448577881, 2.1057465076446533, 2.1416521072387695, 2.2082974910736084, 2.143784761428833, 2.2081634998321533, 1.8169127702713013, 2.085136890411377, 2.229342460632324, 2.08259916305542, 2.0755858421325684, 2.094294309616089]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Running loglikelihood requests:  46%|███████████████████████████████████████████████████████████████▋                                                                         | 93/200 [02:02<01:54,  1.07s/it]Layer: gate_28 - Captured router_logits: [3.6369025707244873, 3.6380436420440674, 3.904390811920166, 3.7197887897491455, 3.84138560295105, 3.6748569011688232, 3.592569351196289, 3.803925037384033, 3.5894389152526855, 3.744236707687378, 3.647470474243164, 3.4528822898864746, 3.7455978393554688, 3.468376398086548, 3.7049293518066406, 3.6102466583251953]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.74691104888916, 8.172521591186523, 7.864715099334717, 7.77013635635376, 7.593942642211914, 7.373578071594238, 8.115965843200684, 8.014673233032227, 8.034658432006836, 7.754910945892334, 7.7521209716796875, 7.617599010467529, 7.8683319091796875, 7.93533182144165, 8.1688232421875, 7.783363342285156]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.876883506774902, 5.109987735748291, 4.952200412750244, 4.542043209075928, 5.199578285217285, 4.9896392822265625, 4.814418315887451, 4.883445739746094, 5.025272846221924, 4.9967498779296875, 4.935246467590332, 4.450008392333984, 4.87484884262085, 5.209550380706787, 5.058617115020752, 4.7399678230285645]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.4707069396972656, 3.2212820053100586, 3.286188840866089, 3.15478253364563, 3.2608137130737305, 3.3423030376434326, 3.260502576828003, 3.1791090965270996, 3.1989309787750244, 3.307249069213867, 3.412208080291748, 2.63481068611145, 3.2224602699279785, 3.2647831439971924, 3.493464708328247, 3.404062271118164]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09096067398786545, 0.11223848909139633, 0.1088566854596138, -0.25224438309669495, -0.22580096125602722, -0.13797733187675476, 0.1258499175310135, -0.15595939755439758, 0.07842715829610825, 0.0945109874010086, 0.10047294944524765, 0.09864390641450882, 0.09320516884326935, 0.10687084496021271, -1.096106767654419, 0.12216751277446747]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07282968610525131, 0.0479268953204155, 0.0354679673910141, 0.03522424399852753, 0.08411067724227905, 0.017551565542817116, 0.04941239580512047, 0.06390494853258133, 0.009183164685964584, 0.05167890340089798, -0.18800681829452515, 0.049317944794893265, 0.0109666483476758, -0.04739850386977196, 0.010978314094245434, 0.039153456687927246]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0779104232788086, 0.039997175335884094, 0.08758226037025452, 0.06082295626401901, 0.09047623723745346, 0.10878954082727432, 0.06818832457065582, -0.1223471611738205, 0.06257394701242447, 0.0704960897564888, -0.0019451521802693605, 0.08580640703439713, -0.20115770399570465, 0.012813478708267212, -0.04769274592399597, 0.10353187471628189]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1266552060842514, 0.13041721284389496, 0.08537939935922623, 0.10951048880815506, 0.09724292159080505, 0.09665944427251816, 0.011854278855025768, 0.14346419274806976, 0.13355016708374023, -0.5469745993614197, -0.06526390463113785, 0.0682837963104248, 0.1249622255563736, -0.26518726348876953, -0.08065545558929443, -0.14629507064819336]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.03490576893091202, 0.06977614015340805, 0.07253121584653854, 0.0845949724316597, 0.0474957711994648, -0.09199602156877518, 0.019879736006259918, -0.011605420149862766, -0.016356900334358215, 0.05179554596543312, -0.19288350641727448, 0.12653020024299622, -0.06187097355723381, -0.22991237044334412, 0.08039969950914383, 0.010491141118109226]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09538889676332474, 0.13160340487957, 0.02888033166527748, 0.057033829391002655, -0.0018736942438408732, -0.03595380485057831, 0.04057791084051132, 0.0739392414689064, 0.10427186638116837, -0.11117111146450043, -0.088961660861969, 0.1366157829761505, -0.19164668023586273, 0.13846495747566223, 0.14841720461845398, 0.06769317388534546]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.11948080360889435, -0.03941788524389267, 0.0643438994884491, 0.2944183349609375, 0.17093753814697266, 0.08116831630468369, -0.12244180589914322, -0.06840024888515472, 0.1335553377866745, 0.13794761896133423, -0.5527336001396179, 0.20087307691574097, 0.33260244131088257, -0.23037083446979523, 0.13700175285339355, 0.21818837523460388]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.10896717011928558, -0.04317152500152588, 0.0922599658370018, 0.03659258037805557, -1.0216361284255981, -0.19554385542869568, -0.15451274812221527, -0.41317087411880493, -0.06472945958375931, -0.1556650698184967, -0.18889504671096802, -0.028093501925468445, 0.14365723729133606, -0.04804691672325134, -0.47071245312690735, 0.006933594588190317]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.14734914898872375, 0.21528811752796173, -0.11613111197948456, 0.4002978503704071, 0.35013672709465027, 0.27071061730384827, 0.17561423778533936, 0.18005704879760742, 0.060075774788856506, 0.287771999835968, 0.20274581015110016, 0.07730728387832642, 0.4167884886264801, -0.16391780972480774, -0.09255590289831161, 0.20444507896900177]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.35989874601364136, 0.29135850071907043, 0.11882306635379791, 0.757239580154419, 0.3232957124710083, 0.3829382658004761, 0.6067007184028625, 0.6949188113212585, 0.37954673171043396, 0.10630805790424347, 0.3307248651981354, 0.5076600909233093, 0.7031977772712708, 0.442142128944397, 0.7695935368537903, 0.6444944739341736]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7645366191864014, 0.7565196752548218, 0.30795782804489136, 0.6320216655731201, 0.6253408789634705, 0.10289954394102097, 0.38035738468170166, 0.7148230671882629, 0.07859159260988235, -0.07587148249149323, 0.4150002598762512, 0.6349033713340759, 0.15591169893741608, 0.20828498899936676, 0.3406684994697571, 0.363497257232666]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.41765522956848145, 0.6715858578681946, 0.6865817308425903, 0.7491719126701355, 0.7954533100128174, 0.5248249769210815, 0.6509358286857605, 0.47742125391960144, 0.6300941109657288, 1.007407784461975, 0.5798144936561584, 0.520301103591919, 0.3070269227027893, 0.23476572334766388, -0.17365895211696625, 0.6459028124809265]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.2628863751888275, 1.005332350730896, 0.8539254069328308, 0.6013741493225098, 0.3211638629436493, 0.6256437301635742, 0.5792527198791504, 0.6547040343284607, 0.6300351023674011, 0.6941903233528137, 1.6129194498062134, 0.7297908663749695, 0.9317662715911865, 0.7477203011512756, 0.5973647832870483, 0.7156633734703064]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.4286855459213257, 1.2641088962554932, 1.5837560892105103, 0.7097105383872986, 1.0691264867782593, 0.6729384064674377, 1.1992632150650024, 1.347652554512024, 1.1551377773284912, 1.264930009841919, 1.1807403564453125, 1.5285165309906006, 0.6597896218299866, 1.56161630153656, 1.2944540977478027, 1.0748881101608276]
Running loglikelihood requests:  48%|██████████████████████████████████████████████████████████████████▍                                                                      | 97/200 [02:06<01:49,  1.06s/it]Layer: gate_13 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7866945862770081, 0.9233635663986206, 1.4191957712173462, 1.4176697731018066, 1.1684269905090332, 1.3530288934707642, 0.19179563224315643, 0.7159029841423035, 0.8189747333526611, 0.5650798678398132, -0.046016011387109756, 0.6811319589614868, 1.1433427333831787, 1.1347745656967163, 1.3607940673828125, 0.5280947089195251]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.7742157578468323, 1.5076487064361572, 1.3587862253189087, 1.0180200338363647, 1.2148246765136719, 1.3783159255981445, 1.4910563230514526, 0.7894393801689148, 0.9757304191589355, 1.0969961881637573, 0.9914036989212036, 0.39355960488319397, 1.8326365947723389, 0.9609522819519043, 0.9746931791305542, 1.1022745370864868]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.176693081855774, 0.746757447719574, 1.2408498525619507, 1.2915763854980469, 0.9311771988868713, 0.7814545631408691, -0.10259431600570679, 2.1027731895446777, -0.3336409330368042, 2.1270785331726074, -0.2750217914581299, 1.2803997993469238, 1.4307349920272827, 1.3696587085723877, 1.282634973526001, 0.04161465913057327]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.1322890520095825, 1.7509973049163818, 1.8396507501602173, 2.253418207168579, 2.1978001594543457, 1.8323150873184204, 1.71066415309906, 2.0667903423309326, 1.8796465396881104, 2.0647544860839844, 2.149188756942749, 1.7437224388122559, 1.6212462186813354, 1.8411442041397095, 1.6896244287490845, 2.331096887588501]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.5878896713256836, 1.3019038438796997, 1.7296981811523438, 2.090172290802002, 1.471737265586853, 1.8485409021377563, 1.5208532810211182, 2.1681067943573, 2.3727171421051025, 1.6148784160614014, 1.652944803237915, 1.5489290952682495, 1.6207150220870972, 1.9382692575454712, 1.7343391180038452, 1.6457390785217285]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.1274008750915527, 1.9565925598144531, 1.6753007173538208, 1.7730194330215454, 1.974643588066101, 1.73616623878479, 1.8593460321426392, 1.9164646863937378, 1.9483745098114014, 1.6983660459518433, 2.052818536758423, 1.8421069383621216, 2.8585331439971924, 2.010181188583374, 1.9485448598861694, 1.8216677904129028]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6618849635124207, 1.3055436611175537, 1.2472950220108032, 0.7962747812271118, 0.5740124583244324, 1.4572113752365112, 1.2772704362869263, 1.2049490213394165, 0.8664332032203674, 1.1103589534759521, 1.2435083389282227, 0.9521870017051697, 1.053360104560852, 1.3122453689575195, 1.3392962217330933, 0.9406976103782654]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.596726417541504, 2.6734888553619385, 2.6034910678863525, 2.7407376766204834, 2.6388914585113525, 2.7922120094299316, 2.801934003829956, 2.8621857166290283, 2.593883991241455, 2.832425832748413, 2.8279290199279785, 2.7838239669799805, 2.6885879039764404, 2.708906888961792, 3.0854201316833496, 2.8728671073913574]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.8763045072555542, 1.653777003288269, 1.8505303859710693, 1.6999009847640991, 1.9202258586883545, 1.8171303272247314, 1.900580883026123, 1.6489981412887573, 1.8727407455444336, 2.1279513835906982, 1.5281519889831543, 1.844099521636963, 1.7785509824752808, 1.761518955230713, 1.8194643259048462, 2.0913186073303223]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5076985359191895, 2.144613265991211, 1.9931285381317139, 1.8718466758728027, 2.149233341217041, 2.028589963912964, 1.925175666809082, 2.1008753776550293, 2.008007526397705, 1.7985122203826904, 2.099738121032715, 1.8986784219741821, 2.096404790878296, 1.477723240852356, 2.233718156814575, 2.0683975219726562]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.7563438415527344, 3.0288989543914795, 2.7941672801971436, 2.737079381942749, 3.0687150955200195, 2.9736738204956055, 2.9847583770751953, 2.823618173599243, 2.943998336791992, 2.9354043006896973, 2.8887486457824707, 2.916775941848755, 2.8428337574005127, 3.0323503017425537, 2.954768419265747, 3.0013515949249268]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.232532501220703, 2.131544828414917, 2.0430078506469727, 2.228111982345581, 1.812018871307373, 2.064802408218384, 2.0706396102905273, 2.150951623916626, 2.115492582321167, 2.1129133701324463, 2.1479945182800293, 2.1093783378601074, 2.1210358142852783, 2.143505811691284, 1.9958263635635376, 2.33392596244812]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.217646837234497, 2.051734209060669, 2.018599033355713, 2.24919056892395, 2.059900999069214, 2.098284959793091, 2.0976157188415527, 2.069392681121826, 2.034971237182617, 2.2766270637512207, 2.4236292839050293, 2.1231529712677, 2.137545108795166, 2.2296864986419678, 2.000197410583496, 2.1284139156341553]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.099695920944214, 2.0344889163970947, 2.074049711227417, 2.182727336883545, 2.076791286468506, 2.112738847732544, 2.1311044692993164, 2.1718122959136963, 2.0931224822998047, 2.206254005432129, 1.8216874599456787, 2.089012384414673, 2.2109110355377197, 2.1413490772247314, 2.0443897247314453, 2.049497604370117]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.545301675796509, 3.5635311603546143, 3.7849960327148438, 3.5999040603637695, 3.7098727226257324, 3.5492594242095947, 3.573524236679077, 3.686521053314209, 3.4641401767730713, 3.6500244140625, 3.5422816276550293, 3.3603241443634033, 3.622016668319702, 3.372783660888672, 3.6876726150512695, 3.4810163974761963]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.633438587188721, 8.074112892150879, 7.786057949066162, 7.660405158996582, 7.544607162475586, 7.375934600830078, 7.94298791885376, 7.9264044761657715, 7.9480671882629395, 7.6720290184021, 7.651813507080078, 7.477049827575684, 7.729522228240967, 7.856110095977783, 8.061397552490234, 7.674846649169922]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.499325752258301, 4.718723297119141, 4.531625747680664, 4.222564697265625, 4.728418350219727, 4.536886215209961, 4.386409759521484, 4.4760050773620605, 4.577907085418701, 4.576422214508057, 4.558601379394531, 4.117855072021484, 4.480299472808838, 4.716060161590576, 4.652734756469727, 4.377081394195557]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.291212797164917, 3.042780637741089, 3.1023592948913574, 2.81166934967041, 3.0751733779907227, 3.175513505935669, 2.9887313842773438, 2.9749631881713867, 3.003699779510498, 3.160147190093994, 3.23869252204895, 2.5780794620513916, 3.0618176460266113, 3.088405132293701, 3.3063840866088867, 3.271822690963745]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.09636781364679337, 0.11055164784193039, 0.10937787592411041, -0.2601354718208313, -0.20569981634616852, -0.14689868688583374, 0.12330851703882217, -0.1559450924396515, 0.09725117683410645, 0.09093689173460007, 0.09695712476968765, 0.09592530131340027, 0.09639294445514679, 0.11474665999412537, -1.023003339767456, 0.12617868185043335]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.08484741300344467, 0.039487890899181366, 0.03388098627328873, 0.04826316982507706, 0.0730813592672348, 0.03134280443191528, 0.03753633797168732, 0.06955166161060333, 0.031323086470365524, 0.0467216819524765, -0.2047959715127945, 0.04959041625261307, 0.02221430279314518, -0.03198873996734619, 0.0013811606913805008, 0.010328995063900948]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.050696030259132385, 0.027375590056180954, 0.08824381977319717, 0.06271883100271225, 0.1116587221622467, 0.08672823756933212, 0.07335615158081055, -0.11775868386030197, 0.0791073590517044, 0.06629601120948792, 0.002252854872494936, 0.0893414318561554, -0.18319207429885864, 0.016692761331796646, -0.02228000946342945, 0.1141805425286293]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.1093703880906105, 0.12121262401342392, 0.08694460242986679, 0.13457825779914856, 0.08626489341259003, 0.1264137476682663, 0.007805740460753441, 0.1483761966228485, 0.12336598336696625, -0.5308337807655334, -0.08656264841556549, 0.10600753873586655, 0.15289941430091858, -0.27229005098342896, -0.09304765611886978, -0.13409732282161713]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.03672610968351364, 0.05065715312957764, 0.07642978429794312, 0.11031440645456314, 0.056796845048666, -0.09693308919668198, 0.03143986687064171, -0.008715251460671425, -0.03377215936779976, 0.03577341139316559, -0.19127698242664337, 0.08317671716213226, -0.08481799066066742, -0.20095092058181763, 0.09395018965005875, 0.035438716411590576]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.10306484997272491, 0.13851360976696014, 0.04208570718765259, 0.07127493619918823, -0.012399730272591114, -0.040352798998355865, 0.03521643206477165, 0.10279478877782822, 0.09946804493665695, -0.09873707592487335, -0.1354500651359558, 0.13500921428203583, -0.21342696249485016, 0.13714392483234406, 0.1548515111207962, 0.06765080243349075]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.1390378326177597, 0.024940483272075653, 0.022664276883006096, 0.26523399353027344, 0.1448258012533188, 0.09518381208181381, -0.0812450498342514, -0.04626915976405144, 0.051549624651670456, 0.14709359407424927, -0.5315007567405701, 0.22473566234111786, 0.344888836145401, -0.23975123465061188, 0.15205316245555878, 0.2319757640361786]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.11239597201347351, 0.0038537560030817986, 0.11653228849172592, -0.02646690048277378, -0.9284960031509399, -0.19458051025867462, -0.09976263344287872, -0.31051814556121826, 0.027182823047041893, -0.1742093563079834, -0.1952916830778122, -0.0027829480823129416, 0.03438194841146469, 0.030708713456988335, -0.5583596229553223, -0.10100076347589493]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.14488129317760468, 0.2573243975639343, -0.08029720187187195, 0.40144580602645874, 0.21630483865737915, 0.35593461990356445, 0.13246724009513855, 0.18812869489192963, 0.014130182564258575, 0.42751172184944153, 0.28050336241722107, 0.040674060583114624, 0.49486443400382996, -0.26895275712013245, -0.13235870003700256, 0.1977977603673935]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.38856041431427, 0.2316131293773651, 0.1644386500120163, 0.7425240278244019, 0.28928178548812866, 0.3381679356098175, 0.6057965159416199, 0.6802448034286499, 0.22287455201148987, 0.08193109184503555, 0.18343865871429443, 0.4671447277069092, 0.5410007238388062, 0.29408520460128784, 0.8182272911071777, 0.6127892136573792]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7401906847953796, 0.7605223059654236, 0.30579087138175964, 0.573541522026062, 0.5803673267364502, -0.12985506653785706, 0.41477763652801514, 0.6706479787826538, -0.03959426283836365, -0.16926155984401703, 0.475612074136734, 0.5129870772361755, 0.20619766414165497, 0.2648511826992035, 0.2989548444747925, 0.4009593725204468]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.3814633786678314, 0.5856416821479797, 0.6883690357208252, 0.48527002334594727, 0.7022495865821838, 0.33519303798675537, 0.7613986730575562, 0.3703058958053589, 0.524330198764801, 0.9249723553657532, 0.5345070958137512, 0.3550395965576172, 0.2492077350616455, 0.2556605935096741, -0.18338240683078766, 0.5870418548583984]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.21374598145484924, 0.939940333366394, 0.8208770751953125, 0.603955090045929, 0.2828463613986969, 0.48550984263420105, 0.5507698059082031, 0.6591050624847412, 0.4719558656215668, 0.6481860876083374, 1.5590121746063232, 0.7787619233131409, 0.9071225523948669, 0.706511378288269, 0.7557317614555359, 0.7644571661949158]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.151934027671814, 1.0778392553329468, 1.3417872190475464, 0.4841747581958771, 1.0145553350448608, 0.3920361399650574, 1.0034544467926025, 1.0752016305923462, 1.0284358263015747, 0.9961153864860535, 1.0692676305770874, 1.4475339651107788, 0.39633673429489136, 1.28403902053833, 0.9497154355049133, 0.7848488688468933]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5930970311164856, 0.7583180069923401, 1.2223737239837646, 1.1722259521484375, 1.125234603881836, 1.0149098634719849, 0.058678947389125824, 0.9299392700195312, 0.7339692115783691, 0.6111305952072144, -0.3802730143070221, 0.5769273638725281, 1.1270469427108765, 1.074790120124817, 1.2092463970184326, 0.41342028975486755]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6064059138298035, 1.413834571838379, 1.1519458293914795, 0.9730914831161499, 1.0134333372116089, 1.227718472480774, 1.416296362876892, 0.7464520335197449, 0.9128134846687317, 0.9754952192306519, 0.8552037477493286, 0.36871445178985596, 1.6799445152282715, 0.9071446657180786, 0.8848444819450378, 1.0557515621185303]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.1998087167739868, 0.6862053275108337, 1.075783133506775, 1.1087050437927246, 0.97153639793396, 0.8497680425643921, -0.06485655158758163, 1.7523887157440186, -0.0888320803642273, 2.1183128356933594, -0.2794637680053711, 1.198879361152649, 1.3755443096160889, 1.2835168838500977, 1.2229357957839966, 0.16164252161979675]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  50%|████████████████████████████████████████████████████████████████████▋                                                                   | 101/200 [02:10<01:44,  1.05s/it]Layer: gate_17 - Captured router_logits: [1.124023199081421, 1.8360010385513306, 1.9408977031707764, 2.2354681491851807, 2.146409511566162, 1.8611515760421753, 1.7748287916183472, 2.0054595470428467, 2.0178635120391846, 2.0551528930664062, 2.3848724365234375, 1.650310754776001, 1.4831010103225708, 1.6678874492645264, 1.611638069152832, 2.3114638328552246]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.557085394859314, 1.1621571779251099, 1.653959035873413, 2.021322727203369, 1.4672924280166626, 1.6636499166488647, 1.6158521175384521, 2.100358486175537, 2.6163156032562256, 1.5618505477905273, 1.5941814184188843, 1.4811185598373413, 1.652599573135376, 1.712225317955017, 1.7307676076889038, 1.6060038805007935]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.2612571716308594, 2.019894599914551, 1.8330010175704956, 1.9652334451675415, 2.1209604740142822, 1.8939616680145264, 2.1018035411834717, 2.07568097114563, 2.1681160926818848, 1.768825888633728, 2.1949310302734375, 1.9659810066223145, 2.822986364364624, 2.173168897628784, 2.112614154815674, 1.9976096153259277]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8154525756835938, 1.2790497541427612, 1.2647485733032227, 0.8366324305534363, 0.48544010519981384, 1.5935535430908203, 1.2555500268936157, 1.404105305671692, 0.8178040981292725, 1.1712021827697754, 1.181361436843872, 1.0190317630767822, 1.2113914489746094, 1.3393994569778442, 1.3222315311431885, 1.1025298833847046]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.5374603271484375, 2.66323184967041, 2.6038384437561035, 2.666808843612671, 2.57891583442688, 2.733081102371216, 2.798100233078003, 2.7061514854431152, 2.584052801132202, 2.814326524734497, 2.7766382694244385, 2.7378478050231934, 2.5889954566955566, 2.7032392024993896, 2.867129325866699, 2.768406629562378]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.9305983781814575, 1.6494733095169067, 1.8957105875015259, 1.569930076599121, 1.9633513689041138, 1.817157506942749, 1.8551018238067627, 1.6816720962524414, 1.8352097272872925, 2.041780948638916, 1.6596511602401733, 1.9044814109802246, 1.8286151885986328, 1.778096318244934, 1.8254958391189575, 2.0928139686584473]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.4346635341644287, 2.0509583950042725, 1.8325551748275757, 1.8623576164245605, 1.9479233026504517, 1.8592652082443237, 1.8374018669128418, 1.9717426300048828, 1.871959924697876, 1.8340578079223633, 1.938112735748291, 1.8428462743759155, 1.903823733329773, 1.2425802946090698, 1.883383870124817, 2.029388904571533]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.8700428009033203, 3.059565305709839, 2.8061251640319824, 2.7163002490997314, 3.080925226211548, 2.9066333770751953, 2.9834885597229004, 2.8341543674468994, 2.921290636062622, 3.0304906368255615, 2.983797788619995, 2.9439404010772705, 2.796414613723755, 3.05147123336792, 2.9663360118865967, 3.022336006164551]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.082946300506592, 2.0324366092681885, 2.0114705562591553, 2.2644901275634766, 1.7391177415847778, 2.0542399883270264, 2.0639610290527344, 2.185929536819458, 2.1302709579467773, 2.0722527503967285, 2.122077226638794, 2.0866782665252686, 2.1433775424957275, 2.1498568058013916, 2.052347183227539, 2.2802271842956543]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.164429187774658, 2.048168897628784, 2.0708563327789307, 2.2190542221069336, 2.0405263900756836, 2.1189420223236084, 2.113736391067505, 2.0491738319396973, 2.0194520950317383, 2.25462007522583, 2.4563052654266357, 2.09047532081604, 2.1603150367736816, 2.2699170112609863, 2.0574727058410645, 2.1390552520751953]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.0542993545532227, 1.966800332069397, 2.0171666145324707, 2.098344326019287, 1.9438947439193726, 2.029653787612915, 2.090333938598633, 2.0838592052459717, 2.0558109283447266, 2.0976617336273193, 1.7301603555679321, 1.9419336318969727, 2.1600887775421143, 2.095731258392334, 1.989530324935913, 2.0167996883392334]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.3928730487823486, 3.4786601066589355, 3.7218198776245117, 3.496281147003174, 3.571692705154419, 3.415140390396118, 3.544008255004883, 3.586305618286133, 3.376722574234009, 3.5176968574523926, 3.4605743885040283, 3.2534804344177246, 3.465803623199463, 3.289252519607544, 3.4852840900421143, 3.4032771587371826]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.106906890869141, 7.6254754066467285, 7.261056423187256, 7.1375322341918945, 7.018581867218018, 6.840697288513184, 7.39141321182251, 7.314030647277832, 7.384413242340088, 7.088306427001953, 7.167544841766357, 6.999139308929443, 7.201131820678711, 7.289758205413818, 7.551225662231445, 7.1873273849487305]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.548043251037598, 4.646897315979004, 4.474506378173828, 4.180303573608398, 4.779666900634766, 4.607288837432861, 4.313360691070557, 4.532194137573242, 4.617056846618652, 4.554372310638428, 4.480931282043457, 3.968733787536621, 4.337653160095215, 4.740137577056885, 4.558544635772705, 4.365513801574707]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.092832565307617, 2.927262783050537, 2.9420382976531982, 2.792375087738037, 2.892857789993286, 2.9867777824401855, 2.899277687072754, 2.821847677230835, 2.9083189964294434, 2.9029364585876465, 3.2050652503967285, 2.340023994445801, 2.911983013153076, 2.928295850753784, 3.1746695041656494, 2.987903356552124]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06983652710914612, 0.08385252207517624, 0.10189712047576904, -0.2672636806964874, -0.2614001929759979, -0.09229305386543274, 0.10339982807636261, -0.11210408806800842, 0.06777315586805344, 0.06770198792219162, 0.07780016958713531, 0.03552175685763359, 0.08026032894849777, 0.10223355889320374, -1.052037000656128, 0.10199599713087082]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07766910642385483, 0.039888422936201096, 0.03201013058423996, 0.04145136848092079, 0.06446497142314911, 0.02667168527841568, 0.06220702454447746, 0.07606129348278046, 0.06547889113426208, 0.04842691123485565, -0.19639776647090912, 0.04752872511744499, 0.011456189677119255, -0.025179624557495117, 0.03663820028305054, 0.027021491900086403]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06879190355539322, 0.02114461548626423, 0.09104876965284348, 0.0651310458779335, 0.12900306284427643, 0.10080553591251373, 0.06538087874650955, -0.12007995694875717, 0.106340691447258, 0.09735031425952911, 0.028416026383638382, 0.09841743856668472, -0.21707122027873993, 0.01226474903523922, -0.02568032406270504, 0.1112692579627037]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1406717151403427, 0.11527799814939499, 0.08125969022512436, 0.10795687884092331, 0.10200875997543335, 0.1069164127111435, -0.05092693492770195, 0.16135463118553162, 0.13945262134075165, -0.6408226490020752, -0.015850022435188293, 0.06511331349611282, 0.22718018293380737, -0.3591040074825287, 0.015458764508366585, -0.1494995504617691]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.02737194299697876, 0.08031805604696274, 0.058852918446063995, 0.17094986140727997, 0.07413238286972046, -0.05721265450119972, 0.006475175265222788, -0.02923978865146637, -0.06597419083118439, 0.031979598104953766, -0.277191162109375, 0.08266059309244156, 0.028525754809379578, -0.24986524879932404, 0.10706982016563416, -0.0798051506280899]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.0724794790148735, 0.14420773088932037, 0.04660283401608467, 0.16078977286815643, -0.09458307176828384, -0.10166008770465851, 0.08246207237243652, 0.042093075811862946, 0.1836719661951065, -0.09700970351696014, -0.06407607346773148, 0.15296974778175354, -0.2667299509048462, 0.19455547630786896, 0.15739497542381287, 0.09858980774879456]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.1903504729270935, -0.047656264156103134, 0.07302486151456833, 0.2934887707233429, 0.16072744131088257, 0.12235631793737411, -0.16703912615776062, -0.10853712260723114, 0.006395020987838507, 0.20261847972869873, -0.670829176902771, 0.21923808753490448, 0.35930055379867554, -0.4193364679813385, 0.23183391988277435, 0.23108291625976562]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [0.006126866675913334, -0.06184128671884537, 0.04573005065321922, 0.01071015652269125, -1.1372219324111938, -0.10868976265192032, -0.04266731068491936, -0.6768830418586731, 0.15168578922748566, -0.13835065066814423, -0.41465431451797485, 0.01988261006772518, -0.10897985100746155, 0.11172940582036972, -0.8142616748809814, -0.2213207632303238]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20650111138820648, 0.2601635456085205, -0.23185423016548157, 0.4732797145843506, 0.34894445538520813, 0.35879459977149963, 0.14141713082790375, 0.1859501600265503, -0.028476418927311897, 0.4282344579696655, 0.3259600102901459, 0.025190457701683044, 0.5329961180686951, -0.33346518874168396, -0.2891433537006378, 0.22792813181877136]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.32595348358154297, 0.18635417520999908, 0.17661306262016296, 0.8859667181968689, 0.2562680244445801, 0.19772601127624512, 0.5393677353858948, 0.6995338797569275, 0.29704782366752625, -0.007560447324067354, 0.0871482715010643, 0.5011836886405945, 0.5305087566375732, 0.06563527882099152, 0.8527655601501465, 0.6320105791091919]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9021776914596558, 0.922978401184082, 0.3195264935493469, 0.5915447473526001, 0.5476435422897339, -0.18004265427589417, 0.4776177406311035, 0.7533680200576782, -0.15340816974639893, -0.3963674008846283, 0.5519763827323914, 0.6105021834373474, 0.11886563152074814, 0.15389776229858398, 0.40582093596458435, 0.33455511927604675]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.34788817167282104, 0.6466572880744934, 0.6535852551460266, 0.34259796142578125, 0.7648215293884277, 0.015234892256557941, 0.610854983329773, 0.1916787028312683, 0.4936114549636841, 1.0703219175338745, 0.47255510091781616, 0.302450954914093, -0.11899293214082718, -0.006819374393671751, -0.478242963552475, 0.4025389552116394]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [-0.11590755730867386, 0.77683424949646, 0.8963001370429993, 0.5007847547531128, 0.011871042661368847, 0.2956181466579437, 0.532184898853302, 0.7105516791343689, 0.2687824070453644, 0.4452663064002991, 1.5424991846084595, 0.6891292333602905, 0.6573769450187683, 0.694264829158783, 0.5566279292106628, 0.7061715722084045]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8273351192474365, 1.013856291770935, 1.2245711088180542, 0.19460918009281158, 0.9147441983222961, -0.02458338625729084, 0.9351249933242798, 1.184836506843567, 0.8648097515106201, 0.8919386267662048, 0.9717134237289429, 1.3397058248519897, 0.002003437140956521, 0.8942023515701294, 0.8713520169258118, 0.4736758768558502]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5134779214859009, 0.6689032912254333, 1.2729424238204956, 1.074776530265808, 1.0642979145050049, 1.0296846628189087, -0.24966219067573547, 0.9735427498817444, 0.7092729806900024, 0.5132013559341431, -0.6137681603431702, 0.47137606143951416, 0.985048234462738, 1.1036134958267212, 1.2829887866973877, 0.20075178146362305]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.3446039855480194, 1.4774293899536133, 1.0397388935089111, 0.9541003108024597, 0.9475529193878174, 1.0771620273590088, 1.4759082794189453, 0.6549109816551208, 0.6995202898979187, 0.9750358462333679, 0.682976484298706, -0.08376515656709671, 1.6944804191589355, 0.8041612505912781, 0.7289364337921143, 0.934501588344574]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.97844398021698, 0.31661292910575867, 1.010825276374817, 1.2013412714004517, 0.8714150786399841, 0.6007557511329651, -0.3200322985649109, 1.8057551383972168, -0.6642089486122131, 2.1977765560150146, -0.6493055820465088, 0.9324621558189392, 1.125859260559082, 1.2477359771728516, 1.1122230291366577, -0.1470635086297989]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.5746486186981201, 1.4408596754074097, 1.7917954921722412, 1.9687318801879883, 2.008415460586548, 1.7033926248550415, 1.603083848953247, 1.7709153890609741, 1.947566032409668, 1.620245337486267, 2.278282642364502, 1.621264934539795, 1.5659925937652588, 1.6920009851455688, 1.104705572128296, 2.0021538734436035]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5989108085632324, 1.1766624450683594, 1.7762075662612915, 2.159881830215454, 1.28304922580719, 1.4767804145812988, 1.7226619720458984, 2.0448131561279297, 2.6819705963134766, 1.522337555885315, 1.4870247840881348, 1.1992331743240356, 1.44577956199646, 1.52072012424469, 1.6124688386917114, 1.5999900102615356]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.9103316068649292, 1.7518527507781982, 1.574601411819458, 1.6659934520721436, 1.9154547452926636, 1.4679027795791626, 1.7506576776504517, 1.6726806163787842, 1.9857268333435059, 1.6233669519424438, 1.9556646347045898, 1.6196621656417847, 2.5196876525878906, 1.921241283416748, 1.8531781435012817, 1.7370473146438599]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.7874950170516968, 1.2127846479415894, 1.1660836935043335, 0.8487392663955688, 0.2242569923400879, 1.5874963998794556, 1.1682829856872559, 1.4104098081588745, 0.8195728659629822, 1.21878981590271, 1.1875033378601074, 1.1476761102676392, 1.1939287185668945, 1.2913659811019897, 1.180322289466858, 0.8291545510292053]
Running loglikelihood requests:  52%|███████████████████████████████████████████████████████████████████████▍                                                                | 105/200 [02:14<01:37,  1.03s/it]Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.4836320877075195, 2.615957021713257, 2.5332300662994385, 2.497468948364258, 2.487149238586426, 2.6908063888549805, 2.62441086769104, 2.5929808616638184, 2.4051473140716553, 2.6635897159576416, 2.7224442958831787, 2.71579909324646, 2.3032875061035156, 2.625941038131714, 2.7091598510742188, 2.6523523330688477]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.7049174308776855, 1.601806879043579, 1.8738856315612793, 1.4333438873291016, 1.735338807106018, 1.7307623624801636, 1.6738853454589844, 1.5176366567611694, 1.713124394416809, 1.9036320447921753, 1.5742778778076172, 1.8022382259368896, 1.6639949083328247, 1.6497201919555664, 1.5882716178894043, 1.9426336288452148]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.554901599884033, 2.0853934288024902, 1.6594702005386353, 1.8290740251541138, 2.004804849624634, 1.8689953088760376, 1.7966065406799316, 1.8739323616027832, 1.914890170097351, 1.7291673421859741, 1.9372535943984985, 1.9055250883102417, 1.8096013069152832, 1.054945945739746, 1.863480806350708, 1.9284279346466064]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.8329074382781982, 2.969470500946045, 2.7524056434631348, 2.5730671882629395, 2.8798158168792725, 2.680889129638672, 2.8463127613067627, 2.5707907676696777, 2.6707170009613037, 2.7854537963867188, 2.95915150642395, 2.679234027862549, 2.7128827571868896, 2.944108009338379, 2.788377046585083, 2.860955238342285]
Layer: gate_24 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.0779013633728027, 2.022249937057495, 2.003631114959717, 2.2220587730407715, 1.6391940116882324, 2.0046119689941406, 2.0519087314605713, 2.2406272888183594, 2.1039540767669678, 2.00807523727417, 2.112372875213623, 1.9943479299545288, 2.026048421859741, 2.022475481033325, 2.1467697620391846, 2.287369966506958]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.1211812496185303, 2.110630512237549, 2.067326784133911, 2.180237054824829, 2.02813982963562, 2.1737008094787598, 2.055264472961426, 1.9743340015411377, 2.058840036392212, 2.2604820728302, 2.585437059402466, 2.075094223022461, 2.1105737686157227, 2.234267234802246, 2.109109401702881, 2.194912910461426]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.1150596141815186, 1.9931007623672485, 1.9985473155975342, 2.099493980407715, 1.9378082752227783, 2.001521587371826, 2.1253085136413574, 2.142535448074341, 2.0213236808776855, 2.112891674041748, 1.642870545387268, 2.0098280906677246, 2.252575159072876, 2.265352725982666, 1.9918698072433472, 2.0077760219573975]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [3.3073911666870117, 3.386706590652466, 3.673112392425537, 3.347092866897583, 3.559443473815918, 3.3218066692352295, 3.5250437259674072, 3.5028162002563477, 3.2657570838928223, 3.3757967948913574, 3.3209354877471924, 3.0721867084503174, 3.336622476577759, 3.127836227416992, 3.491051435470581, 3.2776777744293213]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [6.9639716148376465, 7.492123126983643, 7.135365009307861, 6.8680500984191895, 6.801417827606201, 6.663006782531738, 7.183382511138916, 7.19019079208374, 7.252697944641113, 6.928092956542969, 6.944953441619873, 6.907663345336914, 7.051248550415039, 7.144050121307373, 7.361728191375732, 7.013795375823975]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.406466484069824, 4.474506378173828, 4.2414445877075195, 3.9076924324035645, 4.616533279418945, 4.377607822418213, 4.026525974273682, 4.376506328582764, 4.511099338531494, 4.3355183601379395, 4.260287761688232, 3.6341919898986816, 4.070736885070801, 4.591450214385986, 4.393285274505615, 4.183564186096191]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.944915294647217, 2.84356951713562, 2.876645565032959, 2.7612156867980957, 2.8045830726623535, 2.8655083179473877, 2.8471171855926514, 2.6312246322631836, 2.7502849102020264, 2.7139720916748047, 3.2118215560913086, 2.090646743774414, 2.7465298175811768, 2.7183241844177246, 3.10805606842041, 2.857480049133301]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.08761651813983917, 0.09113419055938721, 0.09910338371992111, -0.1952696144580841, -0.15994824469089508, -0.07487177103757858, 0.11628422886133194, -0.20329974591732025, 0.1104569360613823, 0.08145613223314285, 0.08715479075908661, 0.07629332691431046, 0.08241313695907593, 0.1024850606918335, -0.9238176345825195, 0.10612586885690689]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08919019252061844, 0.036402057856321335, 0.018618078902363777, 0.05136095732450485, 0.06279788911342621, 0.04603973776102066, 0.0391022153198719, 0.04666626453399658, -0.008006195537745953, 0.04518377035856247, -0.14184236526489258, 0.04731460288167, 0.012610755860805511, -0.0041002086363732815, -0.003965495619922876, -0.0045127407647669315]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.039111968129873276, 0.04740089178085327, 0.1063765212893486, 0.04414815455675125, 0.08430825918912888, 0.08414696902036667, 0.03702697157859802, -0.10048823803663254, 0.06689666956663132, 0.04638824611902237, -0.008487354032695293, 0.08387548476457596, -0.12795120477676392, -0.003179565304890275, -0.06150323152542114, 0.0989871621131897]
Layer: gate_2 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.10545401275157928, 0.1173449382185936, 0.09730906784534454, 0.13617700338363647, 0.1041531041264534, 0.09626535326242447, -0.032046400010585785, 0.1326739639043808, 0.12155167013406754, -0.51237553358078, -0.03213035315275192, 0.11106208711862564, 0.06639917194843292, -0.2913288474082947, -0.0980115756392479, -0.08066050708293915]
Layer: gate_3 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0687522441148758, 0.0793381929397583, 0.08213243633508682, 0.032171979546546936, 0.0728008821606636, -0.10329277813434601, 0.03215688467025757, 0.00025016869767569005, 0.019162308424711227, 0.015288262628018856, -0.18965290486812592, 0.11212005466222763, -0.16105076670646667, -0.1997113972902298, 0.09559150785207748, -0.04324670881032944]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.11933091282844543, 0.14287902414798737, 0.07027182728052139, 0.025805678218603134, -0.0032398218754678965, -0.051470573991537094, 0.03628532215952873, 0.12559516727924347, 0.015026413835585117, -0.064938485622406, -0.14109019935131073, 0.14905978739261627, -0.1924998164176941, 0.14445504546165466, 0.17806479334831238, 0.07763457298278809]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.038322631269693375, -0.3020303547382355, -0.1977333128452301, 0.07845159620046616, 0.018775342032313347, -0.09631554782390594, -0.29720792174339294, -0.13092951476573944, 0.07230497151613235, 0.041482310742139816, -0.4068935513496399, 0.14844001829624176, 0.3510432541370392, -0.2164730727672577, -0.0021955808624625206, 0.01756024919450283]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.12891067564487457, 0.06096480041742325, 0.15622539818286896, -0.07382679730653763, -0.724128782749176, 0.19264592230319977, -0.22220692038536072, -0.11394023150205612, -0.08734171092510223, -0.029489796608686447, -0.3061997592449188, 0.04147053137421608, 0.048730943351984024, 0.03663253039121628, -0.4626111090183258, 0.08523034304380417]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20468896627426147, 0.17861579358577728, -0.18812403082847595, 0.18931369483470917, 0.13693372905254364, 0.09250951558351517, 0.19879265129566193, 0.18843404948711395, -0.1308760643005371, 0.26329925656318665, 0.03821418061852455, 0.16655780375003815, 0.4353032410144806, -0.4405556321144104, -0.4352720379829407, 0.2881681025028229]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.10404928773641586, 0.05199826881289482, 0.09731805324554443, 0.42446333169937134, 0.09087073802947998, 0.14756719768047333, 0.3826465308666229, 0.4995109736919403, 0.26007428765296936, -0.029559249058365822, 0.053953710943460464, 0.374729186296463, 0.5313571691513062, 0.16446326673030853, 0.668868899345398, 0.43070533871650696]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [1.1351661682128906, 0.8465343117713928, 0.4530846178531647, 0.6843117475509644, 0.5886884331703186, 0.22122599184513092, 0.5078663229942322, 0.8308126330375671, 0.05816611647605896, 0.23382379114627838, 0.5009451508522034, 0.6295614838600159, 0.24375499784946442, 0.15952876210212708, 0.308727502822876, 0.3020021915435791]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.28603002429008484, 0.38593894243240356, 0.6545336842536926, 0.06269967555999756, 0.4609980285167694, -0.7921848893165588, 0.5860952734947205, 0.18317706882953644, 0.5937066674232483, 0.5883716344833374, 0.4872732162475586, -0.24992559850215912, -0.7173261642456055, -0.6196189522743225, -0.9059252142906189, 0.19446532428264618]
Layer: gate_11 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [-0.5192103385925293, 0.34196195006370544, 0.3997715413570404, 0.4158933758735657, -0.7537126541137695, 0.24002021551132202, 0.2972899079322815, 0.15057338774204254, -0.23368768393993378, -0.039879344403743744, 1.1941627264022827, 0.5270809531211853, 0.4309176504611969, 0.5938585996627808, 0.26691877841949463, 0.4054318964481354]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [-0.0968291386961937, 1.0319825410842896, 0.3052748441696167, -0.5955464839935303, 0.5852646231651306, -1.0683655738830566, 0.9874735474586487, 0.3385900557041168, 0.5592901110649109, 0.3989782929420471, 0.39148402214050293, 1.042276382446289, -0.9180353879928589, 0.29037943482398987, 1.0550042390823364, 0.21598459780216217]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_14 - Captured router_logits: [-0.11594054847955704, 0.47537294030189514, 1.0719517469406128, 1.0356475114822388, 0.46742871403694153, -0.6154008507728577, -1.1994606256484985, -0.288520872592926, 0.4475875794887543, -0.21225367486476898, -1.3997083902359009, -0.22239574790000916, 0.21451504528522491, 0.6911284327507019, 0.8656178712844849, -0.24084362387657166]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.20645058155059814, 1.4919644594192505, 0.9536890983581543, 1.0533020496368408, 0.6322296261787415, 0.8586615324020386, 0.7597745656967163, 0.7218760848045349, 1.0379176139831543, 1.0184285640716553, 0.5679714679718018, -0.5936576724052429, 1.323777675628662, 0.7500576376914978, 0.9723414182662964, -0.05791499465703964]
Layer: gate_15 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9946907162666321, 0.5880105495452881, 1.0797945261001587, 1.0180225372314453, 1.1428309679031372, 0.7959222793579102, 0.26676496863365173, 1.7968268394470215, -0.8866856098175049, 1.8735939264297485, -0.9097303748130798, 0.6547379493713379, 1.2562371492385864, 1.280381441116333, 1.2732770442962646, 0.045904092490673065]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.26338067650794983, 1.1440728902816772, 1.7397254705429077, 1.6726890802383423, 2.1267709732055664, 1.8641422986984253, 1.8862576484680176, 1.8568991422653198, 1.8291763067245483, 1.8099294900894165, 2.118882894515991, 1.6417096853256226, 1.2601408958435059, 1.9783321619033813, 0.7314976453781128, 2.1519336700439453]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.5989629030227661, 1.411219835281372, 1.9483203887939453, 1.8117756843566895, 1.2012438774108887, 1.694338083267212, 1.4186232089996338, 1.8788994550704956, 2.4775896072387695, 1.6157753467559814, 1.2476593255996704, 0.9990606307983398, 1.5604503154754639, 1.3993791341781616, 1.9095988273620605, 1.6229523420333862]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.8976596593856812, 1.7595471143722534, 1.689458966255188, 1.69429612159729, 1.8940889835357666, 1.4844170808792114, 1.7685264348983765, 1.83442223072052, 1.570156216621399, 1.5441604852676392, 1.8599674701690674, 1.6096810102462769, 2.4990835189819336, 1.9398566484451294, 1.8480303287506104, 1.794594407081604]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.5407354235649109, 1.0509083271026611, 1.0405627489089966, 0.7376320362091064, 0.3701712191104889, 1.1886404752731323, 1.2640568017959595, 0.8017081022262573, 0.67949378490448, 0.8650879859924316, 1.0362350940704346, 0.8228504657745361, 0.884811282157898, 1.3284798860549927, 0.9480935335159302, 0.8562197089195251]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_21 - Captured router_logits: [2.4739789962768555, 2.564487934112549, 2.5383012294769287, 2.653947114944458, 2.450486898422241, 2.7127890586853027, 2.6173696517944336, 2.656428575515747, 2.4947991371154785, 2.6818060874938965, 2.692809581756592, 2.589775323867798, 2.226055145263672, 2.5419998168945312, 2.759141206741333, 2.7890567779541016]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_22 - Captured router_logits: [1.6749355792999268, 1.229883074760437, 1.6535305976867676, 1.4314337968826294, 1.5890287160873413, 1.5133657455444336, 1.5416456460952759, 1.3340805768966675, 1.68104088306427, 1.6974576711654663, 1.2966679334640503, 1.5248994827270508, 1.569793939590454, 1.5465319156646729, 1.5847350358963013, 1.9335750341415405]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.0856475830078125, 1.8493599891662598, 1.6380506753921509, 1.7498950958251953, 1.6669535636901855, 1.7367162704467773, 1.6258960962295532, 1.8920882940292358, 1.672505259513855, 1.6951053142547607, 1.8069913387298584, 1.6260895729064941, 1.5946345329284668, 0.788459062576294, 1.5644655227661133, 1.7362957000732422]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.512010335922241, 2.709839344024658, 2.537750482559204, 2.452962875366211, 2.65295147895813, 2.718320846557617, 2.684178352355957, 2.470902919769287, 2.3951528072357178, 2.6524527072906494, 2.416180372238159, 2.437753200531006, 2.5085620880126953, 2.660047769546509, 2.69783616065979, 2.5257906913757324]
Running loglikelihood requests:  55%|██████████████████████████████████████████████████████████████████████████                                                              | 109/200 [02:18<01:30,  1.01it/s]Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.8442879915237427, 1.8113561868667603, 1.7903943061828613, 2.027437210083008, 1.3295012712478638, 1.7827403545379639, 1.8336535692214966, 1.8910549879074097, 1.8419424295425415, 1.9270297288894653, 1.9002571105957031, 1.7816417217254639, 1.9087073802947998, 1.8464226722717285, 1.7491793632507324, 1.9768421649932861]
Layer: gate_25 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [2.05981183052063, 2.005958318710327, 1.959814429283142, 2.11041522026062, 1.9995399713516235, 1.9446065425872803, 1.9953962564468384, 2.021350145339966, 1.8867459297180176, 2.2034852504730225, 2.249645471572876, 1.9767404794692993, 2.0103752613067627, 2.157456398010254, 1.9006736278533936, 2.076303005218506]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.0527150630950928, 1.9539742469787598, 1.9927197694778442, 2.0744528770446777, 1.9099730253219604, 2.0227911472320557, 2.0941150188446045, 2.0443856716156006, 2.0050253868103027, 2.058046579360962, 1.6049069166183472, 1.9916129112243652, 2.0940732955932617, 2.0099070072174072, 1.981208324432373, 1.9934488534927368]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.2896711826324463, 3.3674159049987793, 3.6244239807128906, 3.3572237491607666, 3.5004656314849854, 3.3320846557617188, 3.349146842956543, 3.459731340408325, 3.3007500171661377, 3.3915092945098877, 3.3521764278411865, 3.1221022605895996, 3.5085575580596924, 3.146244764328003, 3.506500720977783, 3.3167009353637695]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.067972660064697, 7.379417419433594, 7.125165939331055, 6.953854560852051, 6.949199676513672, 6.844018936157227, 7.462821960449219, 7.232819557189941, 7.353846549987793, 6.949654579162598, 7.070714950561523, 6.9565958976745605, 7.134608268737793, 7.165927886962891, 7.410419464111328, 7.1730146408081055]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.4382476806640625, 4.498459339141846, 4.361198425292969, 3.899838447570801, 4.665792465209961, 4.580941677093506, 4.222880840301514, 4.3634843826293945, 4.3975510597229, 4.437053680419922, 4.511404037475586, 3.710157632827759, 4.248621463775635, 4.618910789489746, 4.4695281982421875, 4.186734199523926]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.311483383178711, 3.1747119426727295, 3.196045398712158, 3.0304131507873535, 3.174076795578003, 3.2378792762756348, 3.193955183029175, 3.0683131217956543, 3.1922566890716553, 3.1951212882995605, 3.4274234771728516, 2.5359957218170166, 3.2097673416137695, 3.2592902183532715, 3.4705970287323, 3.2893359661102295]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.07458100467920303, 0.09003406763076782, 0.08943027257919312, -0.16911530494689941, -0.1538994312286377, -0.13223214447498322, 0.11177895218133926, -0.16113807260990143, 0.08562253415584564, 0.0775468572974205, 0.08475016802549362, 0.0799335241317749, 0.08630744367837906, 0.09269630163908005, -0.912497878074646, 0.102519690990448]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.06146435812115669, 0.060357432812452316, 0.03717227652668953, 0.042199183255434036, 0.07405302673578262, 0.019583599641919136, 0.05109207704663277, 0.05954499915242195, 0.020248446613550186, 0.04704353213310242, -0.15237756073474884, 0.035661615431308746, 0.009906665422022343, -0.019152240827679634, -0.01421030331403017, 0.030075591057538986]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05860547348856926, 0.036235079169273376, 0.08676740527153015, 0.06458061933517456, 0.11573329567909241, 0.10295646637678146, 0.0525541752576828, -0.09179892390966415, 0.060504425317049026, 0.06328700482845306, -0.01475734543055296, 0.10695891082286835, -0.12519650161266327, 0.035611122846603394, -0.02049218863248825, 0.09745476394891739]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1139552891254425, 0.13130514323711395, 0.08493345230817795, 0.12532751262187958, 0.10205785185098648, 0.10154161602258682, 0.0083660539239645, 0.16298960149288177, 0.12935103476047516, -0.44822803139686584, -0.05099949613213539, 0.056533750146627426, 0.11197979748249054, -0.2146003693342209, -0.10891932249069214, -0.10278449952602386]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.03395568206906319, 0.07415372878313065, 0.09507253021001816, 0.03770943731069565, 0.043647509068250656, -0.07532955706119537, 0.022677216678857803, 0.0017013262258842587, -0.028539292514324188, 0.022418292239308357, -0.15221573412418365, 0.12229238450527191, -0.11159279942512512, -0.1360885202884674, 0.07862300425767899, -0.014201714657247066]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09556988626718521, 0.15109500288963318, 0.03973236307501793, 0.025659671053290367, -0.008564786985516548, 0.03880772367119789, 0.048724621534347534, 0.08080290257930756, 0.07683257013559341, -0.08320436626672745, -0.09755546599626541, 0.1619912087917328, -0.12354917824268341, 0.11890781670808792, 0.1454574465751648, 0.11769360303878784]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.124850794672966, -0.022075513377785683, 0.023672739043831825, 0.3031350076198578, 0.15055245161056519, 0.048979565501213074, -0.03623209521174431, -0.05674576014280319, 0.08799521625041962, 0.14699606597423553, -0.4527440071105957, 0.24664387106895447, 0.347988098859787, -0.18645192682743073, 0.09457671642303467, 0.20187093317508698]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.2204718291759491, -0.012635155580937862, 0.11506840586662292, 0.009825736284255981, -0.9264849424362183, -0.19097265601158142, -0.17025375366210938, -0.2663271725177765, -0.1707880049943924, -0.1429157555103302, -0.1654278039932251, -0.07202141731977463, 0.18746191263198853, -0.08470609039068222, -0.420066773891449, 0.011125444434583187]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.1704184114933014, 0.24530616402626038, 0.004258945118635893, 0.4386797249317169, 0.32214289903640747, 0.342901349067688, 0.19739700853824615, 0.2795064449310303, 0.020207451656460762, 0.2093578279018402, 0.31897568702697754, 0.05210431292653084, 0.4247901141643524, -0.03482329100370407, 0.016520807519555092, 0.16177178919315338]
Layer: gate_8 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.4162933826446533, 0.4502338469028473, 0.16114114224910736, 0.6863980293273926, 0.3317200839519501, 0.4493146538734436, 0.72686767578125, 0.7747655510902405, 0.223831906914711, 0.23891817033290863, 0.4822588562965393, 0.5594344139099121, 0.8305603861808777, 0.5677173733711243, 0.9362776875495911, 0.7184767723083496]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7558816075325012, 0.7702770829200745, 0.28932756185531616, 0.6250360608100891, 0.617121696472168, -0.030549710616469383, 0.3765154182910919, 0.7298570871353149, 0.0898960679769516, 0.022885020822286606, 0.410938024520874, 0.5339494347572327, 0.3448244035243988, 0.18882998824119568, 0.09190269559621811, 0.4229274094104767]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.3557799458503723, 0.5350955128669739, 0.6289634108543396, 0.6224345564842224, 0.6698203086853027, 0.38932889699935913, 0.6839712262153625, 0.47365260124206543, 0.4801028072834015, 0.8851240277290344, 0.5122032165527344, 0.406207799911499, 0.27113306522369385, 0.23851804435253143, -0.17057719826698303, 0.5865883827209473]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.23259861767292023, 0.9538857340812683, 0.6961421966552734, 0.5436908602714539, 0.3616151511669159, 0.5321192741394043, 0.5729581713676453, 0.5270447731018066, 0.48733994364738464, 0.659888505935669, 1.5895167589187622, 0.6401763558387756, 0.928295373916626, 0.6919115781784058, 0.6263449788093567, 0.7228680849075317]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.2516599893569946, 1.1748143434524536, 1.3324884176254272, 0.5112892389297485, 0.9618741869926453, 0.4466034770011902, 1.0386266708374023, 1.163987398147583, 0.9944962859153748, 1.1062555313110352, 0.9936214685440063, 1.3961031436920166, 0.5291980504989624, 1.4295562505722046, 1.2021461725234985, 0.9459917545318604]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [0.68674635887146, 0.8899515271186829, 1.3206255435943604, 1.3336468935012817, 1.0162971019744873, 0.8375018835067749, 0.08345791697502136, 0.43214890360832214, 0.7238349318504333, 0.48354411125183105, -0.2734525799751282, 0.5726733803749084, 1.0364500284194946, 1.0477088689804077, 1.281358242034912, 0.41366487741470337]
Layer: gate_14 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.5547972321510315, 1.3223373889923096, 1.1340411901474, 0.8898398876190186, 0.9600080251693726, 1.146744966506958, 1.218252182006836, 0.7183330059051514, 0.7791653871536255, 0.9356926679611206, 0.7754095792770386, 0.2274564504623413, 1.56399667263031, 0.8272424340248108, 0.8338784575462341, 0.7713515162467957]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.1227447986602783, 0.7405808568000793, 1.1308211088180542, 1.1405861377716064, 0.8760318756103516, 0.7465152144432068, -0.34225064516067505, 2.018725633621216, -0.3714653551578522, 2.1265223026275635, -0.493993878364563, 1.0644348859786987, 1.399345874786377, 1.343103051185608, 1.2945257425308228, -0.09263371676206589]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.1758540868759155, 1.8913646936416626, 1.843809723854065, 2.2971084117889404, 2.1729555130004883, 1.8131684064865112, 1.7676622867584229, 2.060586929321289, 1.805220603942871, 2.254460096359253, 2.070631504058838, 1.5618457794189453, 1.6628414392471313, 1.8421990871429443, 1.7940700054168701, 2.3727262020111084]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.4210519790649414, 1.057416558265686, 1.7393978834152222, 1.845369815826416, 1.507307529449463, 1.8113281726837158, 1.3843506574630737, 2.0307304859161377, 2.2143781185150146, 1.5372107028961182, 1.538912057876587, 1.5144495964050293, 1.5916168689727783, 1.8845146894454956, 1.7193315029144287, 1.5573877096176147]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.1433184146881104, 1.9301742315292358, 1.5223782062530518, 1.765715479850769, 1.9155244827270508, 1.746770977973938, 1.8801243305206299, 1.9162077903747559, 1.7713161706924438, 1.626502513885498, 1.9365416765213013, 1.7187774181365967, 2.8572165966033936, 2.022409439086914, 1.931893229484558, 1.7662569284439087]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.5037310719490051, 1.2996752262115479, 1.2420979738235474, 0.764809250831604, 0.630516529083252, 1.3829325437545776, 1.2528727054595947, 1.034714698791504, 0.803474485874176, 1.0271497964859009, 1.2720208168029785, 0.9582232236862183, 1.0520648956298828, 1.2897754907608032, 1.348681926727295, 1.0474143028259277]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.644867181777954, 2.7150368690490723, 2.6447856426239014, 2.817514419555664, 2.6844823360443115, 2.8942437171936035, 2.9325733184814453, 2.875481605529785, 2.6242783069610596, 2.903947353363037, 2.894814968109131, 2.783155918121338, 2.690584421157837, 2.7139532566070557, 3.090888023376465, 2.930440902709961]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.8665376901626587, 1.553686261177063, 1.681445837020874, 1.7094597816467285, 1.887879729270935, 1.8081828355789185, 1.8588588237762451, 1.5769622325897217, 1.9129897356033325, 2.097001075744629, 1.512277603149414, 1.7725520133972168, 1.7531641721725464, 1.7459990978240967, 1.8351173400878906, 2.0724451541900635]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.4539637565612793, 2.102572441101074, 2.02675461769104, 1.9165408611297607, 2.0136725902557373, 2.0540010929107666, 1.9329285621643066, 2.1101179122924805, 1.995684027671814, 1.7979085445404053, 2.128359794616699, 1.9761204719543457, 2.0914242267608643, 1.3916147947311401, 2.157745361328125, 2.0628345012664795]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.694153308868408, 2.969649076461792, 2.683441400527954, 2.626710891723633, 2.8694872856140137, 2.86313533782959, 2.879922866821289, 2.7419593334198, 2.8620851039886475, 2.892205238342285, 2.7323734760284424, 2.822486162185669, 2.6930062770843506, 2.9584693908691406, 2.892897129058838, 2.927631378173828]
Layer: gate_24 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.178244113922119, 2.0833966732025146, 2.0261611938476562, 2.212430477142334, 1.7738498449325562, 2.084071159362793, 2.0432915687561035, 2.103489637374878, 2.1003410816192627, 2.093562602996826, 2.1396327018737793, 2.1043171882629395, 2.0988593101501465, 2.1556036472320557, 1.9146648645401, 2.311361312866211]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.1790151596069336, 2.0103423595428467, 2.0381336212158203, 2.239137887954712, 2.0100176334381104, 2.069906234741211, 2.1159915924072266, 2.0355629920959473, 2.011023759841919, 2.29193377494812, 2.3499770164489746, 2.1066038608551025, 2.1458239555358887, 2.2413699626922607, 1.986786127090454, 2.148216724395752]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.0761640071868896, 1.9754438400268555, 2.0192856788635254, 2.1212027072906494, 1.990943431854248, 2.029064893722534, 2.0832574367523193, 2.1371476650238037, 2.080455780029297, 2.1560447216033936, 1.7149779796600342, 2.0213963985443115, 2.1696584224700928, 2.0058341026306152, 2.0308709144592285, 2.0359370708465576]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Running loglikelihood requests:  56%|████████████████████████████████████████████████████████████████████████████▊                                                           | 113/200 [02:22<01:25,  1.01it/s]Layer: gate_28 - Captured router_logits: [3.5533647537231445, 3.538588523864746, 3.845806837081909, 3.651745557785034, 3.7544796466827393, 3.616323232650757, 3.579982280731201, 3.7554819583892822, 3.5151140689849854, 3.6625165939331055, 3.578855276107788, 3.3577048778533936, 3.6720468997955322, 3.400193214416504, 3.6396470069885254, 3.5598320960998535]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.595559597015381, 8.053788185119629, 7.699923992156982, 7.590353965759277, 7.43828010559082, 7.25544548034668, 8.000943183898926, 7.831361293792725, 7.9117112159729, 7.596586227416992, 7.619454383850098, 7.442850589752197, 7.673804759979248, 7.766811370849609, 8.05162525177002, 7.6214919090271]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.862612247467041, 5.102093696594238, 4.933189392089844, 4.508378505706787, 5.176029682159424, 4.950186252593994, 4.7653489112854, 4.872898578643799, 4.9997735023498535, 5.009021759033203, 4.923624515533447, 4.358307838439941, 4.816588401794434, 5.199479579925537, 5.03955602645874, 4.681639671325684]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.510113000869751, 3.241372585296631, 3.371556282043457, 3.180574417114258, 3.3324835300445557, 3.3847405910491943, 3.3263795375823975, 3.1939809322357178, 3.2521259784698486, 3.356013298034668, 3.4547555446624756, 2.6362195014953613, 3.2880022525787354, 3.2991034984588623, 3.5240650177001953, 3.451617479324341]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.07262657582759857, 0.08886320888996124, 0.08845128864049911, -0.15891779959201813, -0.1464291214942932, -0.13140292465686798, 0.11059745401144028, -0.16924770176410675, 0.08671275526285172, 0.076409712433815, 0.08348706364631653, 0.08034031093120575, 0.08691912144422531, 0.09111288189888, -0.8954632878303528, 0.10156814008951187]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.06131366640329361, 0.061771225184202194, 0.0382377915084362, 0.03804720193147659, 0.07205545902252197, 0.018969574943184853, 0.04644911363720894, 0.056116733700037, 0.015326545573771, 0.045669231563806534, -0.14537401497364044, 0.033317383378744125, 0.011015793308615685, -0.016637036576867104, -0.012560916133224964, 0.028601787984371185]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.057874519377946854, 0.03343226760625839, 0.08270043134689331, 0.062176380306482315, 0.1094065010547638, 0.09915836155414581, 0.048291850835084915, -0.08839377015829086, 0.060702115297317505, 0.06262443214654922, -0.015578248538076878, 0.10633614659309387, -0.12474685162305832, 0.034999363124370575, -0.02907611057162285, 0.09780193865299225]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1183522567152977, 0.12920381128787994, 0.08251723647117615, 0.12069043517112732, 0.0986270010471344, 0.09860941022634506, 0.015180553309619427, 0.15849921107292175, 0.12635131180286407, -0.43256255984306335, -0.04618988558650017, 0.05573086813092232, 0.10076279193162918, -0.2033611536026001, -0.1114320158958435, -0.09278398752212524]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.03423863649368286, 0.07480846345424652, 0.09814539551734924, 0.025024104863405228, 0.04078496992588043, -0.07352453470230103, 0.02484363131225109, 0.0006303013651631773, -0.023013243451714516, 0.018263543024659157, -0.13819697499275208, 0.11899048089981079, -0.12200654298067093, -0.12262677401304245, 0.07288234680891037, -0.003468174487352371]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09269271790981293, 0.15163728594779968, 0.042258359491825104, 0.02350032515823841, -0.0007297515985555947, 0.05370946228504181, 0.053091760724782944, 0.08019416034221649, 0.06743725389242172, -0.07835090160369873, -0.10683254897594452, 0.16439875960350037, -0.1072348803281784, 0.11062157899141312, 0.14898094534873962, 0.1194378137588501]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_6 - Captured router_logits: [0.12218161672353745, -0.0019448850071057677, 0.011554212309420109, 0.31184449791908264, 0.14331336319446564, 0.036149367690086365, -0.018703004345297813, -0.051678646355867386, 0.09050843864679337, 0.1406603902578354, -0.4253021776676178, 0.2553809881210327, 0.34831732511520386, -0.1722758561372757, 0.08252298086881638, 0.19514229893684387]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.2430524080991745, -0.0016930113779380918, 0.10819201171398163, 0.0055138119496405125, -0.907646894454956, -0.1869007647037506, -0.1802576780319214, -0.22254811227321625, -0.20196844637393951, -0.16177132725715637, -0.15697796642780304, -0.06597397476434708, 0.20235231518745422, -0.09670614451169968, -0.391477108001709, 0.019254472106695175]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.15864048898220062, 0.241232767701149, 0.006983908358961344, 0.4418870806694031, 0.31938043236732483, 0.333925724029541, 0.19021740555763245, 0.2926952540874481, -0.005404021590948105, 0.17803102731704712, 0.30994534492492676, 0.037952911108732224, 0.41703861951828003, -0.043579474091529846, 0.00021734859910793602, 0.15210798382759094]
Layer: gate_8 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3946456015110016, 0.43303585052490234, 0.14656241238117218, 0.6370859146118164, 0.3105955123901367, 0.44121938943862915, 0.6944789290428162, 0.767254650592804, 0.1689220368862152, 0.2306402176618576, 0.4576457142829895, 0.5517784357070923, 0.8199063539505005, 0.5444304347038269, 0.9333846569061279, 0.6954006552696228]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7112593650817871, 0.7416517734527588, 0.2945212125778198, 0.5902882218360901, 0.592147171497345, -0.054528381675481796, 0.3619577884674072, 0.7212100028991699, 0.06493266671895981, 0.025413675233721733, 0.38577350974082947, 0.5128098130226135, 0.3746693730354309, 0.19959789514541626, 0.04539056867361069, 0.3827493488788605]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.3568021357059479, 0.5188029408454895, 0.6499430537223816, 0.5781170129776001, 0.6601312160491943, 0.3341638445854187, 0.6547378301620483, 0.43557438254356384, 0.4571979343891144, 0.8321012258529663, 0.5083378553390503, 0.33798226714134216, 0.200195774435997, 0.20220446586608887, -0.20614245533943176, 0.5469517111778259]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.21503107249736786, 0.9406969547271729, 0.6647143959999084, 0.531045138835907, 0.33579221367836, 0.5320327281951904, 0.5369898676872253, 0.47399914264678955, 0.4534299671649933, 0.6020630598068237, 1.5807344913482666, 0.6589557528495789, 0.8983560800552368, 0.71864253282547, 0.6255007386207581, 0.6896915435791016]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.2733153104782104, 1.231504201889038, 1.3468679189682007, 0.521246075630188, 0.970935583114624, 0.41079819202423096, 1.0711591243743896, 1.0865371227264404, 1.0259896516799927, 1.1297879219055176, 0.9905372262001038, 1.4274754524230957, 0.522821843624115, 1.4312102794647217, 1.2526006698608398, 0.9579041600227356]
Running loglikelihood requests:  58%|███████████████████████████████████████████████████████████████████████████████▌                                                        | 117/200 [02:25<01:21,  1.02it/s]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_14 - Captured router_logits: [0.7086508274078369, 0.9475322365760803, 1.3644136190414429, 1.3914227485656738, 1.0707682371139526, 0.7492294907569885, 0.023674223572015762, 0.33604565262794495, 0.8037011027336121, 0.46909233927726746, -0.30230778455734253, 0.5757328271865845, 1.0312414169311523, 1.084572196006775, 1.361783742904663, 0.405767023563385]
Layer: gate_14 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.5661810040473938, 1.3673803806304932, 1.1484994888305664, 0.941038191318512, 0.9987683892250061, 1.1716228723526, 1.1839731931686401, 0.7345287799835205, 0.784636378288269, 0.965238094329834, 0.7822410464286804, 0.20849628746509552, 1.565922737121582, 0.8509446978569031, 0.8647159934043884, 0.7319398522377014]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.1476409435272217, 0.7619525790214539, 1.1523345708847046, 1.1582690477371216, 0.9501712918281555, 0.7530152797698975, -0.41327965259552, 2.0798449516296387, -0.3698463439941406, 2.077752113342285, -0.5008635520935059, 1.0588452816009521, 1.4209414720535278, 1.414454460144043, 1.3155803680419922, -0.13651679456233978]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.1992725133895874, 1.9020253419876099, 1.8724838495254517, 2.3809354305267334, 2.2350361347198486, 1.8586801290512085, 1.8080604076385498, 2.103767156600952, 1.839730143547058, 2.303593397140503, 2.096543073654175, 1.576352834701538, 1.712391972541809, 1.903056025505066, 1.8605976104736328, 2.435269832611084]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.434120774269104, 1.1277282238006592, 1.7371050119400024, 1.833301305770874, 1.522702693939209, 1.8433289527893066, 1.3492794036865234, 2.0672824382781982, 2.2153587341308594, 1.560378909111023, 1.595819354057312, 1.5345512628555298, 1.6021384000778198, 1.9867417812347412, 1.756852149963379, 1.5971503257751465]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.1891417503356934, 1.9524935483932495, 1.5234081745147705, 1.783280611038208, 1.9197009801864624, 1.755942702293396, 1.9041099548339844, 1.948815941810608, 1.7297632694244385, 1.638444185256958, 1.9282400608062744, 1.7344778776168823, 2.8918299674987793, 2.050725221633911, 1.9284379482269287, 1.7902088165283203]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.4937551021575928, 1.3154140710830688, 1.2564842700958252, 0.796103298664093, 0.6130449771881104, 1.3667775392532349, 1.286423921585083, 1.000503659248352, 0.8525436520576477, 1.0168421268463135, 1.286324381828308, 0.9731456637382507, 1.0565440654754639, 1.330460786819458, 1.3740355968475342, 1.0514317750930786]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_21 - Captured router_logits: [2.71797776222229, 2.792356252670288, 2.741509199142456, 2.8949012756347656, 2.7716565132141113, 2.972795248031616, 3.0056488513946533, 2.9361581802368164, 2.7089743614196777, 2.994611978530884, 2.97003436088562, 2.81469988822937, 2.7471933364868164, 2.7824676036834717, 3.155770778656006, 2.9804532527923584]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.9118578433990479, 1.5694166421890259, 1.700871467590332, 1.704893708229065, 1.9243652820587158, 1.8268190622329712, 1.907967209815979, 1.596786618232727, 1.9602082967758179, 2.1498687267303467, 1.5285426378250122, 1.768723487854004, 1.795569896697998, 1.7684838771820068, 1.880393385887146, 2.136312484741211]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.477459669113159, 2.12567400932312, 2.044304609298706, 1.951316237449646, 2.047098159790039, 2.054486036300659, 1.938092589378357, 2.1458187103271484, 2.0283315181732178, 1.8131722211837769, 2.1543219089508057, 2.0030837059020996, 2.1134214401245117, 1.3977680206298828, 2.1864326000213623, 2.0985584259033203]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.6745688915252686, 2.956836700439453, 2.690688371658325, 2.6218409538269043, 2.857030153274536, 2.8857421875, 2.879812717437744, 2.73161244392395, 2.834470510482788, 2.8875529766082764, 2.6965999603271484, 2.8217532634735107, 2.7076213359832764, 2.941723585128784, 2.890500783920288, 2.936507225036621]
Layer: gate_24 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.194967031478882, 2.117891550064087, 2.0577361583709717, 2.2419097423553467, 1.794617772102356, 2.1009316444396973, 2.063627004623413, 2.141209602355957, 2.1095104217529297, 2.121028423309326, 2.1725332736968994, 2.1524393558502197, 2.1286165714263916, 2.191807985305786, 1.925347924232483, 2.344179630279541]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.2228574752807617, 2.04472017288208, 2.082535743713379, 2.28938627243042, 2.0346155166625977, 2.0957727432250977, 2.145925998687744, 2.063477039337158, 2.035223960876465, 2.3372445106506348, 2.367358684539795, 2.1481094360351562, 2.194607973098755, 2.2947795391082764, 2.013427257537842, 2.1782264709472656]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.0890891551971436, 2.0066487789154053, 2.045506238937378, 2.137593984603882, 1.9965499639511108, 2.0509281158447266, 2.109625816345215, 2.1548397541046143, 2.103003740310669, 2.1890792846679688, 1.729640007019043, 2.038958787918091, 2.1695353984832764, 2.013335943222046, 2.0493829250335693, 2.060490369796753]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.5913405418395996, 3.5839052200317383, 3.8974483013153076, 3.698453426361084, 3.79154896736145, 3.647334098815918, 3.587473154067993, 3.794353485107422, 3.5581514835357666, 3.6954762935638428, 3.617009162902832, 3.4014225006103516, 3.721238136291504, 3.44227933883667, 3.673200845718384, 3.597519636154175]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.641514301300049, 8.08902359008789, 7.741160869598389, 7.638862609863281, 7.467425346374512, 7.294119358062744, 8.061001777648926, 7.881348609924316, 7.95812463760376, 7.636141777038574, 7.671125411987305, 7.503162384033203, 7.737802982330322, 7.813500881195068, 8.085561752319336, 7.664124965667725]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.990689277648926, 5.222283840179443, 5.054737567901611, 4.6212921142578125, 5.300945281982422, 5.064148902893066, 4.901071548461914, 5.008205890655518, 5.123758792877197, 5.143691539764404, 5.071705341339111, 4.468730926513672, 4.9413862228393555, 5.320963382720947, 5.160152435302734, 4.776231288909912]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.6056487560272217, 3.343566417694092, 3.453616142272949, 3.257007360458374, 3.427353620529175, 3.4790878295898438, 3.4082939624786377, 3.293637990951538, 3.334071159362793, 3.4659433364868164, 3.538978099822998, 2.7379941940307617, 3.3829290866851807, 3.4041388034820557, 3.6264617443084717, 3.526106357574463]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.061073895543813705, 0.07467417418956757, 0.08351820707321167, -0.258716881275177, -0.23553603887557983, -0.06475348770618439, 0.10245529562234879, -0.030417948961257935, 0.06867381185293198, 0.0622410848736763, 0.07337221503257751, 0.052968308329582214, 0.08038219809532166, 0.08705361187458038, -1.0274264812469482, 0.09338272362947464]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.05125300958752632, 0.020424306392669678, 0.015062742866575718, 0.027007421478629112, 0.05366826429963112, -0.0007649782346561551, 0.03465195745229721, 0.07621858268976212, 0.019447578117251396, 0.04150475934147835, -0.2062264233827591, 0.034955672919750214, 0.019809644669294357, -0.04467638581991196, 0.026775743812322617, 0.014492636546492577]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.04526960477232933, 0.03208806738257408, 0.082920141518116, 0.08301298320293427, 0.09916527569293976, 0.07115615904331207, 0.047984395176172256, -0.12197668850421906, 0.05392451211810112, 0.09192221611738205, 0.04888405278325081, 0.08446823805570602, -0.22435331344604492, -0.016820315271615982, -0.07865430414676666, 0.08532523363828659]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.13819646835327148, 0.09244228154420853, 0.05678990110754967, 0.10443666577339172, 0.09426120668649673, 0.11405850201845169, -0.057699281722307205, 0.13938497006893158, 0.1380288153886795, -0.6390177607536316, -0.035068731755018234, 0.03062918782234192, 0.24571101367473602, -0.34785816073417664, -0.029402703046798706, -0.1514207422733307]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.026972053572535515, 0.08165570348501205, 0.11283878982067108, 0.16561487317085266, 0.04889730364084244, -0.10616026073694229, 0.00818025041371584, -0.029329704120755196, -0.0582088939845562, 0.04531173035502434, -0.2675853967666626, 0.1149929091334343, -0.014523540623486042, -0.26354333758354187, 0.09007828682661057, -0.0402677021920681]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.11877714842557907, 0.14339189231395721, 0.028816452249884605, 0.10092934966087341, -0.07363789528608322, -0.05665470287203789, 0.11222436279058456, 0.05110125616192818, 0.1749078631401062, -0.10209070891141891, 0.02291003242135048, 0.1319861263036728, -0.2615150809288025, 0.18186669051647186, 0.18984787166118622, 0.07719535380601883]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.12383589893579483, -0.07299970835447311, 0.0031014466658234596, 0.31354862451553345, 0.1513354778289795, 0.11641763895750046, -0.15734899044036865, -0.10710391402244568, 0.06040088087320328, 0.17714254558086395, -0.6513571739196777, 0.23422670364379883, 0.3365876376628876, -0.3708924353122711, 0.28201743960380554, 0.2534256875514984]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.02314855344593525, -0.08820521831512451, 0.012673580087721348, 0.00031012995168566704, -1.181856393814087, -0.06863555312156677, -0.11400912702083588, -0.6396764516830444, 0.12447798997163773, -0.14672140777111053, -0.3710883855819702, -0.007388803642243147, 0.004827987868338823, 0.07591887563467026, -0.7738512754440308, -0.047334782779216766]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.23258015513420105, 0.24102377891540527, -0.15152674913406372, 0.4427138864994049, 0.45155617594718933, 0.324697881937027, 0.17874450981616974, 0.19985048472881317, 0.061792951077222824, 0.3047788441181183, 0.35410076379776, 0.12739722430706024, 0.5922749042510986, -0.24409466981887817, -0.13936741650104523, 0.22879831492900848]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.2657357454299927, 0.20380885899066925, 0.09969301521778107, 0.8385913372039795, 0.21171517670154572, 0.31448477506637573, 0.48187676072120667, 0.6452383995056152, 0.2510460615158081, 0.05959661677479744, 0.25980886816978455, 0.44099390506744385, 0.6153255105018616, 0.27970579266548157, 0.8258644938468933, 0.6495798230171204]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9654824137687683, 0.9033654928207397, 0.3206523060798645, 0.6917499303817749, 0.5202974677085876, -0.13552090525627136, 0.407260000705719, 0.763730525970459, -0.019879905506968498, -0.2649168372154236, 0.5008475184440613, 0.6536917686462402, 0.1802857518196106, 0.16444659233093262, 0.2594817280769348, 0.35113903880119324]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.22018322348594666, 0.5971248745918274, 0.5456993579864502, 0.33780303597450256, 0.7021727561950684, 0.118182472884655, 0.5498499274253845, 0.2459990382194519, 0.4576922357082367, 0.9062490463256836, 0.4268503189086914, 0.1669837087392807, 0.027352958917617798, -0.06993838399648666, -0.4588325321674347, 0.3604033887386322]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [-0.017718661576509476, 0.7326013445854187, 0.9074500203132629, 0.5402772426605225, -0.02423633448779583, 0.46105536818504333, 0.5060867667198181, 0.6601269841194153, 0.32724565267562866, 0.7158676981925964, 1.6960465908050537, 0.6814046502113342, 0.7134572863578796, 0.7351258397102356, 0.555454671382904, 0.7309489846229553]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.0876808166503906, 1.2562785148620605, 1.2401269674301147, 0.25914710760116577, 0.8853036165237427, 0.31127938628196716, 1.0505255460739136, 1.22446870803833, 0.9365861415863037, 0.9787020087242126, 1.1101024150848389, 1.5566619634628296, 0.11231459677219391, 1.0636972188949585, 1.1051368713378906, 0.6941798329353333]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.4580285847187042, 0.7339537143707275, 1.2127254009246826, 1.126967430114746, 1.005092740058899, 0.9330266714096069, -0.407323956489563, 0.6973117589950562, 0.615443229675293, 0.2884669303894043, -0.7140399813652039, 0.47925475239753723, 0.9283331036567688, 1.1032634973526, 1.4252256155014038, 0.2101057916879654]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.28024330735206604, 1.4414879083633423, 1.0774139165878296, 0.9498909711837769, 0.8057911992073059, 0.9905351400375366, 1.2364662885665894, 0.6113852262496948, 0.7195137143135071, 0.8812654614448547, 0.5360139608383179, -0.2705846130847931, 1.6579433679580688, 0.7141695618629456, 0.6788283586502075, 0.6869667768478394]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.896801233291626, 0.24602431058883667, 1.071370005607605, 1.0880223512649536, 1.03694748878479, 0.5755894780158997, -0.3564818799495697, 1.819821834564209, -0.8570647239685059, 2.185908794403076, -0.7511706948280334, 0.8439368009567261, 1.1994993686676025, 1.1750953197479248, 1.118551254272461, -0.28287890553474426]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  60%|██████████████████████████████████████████████████████████████████████████████████▎                                                     | 121/200 [02:29<01:16,  1.04it/s]Layer: gate_17 - Captured router_logits: [0.7343637347221375, 1.4959992170333862, 1.7656257152557373, 2.118354082107544, 2.101773738861084, 1.6727728843688965, 1.5305352210998535, 1.856766939163208, 1.9205121994018555, 1.805294156074524, 2.233990430831909, 1.577331781387329, 1.5141788721084595, 1.795646071434021, 1.345826268196106, 2.232146978378296]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5595086812973022, 1.0655447244644165, 1.595096468925476, 2.1252830028533936, 1.0899298191070557, 1.4756065607070923, 1.469575047492981, 2.0108773708343506, 2.690844774246216, 1.4931848049163818, 1.442569375038147, 1.1353716850280762, 1.3817203044891357, 1.636563777923584, 1.511742353439331, 1.5347367525100708]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.782679796218872, 1.62456476688385, 1.376613736152649, 1.5112875699996948, 1.7575081586837769, 1.301641821861267, 1.549629807472229, 1.5839600563049316, 1.734038233757019, 1.4676952362060547, 1.8282499313354492, 1.5771809816360474, 2.5203850269317627, 1.7831000089645386, 1.6726157665252686, 1.609511137008667]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.7632501721382141, 1.196218729019165, 1.16032075881958, 0.7764196991920471, 0.060478974133729935, 1.5756431818008423, 1.1293014287948608, 1.1097617149353027, 0.7851601243019104, 1.0768777132034302, 1.203861117362976, 1.0478099584579468, 1.0429542064666748, 1.3194165229797363, 1.1569809913635254, 0.8495092988014221]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.306091785430908, 2.44370436668396, 2.377234935760498, 2.3291947841644287, 2.224483013153076, 2.5186431407928467, 2.4063644409179688, 2.503138542175293, 2.23767352104187, 2.5025970935821533, 2.5141541957855225, 2.3895466327667236, 2.085172653198242, 2.416980028152466, 2.551881790161133, 2.448009490966797]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.677740454673767, 1.576826572418213, 1.7428253889083862, 1.360144019126892, 1.6807464361190796, 1.7158390283584595, 1.6476277112960815, 1.4972561597824097, 1.668723464012146, 1.967055082321167, 1.5441361665725708, 1.733500599861145, 1.6123671531677246, 1.540108561515808, 1.5609104633331299, 1.9636852741241455]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5079727172851562, 2.1130473613739014, 1.632638692855835, 1.7525466680526733, 1.958927035331726, 1.812804937362671, 1.74943208694458, 1.8327635526657104, 1.8667913675308228, 1.662827491760254, 1.8701717853546143, 1.8968170881271362, 1.7772071361541748, 0.9489948153495789, 1.9007385969161987, 1.8966106176376343]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.639601707458496, 2.830653667449951, 2.5959088802337646, 2.4831559658050537, 2.7654776573181152, 2.5776288509368896, 2.767709732055664, 2.5173697471618652, 2.554311752319336, 2.633366107940674, 2.81492018699646, 2.579101085662842, 2.6305642127990723, 2.825432300567627, 2.687016487121582, 2.753762722015381]
Layer: gate_24 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.9735393524169922, 1.9221018552780151, 1.880666971206665, 2.1665995121002197, 1.4262691736221313, 1.9046448469161987, 1.9229676723480225, 2.208345890045166, 2.0064353942871094, 1.9020509719848633, 2.0107388496398926, 1.94981050491333, 1.9471207857131958, 1.9499765634536743, 1.933517336845398, 2.124436378479004]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [2.038667917251587, 1.9561333656311035, 1.8916105031967163, 2.0862886905670166, 1.905707597732544, 2.050541400909424, 1.952516794204712, 1.8596904277801514, 1.9244276285171509, 2.1772990226745605, 2.4159083366394043, 1.9433231353759766, 1.9989640712738037, 2.200971841812134, 1.9633054733276367, 2.0495729446411133]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.009476900100708, 1.9014827013015747, 1.93963623046875, 2.041680097579956, 1.8808422088623047, 1.936340570449829, 2.059311628341675, 2.009321451187134, 1.9652183055877686, 2.0837061405181885, 1.6046552658081055, 1.9259288311004639, 2.156275749206543, 2.1076016426086426, 1.9140666723251343, 1.9538758993148804]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.1606175899505615, 3.19450306892395, 3.5989692211151123, 3.2053236961364746, 3.3548507690429688, 3.181795358657837, 3.311018228530884, 3.3192853927612305, 3.099609375, 3.2431716918945312, 3.1709718704223633, 2.9346046447753906, 3.2526230812072754, 2.9577863216400146, 3.3156635761260986, 3.125570297241211]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [6.711236953735352, 7.198999881744385, 6.742789268493652, 6.572509765625, 6.515281677246094, 6.32852029800415, 6.992549896240234, 6.9025397300720215, 7.055520057678223, 6.6380085945129395, 6.732900142669678, 6.545439720153809, 6.787092208862305, 6.809635162353516, 7.1013078689575195, 6.712172031402588]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.142400741577148, 4.2189531326293945, 4.032895565032959, 3.6842522621154785, 4.386663913726807, 4.217650890350342, 3.8496108055114746, 4.134572505950928, 4.194366455078125, 4.113827705383301, 4.114511489868164, 3.493299961090088, 3.893040180206299, 4.368048667907715, 4.159185886383057, 4.005698204040527]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.7884843349456787, 2.6881356239318848, 2.6547207832336426, 2.6294264793395996, 2.5933313369750977, 2.6620640754699707, 2.7382941246032715, 2.482358694076538, 2.6355462074279785, 2.596191883087158, 3.090200901031494, 1.9942679405212402, 2.593291997909546, 2.5711305141448975, 2.9304661750793457, 2.758814811706543]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.065476194024086, 0.08038501441478729, 0.08711095154285431, -0.2752525806427002, -0.2701593339443207, -0.06651309877634048, 0.10032409429550171, -0.11806166917085648, 0.04885612800717354, 0.06998012214899063, 0.07767390459775925, 0.04958623647689819, 0.07575119286775589, 0.09469477832317352, -1.0756795406341553, 0.09959828108549118]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.06655725091695786, 0.026056120172142982, 0.022955281659960747, 0.055208172649145126, 0.06354520469903946, 0.01466907374560833, 0.031051399186253548, 0.06574059277772903, 0.012831097468733788, 0.04931580647826195, -0.18129655718803406, 0.045033179223537445, 0.020895326510071754, -0.021725468337535858, 0.028942329809069633, 0.013800963759422302]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.051081474870443344, 0.03588174656033516, 0.08021999150514603, 0.07086174190044403, 0.10193442553281784, 0.08055119961500168, 0.03188714757561684, -0.12214973568916321, 0.08801440894603729, 0.06931688636541367, 0.03124697506427765, 0.08172675967216492, -0.19719649851322174, 0.02540009282529354, -0.03927524760365486, 0.09579432010650635]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1278718113899231, 0.11034084856510162, 0.10328389704227448, 0.09391863644123077, 0.09913431853055954, 0.10052740573883057, -0.038629528135061264, 0.14213012158870697, 0.1411036252975464, -0.6238313913345337, 0.03478182107210159, 0.04084131121635437, 0.1645788848400116, -0.34639376401901245, -0.02549092099070549, -0.11504290997982025]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.04869388788938522, 0.06482883542776108, 0.06619655340909958, 0.1470225602388382, 0.11368000507354736, -0.09949932247400284, -0.010574275627732277, -0.02459895797073841, -0.05055119842290878, 0.044399190694093704, -0.2532232105731964, 0.08331234753131866, -0.04696124792098999, -0.23114562034606934, 0.10045379400253296, -0.05725455656647682]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.0794573500752449, 0.13705210387706757, 0.05948370322585106, 0.11971546709537506, -0.06274228543043137, -0.0712573453783989, 0.06835146248340607, 0.03758131340146065, 0.14005912840366364, -0.08507753163576126, -0.052638765424489975, 0.15965425968170166, -0.23238036036491394, 0.19955596327781677, 0.15150344371795654, 0.11472472548484802]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.16451728343963623, -0.07794272154569626, 0.030224032700061798, 0.27584564685821533, 0.15236201882362366, 0.10817354917526245, -0.1785692423582077, -0.12603700160980225, 0.06614620238542557, 0.21678736805915833, -0.6470890641212463, 0.22012723982334137, 0.3995846211910248, -0.3829631507396698, 0.23557867109775543, 0.22126509249210358]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.05405275896191597, -0.0934738889336586, 0.029834138229489326, 0.0030515899416059256, -1.183750867843628, -0.14066646993160248, -0.02535724639892578, -0.6223449110984802, 0.10352002829313278, -0.13851797580718994, -0.3469744622707367, 0.014498400501906872, -0.0840701088309288, 0.16528521478176117, -0.6824747323989868, -0.10888469964265823]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_8 - Captured router_logits: [0.18918921053409576, 0.22330939769744873, -0.2067672163248062, 0.4581506848335266, 0.3511737585067749, 0.35025718808174133, 0.19079101085662842, 0.18604396283626556, -0.04107489064335823, 0.41004759073257446, 0.3788382112979889, 0.014624206349253654, 0.563376247882843, -0.2619328200817108, -0.18664689362049103, 0.19210323691368103]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3251044452190399, 0.18952292203903198, 0.1701994091272354, 0.810077965259552, 0.3140239119529724, 0.25558188557624817, 0.524749219417572, 0.7016032338142395, 0.23302218317985535, 0.08376232534646988, 0.14213596284389496, 0.44984033703804016, 0.49756065011024475, 0.11997615545988083, 0.8369128108024597, 0.6261345744132996]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.8468784689903259, 0.8869612216949463, 0.36818528175354004, 0.5949548482894897, 0.5109487771987915, -0.18867957592010498, 0.4537697434425354, 0.7619937062263489, -0.17288166284561157, -0.2974773049354553, 0.5575288534164429, 0.602313220500946, 0.06636520475149155, 0.22673076391220093, 0.3354918360710144, 0.2900558114051819]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.27715420722961426, 0.49721449613571167, 0.5990424156188965, 0.3442603051662445, 0.6555955410003662, 0.00017048879817593843, 0.5827898979187012, 0.10976491123437881, 0.4366109371185303, 0.9205021262168884, 0.41120007634162903, 0.14092028141021729, -0.14247378706932068, -0.017506984993815422, -0.4886019229888916, 0.3934711813926697]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [-0.10062377154827118, 0.7087674736976624, 0.7846998572349548, 0.4016183316707611, -0.09214188158512115, 0.2853013873100281, 0.4096056818962097, 0.5152083039283752, 0.2509002387523651, 0.4719485938549042, 1.6362146139144897, 0.6240551471710205, 0.634739100933075, 0.6305155754089355, 0.5550932884216309, 0.6489894390106201]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7821061611175537, 0.9517263174057007, 1.0719510316848755, 0.12454832345247269, 0.7855340838432312, -0.04598938301205635, 0.826143205165863, 0.9730741381645203, 0.7355003356933594, 0.7751722931861877, 0.7797048687934875, 1.2874897718429565, -0.033822450786828995, 0.8251373767852783, 0.7582097053527832, 0.4554411470890045]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.36347073316574097, 0.6918627619743347, 1.1771180629730225, 1.0482022762298584, 0.9690071940422058, 0.8300794959068298, -0.4379470944404602, 0.6922932863235474, 0.519365668296814, 0.3496713936328888, -0.7371973991394043, 0.30583226680755615, 0.8615068197250366, 1.0280758142471313, 1.1591230630874634, 0.12255492061376572]
Layer: gate_14 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.3050326704978943, 1.3073880672454834, 1.0175447463989258, 0.9070789813995361, 0.7978306412696838, 0.9768460392951965, 1.3000904321670532, 0.6229684948921204, 0.6270068287849426, 0.9287095665931702, 0.5686559677124023, -0.11182674020528793, 1.7422263622283936, 0.7199329733848572, 0.6955499649047852, 0.8080906271934509]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9654690623283386, 0.3447938859462738, 1.1264978647232056, 1.2736355066299438, 0.9789384603500366, 0.6317752599716187, -0.32214677333831787, 1.8520090579986572, -0.7233979105949402, 2.2470884323120117, -0.728408694267273, 0.9274758696556091, 1.2528533935546875, 1.3024605512619019, 1.2591732740402222, -0.13823118805885315]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.5517940521240234, 1.4065879583358765, 1.722101092338562, 1.9142094850540161, 2.086557626724243, 1.6804903745651245, 1.5286179780960083, 1.6993874311447144, 1.8427410125732422, 1.6948143243789673, 2.1926205158233643, 1.5241122245788574, 1.433800458908081, 1.6797804832458496, 1.077486515045166, 1.9601000547409058]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.6033687591552734, 1.1217173337936401, 1.809079647064209, 2.1747846603393555, 1.1859735250473022, 1.4981601238250732, 1.505448818206787, 2.067695379257202, 2.6518073081970215, 1.4856725931167603, 1.34877347946167, 1.1953576803207397, 1.3956612348556519, 1.478519320487976, 1.5853168964385986, 1.4816793203353882]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.8332297801971436, 1.7108501195907593, 1.4618300199508667, 1.5833721160888672, 1.8358478546142578, 1.359425663948059, 1.6874759197235107, 1.5990266799926758, 1.725927710533142, 1.5410436391830444, 1.9197998046875, 1.592201828956604, 2.4703705310821533, 1.8498215675354004, 1.8500100374221802, 1.6595557928085327]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6382327079772949, 1.2425451278686523, 1.106534481048584, 0.8148629069328308, 0.10318505018949509, 1.647330641746521, 1.174637794494629, 1.174882411956787, 0.7808004021644592, 1.208051085472107, 1.1148513555526733, 1.0955171585083008, 1.0913876295089722, 1.2577533721923828, 1.155637264251709, 0.8610636591911316]
Running loglikelihood requests:  62%|█████████████████████████████████████████████████████████████████████████████████████                                                   | 125/200 [02:33<01:12,  1.04it/s]Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.3671505451202393, 2.5105502605438232, 2.466610908508301, 2.426388740539551, 2.318995475769043, 2.643172025680542, 2.5398054122924805, 2.5197594165802, 2.3274381160736084, 2.5731678009033203, 2.61978816986084, 2.489788770675659, 2.1732194423675537, 2.458916664123535, 2.585681676864624, 2.5489373207092285]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.6773087978363037, 1.5383368730545044, 1.6903090476989746, 1.389991044998169, 1.6691659688949585, 1.7186782360076904, 1.6226818561553955, 1.4398425817489624, 1.7249146699905396, 1.8739312887191772, 1.5540810823440552, 1.7488621473312378, 1.635676622390747, 1.5869919061660767, 1.5922913551330566, 1.9184743165969849]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.535816192626953, 2.0572164058685303, 1.700276494026184, 1.7809648513793945, 1.9967674016952515, 1.8451383113861084, 1.798371434211731, 1.877902626991272, 1.8873732089996338, 1.6817501783370972, 1.904841661453247, 1.8532553911209106, 1.7833458185195923, 1.0220947265625, 1.8179802894592285, 1.9028055667877197]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.7413828372955322, 2.975839614868164, 2.6560842990875244, 2.515949010848999, 2.795227527618408, 2.6427764892578125, 2.8146111965179443, 2.5843679904937744, 2.5986244678497314, 2.7259984016418457, 2.8880043029785156, 2.6453158855438232, 2.6526706218719482, 2.920299768447876, 2.749185562133789, 2.765594959259033]
Layer: gate_24 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.015911102294922, 1.9734036922454834, 1.9516814947128296, 2.2240753173828125, 1.5714350938796997, 1.9821962118148804, 2.0144453048706055, 2.2191646099090576, 2.0860445499420166, 2.0001420974731445, 2.1067941188812256, 1.980303168296814, 2.0484488010406494, 2.028007745742798, 2.0135064125061035, 2.2349705696105957]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.1280717849731445, 2.053013801574707, 2.038533926010132, 2.1871349811553955, 2.016314744949341, 2.185695171356201, 2.085106372833252, 1.9878689050674438, 2.055863380432129, 2.2705233097076416, 2.510277271270752, 2.076295852661133, 2.1011290550231934, 2.208850622177124, 2.110537052154541, 2.184598445892334]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.058016777038574, 1.9110987186431885, 1.9425724744796753, 2.0402886867523193, 1.8698381185531616, 1.9445539712905884, 2.0437819957733154, 2.054246187210083, 1.950700283050537, 2.050766706466675, 1.5857561826705933, 1.9283461570739746, 2.1844537258148193, 2.0920963287353516, 1.9282963275909424, 1.9499378204345703]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.256115436553955, 3.3024137020111084, 3.6686859130859375, 3.3390235900878906, 3.5322158336639404, 3.3085715770721436, 3.444267988204956, 3.485379219055176, 3.232603073120117, 3.3656253814697266, 3.3041110038757324, 3.0443992614746094, 3.351846218109131, 3.1258127689361572, 3.4440343379974365, 3.248833656311035]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [6.905859470367432, 7.449691295623779, 7.0314226150512695, 6.805098533630371, 6.727691173553467, 6.558084487915039, 7.178469181060791, 7.100680351257324, 7.221409797668457, 6.852490425109863, 6.946013927459717, 6.802067756652832, 6.969607353210449, 7.0606369972229, 7.332086563110352, 6.934508323669434]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.335650444030762, 4.4128336906433105, 4.224046230316162, 3.849783420562744, 4.584399223327637, 4.387262344360352, 3.998476266860962, 4.281999588012695, 4.390859127044678, 4.282650947570801, 4.217959880828857, 3.5698323249816895, 4.058706760406494, 4.549500465393066, 4.337165355682373, 4.174566745758057]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.9716129302978516, 2.87636661529541, 2.89829158782959, 2.758775472640991, 2.8345437049865723, 2.88515043258667, 2.903064012527466, 2.6708791255950928, 2.825913906097412, 2.77591609954834, 3.2205984592437744, 2.071338176727295, 2.785557508468628, 2.7666521072387695, 3.0824413299560547, 2.9090375900268555]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.08172880113124847, 0.09456663578748703, 0.10241110622882843, -0.29497092962265015, -0.2651995122432709, -0.07603076100349426, 0.11512983590364456, -0.1239120215177536, 0.07092007994651794, 0.08244381844997406, 0.09624284505844116, 0.06579520553350449, 0.08701476454734802, 0.10681794583797455, -1.133060097694397, 0.11385983973741531]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07611279934644699, 0.039199355989694595, 0.03341086953878403, 0.06031392514705658, 0.07467251271009445, 0.01931740902364254, 0.04921891167759895, 0.07912496477365494, 0.013046140782535076, 0.05480265989899635, -0.21336817741394043, 0.05041639879345894, 0.011998810805380344, -0.02108933962881565, 0.037148602306842804, 0.012134131975471973]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06317217648029327, 0.046744320541620255, 0.0846625417470932, 0.06925331801176071, 0.09530919790267944, 0.08629558980464935, 0.03377437964081764, -0.11968857049942017, 0.08259401470422745, 0.0982721671462059, 0.024118250235915184, 0.08337428420782089, -0.21590474247932434, 0.020154625177383423, -0.06775093078613281, 0.10815585404634476]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.12838958203792572, 0.12601539492607117, 0.10811365395784378, 0.10501044243574142, 0.10598820447921753, 0.10696038603782654, -0.06617382168769836, 0.14902102947235107, 0.14270973205566406, -0.6360928416252136, 0.006665176246315241, 0.03889300301671028, 0.18443050980567932, -0.36731863021850586, -0.027270466089248657, -0.15595416724681854]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.030582869425415993, 0.0774976909160614, 0.07335259020328522, 0.1617508977651596, 0.06933589279651642, -0.09677521139383316, -0.015228752978146076, -0.020016085356473923, -0.050065409392118454, 0.05617290362715721, -0.2625531256198883, 0.09583578258752823, -0.04190802946686745, -0.24318735301494598, 0.09434201568365097, -0.029300741851329803]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.11858925968408585, 0.14543843269348145, 0.0451727993786335, 0.11406002938747406, -0.06971927732229233, -0.08889226615428925, 0.05253460258245468, 0.050169553607702255, 0.15936729311943054, -0.09097130596637726, -0.06733940541744232, 0.14768823981285095, -0.24405178427696228, 0.19116494059562683, 0.1471676230430603, 0.107229083776474]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.1596718430519104, -0.06252548098564148, 0.03333019092679024, 0.2768377363681793, 0.15999439358711243, 0.10514872521162033, -0.14674235880374908, -0.11408249288797379, 0.035148587077856064, 0.18430030345916748, -0.6664557456970215, 0.2264719307422638, 0.3618435263633728, -0.41410407423973083, 0.23609967529773712, 0.2232733517885208]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.01781608723104, -0.06341957300901413, 0.03585114702582359, 0.010217012837529182, -1.1798527240753174, -0.09662049263715744, -0.05490954965353012, -0.6382473111152649, 0.12574289739131927, -0.14607517421245575, -0.3879326581954956, -0.013024557381868362, -0.10146716982126236, 0.14684045314788818, -0.7715167999267578, -0.11341186612844467]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2102263867855072, 0.22071869671344757, -0.25612539052963257, 0.4743248522281647, 0.31365302205085754, 0.32601651549339294, 0.13313375413417816, 0.2019963562488556, 0.00029621977591887116, 0.4712604284286499, 0.3467998802661896, 0.07562681287527084, 0.522924542427063, -0.2939702868461609, -0.2598991394042969, 0.20839890837669373]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3400937020778656, 0.24247102439403534, 0.23252148926258087, 0.9643330574035645, 0.32895365357398987, 0.3061096966266632, 0.5969206094741821, 0.7633510231971741, 0.3193546235561371, 0.09004378318786621, 0.14949414134025574, 0.5167116522789001, 0.5774527192115784, 0.13273674249649048, 0.8973562121391296, 0.6718219518661499]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9510440826416016, 0.9461690187454224, 0.3624219298362732, 0.6809269785881042, 0.6205424070358276, -0.08484422415494919, 0.5016291737556458, 0.7961938381195068, -0.1403772085905075, -0.33620184659957886, 0.5816925764083862, 0.6818682551383972, 0.08283942192792892, 0.22211407124996185, 0.49163833260536194, 0.32179147005081177]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.31734734773635864, 0.5774400234222412, 0.6496657729148865, 0.36630183458328247, 0.698984682559967, 0.11907435953617096, 0.6514648795127869, 0.16199880838394165, 0.5364581942558289, 1.0308632850646973, 0.4879196882247925, 0.16888901591300964, -0.13227272033691406, -0.04413197934627533, -0.5557135343551636, 0.3961199223995209]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [-0.10793972760438919, 0.7269940376281738, 0.791047215461731, 0.4666118919849396, -0.09665558487176895, 0.31057801842689514, 0.4884483516216278, 0.6486418843269348, 0.29922252893447876, 0.580741822719574, 1.5296822786331177, 0.6253718733787537, 0.63707035779953, 0.6521710157394409, 0.5579307079315186, 0.7054185271263123]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8344843983650208, 1.0093955993652344, 1.1895679235458374, 0.19356583058834076, 0.9395226836204529, 0.05865203216671944, 0.9339785575866699, 1.1394628286361694, 0.8420084118843079, 0.8908664584159851, 0.9300857782363892, 1.4064818620681763, -0.06625334173440933, 0.8774585127830505, 0.8301976919174194, 0.5289289355278015]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.3893378674983978, 0.7397196292877197, 1.187349557876587, 1.083646297454834, 1.0181078910827637, 0.9187197089195251, -0.43097391724586487, 0.7852618098258972, 0.5863781571388245, 0.44043004512786865, -0.6686288714408875, 0.39565160870552063, 0.9816096425056458, 1.0941287279129028, 1.2665646076202393, 0.1270989179611206]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.26243382692337036, 1.404764175415039, 1.0307296514511108, 0.9521945118904114, 0.8021228909492493, 1.0101561546325684, 1.2968636751174927, 0.6708576083183289, 0.6870415806770325, 0.9424005746841431, 0.5817038416862488, -0.17951948940753937, 1.703042984008789, 0.7027333378791809, 0.6950601935386658, 0.8083477020263672]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.0384259223937988, 0.36952152848243713, 1.117867112159729, 1.3419568538665771, 0.9636462330818176, 0.6412700414657593, -0.336606502532959, 1.924345850944519, -0.7310248017311096, 2.269864320755005, -0.7221192717552185, 1.0200285911560059, 1.2903882265090942, 1.349133849143982, 1.24822998046875, -0.10862579941749573]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.7078778147697449, 1.4575145244598389, 1.7801506519317627, 1.962473750114441, 2.049002170562744, 1.6746084690093994, 1.570515751838684, 1.731655478477478, 1.8122421503067017, 1.6852105855941772, 2.1011035442352295, 1.5793286561965942, 1.566497564315796, 1.6989673376083374, 1.1407777070999146, 2.1343801021575928]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.5721819400787354, 1.1725143194198608, 1.8239516019821167, 2.226191282272339, 1.214254379272461, 1.45701265335083, 1.5924144983291626, 2.0861427783966064, 2.597262144088745, 1.5474673509597778, 1.4333902597427368, 1.2520252466201782, 1.4383407831192017, 1.6753756999969482, 1.5365657806396484, 1.5557790994644165]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.907246470451355, 1.765567421913147, 1.4744387865066528, 1.6163402795791626, 1.901153802871704, 1.3921916484832764, 1.7403454780578613, 1.6358016729354858, 1.7816970348358154, 1.5811878442764282, 1.93672776222229, 1.6598014831542969, 2.518866777420044, 1.9286017417907715, 1.8555107116699219, 1.7289057970046997]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6930847764015198, 1.2336519956588745, 1.114064335823059, 0.8531486392021179, 0.13780252635478973, 1.6509294509887695, 1.2230966091156006, 1.2169004678726196, 0.7609747648239136, 1.2127422094345093, 1.2419908046722412, 1.0877197980880737, 1.1423134803771973, 1.3506388664245605, 1.1907182931900024, 0.914248526096344]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.468398332595825, 2.613896131515503, 2.529653310775757, 2.5288658142089844, 2.4552114009857178, 2.7599995136260986, 2.614030361175537, 2.631566047668457, 2.4171671867370605, 2.6557586193084717, 2.719578266143799, 2.6773600578308105, 2.3093948364257812, 2.5667669773101807, 2.6923646926879883, 2.684053659439087]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.7149322032928467, 1.599387526512146, 1.7316672801971436, 1.433597445487976, 1.720839500427246, 1.7571183443069458, 1.6541765928268433, 1.4846973419189453, 1.7428380250930786, 1.9797248840332031, 1.549301266670227, 1.7708102464675903, 1.6626489162445068, 1.6644511222839355, 1.613765001296997, 2.009974718093872]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.617412805557251, 2.098003387451172, 1.7307465076446533, 1.844262957572937, 1.9880651235580444, 1.9056862592697144, 1.8205854892730713, 1.911380648612976, 1.9222031831741333, 1.7023898363113403, 1.9728399515151978, 1.9208518266677856, 1.8191699981689453, 1.0727049112319946, 1.8568377494812012, 1.935458779335022]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.741518259048462, 2.941481590270996, 2.7025485038757324, 2.534587860107422, 2.8343212604522705, 2.6514856815338135, 2.827286720275879, 2.588033437728882, 2.6360628604888916, 2.749199867248535, 2.8526339530944824, 2.6459109783172607, 2.674699306488037, 2.914867639541626, 2.7946505546569824, 2.7852869033813477]
Running loglikelihood requests:  64%|███████████████████████████████████████████████████████████████████████████████████████▋                                                | 129/200 [02:37<01:08,  1.04it/s]Layer: gate_24 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.036811590194702, 2.0013904571533203, 1.97483229637146, 2.2510311603546143, 1.6009101867675781, 2.013580799102783, 2.0297205448150635, 2.2507736682891846, 2.101679563522339, 2.0095741748809814, 2.0988385677337646, 2.021425485610962, 2.032958984375, 2.0437092781066895, 2.0190012454986572, 2.2491745948791504]
Layer: gate_25 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [2.1271753311157227, 2.056002378463745, 2.0217080116271973, 2.1965739727020264, 2.004629373550415, 2.160109519958496, 2.055567741394043, 1.9575011730194092, 2.0249900817871094, 2.2841358184814453, 2.512629985809326, 2.0714237689971924, 2.0956945419311523, 2.306358575820923, 2.1033496856689453, 2.181781768798828]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.0842809677124023, 1.962236762046814, 1.9831162691116333, 2.11466646194458, 1.8975245952606201, 1.9763288497924805, 2.0901739597320557, 2.1101200580596924, 1.9815996885299683, 2.109308958053589, 1.6287258863449097, 1.986276626586914, 2.2315304279327393, 2.2082090377807617, 1.9473087787628174, 1.997069001197815]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.281773567199707, 3.2887349128723145, 3.7098662853240967, 3.336247205734253, 3.5365424156188965, 3.2863965034484863, 3.403611421585083, 3.4741737842559814, 3.2024309635162354, 3.349412202835083, 3.288048505783081, 3.0500895977020264, 3.367729902267456, 3.074655055999756, 3.476618528366089, 3.2454464435577393]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [6.737875938415527, 7.283234119415283, 6.92294979095459, 6.6703267097473145, 6.568240165710449, 6.424332141876221, 7.041019916534424, 7.021331310272217, 7.104252338409424, 6.711328983306885, 6.812392711639404, 6.673797130584717, 6.826169967651367, 6.922834873199463, 7.202978134155273, 6.7986578941345215]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.415918350219727, 4.53115177154541, 4.303008556365967, 3.9559435844421387, 4.682486057281494, 4.453813076019287, 4.102809906005859, 4.326656818389893, 4.477816104888916, 4.406771183013916, 4.3257832527160645, 3.6983938217163086, 4.131711006164551, 4.692193508148193, 4.423847198486328, 4.23175048828125]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [3.0645058155059814, 2.9295809268951416, 2.980804204940796, 2.9148800373077393, 2.892160654067993, 2.987569808959961, 3.014195203781128, 2.7449839115142822, 2.8836395740509033, 2.836822986602783, 3.2707104682922363, 2.1856069564819336, 2.837399482727051, 2.8048181533813477, 3.159976005554199, 2.96323561668396]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.08630599081516266, 0.09766875952482224, 0.1016964316368103, -0.27936607599258423, -0.24461624026298523, -0.1196412667632103, 0.1204092726111412, -0.09317293018102646, 0.07302144914865494, 0.08649042248725891, 0.09843580424785614, 0.0604180209338665, 0.08711665868759155, 0.10610992461442947, -1.103255271911621, 0.11581695824861526]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.06568071991205215, 0.028280960395932198, 0.02257181890308857, 0.03830159828066826, 0.06964541226625443, 0.012631393037736416, 0.040266554802656174, 0.07342799007892609, 0.012900213710963726, 0.05071588233113289, -0.22213545441627502, 0.03123878315091133, 0.033715151250362396, -0.02988715097308159, 0.012712528929114342, 0.0010603766422718763]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05381345376372337, 0.038781050592660904, 0.07915913313627243, 0.08061741292476654, 0.08489580452442169, 0.0900462344288826, 0.06333251297473907, -0.13497161865234375, 0.06645071506500244, 0.07714596390724182, 0.012603416107594967, 0.08293016999959946, -0.21423129737377167, -0.0012352041667327285, -0.06068803742527962, 0.09242194890975952]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13113966584205627, 0.11968603730201721, 0.08431770652532578, 0.11818621307611465, 0.07495066523551941, 0.10372450947761536, -0.04922603815793991, 0.17901723086833954, 0.14378097653388977, -0.6381776928901672, -0.05439278855919838, 0.0438198521733284, 0.17538481950759888, -0.29594025015830994, -0.09014122933149338, -0.17316608130931854]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.038121823221445084, 0.07581270486116409, 0.10185576230287552, 0.10777316987514496, 0.03859411180019379, -0.09948209673166275, 0.011366399936378002, -0.020770182833075523, 0.0056671625934541225, 0.05363708361983299, -0.23284190893173218, 0.11482897400856018, -0.03975696861743927, -0.27269166707992554, 0.08026741445064545, -0.03051307424902916]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.11117760092020035, 0.1305365115404129, 0.031509608030319214, 0.057264067232608795, -0.04913945123553276, -0.05911334604024887, 0.09323671460151672, 0.09092572331428528, 0.1382664293050766, -0.10593360662460327, -0.03493361920118332, 0.13393327593803406, -0.2023693025112152, 0.1559324562549591, 0.18733246624469757, 0.08520055562257767]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.10874360799789429, -0.03391074016690254, -0.014623938128352165, 0.29381829500198364, 0.1536916047334671, 0.06582444161176682, -0.15722599625587463, -0.05002664774656296, 0.06511368602514267, 0.15803731977939606, -0.6517714858055115, 0.26422610878944397, 0.3177430033683777, -0.3315936028957367, 0.22451172769069672, 0.2318115234375]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.09099012613296509, -0.06920292973518372, 0.037079207599163055, -0.02761504426598549, -1.2025256156921387, -0.11252675950527191, -0.08254165947437286, -0.597244143486023, 0.0876360535621643, -0.175140380859375, -0.26753494143486023, -0.015644395723938942, -0.023843729868531227, 0.05755503103137016, -0.6192779541015625, -0.056504279375076294]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20784354209899902, 0.20875070989131927, -0.1611403077840805, 0.3774813115596771, 0.3566727936267853, 0.3035246729850769, 0.12919923663139343, 0.1603829562664032, -0.0212458074092865, 0.3869856595993042, 0.31986933946609497, 0.08448892086744308, 0.511040210723877, -0.2480139136314392, -0.1654651015996933, 0.1709415167570114]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.25969091057777405, 0.18198548257350922, 0.13279485702514648, 0.8488529920578003, 0.27091485261917114, 0.2907726466655731, 0.5486642122268677, 0.6751216053962708, 0.21322493255138397, 0.11061659455299377, 0.2645101249217987, 0.494289368391037, 0.5664495825767517, 0.1918928176164627, 0.8061859607696533, 0.6579388380050659]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.9233559370040894, 0.9289558529853821, 0.3103553354740143, 0.6461880207061768, 0.5285249352455139, -0.17944833636283875, 0.3993839621543884, 0.7269343137741089, -0.07386080920696259, -0.22964443266391754, 0.4681848883628845, 0.6248161792755127, 0.09693735092878342, 0.17836950719356537, 0.2719612419605255, 0.3309767246246338]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.30315452814102173, 0.5685547590255737, 0.5521096587181091, 0.3519609570503235, 0.7390651702880859, 0.24875138700008392, 0.5996822118759155, 0.27312421798706055, 0.47420498728752136, 0.9592670798301697, 0.48190411925315857, 0.19813400506973267, 0.04956503584980965, 0.01314616110175848, -0.3920723497867584, 0.4951264262199402]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.13941821455955505, 0.8078007102012634, 0.9061946868896484, 0.5439143180847168, 0.0650956928730011, 0.4176623523235321, 0.519728422164917, 0.6480181217193604, 0.4369402527809143, 0.8069145679473877, 1.698368787765503, 0.6580609679222107, 0.7618688344955444, 0.6692774891853333, 0.5574273467063904, 0.7126677632331848]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.880990207195282, 0.9750524163246155, 1.155261516571045, 0.25758808851242065, 0.809289276599884, 0.26334819197654724, 0.8639189004898071, 1.1585345268249512, 0.7223122715950012, 0.7842853665351868, 0.8293325304985046, 1.4521796703338623, 0.04683639109134674, 0.881192684173584, 0.7252814769744873, 0.5330745577812195]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.4037494361400604, 0.6283749341964722, 1.0458738803863525, 0.9330559372901917, 0.7981780171394348, 0.8853927850723267, -0.160677969455719, 0.779219388961792, 0.4377838969230652, 0.413095623254776, -0.519851803779602, 0.3275083601474762, 0.9418483376502991, 0.9678536057472229, 1.1715465784072876, 0.172367662191391]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.4467959702014923, 1.3850276470184326, 1.0175292491912842, 0.8005597591400146, 0.835243284702301, 1.0071316957473755, 1.307409405708313, 0.5803395509719849, 0.7531331181526184, 0.849339485168457, 0.6352822780609131, 0.16412165760993958, 1.6474192142486572, 0.7507210969924927, 0.8397364020347595, 1.0595412254333496]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9472904205322266, 0.44941896200180054, 0.9341030120849609, 1.0444731712341309, 0.8506659865379333, 0.6649313569068909, -0.20754043757915497, 1.5114048719406128, -0.24489723145961761, 2.3161535263061523, -0.42835187911987305, 0.9509099125862122, 1.1543002128601074, 1.154477834701538, 1.0246835947036743, 0.07762075215578079]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.8115116953849792, 1.5755870342254639, 1.6775866746902466, 1.8727664947509766, 1.9341604709625244, 1.5494023561477661, 1.5023092031478882, 1.7143957614898682, 1.8165017366409302, 1.8382848501205444, 2.1125998497009277, 1.3650076389312744, 1.2746269702911377, 1.4012635946273804, 1.2521777153015137, 2.031620740890503]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.623386263847351, 1.1984654664993286, 1.8261350393295288, 2.227640151977539, 1.4432029724121094, 1.69484281539917, 1.6890755891799927, 2.0568065643310547, 2.8102669715881348, 1.6110398769378662, 1.540964961051941, 1.3991724252700806, 1.6071721315383911, 1.7262645959854126, 1.659854769706726, 1.5965853929519653]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.874224066734314, 1.7420226335525513, 1.50981605052948, 1.5499470233917236, 1.8729538917541504, 1.4959572553634644, 1.7230194807052612, 1.704400897026062, 1.8851393461227417, 1.479027271270752, 1.953888177871704, 1.6506953239440918, 2.527733564376831, 1.881983995437622, 1.8178850412368774, 1.7582844495773315]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.7932656407356262, 1.2146631479263306, 1.1056948900222778, 0.6764041781425476, 0.3513593077659607, 1.4707602262496948, 1.1857669353485107, 1.2213510274887085, 0.7456706762313843, 1.1285430192947388, 1.1753679513931274, 1.0474152565002441, 1.0397536754608154, 1.3050734996795654, 1.1416985988616943, 1.0653042793273926]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.2659294605255127, 2.4391491413116455, 2.360304594039917, 2.3431174755096436, 2.2612926959991455, 2.534196138381958, 2.4304027557373047, 2.4801294803619385, 2.2876925468444824, 2.5068304538726807, 2.5352509021759033, 2.519634485244751, 2.183302164077759, 2.479243278503418, 2.535412073135376, 2.5060811042785645]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.6119837760925293, 1.5015645027160645, 1.6896742582321167, 1.335086703300476, 1.630747675895691, 1.6186330318450928, 1.5904759168624878, 1.427322506904602, 1.6018476486206055, 1.853668451309204, 1.5214980840682983, 1.7736175060272217, 1.5772490501403809, 1.5324546098709106, 1.5280473232269287, 1.8988672494888306]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.547950029373169, 2.0398991107940674, 1.660892128944397, 1.6728789806365967, 1.9008774757385254, 1.7601256370544434, 1.7357455492019653, 1.8385875225067139, 1.7871116399765015, 1.6790786981582642, 1.8150111436843872, 1.8350764513015747, 1.7420517206192017, 1.0656776428222656, 1.7569822072982788, 1.8748631477355957]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.590646266937256, 2.7369577884674072, 2.5657601356506348, 2.372389554977417, 2.717780828475952, 2.571357488632202, 2.7298269271850586, 2.5172858238220215, 2.5046355724334717, 2.6134519577026367, 2.754171133041382, 2.5223584175109863, 2.5331244468688965, 2.7548911571502686, 2.6143901348114014, 2.6283202171325684]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_25 - Captured router_logits: [1.95892333984375, 1.9386775493621826, 1.8328181505203247, 2.159850597381592, 1.5489583015441895, 1.9325746297836304, 1.9208067655563354, 2.193629741668701, 1.9625719785690308, 1.9361772537231445, 1.9838651418685913, 1.9502812623977661, 1.9876590967178345, 1.9498308897018433, 1.9294205904006958, 2.133388042449951]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [2.05422043800354, 1.9413834810256958, 1.9243062734603882, 2.096842050552368, 1.9380985498428345, 2.0242838859558105, 2.007155179977417, 1.9116891622543335, 1.9471429586410522, 2.194884777069092, 2.4575035572052, 1.9548871517181396, 2.0261130332946777, 2.2169649600982666, 1.9622200727462769, 2.0561721324920654]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.0162885189056396, 1.9380927085876465, 1.9782918691635132, 2.0671932697296143, 1.913969874382019, 1.932963490486145, 2.0629470348358154, 2.0616986751556396, 1.9669697284698486, 2.056868076324463, 1.66367506980896, 1.9436595439910889, 2.140627145767212, 2.1953511238098145, 1.9170689582824707, 1.9625684022903442]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Running loglikelihood requests:  66%|██████████████████████████████████████████████████████████████████████████████████████████▍                                             | 133/200 [02:40<01:03,  1.05it/s]Layer: gate_28 - Captured router_logits: [3.220113754272461, 3.260070323944092, 3.6593644618988037, 3.2559399604797363, 3.4136667251586914, 3.2296524047851562, 3.390195608139038, 3.3995296955108643, 3.1992578506469727, 3.3188424110412598, 3.243842124938965, 3.0514094829559326, 3.310749053955078, 3.073880910873413, 3.456188201904297, 3.1974551677703857]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [6.726385593414307, 7.239071846008301, 6.862624645233154, 6.673077583312988, 6.586843013763428, 6.435163497924805, 7.032397747039795, 6.983341217041016, 7.034295082092285, 6.6795759201049805, 6.807635307312012, 6.574758529663086, 6.819116592407227, 6.935550689697266, 7.1726579666137695, 6.8588762283325195]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.140959739685059, 4.207372188568115, 4.03205680847168, 3.6993846893310547, 4.361439228057861, 4.21221399307251, 3.898434638977051, 4.084158420562744, 4.215151309967041, 4.10219144821167, 4.083478927612305, 3.5053977966308594, 3.9222402572631836, 4.328429698944092, 4.166068077087402, 3.9822959899902344]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.9075911045074463, 2.770993232727051, 2.798031806945801, 2.6676011085510254, 2.7268216609954834, 2.807175397872925, 2.802229881286621, 2.612924337387085, 2.702157735824585, 2.7198450565338135, 3.124924898147583, 2.1170520782470703, 2.7254111766815186, 2.7126593589782715, 3.040269136428833, 2.8204715251922607]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06440191715955734, 0.07585961371660233, 0.084434874355793, -0.2551463544368744, -0.22957849502563477, -0.053104665130376816, 0.10519080609083176, -0.05124200880527496, 0.0702512338757515, 0.060901835560798645, 0.07366646826267242, 0.06347339600324631, 0.0833134800195694, 0.08696628361940384, -1.0322041511535645, 0.09620537608861923]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.052934497594833374, 0.022464700043201447, 0.02384938858449459, 0.027916084975004196, 0.065480537712574, 0.0029464082326740026, 0.03852967917919159, 0.07987050712108612, 0.002920509548857808, 0.04815065488219261, -0.21079204976558685, 0.03152671083807945, 0.026555219665169716, -0.0432128831744194, 0.023953301832079887, 0.006992959417402744]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.046009745448827744, 0.027101734653115273, 0.08045531809329987, 0.0867185965180397, 0.08639822900295258, 0.07962331920862198, 0.037807218730449677, -0.12459506839513779, 0.06831253319978714, 0.11130596697330475, 0.03066515550017357, 0.08251749724149704, -0.23805762827396393, -0.01627918891608715, -0.08863994479179382, 0.0897306576371193]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12879325449466705, 0.09130677580833435, 0.05982920899987221, 0.09903322905302048, 0.08867939561605453, 0.09545499831438065, -0.05702148377895355, 0.15499241650104523, 0.1279202401638031, -0.627524197101593, -0.020803995430469513, 0.029976574704051018, 0.2165674865245819, -0.3395698070526123, -0.01600484549999237, -0.1484200656414032]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.013255552388727665, 0.09232412278652191, 0.11508888006210327, 0.1515902429819107, 0.05455984175205231, -0.11989457160234451, -0.015716716647148132, -0.026572834700345993, -0.05058660730719566, 0.02190704084932804, -0.25742700695991516, 0.12123190611600876, -0.028826916590332985, -0.25971484184265137, 0.09608129411935806, -0.04239773750305176]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.12723837792873383, 0.13557511568069458, 0.03610900044441223, 0.1081719920039177, -0.07065006345510483, -0.06324107199907303, 0.1341145634651184, 0.056380774825811386, 0.1703682839870453, -0.11427535116672516, 0.028307953849434853, 0.13143029808998108, -0.22829411923885345, 0.1753997951745987, 0.18267594277858734, 0.07770346850156784]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.14571090042591095, -0.07802724838256836, 0.04549708217382431, 0.3293102979660034, 0.17243406176567078, 0.12229804694652557, -0.15476840734481812, -0.0980149582028389, 0.03795164078474045, 0.16827943921089172, -0.6691163182258606, 0.2433215230703354, 0.32007959485054016, -0.4026552140712738, 0.2976548373699188, 0.2449808120727539]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.04026195779442787, -0.07541511952877045, 0.005672239698469639, -0.011883064173161983, -1.2225263118743896, -0.03752162680029869, -0.13866691291332245, -0.6538304686546326, 0.11500291526317596, -0.16020062565803528, -0.4263960123062134, -0.02817060798406601, -0.0449397899210453, 0.09157954156398773, -0.7908260226249695, -0.06857012957334518]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.2061508297920227, 0.25068822503089905, -0.1528875231742859, 0.4262113869190216, 0.46087202429771423, 0.3096342384815216, 0.18233804404735565, 0.2045747935771942, 0.05642365664243698, 0.29118430614471436, 0.33380863070487976, 0.1469506174325943, 0.5565192103385925, -0.26714828610420227, -0.12931784987449646, 0.19843704998493195]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.27158835530281067, 0.17964476346969604, 0.11217369884252548, 0.8329108953475952, 0.20557881891727448, 0.2929203510284424, 0.5004957318305969, 0.655665934085846, 0.24177467823028564, 0.02821238897740841, 0.19598537683486938, 0.4964865446090698, 0.6604556441307068, 0.2277194708585739, 0.839999794960022, 0.6812331676483154]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.9451528191566467, 0.8912560939788818, 0.3370431661605835, 0.6961255073547363, 0.5599134564399719, -0.13460208475589752, 0.4274832308292389, 0.7378963828086853, -0.09215573966503143, -0.37910744547843933, 0.4798329174518585, 0.6552591919898987, 0.2133513242006302, 0.16132000088691711, 0.2842235565185547, 0.2988540530204773]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.21787500381469727, 0.5503460764884949, 0.5718989968299866, 0.301992803812027, 0.7451980113983154, 0.09536068886518478, 0.5129673480987549, 0.21230080723762512, 0.4874191880226135, 0.8539659976959229, 0.4347522258758545, 0.10876782983541489, -0.09326468408107758, -0.1566466987133026, -0.5760026574134827, 0.3100866973400116]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [-0.13577093183994293, 0.6863497495651245, 0.957431435585022, 0.48245131969451904, -0.1448463350534439, 0.43152427673339844, 0.4744853973388672, 0.6237782835960388, 0.3083548843860626, 0.6481436491012573, 1.6883044242858887, 0.6729909181594849, 0.6652531623840332, 0.7507210969924927, 0.5351323485374451, 0.7253672480583191]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.9322749376296997, 1.2590444087982178, 1.1681129932403564, 0.09891080111265182, 0.854107141494751, 0.16790072619915009, 1.06196129322052, 1.200243592262268, 0.8738067150115967, 0.9431536793708801, 1.0811781883239746, 1.6292564868927002, -0.07682447135448456, 0.9595140814781189, 1.0630487203598022, 0.5928559303283691]
Running loglikelihood requests:  68%|█████████████████████████████████████████████████████████████████████████████████████████████▏                                          | 137/200 [02:44<00:59,  1.06it/s]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.4071942865848541, 0.7064177989959717, 1.2062662839889526, 1.0767734050750732, 1.0036078691482544, 0.779299259185791, -0.6563225388526917, 0.7224827408790588, 0.5749256610870361, 0.22117923200130463, -0.8140782117843628, 0.3879198729991913, 0.870957612991333, 1.1066590547561646, 1.4141989946365356, 0.08190227299928665]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.21286021173000336, 1.43683922290802, 1.0637245178222656, 1.00984787940979, 0.7524120211601257, 0.9264653325080872, 1.2399886846542358, 0.6149342656135559, 0.7407070398330688, 0.8862884044647217, 0.4786887466907501, -0.4498472511768341, 1.6323277950286865, 0.6980652809143066, 0.6715881824493408, 0.7480141520500183]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.8457965850830078, 0.1900523453950882, 1.0100698471069336, 1.1359847784042358, 1.0622014999389648, 0.4866634011268616, -0.3930629789829254, 1.819946527481079, -0.949480414390564, 2.3225109577178955, -0.8589707612991333, 0.8575443625450134, 1.227141261100769, 1.1804933547973633, 1.1113672256469727, -0.31469178199768066]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.6660690307617188, 1.3817250728607178, 1.7086126804351807, 2.0565357208251953, 2.105328321456909, 1.6318780183792114, 1.5153573751449585, 1.7880022525787354, 1.941022276878357, 1.8143731355667114, 2.2697231769561768, 1.5231719017028809, 1.4798146486282349, 1.8263318538665771, 1.2581537961959839, 2.1969501972198486]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.569104552268982, 1.0880569219589233, 1.6533247232437134, 2.23419189453125, 1.0302506685256958, 1.4850614070892334, 1.5072283744812012, 2.0371689796447754, 2.80588436126709, 1.5411828756332397, 1.4107205867767334, 1.1028015613555908, 1.3243719339370728, 1.6326072216033936, 1.5199189186096191, 1.5542365312576294]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.8386056423187256, 1.6994805335998535, 1.4302842617034912, 1.5721909999847412, 1.7886592149734497, 1.3576805591583252, 1.6330128908157349, 1.6941514015197754, 1.8562294244766235, 1.5430926084518433, 1.9220890998840332, 1.6569349765777588, 2.5985910892486572, 1.8468736410140991, 1.7357150316238403, 1.6615188121795654]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8637557029724121, 1.287163257598877, 1.2296608686447144, 0.8866589665412903, 0.02708485536277294, 1.6734037399291992, 1.2081189155578613, 1.2584710121154785, 0.8443678617477417, 1.2137700319290161, 1.3272781372070312, 1.119776725769043, 1.0779491662979126, 1.3824982643127441, 1.205837368965149, 0.8924423456192017]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.3090641498565674, 2.450335741043091, 2.439314603805542, 2.3590917587280273, 2.2522566318511963, 2.5586705207824707, 2.4203903675079346, 2.5193629264831543, 2.2433948516845703, 2.499525785446167, 2.547635078430176, 2.476247787475586, 2.1055963039398193, 2.437401056289673, 2.580282688140869, 2.453490734100342]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.7316781282424927, 1.6537421941757202, 1.796617031097412, 1.3965057134628296, 1.7612298727035522, 1.7662427425384521, 1.745177984237671, 1.5466923713684082, 1.7504826784133911, 2.0423221588134766, 1.639278531074524, 1.8168226480484009, 1.6835812330245972, 1.6111692190170288, 1.6267234086990356, 2.0523250102996826]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.6391754150390625, 2.1484360694885254, 1.658721923828125, 1.8104122877120972, 2.024399757385254, 1.8230786323547363, 1.779008388519287, 1.850306510925293, 1.907317042350769, 1.6755388975143433, 1.9028501510620117, 1.9303820133209229, 1.8389273881912231, 0.9507672190666199, 1.919479250907898, 1.9498533010482788]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.71392822265625, 2.9272477626800537, 2.6749892234802246, 2.5327107906341553, 2.8641223907470703, 2.685523509979248, 2.8672266006469727, 2.6430206298828125, 2.65958309173584, 2.680053949356079, 3.0137012004852295, 2.6795756816864014, 2.7389278411865234, 2.917581081390381, 2.773279905319214, 2.8653745651245117]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.060744047164917, 2.028320789337158, 1.9335960149765015, 2.265756607055664, 1.5321394205093384, 1.9977567195892334, 1.991977572441101, 2.2551772594451904, 2.0581214427948, 1.9730039834976196, 2.0782368183135986, 2.0151519775390625, 2.03024959564209, 2.0135645866394043, 2.0139851570129395, 2.1933255195617676]
Layer: gate_25 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [2.171553611755371, 2.059312582015991, 1.9861009120941162, 2.173492193222046, 2.026013135910034, 2.137972354888916, 2.026792526245117, 1.9655097723007202, 1.9969964027404785, 2.2677438259124756, 2.5355570316314697, 2.046600818634033, 2.0973241329193115, 2.342163324356079, 2.061706066131592, 2.1466283798217773]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.0355241298675537, 1.911726951599121, 1.966814637184143, 2.075374126434326, 1.9304403066635132, 1.9535162448883057, 2.082240581512451, 2.0515005588531494, 1.977809190750122, 2.1170129776000977, 1.6357994079589844, 1.9584929943084717, 2.1964807510375977, 2.1126527786254883, 1.9459975957870483, 1.979919672012329]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.226994276046753, 3.2544071674346924, 3.697361707687378, 3.2827515602111816, 3.4469099044799805, 3.2598390579223633, 3.41512393951416, 3.417717456817627, 3.1867754459381104, 3.337252616882324, 3.251088857650757, 3.0230605602264404, 3.363182544708252, 3.0267012119293213, 3.456712007522583, 3.189521551132202]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [6.8395819664001465, 7.366327285766602, 6.892373085021973, 6.722423553466797, 6.638067245483398, 6.446805477142334, 7.107904434204102, 7.039591312408447, 7.162100791931152, 6.776598930358887, 6.899017333984375, 6.6495361328125, 6.89713716506958, 6.9388346672058105, 7.255560874938965, 6.842336654663086]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.161357402801514, 4.220134258270264, 4.000817775726318, 3.646864175796509, 4.314022064208984, 4.200231075286865, 3.8365399837493896, 4.072778224945068, 4.113632678985596, 4.113752365112305, 4.067074298858643, 3.4994359016418457, 3.889897584915161, 4.348268508911133, 4.158779144287109, 3.9861257076263428]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [2.846940755844116, 2.7555673122406006, 2.720612049102783, 2.632200002670288, 2.6542282104492188, 2.705909490585327, 2.8028762340545654, 2.5281474590301514, 2.6742618083953857, 2.655740737915039, 3.1451098918914795, 2.0333669185638428, 2.637650728225708, 2.613103151321411, 2.938164234161377, 2.81196928024292]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.0949626937508583, 0.10381948947906494, 0.10580217838287354, -0.19161786139011383, -0.13537490367889404, -0.14314347505569458, 0.1248428076505661, -0.1898220330476761, 0.07712581008672714, 0.08657315373420715, 0.10123132914304733, 0.057328350841999054, 0.0894438847899437, 0.10391578078269958, -0.8429108262062073, 0.12798641622066498]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.07257220149040222, 0.04349246993660927, 0.027995355427265167, 0.049542974680662155, 0.07390912622213364, 0.04694709554314613, 0.03360716253519058, 0.05450023338198662, -0.00303800031542778, 0.053800664842128754, -0.1589561551809311, 0.038753774017095566, 0.02285570651292801, -0.012220911681652069, -0.0061219846829771996, 0.008400220423936844]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.050898920744657516, 0.05334422364830971, 0.10820037871599197, 0.06798699498176575, 0.0819212794303894, 0.1082284078001976, 0.043374575674533844, -0.09574273228645325, 0.061292700469493866, 0.03289517015218735, -0.009763994254171848, 0.10190418362617493, -0.1666772961616516, 0.025199975818395615, -0.047947172075510025, 0.1239921823143959]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.10289999097585678, 0.1332617700099945, 0.11064884066581726, 0.1545151025056839, 0.10042355954647064, 0.10094356536865234, 0.0142449252307415, 0.17221283912658691, 0.1331939548254013, -0.5129539370536804, -0.09463857859373093, 0.06968317925930023, 0.06194806471467018, -0.2301255464553833, -0.1497011035680771, -0.13366904854774475]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07439588755369186, 0.09446354955434799, 0.09375590831041336, 0.022978225722908974, 0.03213199973106384, -0.11755456030368805, 0.04093899205327034, 0.017352323979139328, 0.06781409680843353, 0.026619214564561844, -0.13672155141830444, 0.12618674337863922, -0.16366508603096008, -0.2236984521150589, 0.06949593126773834, 0.0038766716606914997]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.12471230328083038, 0.1473245769739151, 0.05922820419073105, 0.025357147678732872, -0.0062920753844082355, -0.0073011452332139015, 0.019987961277365685, 0.13273081183433533, 0.03315664082765579, -0.06351572275161743, -0.1635511815547943, 0.16690489649772644, -0.1287681758403778, 0.13375064730644226, 0.22813814878463745, 0.08552512526512146]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.14725282788276672, 0.015722990036010742, 0.0027230866253376007, 0.29243820905685425, 0.15361037850379944, 0.06084286421537399, -0.12005242705345154, -0.01480274461209774, -0.023782946169376373, 0.1649608612060547, -0.4427047371864319, 0.24168169498443604, 0.3979843556880951, -0.1876142919063568, 0.12359432876110077, 0.17872482538223267]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.3057253956794739, -0.01387693453580141, 0.19335897266864777, 0.000517766980919987, -0.8462414741516113, -0.18685314059257507, -0.23340235650539398, -0.24960675835609436, -0.23072382807731628, -0.22317436337471008, -0.1183355450630188, -0.09531857818365097, 0.08371639996767044, -0.1467994600534439, -0.3354673981666565, 0.11657615005970001]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.1259419023990631, 0.16825027763843536, 0.04583633691072464, 0.3502819240093231, 0.24707138538360596, 0.32997334003448486, 0.12552618980407715, 0.2631402611732483, -0.06353643536567688, 0.09218986332416534, 0.2667739689350128, 0.02402830310165882, 0.4212053418159485, -0.12499672174453735, -0.02069968543946743, 0.1034160703420639]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.32650306820869446, 0.19090011715888977, 0.16262909770011902, 0.5713066458702087, 0.3720239996910095, 0.49846789240837097, 0.7765701413154602, 0.7406128644943237, 0.24779468774795532, 0.2235594540834427, 0.48630577325820923, 0.5939715504646301, 0.7482929825782776, 0.4178733825683594, 0.8795827627182007, 0.7421414256095886]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7807399034500122, 0.8649290800094604, 0.3443337082862854, 0.7117829322814941, 0.7429137825965881, -0.057300858199596405, 0.5217182040214539, 0.7862288355827332, 0.08870020508766174, -0.1666608601808548, 0.5578957200050354, 0.6044962406158447, 0.31519511342048645, 0.22566716372966766, 0.18019701540470123, 0.5508838295936584]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.39050552248954773, 0.6225168704986572, 0.7848302125930786, 0.6012496948242188, 0.7718773484230042, 0.5926169157028198, 0.8768311142921448, 0.5520880222320557, 0.5252792239189148, 0.8225074410438538, 0.6471362709999084, 0.6024619936943054, 0.3282111883163452, 0.39800816774368286, -0.10957800596952438, 0.7542835474014282]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.27901896834373474, 0.990683913230896, 0.7397468686103821, 0.5641654133796692, 0.38243481516838074, 0.48280882835388184, 0.3953784108161926, 0.44916442036628723, 0.6883012056350708, 0.9041994214057922, 1.414505958557129, 0.6115886569023132, 0.902928352355957, 0.7087475657463074, 0.5660359859466553, 0.691755473613739]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8772900700569153, 0.8268906474113464, 1.1009564399719238, 0.5985000133514404, 0.8530218601226807, 0.48994743824005127, 0.7414294481277466, 0.6877288222312927, 0.6379286050796509, 0.7262094616889954, 0.7386582493782043, 1.2242494821548462, 0.460265189409256, 1.1313252449035645, 0.8372198343276978, 0.7814425826072693]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5013590455055237, 0.47040852904319763, 0.8675894141197205, 0.8129234910011292, 0.5443029999732971, 0.6043106913566589, 0.25542357563972473, 0.4304217994213104, 0.5460120439529419, 0.5941519737243652, 0.024306271225214005, 0.37623390555381775, 0.8742062449455261, 0.6157686710357666, 0.8436904549598694, 0.38630780577659607]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6836668848991394, 0.9326715469360352, 0.9998363256454468, 0.73332279920578, 0.7242347598075867, 1.055916428565979, 1.0580087900161743, 0.6707738041877747, 0.6743518114089966, 0.7674462795257568, 0.7986280918121338, 0.6351502537727356, 1.266747236251831, 0.7641521096229553, 0.8761134743690491, 0.8177230954170227]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.095963954925537, 0.6728739738464355, 0.9618530869483948, 1.1533900499343872, 0.7753851413726807, 0.9205300807952881, -0.2888279855251312, 1.3905014991760254, 0.04046936705708504, 1.7133575677871704, -0.35680776834487915, 1.0180363655090332, 1.2821391820907593, 1.0936485528945923, 1.0395963191986084, 0.21513447165489197]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  70%|███████████████████████████████████████████████████████████████████████████████████████████████▉                                        | 141/200 [02:48<00:55,  1.07it/s]Layer: gate_17 - Captured router_logits: [0.8057249784469604, 1.4803082942962646, 1.3880469799041748, 1.7000020742416382, 1.6384047269821167, 1.3608709573745728, 1.6437326669692993, 1.621411919593811, 1.2762731313705444, 1.6580240726470947, 1.5556368827819824, 1.1207664012908936, 1.1027151346206665, 1.3389265537261963, 1.372393250465393, 1.9584487676620483]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.395733118057251, 1.0418297052383423, 1.5228804349899292, 1.8114298582077026, 1.628526210784912, 1.8873775005340576, 1.4701619148254395, 1.862776517868042, 2.139965772628784, 1.4528074264526367, 1.486525058746338, 1.4803348779678345, 1.6961456537246704, 1.6616222858428955, 1.7961230278015137, 1.5534898042678833]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.0265324115753174, 1.9093979597091675, 1.585317611694336, 1.816870093345642, 1.960432529449463, 1.8251330852508545, 1.8784165382385254, 2.0037295818328857, 1.84030020236969, 1.4865849018096924, 1.977115273475647, 1.6020092964172363, 2.5092873573303223, 2.013930320739746, 1.8989235162734985, 1.9727630615234375]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.7320327758789062, 1.3254140615463257, 1.2751753330230713, 0.8972334861755371, 0.778897762298584, 1.4313315153121948, 1.3468682765960693, 1.2455693483352661, 1.091922402381897, 1.0302776098251343, 1.208299160003662, 0.979981541633606, 1.1606462001800537, 1.542877435684204, 1.3680973052978516, 1.1067922115325928]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_21 - Captured router_logits: [2.572397470474243, 2.6564724445343018, 2.6890194416046143, 2.7694904804229736, 2.690206289291382, 2.834596633911133, 2.8143742084503174, 2.685166835784912, 2.729051113128662, 2.9078516960144043, 2.8468878269195557, 2.6992599964141846, 2.8194007873535156, 2.7552876472473145, 2.956345796585083, 2.8274130821228027]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.6674213409423828, 1.298532247543335, 1.462845802307129, 1.359666109085083, 1.6946964263916016, 1.465767741203308, 1.6639620065689087, 1.3365195989608765, 1.5867646932601929, 1.7369741201400757, 1.2930330038070679, 1.5720553398132324, 1.6953763961791992, 1.5438947677612305, 1.6746588945388794, 1.8310816287994385]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.1732895374298096, 1.8582383394241333, 1.853564977645874, 1.7725251913070679, 1.8690741062164307, 1.8237154483795166, 1.8436800241470337, 2.057786464691162, 1.6944782733917236, 1.9143821001052856, 1.9152441024780273, 1.637172818183899, 1.9095326662063599, 1.38670814037323, 1.736608862876892, 1.9547643661499023]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.5597219467163086, 2.69720458984375, 2.510119676589966, 2.5461580753326416, 2.654524564743042, 2.907210350036621, 2.7075512409210205, 2.6342954635620117, 2.584009885787964, 2.8012797832489014, 2.4753470420837402, 2.8129193782806396, 2.6086812019348145, 2.750293016433716, 2.7126903533935547, 2.7042765617370605]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.9889007806777954, 1.9517608880996704, 1.9132139682769775, 2.1361138820648193, 1.8405433893203735, 1.94100821018219, 1.9203007221221924, 1.9602919816970825, 1.9548512697219849, 2.027890682220459, 2.022237539291382, 2.03603458404541, 2.01871657371521, 2.0263125896453857, 1.7345401048660278, 2.1680259704589844]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.0029118061065674, 1.909851312637329, 1.975447654724121, 2.114326238632202, 2.030174732208252, 1.9075919389724731, 2.026787757873535, 1.964656949043274, 1.984721064567566, 2.2121806144714355, 2.2867512702941895, 2.1042728424072266, 2.1176369190216064, 2.1225550174713135, 1.9043195247650146, 2.0023744106292725]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.012162923812866, 1.959701657295227, 2.0767931938171387, 2.0834908485412598, 1.9393655061721802, 2.046339273452759, 2.11590838432312, 2.095926284790039, 2.0245678424835205, 2.0694165229797363, 1.8485101461410522, 2.025660276412964, 2.054792642593384, 2.0253326892852783, 2.103973388671875, 2.064232110977173]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.417980670928955, 3.445014238357544, 3.762061595916748, 3.5628459453582764, 3.6677498817443848, 3.4422061443328857, 3.6227688789367676, 3.5615482330322266, 3.397341251373291, 3.545424461364746, 3.5212833881378174, 3.405024766921997, 3.587395429611206, 3.357401132583618, 3.6480236053466797, 3.4834492206573486]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.158868789672852, 7.488953590393066, 7.224445343017578, 6.9655632972717285, 7.130784511566162, 6.881800651550293, 7.352296829223633, 7.296244144439697, 7.38200044631958, 7.084122657775879, 7.229069709777832, 7.032748222351074, 7.164400577545166, 7.272374629974365, 7.467471122741699, 7.283151149749756]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.5631103515625, 4.4716477394104, 4.385187149047852, 3.948662042617798, 4.743718147277832, 4.646108627319336, 4.143354415893555, 4.288194179534912, 4.422243595123291, 4.416635990142822, 4.306063175201416, 3.7373046875, 4.2163801193237305, 4.69328498840332, 4.420240879058838, 4.42581844329834]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1801939010620117, 3.254530191421509, 3.0950021743774414, 3.176379680633545, 3.0357096195220947, 3.1573963165283203, 3.1813411712646484, 2.9781346321105957, 3.171705722808838, 2.9991278648376465, 3.38666033744812, 2.2264652252197266, 3.080604314804077, 3.071012020111084, 3.374042510986328, 3.0587522983551025]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09189761430025101, 0.10282013565301895, 0.1045229583978653, -0.20762969553470612, -0.14807119965553284, -0.15588140487670898, 0.12709976732730865, -0.18045911192893982, 0.0798928439617157, 0.08656690269708633, 0.10252665728330612, 0.060676876455545425, 0.09219086915254593, 0.10275281965732574, -0.8963738083839417, 0.1262167990207672]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07315182685852051, 0.04231462627649307, 0.03233621269464493, 0.04180488735437393, 0.07224904745817184, 0.03729512542486191, 0.03009818121790886, 0.05176413059234619, 0.005943150259554386, 0.05117197707295418, -0.17341642081737518, 0.03263373672962189, 0.030008811503648758, -0.024794790893793106, -0.009671024978160858, 0.006404750049114227]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.04747871309518814, 0.04082583263516426, 0.10115911811590195, 0.07062999904155731, 0.07952094823122025, 0.10497530549764633, 0.043122388422489166, -0.11107677221298218, 0.056244753301143646, 0.02846040204167366, -0.010442622937262058, 0.09863720834255219, -0.16461898386478424, 0.01626533269882202, -0.03591509163379669, 0.12380590289831161]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.09886527061462402, 0.136949360370636, 0.10227403044700623, 0.1420602947473526, 0.09140786528587341, 0.10684822499752045, -0.0029779316391795874, 0.16689223051071167, 0.13100452721118927, -0.5202823281288147, -0.10040471702814102, 0.051159895956516266, 0.07156164944171906, -0.20926888287067413, -0.15617822110652924, -0.13595688343048096]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.06010597571730614, 0.09419331699609756, 0.09415630996227264, 0.02265058644115925, 0.024691591039299965, -0.13177813589572906, 0.041730333119630814, 0.009903762489557266, 0.08270248025655746, 0.04212324693799019, -0.1555187702178955, 0.1224837675690651, -0.1664034128189087, -0.2128494381904602, 0.0664035752415657, 0.03098919987678528]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.1288418471813202, 0.1486077904701233, 0.048824504017829895, 0.006503010168671608, 0.01598373055458069, 0.01223782915621996, 0.04595838487148285, 0.14433178305625916, 0.0458400659263134, -0.06516470015048981, -0.12059894949197769, 0.17123189568519592, -0.1465340256690979, 0.13338367640972137, 0.2352772206068039, 0.088210828602314]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.14055363833904266, 0.03190123662352562, -0.03448929265141487, 0.2862981855869293, 0.15148822963237762, 0.053542908281087875, -0.10759946703910828, -0.019350847229361534, -0.035984039306640625, 0.15099811553955078, -0.46066030859947205, 0.25558626651763916, 0.4027930498123169, -0.18441909551620483, 0.1244533434510231, 0.19552724063396454]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.29714953899383545, -0.025950897485017776, 0.1581384539604187, -0.00696589658036828, -0.9199610352516174, -0.20090407133102417, -0.20623049139976501, -0.25925785303115845, -0.2020256370306015, -0.2540384829044342, -0.14724783599376678, -0.07301369309425354, 0.11054243892431259, -0.11310099810361862, -0.4037935733795166, 0.15577176213264465]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.1398334503173828, 0.1731264889240265, 0.002126213163137436, 0.37025243043899536, 0.2490626573562622, 0.3225785195827484, 0.11522690206766129, 0.2633228898048401, -0.05988502502441406, 0.0999336689710617, 0.2951599061489105, 0.01729300618171692, 0.4457953870296478, -0.16348181664943695, -0.05445251986384392, 0.12258867174386978]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3322063386440277, 0.18343499302864075, 0.1512012928724289, 0.6124809980392456, 0.3583621382713318, 0.49454957246780396, 0.7293550372123718, 0.7835814952850342, 0.19114288687705994, 0.21004249155521393, 0.4609536826610565, 0.5874726176261902, 0.7760812640190125, 0.3429562449455261, 0.9340590834617615, 0.7278625965118408]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7928285598754883, 0.8777679800987244, 0.35334518551826477, 0.6893740296363831, 0.6836600303649902, -0.11361467093229294, 0.497879296541214, 0.7958346009254456, 0.0874938890337944, -0.19793254137039185, 0.5376336574554443, 0.5775254368782043, 0.34567537903785706, 0.23016348481178284, 0.15468287467956543, 0.5042580366134644]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.365207701921463, 0.571683406829834, 0.766241192817688, 0.5365532636642456, 0.7659602165222168, 0.5233844518661499, 0.8358156085014343, 0.5062340497970581, 0.4916636645793915, 0.8298466205596924, 0.595858633518219, 0.49618595838546753, 0.2714044153690338, 0.38299766182899475, -0.13216710090637207, 0.7013831734657288]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.2780064642429352, 0.9591680765151978, 0.7478719353675842, 0.5693280100822449, 0.3953835368156433, 0.4827314019203186, 0.43253880739212036, 0.46590855717658997, 0.6520617008209229, 0.9447094202041626, 1.5117192268371582, 0.617220938205719, 0.8795880675315857, 0.7048202753067017, 0.5856756567955017, 0.7156233191490173]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8772193193435669, 0.8232574462890625, 1.089282512664795, 0.6141295433044434, 0.8149805665016174, 0.4705059826374054, 0.7284572124481201, 0.6974446177482605, 0.6311945915222168, 0.6894153356552124, 0.7047082781791687, 1.2001020908355713, 0.48937925696372986, 1.1123656034469604, 0.8405354022979736, 0.7397474646568298]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.4117351472377777, 0.48706912994384766, 0.7474468350410461, 0.7169673442840576, 0.46802258491516113, 0.6239475607872009, 0.31906595826148987, 0.4115082025527954, 0.41992998123168945, 0.504115641117096, 0.09714552015066147, 0.3384224474430084, 0.7861784100532532, 0.5360264778137207, 0.7248727679252625, 0.39934608340263367]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.7311962246894836, 0.8798782825469971, 1.032035231590271, 0.7219106554985046, 0.703094482421875, 1.090096354484558, 0.9157761931419373, 0.6723842620849609, 0.6674018502235413, 0.6895869374275208, 0.7828101515769958, 0.6959083676338196, 1.2026710510253906, 0.7017059326171875, 0.8375009894371033, 0.7843695878982544]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.105630874633789, 0.6557228565216064, 0.9169329404830933, 1.1046805381774902, 0.65109783411026, 0.9866778254508972, -0.23373253643512726, 1.46397066116333, 0.1800665259361267, 1.7013440132141113, -0.05397454649209976, 1.0100122690200806, 1.148258090019226, 1.0827187299728394, 0.9702231884002686, 0.2600661516189575]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.8751023411750793, 1.3997352123260498, 1.4052048921585083, 1.6890852451324463, 1.5464308261871338, 1.2607184648513794, 1.6858150959014893, 1.5168989896774292, 1.160700798034668, 1.5967034101486206, 1.5064098834991455, 1.0311458110809326, 1.066718578338623, 1.3433163166046143, 1.4567592144012451, 1.897207260131836]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.2723978757858276, 0.8719469904899597, 1.4416165351867676, 1.7341607809066772, 1.525969386100769, 1.7691949605941772, 1.312294363975525, 1.7690019607543945, 2.003983974456787, 1.3397126197814941, 1.4169758558273315, 1.4737900495529175, 1.634661078453064, 1.60176420211792, 1.761364459991455, 1.4948234558105469]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.1087708473205566, 1.9461742639541626, 1.5712943077087402, 1.8197190761566162, 1.9521980285644531, 1.8046705722808838, 1.9456133842468262, 2.0227103233337402, 1.7588266134262085, 1.4535728693008423, 2.0037057399749756, 1.6056493520736694, 2.6263394355773926, 2.0320889949798584, 1.9339683055877686, 1.9600305557250977]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6261504292488098, 1.2964813709259033, 1.2849764823913574, 0.9401777386665344, 0.7867866158485413, 1.3849540948867798, 1.3067747354507446, 1.0632433891296387, 1.0545010566711426, 0.9416322112083435, 1.114256501197815, 0.8045560121536255, 1.120492935180664, 1.5000485181808472, 1.3441829681396484, 1.1917835474014282]
Running loglikelihood requests:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                                     | 145/200 [02:51<00:50,  1.10it/s]Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_21 - Captured router_logits: [2.474025011062622, 2.5404748916625977, 2.5477283000946045, 2.7587594985961914, 2.6114494800567627, 2.812314987182617, 2.766580581665039, 2.675400495529175, 2.585702896118164, 2.858623743057251, 2.7546029090881348, 2.638845205307007, 2.7198455333709717, 2.5705974102020264, 2.881740093231201, 2.778139114379883]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.6223070621490479, 1.2118690013885498, 1.3411023616790771, 1.3392269611358643, 1.6928759813308716, 1.4195603132247925, 1.5923430919647217, 1.2887030839920044, 1.5340932607650757, 1.7450528144836426, 1.2024506330490112, 1.5156561136245728, 1.6089868545532227, 1.5522159337997437, 1.623678207397461, 1.7901910543441772]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.247699737548828, 1.8983373641967773, 1.9562584161758423, 1.8238639831542969, 1.9685732126235962, 1.8432587385177612, 1.836590051651001, 2.1280274391174316, 1.7822086811065674, 1.9129432439804077, 1.9821847677230835, 1.6378663778305054, 1.9728202819824219, 1.422023892402649, 1.840164065361023, 1.9965721368789673]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.703054666519165, 2.9130821228027344, 2.657437801361084, 2.7148120403289795, 2.8174922466278076, 3.0696539878845215, 2.8890793323516846, 2.786626100540161, 2.8243210315704346, 2.9783437252044678, 2.6448514461517334, 2.9778106212615967, 2.8133463859558105, 2.941192865371704, 2.9351248741149902, 2.9046294689178467]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.0992608070373535, 2.0267984867095947, 2.0048277378082275, 2.2295608520507812, 1.8659148216247559, 2.021212100982666, 2.0293588638305664, 2.0218114852905273, 2.063288450241089, 2.118896484375, 2.090059280395508, 2.1049976348876953, 2.1231980323791504, 2.1393239498138428, 1.7933616638183594, 2.276336431503296]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.063828945159912, 1.9122172594070435, 1.9915748834609985, 2.1402974128723145, 2.0392017364501953, 1.9164986610412598, 2.081127166748047, 1.9762948751449585, 1.9850085973739624, 2.2181174755096436, 2.2909996509552, 2.1011619567871094, 2.1098825931549072, 2.126845598220825, 1.896242380142212, 2.0239436626434326]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.0638341903686523, 1.9506103992462158, 2.0828022956848145, 2.1132192611694336, 1.984893798828125, 2.0702626705169678, 2.117142915725708, 2.163642168045044, 2.079742431640625, 2.1394572257995605, 1.8474241495132446, 2.06180739402771, 2.105713129043579, 2.018418312072754, 2.114842176437378, 2.060123920440674]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.548506498336792, 3.5772407054901123, 3.947171211242676, 3.7366504669189453, 3.8178889751434326, 3.623342514038086, 3.7384631633758545, 3.7802574634552, 3.59211802482605, 3.7010622024536133, 3.6653614044189453, 3.575115203857422, 3.727997064590454, 3.490057945251465, 3.7500147819519043, 3.6682863235473633]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.4277238845825195, 7.740997791290283, 7.485689640045166, 7.242806911468506, 7.358785629272461, 7.065945148468018, 7.601941108703613, 7.577200412750244, 7.551122665405273, 7.262109756469727, 7.495902061462402, 7.274255752563477, 7.384946823120117, 7.534983158111572, 7.74407958984375, 7.5410475730896]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_30 - Captured router_logits: [4.594363212585449, 4.522589683532715, 4.404348850250244, 3.9462778568267822, 4.740200519561768, 4.624273777008057, 4.202331066131592, 4.298303604125977, 4.394641399383545, 4.43604850769043, 4.313632965087891, 3.789419174194336, 4.250765323638916, 4.692689418792725, 4.501801013946533, 4.4048285484313965]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1707985401153564, 3.2561864852905273, 3.0719428062438965, 3.150278329849243, 3.00750732421875, 3.1396496295928955, 3.1838197708129883, 2.9814565181732178, 3.127377986907959, 3.014451742172241, 3.363605260848999, 2.2509765625, 3.0158870220184326, 3.0471065044403076, 3.334662437438965, 3.0604772567749023]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.09237700700759888, 0.10580160468816757, 0.10760931670665741, -0.16279004514217377, -0.12262105196714401, -0.15863074362277985, 0.12961284816265106, -0.19233787059783936, 0.0905020460486412, 0.09281137585639954, 0.09857640415430069, 0.07957091182470322, 0.08966532349586487, 0.10157954692840576, -0.8632550239562988, 0.1284143626689911]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07190202176570892, 0.03801316022872925, 0.020139232277870178, 0.03604991361498833, 0.08137871325016022, 0.0505497045814991, 0.04667860269546509, 0.0545421801507473, 0.0043043638579547405, 0.04731842875480652, -0.14368796348571777, 0.0398346371948719, 0.031005315482616425, -0.018724633380770683, -0.0032818512991070747, 0.006178759969770908]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05807355046272278, 0.035649046301841736, 0.10431119799613953, 0.07850631326436996, 0.09012655913829803, 0.10532517731189728, 0.05513808876276016, -0.10927154123783112, 0.06085270643234253, 0.03412073478102684, -0.010165557265281677, 0.10065305978059769, -0.15369386970996857, 0.027409857138991356, -0.04941318929195404, 0.11270339787006378]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.10833795368671417, 0.13122674822807312, 0.09702307730913162, 0.1474020928144455, 0.09099750965833664, 0.10895951837301254, 0.009280710481107235, 0.17709322273731232, 0.12329975515604019, -0.4926602840423584, -0.12974457442760468, 0.07753095775842667, 0.07650154829025269, -0.22432656586170197, -0.17681731283664703, -0.14283359050750732]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.07072115689516068, 0.10661550611257553, 0.1111113429069519, 0.01776731200516224, 0.01950059086084366, -0.12566618621349335, 0.03737277165055275, 0.0451510064303875, 0.025373945012688637, 0.03477812185883522, -0.14018575847148895, 0.15205124020576477, -0.17171213030815125, -0.2151128649711609, 0.07393103092908859, -0.011240359395742416]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.14729425311088562, 0.15118210017681122, 0.05784622207283974, 0.011078913696110249, 0.01036654319614172, 0.026781396940350533, 0.02500482089817524, 0.1204080805182457, 0.04495515301823616, -0.07273127138614655, -0.16357454657554626, 0.1577945351600647, -0.10101364552974701, 0.1337452232837677, 0.2114543318748474, 0.0879753977060318]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.15862736105918884, 0.038737062364816666, -0.04331231489777565, 0.31105321645736694, 0.15369370579719543, 0.05670493468642235, -0.0758470967411995, -0.047180745750665665, 0.02758917212486267, 0.16399680078029633, -0.3990394175052643, 0.2840060591697693, 0.35673439502716064, -0.13195554912090302, 0.10194899141788483, 0.20046909153461456]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.24602153897285461, 0.07011666148900986, 0.17815202474594116, 0.016986804082989693, -0.8552232384681702, -0.16475076973438263, -0.1639917492866516, -0.2704509496688843, -0.20792758464813232, -0.27519622445106506, -0.11703277379274368, -0.0358097106218338, 0.13078854978084564, -0.13022072613239288, -0.32683897018432617, 0.08229167759418488]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.17100223898887634, 0.16928303241729736, -0.043929461389780045, 0.33432242274284363, 0.24572746455669403, 0.3163679242134094, 0.09097066521644592, 0.3208383023738861, -0.05668304115533829, 0.11997400969266891, 0.2784605622291565, 0.03257911279797554, 0.41889622807502747, -0.15119928121566772, -0.06061665713787079, 0.11184433847665787]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3619179129600525, 0.259785532951355, 0.15549679100513458, 0.6008238196372986, 0.36186984181404114, 0.4490010142326355, 0.7498940229415894, 0.7688803672790527, 0.31938794255256653, 0.2571978271007538, 0.4821138381958008, 0.5803912878036499, 0.7776849865913391, 0.41931405663490295, 0.8906723260879517, 0.7839400172233582]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7868708372116089, 0.8182977437973022, 0.3909909427165985, 0.6869662404060364, 0.6356494426727295, -0.07388082891702652, 0.48624297976493835, 0.7821452021598816, 0.13705211877822876, -0.13022971153259277, 0.5693596601486206, 0.5916433930397034, 0.3150098919868469, 0.25643041729927063, 0.19073359668254852, 0.41600847244262695]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.2788543701171875, 0.46411240100860596, 0.6578376293182373, 0.47906678915023804, 0.6481128931045532, 0.33310091495513916, 0.7334281802177429, 0.39289864897727966, 0.41736170649528503, 0.746603786945343, 0.5039957165718079, 0.39981865882873535, 0.16229866445064545, 0.16535289585590363, -0.16636250913143158, 0.5085245966911316]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.14953450858592987, 0.9193499088287354, 0.7200819849967957, 0.5086625218391418, 0.25240838527679443, 0.5281299948692322, 0.3702726662158966, 0.4593845307826996, 0.5454956889152527, 0.8145485520362854, 1.4215761423110962, 0.6362237930297852, 0.8477491736412048, 0.6951536536216736, 0.6339601278305054, 0.6840612888336182]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.9595198035240173, 0.9476122856140137, 1.2167994976043701, 0.5413654446601868, 0.8959575891494751, 0.48803433775901794, 0.8432698845863342, 0.7537052035331726, 0.7094041705131531, 0.8345848917961121, 0.8523009419441223, 1.2837575674057007, 0.48957332968711853, 1.2327979803085327, 1.0185809135437012, 0.8655261397361755]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5973159670829773, 0.6581867337226868, 1.0624455213546753, 1.0403687953948975, 0.7764966487884521, 0.8412409424781799, 0.2272326946258545, 0.5580878853797913, 0.6349809169769287, 0.5569208860397339, 0.028357217088341713, 0.5089844465255737, 1.0012400150299072, 0.8442422151565552, 1.0857398509979248, 0.4747675955295563]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.6589688658714294, 1.0569965839385986, 1.1221381425857544, 0.8038439750671387, 0.8671844005584717, 1.149276852607727, 1.0752748250961304, 0.7052577137947083, 0.6928530335426331, 0.8154910802841187, 0.8179905414581299, 0.4480125308036804, 1.3411155939102173, 0.8061698079109192, 0.9041829705238342, 0.9967869520187378]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.3281680345535278, 0.8136601448059082, 1.0734448432922363, 1.1555601358413696, 0.9732626080513, 1.0720460414886475, -0.13116344809532166, 1.7127537727355957, 0.30462536215782166, 2.269676685333252, 0.014408593066036701, 1.2611448764801025, 1.3863576650619507, 1.3323863744735718, 1.1820788383483887, 0.4624665081501007]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.2165367603302002, 1.7282099723815918, 1.592853307723999, 2.074711322784424, 1.9187254905700684, 1.5817164182662964, 1.8738319873809814, 1.8682050704956055, 1.7033419609069824, 2.044896125793457, 2.060873508453369, 1.3703293800354004, 1.3375962972640991, 1.606854796409607, 1.8074345588684082, 2.2707152366638184]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.2234280109405518, 0.9615264534950256, 1.4668519496917725, 1.7858569622039795, 1.5499804019927979, 1.8383468389511108, 1.4463528394699097, 1.9499585628509521, 2.3321027755737305, 1.480609655380249, 1.5773957967758179, 1.7435730695724487, 1.7148301601409912, 1.8828978538513184, 1.862277865409851, 1.5354046821594238]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.2152581214904785, 2.048194408416748, 1.6735869646072388, 1.874155879020691, 1.979568362236023, 1.9361658096313477, 2.0479824542999268, 2.126525640487671, 1.994171142578125, 1.6416621208190918, 2.0928218364715576, 1.7936675548553467, 2.7002205848693848, 2.1359944343566895, 1.9858481884002686, 2.047544002532959]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.7302415370941162, 1.3470579385757446, 1.3384984731674194, 0.924375593662262, 0.8332884907722473, 1.3455063104629517, 1.3242727518081665, 1.3571909666061401, 1.0172303915023804, 1.0056904554367065, 1.2941101789474487, 0.98675936460495, 1.141093134880066, 1.4972286224365234, 1.4191824197769165, 1.2356737852096558]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_21 - Captured router_logits: [2.600130319595337, 2.694394588470459, 2.7514543533325195, 2.858046293258667, 2.71575927734375, 2.8393666744232178, 2.9278807640075684, 2.809297800064087, 2.8011257648468018, 2.952420711517334, 2.876006841659546, 2.7375290393829346, 2.794849395751953, 2.810871124267578, 3.027061939239502, 2.874667167663574]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.927052617073059, 1.5296880006790161, 1.7375820875167847, 1.5663880109786987, 1.9474964141845703, 1.7083619832992554, 1.91754150390625, 1.6184312105178833, 1.8429301977157593, 2.091722249984741, 1.5665572881698608, 1.8172292709350586, 1.8922383785247803, 1.7809944152832031, 1.9295575618743896, 2.1656136512756348]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.326606273651123, 2.0142862796783447, 1.9787712097167969, 1.8597877025604248, 1.9267916679382324, 1.847372055053711, 1.8948490619659424, 2.1519253253936768, 1.7900065183639526, 1.852338433265686, 2.017150640487671, 1.8167965412139893, 2.0031704902648926, 1.3725231885910034, 1.9371916055679321, 2.1045455932617188]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.6321239471435547, 2.785104513168335, 2.662383794784546, 2.58290958404541, 2.8112435340881348, 2.906564474105835, 2.819652795791626, 2.7588062286376953, 2.7612335681915283, 2.915656805038452, 2.6228928565979004, 2.859426259994507, 2.7353570461273193, 2.7928478717803955, 2.807863473892212, 2.822866439819336]
Running loglikelihood requests:  74%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                  | 149/200 [02:55<00:46,  1.09it/s]Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.0077924728393555, 1.9842525720596313, 1.9230797290802002, 2.1756246089935303, 1.7192610502243042, 1.9906973838806152, 1.9013656377792358, 2.061404228210449, 1.9811949729919434, 1.9918080568313599, 2.0551085472106934, 2.0572280883789062, 2.0961859226226807, 2.0654916763305664, 1.7537271976470947, 2.1711061000823975]
Layer: gate_25 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [2.1068179607391357, 1.9692353010177612, 2.000326156616211, 2.1446499824523926, 2.069611072540283, 1.9294242858886719, 2.065727710723877, 1.9819419384002686, 1.9327635765075684, 2.2240803241729736, 2.30478572845459, 2.09611177444458, 2.115328311920166, 2.284468412399292, 1.906623125076294, 2.054591417312622]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.9653187990188599, 1.9079821109771729, 2.0428779125213623, 2.0593061447143555, 1.938356637954712, 2.031104564666748, 2.082949161529541, 2.051189422607422, 2.011394739151001, 2.0741934776306152, 1.7550128698349, 1.9899481534957886, 2.012877941131592, 2.006077527999878, 1.9890968799591064, 1.9818066358566284]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.4915452003479004, 3.569270610809326, 3.8742244243621826, 3.6610710620880127, 3.6985180377960205, 3.5141801834106445, 3.560488700866699, 3.675330638885498, 3.493788480758667, 3.6495680809020996, 3.588890552520752, 3.427018880844116, 3.646955966949463, 3.413623571395874, 3.710829496383667, 3.5534508228302]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.027092933654785, 7.338339328765869, 7.1681904792785645, 6.9858880043029785, 6.947513580322266, 6.816065788269043, 7.384885311126709, 7.233550071716309, 7.311185359954834, 7.006622791290283, 7.144092082977295, 6.992431640625, 7.096193790435791, 7.1851372718811035, 7.359000205993652, 7.171684265136719]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.290005683898926, 4.243655204772949, 4.159675598144531, 3.8168985843658447, 4.510250091552734, 4.340481758117676, 4.02742338180542, 4.1861348152160645, 4.2215166091918945, 4.2437872886657715, 4.216072082519531, 3.672333002090454, 4.089738368988037, 4.366968154907227, 4.215496063232422, 4.104729175567627]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.1697349548339844, 3.1003024578094482, 2.9982447624206543, 2.9061317443847656, 2.9676334857940674, 3.085592031478882, 3.0132596492767334, 2.9251153469085693, 3.039468765258789, 3.0484619140625, 3.2684812545776367, 2.362039804458618, 3.0480549335479736, 3.069577693939209, 3.294248580932617, 3.079078197479248]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.08807609230279922, 0.09902159124612808, 0.1080334484577179, -0.23930902779102325, -0.22651422023773193, -0.08923932164907455, 0.12427138537168503, -0.11513194441795349, 0.08172585815191269, 0.08610134571790695, 0.10119817405939102, 0.07110122591257095, 0.0961446762084961, 0.1096060574054718, -1.0448001623153687, 0.1182071641087532]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.06313762813806534, 0.03532977029681206, 0.029684970155358315, 0.042068783193826675, 0.07171207666397095, 0.01695689745247364, 0.03865843638777733, 0.06116228178143501, 0.015425642021000385, 0.05937005206942558, -0.20758748054504395, 0.04315899685025215, 0.03662364184856415, -0.02333647944033146, 0.01779359020292759, 0.0055472650565207005]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.06304779648780823, 0.046106141060590744, 0.0843769833445549, 0.07937855273485184, 0.08435755968093872, 0.1080750823020935, 0.058034349232912064, -0.1273752748966217, 0.07837210595607758, 0.06398838013410568, 0.021394535899162292, 0.0890253484249115, -0.21959872543811798, 0.02804148755967617, -0.07379747182130814, 0.10807617753744125]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.12793488800525665, 0.10345863550901413, 0.08496623486280441, 0.1197449266910553, 0.08470293134450912, 0.09273576736450195, -0.03250167891383171, 0.17459507286548615, 0.14281976222991943, -0.5911270976066589, -0.03440461307764053, 0.04150497540831566, 0.1341129094362259, -0.267928808927536, -0.09681078046560287, -0.1438591182231903]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.04213309660553932, 0.07956544309854507, 0.10772150754928589, 0.09266948699951172, 0.05888236686587334, -0.09803051501512527, -0.002241137670353055, -0.013022246770560741, 0.007325112819671631, 0.053422775119543076, -0.21696354448795319, 0.09955448657274246, -0.07732585817575455, -0.2450992614030838, 0.08546902984380722, -0.009450250305235386]
Layer: gate_4 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.1005367711186409, 0.1532723307609558, 0.05697594955563545, 0.048016924411058426, -0.02066420204937458, -0.06301390379667282, 0.07537014037370682, 0.08617552369832993, 0.10360557585954666, -0.0890987291932106, -0.021100535988807678, 0.15652184188365936, -0.23736345767974854, 0.16010606288909912, 0.18981434404850006, 0.09455672651529312]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.13322477042675018, -0.052344124764204025, -0.005224175285547972, 0.29643937945365906, 0.15621280670166016, 0.06366408616304398, -0.15178021788597107, -0.08850362151861191, 0.10228433459997177, 0.15128743648529053, -0.6239571571350098, 0.2512427866458893, 0.3426431119441986, -0.338543564081192, 0.1622762233018875, 0.24483056366443634]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.141233429312706, -0.08973535150289536, 0.07211471349000931, -0.005755288060754538, -1.0464016199111938, -0.12094337493181229, -0.11153829097747803, -0.5357621312141418, 0.015936387702822685, -0.14082609117031097, -0.33685147762298584, -0.04339304193854332, 0.011819228529930115, 0.04545661807060242, -0.6809051632881165, 0.015924444422125816]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.21941180527210236, 0.21403300762176514, -0.11928293853998184, 0.3815169632434845, 0.3456231355667114, 0.30539825558662415, 0.1051262840628624, 0.12897001206874847, 0.006293810438364744, 0.36031079292297363, 0.33242687582969666, 0.09112179279327393, 0.5027823448181152, -0.26721909642219543, -0.20151294767856598, 0.17983411252498627]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.307174950838089, 0.193648099899292, 0.13024769723415375, 0.8504474759101868, 0.27651432156562805, 0.33458444476127625, 0.5868386626243591, 0.7358723282814026, 0.2598002254962921, 0.19595931470394135, 0.24002306163311005, 0.5274756550788879, 0.652921736240387, 0.20849545300006866, 0.8220639228820801, 0.6429398655891418]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [0.916344940662384, 0.9257585406303406, 0.329648494720459, 0.6606051921844482, 0.6227198243141174, -0.10903606563806534, 0.40579426288604736, 0.778906524181366, -0.08696494251489639, -0.21049761772155762, 0.5105648636817932, 0.6364429593086243, 0.21020515263080597, 0.21853812038898468, 0.3378112316131592, 0.3655589520931244]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.2475445717573166, 0.5110209584236145, 0.5418546795845032, 0.4172845184803009, 0.696815550327301, 0.29905688762664795, 0.6597523093223572, 0.2877242863178253, 0.4746227264404297, 0.9073511958122253, 0.4409193694591522, 0.21386665105819702, 0.031896088272333145, 0.07581094652414322, -0.323021799325943, 0.4757259786128998]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.12649525701999664, 0.8344586491584778, 0.8455279469490051, 0.5327093005180359, 0.17370742559432983, 0.4598979651927948, 0.514894962310791, 0.6238594055175781, 0.4977017343044281, 0.7919743657112122, 1.712278962135315, 0.6707189083099365, 0.8101964592933655, 0.6869726181030273, 0.6549108624458313, 0.7067831158638]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.8572316765785217, 0.9399916529655457, 1.0952132940292358, 0.25805723667144775, 0.7656977772712708, 0.26116952300071716, 0.8499960899353027, 1.0416613817214966, 0.6963591575622559, 0.7894151210784912, 0.795473039150238, 1.4882653951644897, 0.14364393055438995, 0.9960036277770996, 0.7852156758308411, 0.5839969515800476]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5244810581207275, 0.7499497532844543, 1.1231918334960938, 1.1065367460250854, 0.9063212275505066, 0.930111825466156, -0.1902119666337967, 0.7712673544883728, 0.444578617811203, 0.44665130972862244, -0.39748135209083557, 0.39236903190612793, 1.038132905960083, 1.0732405185699463, 1.2507400512695312, 0.29738977551460266]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.37171173095703125, 1.4161854982376099, 1.0870814323425293, 0.8510108590126038, 0.8049895167350769, 1.0513616800308228, 1.2975229024887085, 0.7117366194725037, 0.7672415375709534, 0.9034704566001892, 0.6796944737434387, 0.06574270129203796, 1.7110117673873901, 0.6797011494636536, 0.8111617565155029, 1.0340780019760132]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.04755437374115, 0.4560883045196533, 1.1596293449401855, 1.278030514717102, 0.9477187991142273, 0.690474808216095, -0.3112296760082245, 1.8380122184753418, -0.5513293147087097, 2.5230748653411865, -0.5841949582099915, 1.1026197671890259, 1.2930898666381836, 1.2574437856674194, 1.236979365348816, -0.044235240668058395]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.9430751800537109, 1.7167190313339233, 1.788861870765686, 2.088844060897827, 2.0735695362091064, 1.6908856630325317, 1.6323920488357544, 1.819501519203186, 1.8684316873550415, 1.9944181442260742, 2.1824302673339844, 1.4554882049560547, 1.5165752172470093, 1.5639986991882324, 1.4182418584823608, 2.198012351989746]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.5387696027755737, 1.1205629110336304, 1.7649987936019897, 2.2444355487823486, 1.4553332328796387, 1.6642152070999146, 1.489344596862793, 2.111633539199829, 2.6022326946258545, 1.573992371559143, 1.5295966863632202, 1.4622999429702759, 1.6395936012268066, 1.9465299844741821, 1.6774845123291016, 1.5679973363876343]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.9111595153808594, 1.7522457838058472, 1.4358643293380737, 1.5492955446243286, 1.8616838455200195, 1.5153789520263672, 1.760787844657898, 1.723441481590271, 1.8162437677383423, 1.4840811491012573, 1.9333381652832031, 1.6224431991577148, 2.6751821041107178, 1.9107681512832642, 1.8263899087905884, 1.6602625846862793]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.632796049118042, 1.2272323369979858, 1.0825371742248535, 0.7369582653045654, 0.4834822118282318, 1.5578047037124634, 1.1989750862121582, 1.2288264036178589, 0.7612627148628235, 1.1484980583190918, 1.3339828252792358, 0.9668123126029968, 1.0700011253356934, 1.2988361120224, 1.2814404964447021, 0.9794771671295166]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.28642201423645, 2.4399726390838623, 2.401630163192749, 2.411245107650757, 2.307715892791748, 2.567747116088867, 2.4426872730255127, 2.536733627319336, 2.3168303966522217, 2.479990005493164, 2.5563058853149414, 2.4773952960968018, 2.2412478923797607, 2.4558677673339844, 2.627202272415161, 2.582153081893921]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.7724977731704712, 1.5770667791366577, 1.7947956323623657, 1.572047233581543, 1.7410601377487183, 1.7496322393417358, 1.7394336462020874, 1.4754704236984253, 1.7640066146850586, 2.021620035171509, 1.5687004327774048, 1.845860481262207, 1.6898937225341797, 1.6435741186141968, 1.6746565103530884, 2.0181989669799805]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.611823797225952, 2.0449306964874268, 1.7839655876159668, 1.8178545236587524, 2.052265167236328, 1.9031909704208374, 1.8118921518325806, 1.9161781072616577, 1.8921418190002441, 1.7013188600540161, 1.9516487121582031, 1.9209505319595337, 1.8644951581954956, 1.1414542198181152, 1.9601346254348755, 1.8863674402236938]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.7138149738311768, 2.9561269283294678, 2.752760648727417, 2.578087329864502, 2.934741973876953, 2.733412981033325, 2.961803436279297, 2.662827253341675, 2.7213993072509766, 2.801867723464966, 2.9203617572784424, 2.711054563522339, 2.7392513751983643, 2.9479503631591797, 2.867443084716797, 2.8774614334106445]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.0585100650787354, 2.0064337253570557, 1.9239336252212524, 2.2129604816436768, 1.5951213836669922, 1.9882031679153442, 1.9726346731185913, 2.145798444747925, 2.0368764400482178, 1.9804638624191284, 2.0606329441070557, 2.0029795169830322, 1.9940109252929688, 2.0035078525543213, 1.9093583822250366, 2.1873388290405273]
Layer: gate_25 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [2.1174042224884033, 1.9954619407653809, 1.9690966606140137, 2.147019147872925, 1.9805489778518677, 2.065051317214966, 2.026535749435425, 1.9525376558303833, 1.9920622110366821, 2.276606798171997, 2.4701101779937744, 2.017437696456909, 2.0670716762542725, 2.242475748062134, 2.0339365005493164, 2.0907065868377686]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.052509307861328, 1.938820242881775, 1.9833914041519165, 2.1266062259674072, 1.9394073486328125, 1.971916675567627, 2.0922605991363525, 2.0945777893066406, 1.9960471391677856, 2.123838186264038, 1.6818262338638306, 1.9835718870162964, 2.2031099796295166, 2.1220712661743164, 1.9652714729309082, 2.0024468898773193]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Running loglikelihood requests:  76%|████████████████████████████████████████████████████████████████████████████████████████████████████████                                | 153/200 [02:58<00:42,  1.11it/s]Layer: gate_28 - Captured router_logits: [3.3185336589813232, 3.331303596496582, 3.7419450283050537, 3.3781559467315674, 3.5553786754608154, 3.3347599506378174, 3.403773307800293, 3.5117225646972656, 3.255905866622925, 3.4334194660186768, 3.338979721069336, 3.1214301586151123, 3.4097461700439453, 3.1499383449554443, 3.5108182430267334, 3.305636167526245]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.070730686187744, 7.62633752822876, 7.170438766479492, 7.008094787597656, 6.901115417480469, 6.739220142364502, 7.391274929046631, 7.361598968505859, 7.431704044342041, 7.053320407867432, 7.118532180786133, 6.903113842010498, 7.150492191314697, 7.219898223876953, 7.533200740814209, 7.114721775054932]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.570469379425049, 4.668882846832275, 4.486862659454346, 4.112342357635498, 4.840714454650879, 4.61361837387085, 4.320172309875488, 4.446361064910889, 4.583240985870361, 4.580140590667725, 4.470573902130127, 3.9530036449432373, 4.384093761444092, 4.805922031402588, 4.621461391448975, 4.395820140838623]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.230236291885376, 3.0572071075439453, 3.07194447517395, 2.872764825820923, 2.9923324584960938, 3.1374282836914062, 3.0011661052703857, 2.8960254192352295, 2.9837558269500732, 3.057079315185547, 3.344219923019409, 2.3558568954467773, 3.000091791152954, 2.988203287124634, 3.219726800918579, 3.1847527027130127]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.060109417885541916, 0.07196659594774246, 0.08042638003826141, -0.2530422508716583, -0.2223893254995346, -0.06246509030461311, 0.10204219818115234, -0.05618805065751076, 0.06459079682826996, 0.05751410126686096, 0.06946325302124023, 0.05749841406941414, 0.08042343705892563, 0.08395609259605408, -1.019281029701233, 0.09325310587882996]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.0562087707221508, 0.02401910163462162, 0.024078398942947388, 0.027089601382613182, 0.06413363665342331, 0.003788416041061282, 0.0374256931245327, 0.07601173967123032, 2.211457831435837e-05, 0.048676133155822754, -0.21506483852863312, 0.02975308522582054, 0.025790724903345108, -0.040597252547740936, 0.020674770697951317, 0.005588687025010586]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.043234750628471375, 0.023392770439386368, 0.07869013398885727, 0.09104020893573761, 0.08825436979532242, 0.07684390246868134, 0.03991904854774475, -0.1189461424946785, 0.06327030807733536, 0.1013026311993599, 0.02731722593307495, 0.08369343727827072, -0.21903082728385925, -0.012392937205731869, -0.08033251762390137, 0.09284297376871109]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.1307387351989746, 0.09346401691436768, 0.061203595250844955, 0.10553820431232452, 0.08523044735193253, 0.09269226342439651, -0.052634213119745255, 0.1621195375919342, 0.12761534750461578, -0.6046963334083557, -0.02671254239976406, 0.03657110780477524, 0.20494084060192108, -0.3325795531272888, -0.02222331427037716, -0.14193876087665558]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.016691133379936218, 0.09007233381271362, 0.11159969121217728, 0.13822400569915771, 0.049390144646167755, -0.10432082414627075, -0.012935124337673187, -0.0275262538343668, -0.04353342577815056, 0.031309787184000015, -0.2506444752216339, 0.12152675539255142, -0.03066353127360344, -0.25399985909461975, 0.1000424325466156, -0.03578619286417961]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.1270570307970047, 0.14277943968772888, 0.034499865025281906, 0.1013794019818306, -0.04923803359270096, -0.0538930706679821, 0.12560629844665527, 0.06561067700386047, 0.16862934827804565, -0.0868515595793724, 0.04199279472231865, 0.13727517426013947, -0.21505166590213776, 0.17782852053642273, 0.19353105127811432, 0.09161711484193802]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.13788753747940063, -0.07810238003730774, 0.0433943048119545, 0.32609015703201294, 0.16567814350128174, 0.11247225105762482, -0.146225243806839, -0.077069491147995, 0.034112486988306046, 0.16182707250118256, -0.6196808815002441, 0.2478475421667099, 0.32657626271247864, -0.36034056544303894, 0.2640182375907898, 0.23331716656684875]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.04993177577853203, -0.08939636498689651, 0.02872811071574688, 0.003587663872167468, -1.1444085836410522, -0.02887296862900257, -0.12734267115592957, -0.5884019732475281, 0.11562102288007736, -0.1178552433848381, -0.3622487485408783, -0.0234773438423872, 0.005365409422665834, 0.09790819138288498, -0.6896124482154846, -0.0450722761452198]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20111438632011414, 0.26241394877433777, -0.16044394671916962, 0.4708700180053711, 0.4580429196357727, 0.30892109870910645, 0.18230795860290527, 0.18212354183197021, 0.008199432864785194, 0.2819119393825531, 0.32209405303001404, 0.11246290057897568, 0.5730177760124207, -0.2439740002155304, -0.15515416860580444, 0.21416063606739044]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.2635461390018463, 0.14163120090961456, 0.13167668879032135, 0.8345857262611389, 0.20418018102645874, 0.2671288847923279, 0.47390690445899963, 0.6387374997138977, 0.24174824357032776, 0.02564140409231186, 0.20956645905971527, 0.4583500623703003, 0.6528081893920898, 0.2189858853816986, 0.8397293090820312, 0.6918495297431946]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.9229927659034729, 0.8918912410736084, 0.3442772328853607, 0.6808323264122009, 0.5546101331710815, -0.15312793850898743, 0.42064735293388367, 0.7318841218948364, -0.04563548415899277, -0.3125268816947937, 0.5001360774040222, 0.6763487458229065, 0.2323339581489563, 0.15968696773052216, 0.28443050384521484, 0.31800520420074463]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.21227549016475677, 0.5144302248954773, 0.540955662727356, 0.31539681553840637, 0.7249561548233032, 0.14812889695167542, 0.5191014409065247, 0.25572994351387024, 0.4757506847381592, 0.8309871554374695, 0.42526233196258545, 0.14814497530460358, -0.005790760740637779, -0.0841599553823471, -0.494902104139328, 0.3361719250679016]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [-0.031341422349214554, 0.7560128569602966, 0.9690067768096924, 0.5452350974082947, -0.04851453751325607, 0.49250033497810364, 0.5102506875991821, 0.6397271156311035, 0.3420952260494232, 0.6982220411300659, 1.6541695594787598, 0.6945649981498718, 0.7366009950637817, 0.7408837676048279, 0.563403844833374, 0.7563812136650085]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.0373337268829346, 1.2155309915542603, 1.204870343208313, 0.2186613827943802, 0.9011980295181274, 0.29470452666282654, 1.055959939956665, 1.2919843196868896, 0.8970242142677307, 0.9996420741081238, 1.0616233348846436, 1.6117233037948608, 0.05062113329768181, 1.0871686935424805, 1.0947692394256592, 0.6781056523323059]
Running loglikelihood requests:  78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▊                             | 157/200 [03:02<00:38,  1.11it/s]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5401248931884766, 0.741973876953125, 1.1896909475326538, 1.1209425926208496, 0.9823513031005859, 0.8492571115493774, -0.3668404817581177, 0.8370548486709595, 0.6387637257575989, 0.33476340770721436, -0.5534802079200745, 0.5217521786689758, 0.9682251811027527, 1.113736867904663, 1.403588056564331, 0.22517839074134827]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.3294641077518463, 1.3841394186019897, 1.0775909423828125, 0.9764748215675354, 0.8219361305236816, 0.9700169563293457, 1.2244855165481567, 0.6426860094070435, 0.7427879571914673, 0.8669096231460571, 0.5664940476417542, -0.22192968428134918, 1.5669938325881958, 0.7229400873184204, 0.7151102423667908, 0.8100175261497498]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9552004933357239, 0.32551509141921997, 1.055907130241394, 1.1526986360549927, 1.0250540971755981, 0.5874888300895691, -0.23524129390716553, 1.8119773864746094, -0.7313997149467468, 2.373538017272949, -0.5836734175682068, 0.899014949798584, 1.239608883857727, 1.1298038959503174, 1.1007200479507446, -0.06710287183523178]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.9079073667526245, 1.550050139427185, 1.7412303686141968, 2.155641794204712, 2.134053945541382, 1.6522254943847656, 1.5914511680603027, 1.8298673629760742, 1.9830563068389893, 1.9652568101882935, 2.2955212593078613, 1.5637749433517456, 1.5613079071044922, 1.8304543495178223, 1.4678961038589478, 2.2213544845581055]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.4398701190948486, 1.014350175857544, 1.5527902841567993, 2.0883960723876953, 1.1251473426818848, 1.5243206024169922, 1.5561906099319458, 1.9916447401046753, 2.654648542404175, 1.470597743988037, 1.4503446817398071, 1.232132911682129, 1.3985881805419922, 1.6721675395965576, 1.538467526435852, 1.5065133571624756]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.7833515405654907, 1.6485832929611206, 1.4004400968551636, 1.5291786193847656, 1.711076259613037, 1.355976939201355, 1.5914376974105835, 1.647161841392517, 1.8304837942123413, 1.497023105621338, 1.821901798248291, 1.5568325519561768, 2.533216953277588, 1.7840427160263062, 1.6728779077529907, 1.5367941856384277]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.7888946533203125, 1.2394415140151978, 1.180057406425476, 0.8461467027664185, 0.20845258235931396, 1.5596840381622314, 1.173091173171997, 1.286512017250061, 0.83969646692276, 1.076812982559204, 1.222816824913025, 1.0573104619979858, 1.061029076576233, 1.3293029069900513, 1.1883137226104736, 0.9060413837432861]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.2588376998901367, 2.4043688774108887, 2.346266031265259, 2.3091037273406982, 2.2420573234558105, 2.5117287635803223, 2.3892264366149902, 2.4302730560302734, 2.1811537742614746, 2.474897623062134, 2.485198736190796, 2.514601469039917, 2.1036295890808105, 2.3416171073913574, 2.54616117477417, 2.4138615131378174]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.6396280527114868, 1.532136082649231, 1.711788296699524, 1.3424617052078247, 1.6657387018203735, 1.6540591716766357, 1.6430590152740479, 1.4355686902999878, 1.6498788595199585, 1.9107874631881714, 1.5283069610595703, 1.7977882623672485, 1.5895599126815796, 1.4968377351760864, 1.55486261844635, 1.911839246749878]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.508355140686035, 2.036565065383911, 1.595355749130249, 1.707850456237793, 1.8632584810256958, 1.738407015800476, 1.6614559888839722, 1.7511403560638428, 1.7682527303695679, 1.5895051956176758, 1.8059457540512085, 1.7720637321472168, 1.7376654148101807, 0.9358950853347778, 1.8054962158203125, 1.8327566385269165]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.6010406017303467, 2.7882919311523438, 2.5881779193878174, 2.452685832977295, 2.7722251415252686, 2.596630573272705, 2.751328706741333, 2.488126754760742, 2.557330369949341, 2.5942347049713135, 2.8535962104797363, 2.557555913925171, 2.6557259559631348, 2.780977487564087, 2.6646440029144287, 2.769321918487549]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.9696213006973267, 1.9481825828552246, 1.8828613758087158, 2.1656453609466553, 1.488422155380249, 1.9113250970840454, 1.9150316715240479, 2.1788175106048584, 1.989145040512085, 1.897401213645935, 1.9993929862976074, 1.92375648021698, 1.94053053855896, 1.956899881362915, 1.9607468843460083, 2.147925853729248]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [2.0971007347106934, 1.9826558828353882, 1.9391515254974365, 2.100663900375366, 1.9542878866195679, 2.0505428314208984, 1.9506691694259644, 1.9031492471694946, 1.9400146007537842, 2.167830467224121, 2.4773600101470947, 1.9596644639968872, 1.9889874458312988, 2.234567403793335, 1.9910435676574707, 2.075925588607788]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.9840799570083618, 1.8814406394958496, 1.9156990051269531, 2.0302062034606934, 1.8851248025894165, 1.915144443511963, 2.050635814666748, 2.001797676086426, 1.9315176010131836, 2.0604684352874756, 1.5929728746414185, 1.936110019683838, 2.125511884689331, 2.085965394973755, 1.9065020084381104, 1.943124532699585]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.0964818000793457, 3.12156081199646, 3.551678419113159, 3.1517446041107178, 3.315092086791992, 3.1288607120513916, 3.324855327606201, 3.2661280632019043, 3.051016330718994, 3.190141201019287, 3.1139025688171387, 2.8832714557647705, 3.225947856903076, 2.90346360206604, 3.30104923248291, 3.0916671752929688]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [6.6618781089782715, 7.1187262535095215, 6.711365222930908, 6.54806661605835, 6.449193000793457, 6.244873523712158, 6.900579452514648, 6.850332260131836, 6.971601486206055, 6.575907230377197, 6.711118221282959, 6.5022358894348145, 6.715550422668457, 6.747613906860352, 7.035241603851318, 6.677480697631836]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.065621852874756, 4.114102840423584, 3.9217522144317627, 3.5713565349578857, 4.251677513122559, 4.120527744293213, 3.76772141456604, 4.030357837677002, 4.055425643920898, 4.016261100769043, 3.983551263809204, 3.4230966567993164, 3.8275482654571533, 4.2781572341918945, 4.066195487976074, 3.9075939655303955]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_31 - Captured router_logits: [2.793971538543701, 2.6825711727142334, 2.660930633544922, 2.635195016860962, 2.6062724590301514, 2.6506361961364746, 2.778488874435425, 2.495591878890991, 2.6239781379699707, 2.5991883277893066, 3.0989432334899902, 1.9572120904922485, 2.5809197425842285, 2.573080062866211, 2.89611554145813, 2.7437126636505127]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.05082029476761818, 0.07156862318515778, 0.07900696992874146, -0.25699278712272644, -0.24278201162815094, -0.07909390330314636, 0.0970848947763443, -0.12034979462623596, 0.04505837336182594, 0.0610896497964859, 0.06503577530384064, 0.053875137120485306, 0.0681590586900711, 0.08162549883127213, -0.9467370510101318, 0.09270798414945602]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.06541046500205994, 0.036753591150045395, 0.01868312433362007, 0.035567380487918854, 0.06988445669412613, 0.023758450523018837, 0.04132622107863426, 0.04355751350522041, 0.012371162883937359, 0.04813474789261818, -0.2068995088338852, 0.056873440742492676, 0.05016833916306496, -0.03838823363184929, 0.029049169272184372, 0.015290326438844204]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05664656683802605, 0.057157836854457855, 0.07964450865983963, 0.07120994478464127, 0.06667649745941162, 0.08364426344633102, 0.04532070457935333, -0.12915872037410736, 0.1014026403427124, 0.06003929674625397, 0.015510797500610352, 0.08433586359024048, -0.24122853577136993, 0.04621170088648796, -0.06711477041244507, 0.1380568891763687]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13050000369548798, 0.12219506502151489, 0.10667163878679276, 0.07830840349197388, 0.09542940557003021, 0.12150298058986664, -0.0217950027436018, 0.17609161138534546, 0.11661480367183685, -0.6468411684036255, 0.04779960587620735, 0.07091246545314789, 0.1046796441078186, -0.33283528685569763, -0.035690970718860626, -0.12769253551959991]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.04328593984246254, 0.07300741970539093, 0.06959275901317596, 0.08212058991193771, 0.1275613158941269, -0.08344131708145142, -0.01645171269774437, -0.026577994227409363, -0.04716121405363083, 0.022849882021546364, -0.22802406549453735, 0.09664472937583923, -0.05397653579711914, -0.23143337666988373, 0.08120045065879822, -0.054626356810331345]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.0988505557179451, 0.14815934002399445, 0.03930150344967842, 0.08578040450811386, -0.054366279393434525, -0.06357865035533905, 0.11792304366827011, 0.08213703334331512, 0.0684099942445755, -0.10098239779472351, 0.030497180297970772, 0.1740724891424179, -0.22288401424884796, 0.19438864290714264, 0.1385028064250946, 0.0954209491610527]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.16611887514591217, -0.049924664199352264, 0.06584624201059341, 0.29693347215652466, 0.13223843276500702, 0.08334953337907791, -0.15245361626148224, -0.14143213629722595, -0.02416791394352913, 0.2170921117067337, -0.6100910902023315, 0.22661298513412476, 0.38742244243621826, -0.3900747001171112, 0.14579075574874878, 0.2024681121110916]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.17890983819961548, -0.029801813885569572, 0.029728449881076813, -0.044977445155382156, -1.0055243968963623, -0.07767003029584885, -0.017592597752809525, -0.5833970308303833, 0.029336553066968918, -0.08267566561698914, -0.3144812285900116, 0.0005436890060082078, -0.023447947576642036, 0.12020085006952286, -0.6130375862121582, 0.01784430257976055]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_8 - Captured router_logits: [0.11329956352710724, 0.2312631458044052, -0.2211008220911026, 0.474835604429245, 0.30136650800704956, 0.3519933819770813, 0.10984201729297638, 0.3138206899166107, -0.07273218780755997, 0.19977466762065887, 0.35166701674461365, -0.010602626949548721, 0.5685381889343262, -0.22417934238910675, -0.1999567598104477, 0.14359667897224426]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.31411316990852356, 0.13040272891521454, 0.10817386209964752, 0.6233463287353516, 0.27566105127334595, 0.2818007171154022, 0.48470911383628845, 0.640562891960144, 0.08916715532541275, 0.082710400223732, 0.17462784051895142, 0.39484813809394836, 0.5118017792701721, 0.06391392648220062, 0.7473554015159607, 0.594301164150238]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.773713231086731, 0.8735828995704651, 0.3182894289493561, 0.5927942395210266, 0.5556245446205139, -0.2690015435218811, 0.46631208062171936, 0.7450571656227112, -0.006638127379119396, -0.44313928484916687, 0.5254213213920593, 0.5098833441734314, 0.31479865312576294, 0.17086562514305115, 0.18108832836151123, 0.29333260655403137]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.3040495812892914, 0.6207973957061768, 0.7837485671043396, 0.4491002559661865, 0.9137205481529236, 0.4758715331554413, 0.6913852691650391, 0.36782005429267883, 0.5304237604141235, 0.9473066329956055, 0.6128940582275391, 0.5066012740135193, 0.36057335138320923, 0.3207888901233673, 0.008883009664714336, 0.6655006408691406]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.028249749913811684, 0.8985419869422913, 0.7541916966438293, 0.36196720600128174, 0.20773926377296448, 0.38606512546539307, 0.38026052713394165, 0.4501384198665619, 0.487539142370224, 0.8430523872375488, 1.623146653175354, 0.7106921672821045, 0.6848589777946472, 0.6016218662261963, 0.6041256785392761, 0.758805513381958]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.9366641044616699, 0.8277896046638489, 1.2801599502563477, 0.3667972683906555, 0.8091294169425964, 0.43339502811431885, 0.7538929581642151, 0.6278300285339355, 0.6166616082191467, 0.7623180747032166, 0.6180000305175781, 1.3243274688720703, 0.38005855679512024, 0.9706797003746033, 0.74140465259552, 0.524759829044342]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6914557218551636, 0.5501877069473267, 1.088554859161377, 1.0523297786712646, 0.9952958822250366, 0.7104217410087585, 0.05253443866968155, 0.5022954344749451, 0.5594739317893982, 0.6608245372772217, -0.18687379360198975, 0.5841158628463745, 1.123056173324585, 1.0310827493667603, 1.1390405893325806, 0.2880327105522156]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.5782676339149475, 1.1123838424682617, 0.9951345920562744, 0.7293661832809448, 0.8976805806159973, 1.0622016191482544, 0.9928569793701172, 0.4774492681026459, 0.568245530128479, 0.8170996904373169, 0.7234122157096863, 0.6883072853088379, 1.5521563291549683, 0.7408525943756104, 0.7053794264793396, 0.7708245515823364]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.0668320655822754, 0.6530272364616394, 0.960271418094635, 0.8334357738494873, 0.688933789730072, 0.7164998054504395, -0.16923068463802338, 1.484379529953003, 0.392835408449173, 1.7354992628097534, 0.059636183083057404, 1.0274343490600586, 1.1194928884506226, 0.9738906621932983, 0.9636629223823547, 0.07425417751073837]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                          | 161/200 [03:05<00:34,  1.13it/s]Layer: gate_17 - Captured router_logits: [1.1430553197860718, 1.837699294090271, 1.447550654411316, 1.9018782377243042, 1.6431523561477661, 1.4159021377563477, 1.3619086742401123, 1.6418840885162354, 1.3346900939941406, 1.6680737733840942, 1.6166094541549683, 1.1464617252349854, 0.9812625050544739, 1.272459626197815, 1.5197029113769531, 1.9909822940826416]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.338754415512085, 0.9636395573616028, 1.4923816919326782, 1.6978564262390137, 1.5255407094955444, 1.5412999391555786, 1.247808814048767, 1.8401051759719849, 2.1137025356292725, 1.3268318176269531, 1.6186928749084473, 1.5927575826644897, 1.6026554107666016, 1.7851431369781494, 1.497556447982788, 1.4153352975845337]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.982619285583496, 1.8291082382202148, 1.6327464580535889, 1.7447764873504639, 1.9227876663208008, 1.8141801357269287, 2.015658140182495, 1.9297269582748413, 1.7881958484649658, 1.5254651308059692, 2.0171921253204346, 1.6682860851287842, 2.4339094161987305, 2.020704507827759, 1.9079468250274658, 1.795851707458496]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8804851770401001, 1.3114057779312134, 1.188120722770691, 0.947860062122345, 0.8332720398902893, 1.5245697498321533, 1.2975037097930908, 1.2546107769012451, 1.028766393661499, 1.163354754447937, 1.0565186738967896, 1.0666546821594238, 1.3173786401748657, 1.481736183166504, 1.3152446746826172, 1.3143870830535889]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.8019630908966064, 2.8817389011383057, 2.8639626502990723, 2.8071091175079346, 2.8726930618286133, 3.017737865447998, 3.0385730266571045, 2.8394618034362793, 2.778715133666992, 3.1025118827819824, 3.0200068950653076, 2.76617169380188, 2.816990375518799, 2.8802950382232666, 3.034435749053955, 3.0590906143188477]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.8323874473571777, 1.5305556058883667, 1.5949299335479736, 1.5485413074493408, 1.7627744674682617, 1.682454228401184, 1.7710756063461304, 1.415891170501709, 1.6897327899932861, 1.8771042823791504, 1.4916963577270508, 1.5830440521240234, 1.7381123304367065, 1.604170799255371, 1.7550016641616821, 1.8878819942474365]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.123236894607544, 1.9425626993179321, 1.8302637338638306, 1.780832052230835, 1.8032362461090088, 1.9491857290267944, 1.7965459823608398, 1.9820646047592163, 1.7816063165664673, 1.710487961769104, 1.8398410081863403, 1.7429338693618774, 1.8291813135147095, 1.295252799987793, 1.7760109901428223, 2.0135629177093506]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.4698445796966553, 2.6738357543945312, 2.4677422046661377, 2.452440023422241, 2.5645670890808105, 2.627117156982422, 2.636353015899658, 2.484318733215332, 2.5207109451293945, 2.7361185550689697, 2.4623546600341797, 2.588984251022339, 2.5662808418273926, 2.65874981880188, 2.63175106048584, 2.685211420059204]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.9187973737716675, 1.914587378501892, 1.8605180978775024, 2.085020065307617, 1.6507622003555298, 1.8671274185180664, 1.8821156024932861, 1.9840811491012573, 1.968019723892212, 1.9870176315307617, 1.9763290882110596, 2.009993314743042, 1.990692138671875, 1.9613103866577148, 1.723320484161377, 2.083981990814209]
Layer: gate_25 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [2.0382723808288574, 1.9037564992904663, 2.0733871459960938, 2.1302037239074707, 1.912791132926941, 2.0100317001342773, 1.9865165948867798, 1.915295958518982, 1.9675134420394897, 2.161694049835205, 2.185471773147583, 1.9984896183013916, 2.1053466796875, 2.133138418197632, 1.8713946342468262, 2.0287115573883057]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.017066240310669, 1.8969491720199585, 2.029736042022705, 2.0298399925231934, 1.9162040948867798, 1.9353467226028442, 2.0339417457580566, 2.0468485355377197, 2.0235981941223145, 2.0624001026153564, 1.7404299974441528, 1.9276254177093506, 2.1007635593414307, 2.009996175765991, 2.012009859085083, 1.993754267692566]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.4510419368743896, 3.4690020084381104, 3.7913782596588135, 3.582508087158203, 3.632556438446045, 3.475370168685913, 3.4835586547851562, 3.622893810272217, 3.4617323875427246, 3.5715038776397705, 3.5782265663146973, 3.3543384075164795, 3.6308019161224365, 3.396493673324585, 3.6117539405822754, 3.5226759910583496]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.224458694458008, 7.563068866729736, 7.254087924957275, 7.135689735412598, 7.118202209472656, 6.856691837310791, 7.400340557098389, 7.415609359741211, 7.46832799911499, 7.13944149017334, 7.164968490600586, 6.951406478881836, 7.2369384765625, 7.270601272583008, 7.543078422546387, 7.258798599243164]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.6101765632629395, 4.713670253753662, 4.569178104400635, 4.197242259979248, 4.862197399139404, 4.685002326965332, 4.341897964477539, 4.566646575927734, 4.661758899688721, 4.603305816650391, 4.6198506355285645, 4.014650821685791, 4.406239986419678, 4.75040340423584, 4.690469264984131, 4.4752655029296875]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.139705181121826, 3.0721194744110107, 3.0668561458587646, 2.9504637718200684, 3.0021917819976807, 3.080723762512207, 3.0383336544036865, 2.95048189163208, 2.978700637817383, 2.9475290775299072, 3.2039291858673096, 2.398181915283203, 3.0215487480163574, 2.9759740829467773, 3.242875814437866, 2.9999988079071045]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_0 - Captured router_logits: [0.0918746292591095, 0.09860486537218094, 0.10222776234149933, -0.175292506814003, -0.14729027450084686, -0.1172412559390068, 0.12847168743610382, -0.14232179522514343, 0.09585562348365784, 0.08757289499044418, 0.09689120948314667, 0.08954986929893494, 0.0922660380601883, 0.1072428748011589, -0.884282648563385, 0.11730888485908508]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.08972325921058655, 0.04090151935815811, 0.029045099392533302, 0.03976932540535927, 0.08759211748838425, 0.060357894748449326, 0.033155910670757294, 0.04902304336428642, -0.0018517391290515661, 0.053412895649671555, -0.17541161179542542, 0.038123488426208496, 0.04049449414014816, -0.0003541935875546187, -0.006172021850943565, 0.0023418772034347057]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.055554620921611786, 0.0465216301381588, 0.11796387284994125, 0.09180698543787003, 0.07837160676717758, 0.10509868711233139, 0.0776730477809906, -0.12431806325912476, 0.06467355042695999, 0.04175407811999321, 0.0008748222608119249, 0.09066540002822876, -0.17712077498435974, 0.018146464601159096, -0.06770265102386475, 0.13132841885089874]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.12829957902431488, 0.12923574447631836, 0.10509289056062698, 0.1525738686323166, 0.1085544154047966, 0.11075073480606079, 0.018035555258393288, 0.1653081178665161, 0.12033243477344513, -0.535756528377533, -0.08860523998737335, 0.08180101960897446, 0.08973819762468338, -0.22970451414585114, -0.1728019267320633, -0.13758143782615662]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.056726448237895966, 0.09260723739862442, 0.11390005797147751, 0.010423914529383183, 0.028105607256293297, -0.10290531069040298, 0.04285683482885361, 0.029848799109458923, 0.011007668450474739, 0.03457200527191162, -0.16776283085346222, 0.1289338320493698, -0.14501959085464478, -0.2243044525384903, 0.0772402286529541, 0.007310235407203436]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.1492631584405899, 0.1475016474723816, 0.083798848092556, 0.0002269425749545917, 0.05743760988116264, -0.02016749233007431, 0.05973810330033302, 0.12225747853517532, 0.017056653276085854, -0.0902630165219307, -0.059206366539001465, 0.15544681251049042, -0.14854936301708221, 0.1237679198384285, 0.1925603747367859, 0.06966503709554672]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.20778821408748627, 0.02651362121105194, -0.034615885466337204, 0.3115304112434387, 0.12782247364521027, 0.04502321034669876, -0.12360163033008575, -0.05850265547633171, 0.006924275308847427, 0.13367338478565216, -0.5081941485404968, 0.28591179847717285, 0.33051595091819763, -0.17451338469982147, 0.1075831949710846, 0.23201267421245575]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.2448803335428238, 0.07105113565921783, 0.14429423213005066, 0.029738636687397957, -0.9363839626312256, -0.11411016434431076, -0.14346979558467865, -0.35632920265197754, -0.15554054081439972, -0.2300158590078354, -0.2045857161283493, -0.05416252464056015, 0.09363975375890732, -0.06054199859499931, -0.4487382769584656, 0.11752599477767944]
Layer: gate_7 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.1596176028251648, 0.19382570683956146, -0.09282071143388748, 0.3535301089286804, 0.2646397054195404, 0.30037999153137207, 0.12453402578830719, 0.3166446089744568, -0.004531383980065584, 0.14405520260334015, 0.2806251645088196, 0.04212171211838722, 0.45717334747314453, -0.2572743892669678, -0.08040006458759308, 0.13349351286888123]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3212512731552124, 0.16352924704551697, 0.14171679317951202, 0.5541066527366638, 0.30142542719841003, 0.3707580864429474, 0.6226785778999329, 0.7005046010017395, 0.12208747863769531, 0.13065440952777863, 0.3669685423374176, 0.4707697331905365, 0.7020502686500549, 0.24215956032276154, 0.7883742451667786, 0.6552016139030457]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7607836723327637, 0.7729758024215698, 0.27576586604118347, 0.6020408868789673, 0.5405076742172241, -0.1888311505317688, 0.35434532165527344, 0.733826220035553, -0.0060195475816726685, -0.18445849418640137, 0.44918662309646606, 0.5093929767608643, 0.2744484841823578, 0.2311987429857254, 0.06951496750116348, 0.3377043902873993]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.3384493887424469, 0.5540128946304321, 0.7176173329353333, 0.5383701920509338, 0.7807832360267639, 0.41806653141975403, 0.8868619799613953, 0.428009033203125, 0.4736303389072418, 0.7883416414260864, 0.5968056917190552, 0.42255666851997375, 0.26533544063568115, 0.26246142387390137, -0.13004237413406372, 0.57343989610672]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.1228620857000351, 1.001298189163208, 0.7294089198112488, 0.4963391423225403, 0.29453393816947937, 0.48624444007873535, 0.3844587504863739, 0.4154323935508728, 0.5262861251831055, 0.8986243605613708, 1.4830416440963745, 0.6673744320869446, 0.8389425873756409, 0.7370008826255798, 0.6439118981361389, 0.6531334519386292]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.9658905863761902, 0.8577158451080322, 1.173586368560791, 0.43937140703201294, 0.8125222325325012, 0.5117290019989014, 0.7718232870101929, 0.6316684484481812, 0.6568816900253296, 0.7240900993347168, 0.6857306361198425, 1.2359704971313477, 0.41457870602607727, 1.1215704679489136, 0.9392776489257812, 0.7346521019935608]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5991836190223694, 0.6068791747093201, 1.0334478616714478, 0.9866384863853455, 0.7335102558135986, 0.7629090547561646, 0.24534034729003906, 0.35157743096351624, 0.6622903347015381, 0.5618038773536682, 0.07033185660839081, 0.44659990072250366, 0.9825121760368347, 0.7908087372779846, 1.0493929386138916, 0.35632917284965515]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.6606230139732361, 1.0689146518707275, 1.1575466394424438, 0.8193678855895996, 0.915869414806366, 1.1961569786071777, 1.0311837196350098, 0.7236428260803223, 0.7354204058647156, 0.8112431168556213, 0.8463290929794312, 0.5466357469558716, 1.374688744544983, 0.7704717516899109, 0.926741898059845, 0.9393802285194397]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.1899415254592896, 0.7630048990249634, 0.9634697437286377, 1.107201099395752, 0.7942686080932617, 0.9009782671928406, -0.28264907002449036, 1.6526589393615723, 0.07813966274261475, 2.01194429397583, -0.16039727628231049, 1.1231738328933716, 1.3118276596069336, 1.237166404724121, 1.062760353088379, 0.3090982735157013]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.9797245860099792, 1.5230199098587036, 1.4484596252441406, 1.967017650604248, 1.8176929950714111, 1.4624227285385132, 1.7171218395233154, 1.7186861038208008, 1.4736860990524292, 1.823041319847107, 1.8476605415344238, 1.2300515174865723, 1.1204642057418823, 1.5344372987747192, 1.5443919897079468, 2.145998954772949]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.313607096672058, 1.040342926979065, 1.5639439821243286, 1.8468176126480103, 1.5511397123336792, 1.8454021215438843, 1.4692802429199219, 2.0056774616241455, 2.386110782623291, 1.4944677352905273, 1.5855921506881714, 1.6216527223587036, 1.7495887279510498, 1.9514412879943848, 1.9090224504470825, 1.5317623615264893]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.1452713012695312, 1.9756108522415161, 1.6503493785858154, 1.8458439111709595, 1.9438735246658325, 1.7903982400894165, 1.969510793685913, 2.041901111602783, 1.8500303030014038, 1.5562070608139038, 2.064335346221924, 1.7148271799087524, 2.780137538909912, 2.0815365314483643, 1.9440771341323853, 2.001258373260498]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.6349866986274719, 1.2380471229553223, 1.2049007415771484, 0.7704948782920837, 0.5781521797180176, 1.2848689556121826, 1.2568289041519165, 1.1399250030517578, 0.979979932308197, 0.8838602304458618, 1.1501952409744263, 0.8754515647888184, 1.0215251445770264, 1.4663985967636108, 1.274035930633545, 1.0703076124191284]
Running loglikelihood requests:  82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                       | 165/200 [03:09<00:30,  1.15it/s]Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_21 - Captured router_logits: [2.4561939239501953, 2.5785014629364014, 2.616118907928467, 2.725999116897583, 2.6134305000305176, 2.7266886234283447, 2.766730785369873, 2.6314313411712646, 2.686185598373413, 2.8237464427948, 2.72465181350708, 2.631348133087158, 2.6573097705841064, 2.6507155895233154, 2.851097822189331, 2.7370030879974365]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.7746849060058594, 1.4119222164154053, 1.5573683977127075, 1.4087086915969849, 1.788978099822998, 1.5416529178619385, 1.7279326915740967, 1.4742180109024048, 1.6702370643615723, 1.9319677352905273, 1.421975016593933, 1.6087664365768433, 1.748411774635315, 1.6059297323226929, 1.74530029296875, 1.946758508682251]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_23 - Captured router_logits: [2.230567216873169, 1.832407832145691, 1.8216584920883179, 1.7260572910308838, 1.7803136110305786, 1.7115787267684937, 1.8062407970428467, 2.0137205123901367, 1.616913080215454, 1.6916600465774536, 1.827230453491211, 1.609729528427124, 1.8391635417938232, 1.2266321182250977, 1.7236608266830444, 1.9664970636367798]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.4832332134246826, 2.634340763092041, 2.494339942932129, 2.442889928817749, 2.675776958465576, 2.764812469482422, 2.685934066772461, 2.6011457443237305, 2.582227945327759, 2.775423765182495, 2.408193826675415, 2.703842878341675, 2.575887441635132, 2.6303560733795166, 2.6191506385803223, 2.6544978618621826]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.868675708770752, 1.8701719045639038, 1.7986211776733398, 2.047593116760254, 1.6231669187545776, 1.8622137308120728, 1.7850176095962524, 1.9113062620162964, 1.8443603515625, 1.9083411693572998, 1.933302879333496, 1.8917187452316284, 1.984876036643982, 1.9748353958129883, 1.618208885192871, 2.079463005065918]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.007761240005493, 1.830801248550415, 1.9480938911437988, 2.0360114574432373, 1.9507019519805908, 1.8362514972686768, 1.9906771183013916, 1.919936180114746, 1.878773808479309, 2.1951944828033447, 2.214381694793701, 1.991929531097412, 2.0038866996765137, 2.194065570831299, 1.8166476488113403, 1.9543248414993286]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.9450886249542236, 1.913299798965454, 2.0144100189208984, 2.039038896560669, 1.9088059663772583, 2.0255234241485596, 2.0631566047668457, 2.0319759845733643, 1.997228980064392, 2.0491411685943604, 1.723557949066162, 1.9526774883270264, 1.9844218492507935, 1.9794360399246216, 1.97720468044281, 1.9691592454910278]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.513340950012207, 3.582122325897217, 3.9082705974578857, 3.668229103088379, 3.751739978790283, 3.5322537422180176, 3.58906888961792, 3.6807217597961426, 3.5294032096862793, 3.656707286834717, 3.623237133026123, 3.4169487953186035, 3.67981219291687, 3.464524984359741, 3.749638795852661, 3.550189256668091]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [6.943418025970459, 7.204946041107178, 7.068169593811035, 6.83185338973999, 6.878479957580566, 6.73834753036499, 7.212923526763916, 7.11017370223999, 7.200517177581787, 6.85890531539917, 7.037939548492432, 6.856993675231934, 7.0237226486206055, 7.082090854644775, 7.275564193725586, 6.998090744018555]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_30 - Captured router_logits: [4.40463924407959, 4.367335796356201, 4.305438041687012, 3.855482339859009, 4.689493179321289, 4.468235015869141, 4.1253662109375, 4.254696846008301, 4.321328639984131, 4.335043430328369, 4.298829078674316, 3.692082405090332, 4.17094087600708, 4.561644554138184, 4.366684436798096, 4.234291076660156]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.30822491645813, 3.269488573074341, 3.1460442543029785, 3.1167402267456055, 3.1402738094329834, 3.2367353439331055, 3.213797092437744, 3.101442575454712, 3.2066304683685303, 3.1722300052642822, 3.4450201988220215, 2.416867256164551, 3.195727825164795, 3.2035672664642334, 3.436770439147949, 3.2314488887786865]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.0733431801199913, 0.08683796972036362, 0.09483225643634796, -0.254866361618042, -0.2071942538022995, -0.08514250814914703, 0.11772576719522476, -0.13084332644939423, 0.07476132363080978, 0.07589814811944962, 0.08343814313411713, 0.07671559602022171, 0.08241372555494308, 0.09423702955245972, -1.0259721279144287, 0.10245021432638168]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.06859506666660309, 0.030789727345108986, 0.021764468401670456, 0.02864145115017891, 0.06744804978370667, 0.014993431977927685, 0.03276441991329193, 0.054595571011304855, 0.0026778157334774733, 0.04965056851506233, -0.20258566737174988, 0.03887078911066055, 0.03127375617623329, -0.0356883779168129, 0.014800000935792923, 0.00033337375498376787]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05150546878576279, 0.039010223001241684, 0.08016814291477203, 0.07670402526855469, 0.07949117571115494, 0.08488879352807999, 0.05718880146741867, -0.12120658159255981, 0.07195959985256195, 0.07380221039056778, 0.008868555538356304, 0.08381952345371246, -0.2058819830417633, 0.008222787640988827, -0.07016696035861969, 0.09062205255031586]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.12344859540462494, 0.1030743271112442, 0.07506424188613892, 0.11399071663618088, 0.0674181580543518, 0.0911417156457901, -0.012704764492809772, 0.17505280673503876, 0.11273928731679916, -0.5659345984458923, -0.03559354320168495, 0.055259089916944504, 0.13562288880348206, -0.2914736270904541, -0.07473118603229523, -0.13034941256046295]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.042970187962055206, 0.08832395821809769, 0.11452546715736389, 0.07053043693304062, 0.060535795986652374, -0.09699894487857819, 0.002936545293778181, -0.0172511488199234, -0.008677716366946697, 0.04099894315004349, -0.20366829633712769, 0.11024468392133713, -0.08382093906402588, -0.2374480962753296, 0.07960917800664902, -0.014071126468479633]
Layer: gate_4 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.10340991616249084, 0.14677606523036957, 0.05177837237715721, 0.030652085319161415, -0.019288908690214157, -0.028360117226839066, 0.09281250089406967, 0.11705604940652847, 0.08840245753526688, -0.0701751857995987, -0.008024649694561958, 0.1426803469657898, -0.1816709339618683, 0.15695379674434662, 0.20906174182891846, 0.09409108012914658]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.11014322936534882, -0.017888592556118965, 0.011117123067378998, 0.30969005823135376, 0.1431969851255417, 0.06176970154047012, -0.13239653408527374, -0.056118227541446686, 0.06451614946126938, 0.1566331535577774, -0.5848639607429504, 0.2644500136375427, 0.32242199778556824, -0.29097726941108704, 0.14482381939888, 0.2108379304409027]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.14964492619037628, -0.04646531492471695, 0.06532411277294159, -0.0531715489923954, -1.040024757385254, -0.07944289594888687, -0.08846956491470337, -0.5034546852111816, 0.023265715688467026, -0.10813313722610474, -0.25453296303749084, -0.013997341506183147, 0.05276957154273987, 0.07342668622732162, -0.5408949851989746, -0.030126383528113365]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_8 - Captured router_logits: [0.17279274761676788, 0.1873517632484436, -0.11323577165603638, 0.39859071373939514, 0.35110461711883545, 0.2993395924568176, 0.11748309433460236, 0.1812695562839508, -0.03223063424229622, 0.29469799995422363, 0.3099246323108673, 0.09958397597074509, 0.5214105248451233, -0.214187890291214, -0.1430276483297348, 0.1448902189731598]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.2934761643409729, 0.17784658074378967, 0.14345216751098633, 0.6775048971176147, 0.23465654253959656, 0.33124375343322754, 0.5288044214248657, 0.6392744183540344, 0.11151175200939178, 0.1864379495382309, 0.2815840244293213, 0.4455223083496094, 0.6592646837234497, 0.2594192624092102, 0.7747752070426941, 0.6435590386390686]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.8357323408126831, 0.8505004048347473, 0.31209737062454224, 0.6445800065994263, 0.5217599868774414, -0.12252460420131683, 0.4278503954410553, 0.7231641411781311, -0.0010516047477722168, -0.14838188886642456, 0.4635739028453827, 0.5856210589408875, 0.36388638615608215, 0.15879175066947937, 0.16518761217594147, 0.3700129985809326]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.2912798821926117, 0.5359150767326355, 0.5857900977134705, 0.44747093319892883, 0.6975515484809875, 0.24795123934745789, 0.5962333083152771, 0.35724979639053345, 0.449191689491272, 0.8229174613952637, 0.4970283508300781, 0.268117755651474, 0.125153586268425, 0.12028560042381287, -0.2419063001871109, 0.5132233500480652]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.140231654047966, 0.828291654586792, 0.8295915722846985, 0.5191659331321716, 0.19596154987812042, 0.38103795051574707, 0.4548725485801697, 0.4906798303127289, 0.428178071975708, 0.7705565094947815, 1.698566198348999, 0.6253708600997925, 0.7523739337921143, 0.6792654991149902, 0.5643447637557983, 0.694387674331665]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7724935412406921, 0.8588457703590393, 0.9557522535324097, 0.14047281444072723, 0.5672137141227722, 0.23383685946464539, 0.691218912601471, 0.9045699238777161, 0.507723331451416, 0.5994745492935181, 0.5915041565895081, 1.2952003479003906, 0.08475034683942795, 0.877413809299469, 0.6772186160087585, 0.5155704617500305]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.44964298605918884, 0.6548498272895813, 1.0400060415267944, 0.9872766137123108, 0.8468950390815735, 0.6889843344688416, -0.12480313330888748, 0.6406892538070679, 0.48541587591171265, 0.3700123429298401, -0.34419775009155273, 0.4068506360054016, 0.885802686214447, 0.982672393321991, 1.1549955606460571, 0.23860281705856323]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.5928769707679749, 1.4080615043640137, 1.156123399734497, 0.9478886127471924, 0.9255112409591675, 1.0920045375823975, 1.2244058847427368, 0.7402606010437012, 0.7774927020072937, 0.9261474609375, 0.797289252281189, 0.32885056734085083, 1.722205400466919, 0.7438355684280396, 0.8206168413162231, 1.0258601903915405]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.1343994140625, 0.5745204091072083, 1.1366215944290161, 1.1549553871154785, 0.8930493593215942, 0.7732714414596558, -0.2864629626274109, 1.8004560470581055, -0.23605944216251373, 2.45041823387146, -0.35506361722946167, 1.0234642028808594, 1.296082615852356, 1.2034854888916016, 1.1639150381088257, 0.14674009382724762]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.164255976676941, 1.811335802078247, 1.8191933631896973, 2.286386728286743, 2.201990842819214, 1.7082033157348633, 1.7194349765777588, 1.944740653038025, 1.8508459329605103, 2.1190433502197266, 2.1514456272125244, 1.4918829202651978, 1.3999134302139282, 1.6264044046401978, 1.7005414962768555, 2.338113784790039]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.54315984249115, 1.1406835317611694, 1.7743780612945557, 2.2178330421447754, 1.5398750305175781, 1.8126144409179688, 1.450897216796875, 2.148784637451172, 2.5821380615234375, 1.5952471494674683, 1.6620614528656006, 1.59197199344635, 1.7278656959533691, 2.1183085441589355, 1.7694432735443115, 1.6687105894088745]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.032454252243042, 1.8667211532592773, 1.4965978860855103, 1.6993775367736816, 1.9212955236434937, 1.6378968954086304, 1.8397035598754883, 1.8364508152008057, 1.769994854927063, 1.4992971420288086, 1.9906847476959229, 1.7272412776947021, 2.8566479682922363, 2.001321315765381, 1.8766177892684937, 1.8014402389526367]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.5420458912849426, 1.2154550552368164, 1.1481308937072754, 0.6715294718742371, 0.3220444321632385, 1.5325429439544678, 1.2660869359970093, 1.127354383468628, 0.8606473803520203, 1.0167913436889648, 1.2156238555908203, 0.9847134947776794, 1.0875111818313599, 1.46133291721344, 1.2549916505813599, 1.0634520053863525]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.335036516189575, 2.4885518550872803, 2.3899357318878174, 2.47916841506958, 2.3390514850616455, 2.5844409465789795, 2.5152337551116943, 2.473871946334839, 2.3457720279693604, 2.5768051147460938, 2.563075065612793, 2.4608805179595947, 2.242605209350586, 2.4735946655273438, 2.690053701400757, 2.5612432956695557]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.7511672973632812, 1.531858205795288, 1.6213593482971191, 1.4404592514038086, 1.7064038515090942, 1.6733235120773315, 1.7469186782836914, 1.4536240100860596, 1.6953506469726562, 1.979920744895935, 1.492957353591919, 1.782208800315857, 1.641271710395813, 1.5485032796859741, 1.6672159433364868, 1.9415605068206787]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.630289077758789, 2.040248394012451, 1.827699899673462, 1.7894130945205688, 1.9278244972229004, 1.8657464981079102, 1.7947973012924194, 1.9450162649154663, 1.8192988634109497, 1.6460689306259155, 1.9468311071395874, 1.8275001049041748, 1.928071141242981, 1.1477720737457275, 1.95429265499115, 1.9636576175689697]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.698831558227539, 2.857423782348633, 2.615419387817383, 2.5512216091156006, 2.790776491165161, 2.7720448970794678, 2.9362239837646484, 2.6209616661071777, 2.659336805343628, 2.7425997257232666, 2.7407779693603516, 2.662149429321289, 2.6696889400482178, 2.8646130561828613, 2.779878854751587, 2.8478877544403076]
Running loglikelihood requests:  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                     | 169/200 [03:12<00:26,  1.16it/s]Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.9831486940383911, 1.9274874925613403, 1.872544765472412, 2.1581363677978516, 1.5377542972564697, 1.928589105606079, 1.8964354991912842, 2.1049365997314453, 1.9843535423278809, 1.9599623680114746, 1.9976977109909058, 1.9706168174743652, 1.9515904188156128, 1.9580997228622437, 1.7709214687347412, 2.1165223121643066]
Layer: gate_25 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [2.151667833328247, 1.9656729698181152, 1.9506394863128662, 2.154496669769287, 1.9943784475326538, 2.056290864944458, 2.027637481689453, 1.986328125, 1.9772082567214966, 2.287348985671997, 2.344695568084717, 2.0286693572998047, 2.073009967803955, 2.2733724117279053, 2.025465726852417, 2.086282968521118]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.032958984375, 1.9516688585281372, 1.9965521097183228, 2.08617901802063, 1.950661301612854, 1.991750955581665, 2.065486431121826, 2.046571731567383, 2.016598701477051, 2.1448841094970703, 1.693239688873291, 1.9766921997070312, 2.162989377975464, 2.0253560543060303, 1.9633796215057373, 1.9980781078338623]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.3579227924346924, 3.3924319744110107, 3.849858522415161, 3.481571674346924, 3.5592167377471924, 3.3950703144073486, 3.4440438747406006, 3.5617334842681885, 3.351691722869873, 3.498556137084961, 3.423210859298706, 3.2115368843078613, 3.5155887603759766, 3.212740898132324, 3.5627894401550293, 3.392146587371826]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.1255784034729, 7.514019966125488, 7.068671703338623, 7.062319278717041, 6.924032211303711, 6.741713047027588, 7.4193243980407715, 7.31667947769165, 7.443721771240234, 7.070078372955322, 7.189787864685059, 6.9475250244140625, 7.132999897003174, 7.202944755554199, 7.505809783935547, 7.240360260009766]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.573575496673584, 4.673364639282227, 4.551625728607178, 4.14394998550415, 4.8088459968566895, 4.634047031402588, 4.390347957611084, 4.503284931182861, 4.527812957763672, 4.582172393798828, 4.587183475494385, 3.977982997894287, 4.463824272155762, 4.763112545013428, 4.606842041015625, 4.431881904602051]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.2115254402160645, 3.0747883319854736, 3.051595687866211, 2.8734989166259766, 2.99798321723938, 3.1175029277801514, 3.066218137741089, 2.934556722640991, 3.0094926357269287, 3.0796327590942383, 3.2503280639648438, 2.436361312866211, 3.0025782585144043, 3.031869888305664, 3.199005126953125, 3.162461519241333]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06890954822301865, 0.08654545247554779, 0.08781270682811737, -0.2649289667606354, -0.2558664083480835, -0.048069991171360016, 0.1070302352309227, -0.15209533274173737, 0.05352909490466118, 0.07383601367473602, 0.08326484262943268, 0.056468404829502106, 0.08154710382223129, 0.1004372090101242, -1.0670154094696045, 0.10852392762899399]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.07458081096410751, 0.03866152465343475, 0.0405263751745224, 0.046621352434158325, 0.07355495542287827, 0.025206515565514565, 0.04931206628680229, 0.045218825340270996, 0.006832143757492304, 0.043248482048511505, -0.1959199458360672, 0.06460456550121307, 0.026095375418663025, -0.015789683908224106, 0.028799530118703842, 0.01811869814991951]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.058430980890989304, 0.029696637764573097, 0.08860983699560165, 0.07284845411777496, 0.10427097231149673, 0.0932067260146141, 0.03173892945051193, -0.12501057982444763, 0.09431932866573334, 0.07178207486867905, 0.014862154610455036, 0.09454536437988281, -0.21858608722686768, 0.037181079387664795, -0.06594005227088928, 0.10592864453792572]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.1380811482667923, 0.12713506817817688, 0.10557820647954941, 0.10368883609771729, 0.08414790779352188, 0.12630073726177216, -0.04648074880242348, 0.1587439477443695, 0.13490092754364014, -0.641839325428009, 0.02209504321217537, 0.08798660337924957, 0.11215408146381378, -0.37955087423324585, -0.02307708188891411, -0.11454393714666367]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.04430069774389267, 0.07980741560459137, 0.0729098916053772, 0.07488036900758743, 0.10123173892498016, -0.09086181223392487, -0.030664360150694847, -0.005085588898509741, -0.06795795261859894, 0.035483524203300476, -0.22072705626487732, 0.10665075480937958, -0.059122294187545776, -0.19571715593338013, 0.10353275388479233, -0.07209081947803497]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09567568451166153, 0.15818539261817932, 0.060831569135189056, 0.10442213714122772, -0.047030191868543625, -0.04180110618472099, 0.11771633476018906, 0.08869978040456772, 0.09236834943294525, -0.08134402334690094, 0.014769298955798149, 0.15722942352294922, -0.22640347480773926, 0.19149725139141083, 0.14026840031147003, 0.10656766593456268]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.19468960165977478, -0.07792927324771881, 0.07287464290857315, 0.31754645705223083, 0.1470707654953003, 0.12777729332447052, -0.0863485336303711, -0.1008983701467514, -0.04141690582036972, 0.22069230675697327, -0.5936140418052673, 0.2527652084827423, 0.3300759792327881, -0.38928255438804626, 0.12964442372322083, 0.21551504731178284]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.16058996319770813, -0.0658189058303833, 0.037306223064661026, -0.008480655029416084, -1.0019409656524658, -0.06126371771097183, -0.017425043508410454, -0.5427118539810181, 0.05207573249936104, -0.08338935673236847, -0.3470376431941986, 0.006943743675947189, -0.014905674383044243, 0.15032117068767548, -0.664995551109314, -0.0811263769865036]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_8 - Captured router_logits: [0.13344433903694153, 0.21743492782115936, -0.24249669909477234, 0.4573509693145752, 0.33218711614608765, 0.3748508095741272, 0.09331660717725754, 0.32107821106910706, -0.02388300932943821, 0.2247300148010254, 0.36407431960105896, -0.008730066940188408, 0.553849458694458, -0.2735671103000641, -0.19201824069023132, 0.12466941773891449]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3570144474506378, 0.22689375281333923, 0.16590844094753265, 0.6344324946403503, 0.23963546752929688, 0.2517184615135193, 0.5090712904930115, 0.6682442426681519, 0.02769451215863228, 0.0864921510219574, 0.15633198618888855, 0.43930691480636597, 0.5520179271697998, 0.12894001603126526, 0.7845534682273865, 0.6360416412353516]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7896414995193481, 0.8480512499809265, 0.2859216630458832, 0.5997337698936462, 0.5327579975128174, -0.328697144985199, 0.4868927001953125, 0.7835195660591125, -0.00749603658914566, -0.3999996781349182, 0.5152627825737, 0.48628172278404236, 0.32968881726264954, 0.21791329979896545, 0.16722290217876434, 0.32151657342910767]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.32116079330444336, 0.5397564172744751, 0.6856279373168945, 0.3966732919216156, 0.7841005325317383, 0.2595132291316986, 0.6498265266418457, 0.3045262396335602, 0.4367087483406067, 0.8870130777359009, 0.4908072352409363, 0.451402485370636, 0.14829914271831512, 0.2206573486328125, -0.09516904503107071, 0.4830546975135803]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [-0.009837942197918892, 0.831659197807312, 0.6808975338935852, 0.34884917736053467, 0.22569015622138977, 0.39631032943725586, 0.41143742203712463, 0.4071032404899597, 0.45783036947250366, 0.7567510008811951, 1.6517131328582764, 0.6425390839576721, 0.6600181460380554, 0.5912657976150513, 0.5553799867630005, 0.7089700698852539]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.6696723699569702, 0.598950982093811, 1.139574408531189, 0.235737606883049, 0.589361310005188, 0.3045399785041809, 0.6276373267173767, 0.6655076146125793, 0.5050325989723206, 0.4604763090610504, 0.4093790054321289, 1.1354560852050781, 0.35854873061180115, 0.8705876469612122, 0.6536545157432556, 0.3709641993045807]
Layer: gate_13 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5308837890625, 0.4742162823677063, 0.9368117451667786, 0.879796028137207, 0.8889151811599731, 0.7736033797264099, 0.4197874069213867, 0.4840560555458069, 0.6310215592384338, 0.5409687757492065, -0.0711628645658493, 0.4095520079135895, 0.947881817817688, 0.787696361541748, 1.0215961933135986, 0.24354210495948792]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.48321977257728577, 1.0894101858139038, 0.9701870083808899, 0.6849954724311829, 0.8804767727851868, 1.0782629251480103, 0.9703366160392761, 0.4969578683376312, 0.6403481364250183, 0.7657386064529419, 0.7511593699455261, 0.7758736610412598, 1.4929969310760498, 0.7241305112838745, 0.7901091575622559, 0.875533401966095]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.964384913444519, 0.7231616973876953, 0.91643226146698, 1.0310494899749756, 0.7004430294036865, 0.7111685872077942, -0.29135507345199585, 1.3839973211288452, 0.2486962378025055, 1.8551654815673828, -0.34371715784072876, 1.0840123891830444, 1.1230523586273193, 1.0709946155548096, 0.9858362078666687, 0.06168947368860245]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.7195557951927185, 1.7224171161651611, 1.3641258478164673, 1.7225170135498047, 1.6253477334976196, 1.324368953704834, 1.3093125820159912, 1.5076926946640015, 1.3488478660583496, 1.647334098815918, 1.7188599109649658, 1.079807996749878, 0.9825422167778015, 1.1357736587524414, 1.1847400665283203, 1.9228042364120483]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.3592716455459595, 0.9799381494522095, 1.555004358291626, 1.9167455434799194, 1.6055059432983398, 1.5599448680877686, 1.5053157806396484, 1.9845441579818726, 2.2760937213897705, 1.4218496084213257, 1.3476967811584473, 1.4005199670791626, 1.5610206127166748, 1.593153953552246, 1.582045555114746, 1.4040806293487549]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.012430191040039, 1.8225505352020264, 1.567169427871704, 1.7099937200546265, 1.960019826889038, 1.7026493549346924, 1.8876736164093018, 1.8003498315811157, 1.9563264846801758, 1.5353257656097412, 2.0595321655273438, 1.643842339515686, 2.296760082244873, 1.9931201934814453, 1.9470077753067017, 1.8453909158706665]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.646990954875946, 1.2716299295425415, 1.158312439918518, 0.6711280345916748, 0.6506034731864929, 1.5901377201080322, 1.2083919048309326, 1.087511658668518, 0.9075912237167358, 0.9983131289482117, 1.1633834838867188, 0.9819236993789673, 1.0867602825164795, 1.4332574605941772, 1.18734872341156, 0.8994618654251099]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.5562472343444824, 2.661161184310913, 2.5924782752990723, 2.5270917415618896, 2.5523619651794434, 2.715282917022705, 2.6690127849578857, 2.5423049926757812, 2.4604949951171875, 2.7742698192596436, 2.7145438194274902, 2.6123850345611572, 2.6226913928985596, 2.659879207611084, 2.7684733867645264, 2.837620973587036]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_22 - Captured router_logits: [1.5749731063842773, 1.3178365230560303, 1.4591093063354492, 1.4235546588897705, 1.5885339975357056, 1.490299940109253, 1.4777871370315552, 1.2990013360977173, 1.5394251346588135, 1.7117000818252563, 1.324795126914978, 1.4962800741195679, 1.5013601779937744, 1.48564612865448, 1.4771225452423096, 1.6760671138763428]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.2688212394714355, 1.839349389076233, 1.6455047130584717, 1.6341087818145752, 1.752529501914978, 1.8656749725341797, 1.7410796880722046, 1.8783996105194092, 1.6485780477523804, 1.614131212234497, 1.7316974401474, 1.6501438617706299, 1.6607475280761719, 1.2727315425872803, 1.6680021286010742, 1.8486175537109375]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.3584446907043457, 2.5273985862731934, 2.257070779800415, 2.2473855018615723, 2.424699068069458, 2.4879775047302246, 2.4340932369232178, 2.363053798675537, 2.3631844520568848, 2.5399651527404785, 2.4364466667175293, 2.407238483428955, 2.406813144683838, 2.542325496673584, 2.4351415634155273, 2.451625347137451]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_25 - Captured router_logits: [1.7900867462158203, 1.8009411096572876, 1.7355324029922485, 2.0440244674682617, 1.5889424085617065, 1.8305975198745728, 1.7823498249053955, 1.929761290550232, 1.871241569519043, 1.9018161296844482, 1.8950468301773071, 1.881643533706665, 1.8726530075073242, 1.8581275939941406, 1.7006937265396118, 2.0244710445404053]
Layer: gate_25 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.9335010051727295, 1.842272162437439, 1.935821533203125, 2.0521366596221924, 1.900964379310608, 1.9282424449920654, 1.92208993434906, 1.8804271221160889, 1.875714898109436, 2.114717960357666, 2.210669994354248, 1.921889066696167, 1.981281042098999, 2.1538612842559814, 1.933301568031311, 1.9620050191879272]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.9573733806610107, 1.864034652709961, 1.9401512145996094, 1.9322713613510132, 1.8284820318222046, 1.887695074081421, 1.9719825983047485, 1.94803786277771, 1.9161949157714844, 2.004380226135254, 1.662027359008789, 1.8979568481445312, 2.0876269340515137, 2.0521020889282227, 1.9388959407806396, 1.9067707061767578]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Running loglikelihood requests:  86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                  | 173/200 [03:15<00:22,  1.18it/s]Layer: gate_28 - Captured router_logits: [3.289278745651245, 3.3459439277648926, 3.6621365547180176, 3.408085584640503, 3.490893602371216, 3.317336320877075, 3.446300506591797, 3.466844081878662, 3.314505100250244, 3.380582332611084, 3.366473436355591, 3.182154655456543, 3.4885921478271484, 3.2434020042419434, 3.492027997970581, 3.330986499786377]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [6.9650115966796875, 7.439363956451416, 7.05501651763916, 6.927926540374756, 6.983848571777344, 6.713873386383057, 7.303860664367676, 7.215146064758301, 7.304780006408691, 6.886584281921387, 6.970977783203125, 6.837921142578125, 7.0179338455200195, 7.1054368019104, 7.415928840637207, 7.130419731140137]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.305302619934082, 4.371335506439209, 4.2014055252075195, 3.8302688598632812, 4.495429039001465, 4.472243785858154, 4.059610843658447, 4.180057048797607, 4.25081729888916, 4.2440080642700195, 4.214169502258301, 3.6207401752471924, 4.027725696563721, 4.442834377288818, 4.2704362869262695, 4.223687171936035]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.0910139083862305, 3.042107582092285, 3.039537191390991, 2.953950881958008, 2.978466033935547, 3.0603020191192627, 2.9988467693328857, 2.8902900218963623, 2.912264347076416, 2.9432129859924316, 3.290503740310669, 2.3815834522247314, 3.044342517852783, 2.92559552192688, 3.2590293884277344, 2.9476699829101562]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06050663813948631, 0.07602103799581528, 0.07668471336364746, -0.2435118407011032, -0.23966744542121887, -0.09674220532178879, 0.1032293513417244, -0.12649573385715485, 0.04988167807459831, 0.06438849121332169, 0.07371631264686584, 0.05742735043168068, 0.07579730451107025, 0.08330366015434265, -0.9109704494476318, 0.10412509739398956]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.06924384087324142, 0.030827250331640244, 0.030208148062229156, 0.037640057504177094, 0.06153043732047081, 0.03335373103618622, 0.03577825799584389, 0.04023678973317146, 0.009700446389615536, 0.048547450453042984, -0.2018066793680191, 0.039123136550188065, 0.046310003846883774, -0.025416579097509384, 0.01419132761657238, 0.01950438879430294]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.044019538909196854, 0.02718028612434864, 0.08843379467725754, 0.08810172975063324, 0.07658940553665161, 0.09395284205675125, 0.03724322468042374, -0.12352538853883743, 0.06490657478570938, 0.055905845016241074, 0.009714782238006592, 0.07874728739261627, -0.2169879525899887, 0.031781334429979324, -0.07937047630548477, 0.15187156200408936]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.13876083493232727, 0.128121480345726, 0.12082275748252869, 0.09133008122444153, 0.096648670732975, 0.13588206470012665, -0.06617094576358795, 0.17111530900001526, 0.11493547260761261, -0.6376292705535889, -0.003802865045145154, 0.08327090740203857, 0.11816100776195526, -0.30250242352485657, -0.06444387882947922, -0.16091808676719666]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.04548857733607292, 0.08865024894475937, 0.08114082366228104, 0.07829020917415619, 0.08866539597511292, -0.07424982637166977, 0.008077891543507576, -0.029692893847823143, -0.026636971160769463, 0.03771819919347763, -0.2445933073759079, 0.08880112320184708, -0.04991493374109268, -0.25286903977394104, 0.08939441293478012, -0.04304441809654236]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_5 - Captured router_logits: [0.06860875338315964, 0.15923340618610382, 0.04230551794171333, 0.0781918466091156, -0.03422955796122551, -0.06395934522151947, 0.1141168624162674, 0.08362223207950592, 0.0746261477470398, -0.08603721112012863, 0.03798919916152954, 0.16082419455051422, -0.22890503704547882, 0.17721974849700928, 0.15004214644432068, 0.08374499529600143]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.1556330770254135, -0.08250392228364944, 0.025603169575333595, 0.29587793350219727, 0.14633874595165253, 0.09807149320840836, -0.15066340565681458, -0.06864288449287415, -0.02751983143389225, 0.19819603860378265, -0.5869089961051941, 0.27461564540863037, 0.3544637858867645, -0.3475600481033325, 0.16583283245563507, 0.24362662434577942]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.1698632389307022, -0.05287715047597885, 0.0774621069431305, -0.06340883672237396, -0.981925368309021, -0.04186498373746872, -0.04412202164530754, -0.5866153240203857, 0.03941833972930908, -0.10204847157001495, -0.2638610005378723, 0.0014085910515859723, 0.055187493562698364, 0.09062357991933823, -0.6083446145057678, 0.07813221961259842]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_8 - Captured router_logits: [0.13782311975955963, 0.222188800573349, -0.2258821576833725, 0.443194717168808, 0.3410347104072571, 0.35242292284965515, 0.10075119882822037, 0.29451271891593933, -0.09915704280138016, 0.17893485724925995, 0.34833741188049316, -0.0013913726434111595, 0.5431649684906006, -0.2598898410797119, -0.1944936215877533, 0.12227052450180054]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.37410768866539, 0.13359324634075165, 0.16467946767807007, 0.6283757090568542, 0.2683677077293396, 0.31772661209106445, 0.4979991316795349, 0.6208556890487671, 0.0731707215309143, 0.11697760969400406, 0.1634710282087326, 0.43397092819213867, 0.5890135765075684, 0.09727700799703598, 0.756494402885437, 0.6307384371757507]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7402459383010864, 0.8431020975112915, 0.3365843892097473, 0.6120871305465698, 0.5078257322311401, -0.19908131659030914, 0.506630003452301, 0.7458474636077881, -0.04376153647899628, -0.3407581150531769, 0.5349473357200623, 0.5018372535705566, 0.2951337397098541, 0.17105433344841003, 0.16794492304325104, 0.31592491269111633]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.23815429210662842, 0.521159827709198, 0.6650225520133972, 0.4556555151939392, 0.7339827418327332, 0.33282873034477234, 0.6544842720031738, 0.3372204601764679, 0.43279027938842773, 0.8379754424095154, 0.4970516860485077, 0.44194304943084717, 0.22133152186870575, 0.275164395570755, -0.058624353259801865, 0.5419586300849915]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.07852477580308914, 0.8551942706108093, 0.7231411337852478, 0.38990259170532227, 0.29272109270095825, 0.3934094309806824, 0.4794759452342987, 0.3763910233974457, 0.47378358244895935, 0.8713104128837585, 1.5807238817214966, 0.6191074252128601, 0.7019892930984497, 0.5716849565505981, 0.5805218815803528, 0.7378672957420349]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7172088027000427, 0.6395385265350342, 1.1549127101898193, 0.28522932529449463, 0.7240548133850098, 0.4579206109046936, 0.6337571740150452, 0.650294303894043, 0.5374080538749695, 0.523614764213562, 0.4223655164241791, 1.2159051895141602, 0.506041407585144, 0.8128753900527954, 0.5698801875114441, 0.45407113432884216]
Running loglikelihood requests:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎               | 177/200 [03:19<00:19,  1.20it/s]Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6331992149353027, 0.46552085876464844, 0.923273503780365, 0.836757242679596, 0.859197199344635, 0.7289084792137146, 0.49350684881210327, 0.6441183090209961, 0.6728284358978271, 0.6167914271354675, 0.03322101756930351, 0.5273138880729675, 0.9647812247276306, 0.8557948470115662, 0.9137637615203857, 0.3333107829093933]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_15 - Captured router_logits: [0.6873445510864258, 1.2228546142578125, 1.1452903747558594, 0.8446446657180786, 1.0149846076965332, 1.1121677160263062, 1.147409439086914, 0.5790982246398926, 0.8013034462928772, 0.9363613724708557, 0.8781234622001648, 0.818310558795929, 1.5780433416366577, 0.7566015720367432, 0.905818521976471, 0.8543814420700073]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9741331338882446, 0.6154950857162476, 0.9255934357643127, 1.0273809432983398, 0.7177565097808838, 0.5366128087043762, -0.1790720373392105, 1.3587877750396729, 0.1245584711432457, 1.772970199584961, -0.31556880474090576, 0.9742165803909302, 1.1247612237930298, 0.903117835521698, 0.9642529487609863, -0.045927293598651886]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.7653277516365051, 1.6451709270477295, 1.2846094369888306, 1.6208921670913696, 1.5557196140289307, 1.3522627353668213, 1.4088410139083862, 1.4245781898498535, 1.2669581174850464, 1.5891138315200806, 1.6832995414733887, 1.0915358066558838, 0.9583473205566406, 1.2348692417144775, 1.206889033317566, 1.8391960859298706]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.3020267486572266, 1.0198209285736084, 1.4699628353118896, 1.837148666381836, 1.486679196357727, 1.572109341621399, 1.5366464853286743, 1.8414126634597778, 2.1087701320648193, 1.3893694877624512, 1.3469914197921753, 1.3296512365341187, 1.4497754573822021, 1.5319726467132568, 1.5371304750442505, 1.3472073078155518]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.8816159963607788, 1.7645751237869263, 1.614859938621521, 1.667697787284851, 1.923875093460083, 1.7115039825439453, 1.8336267471313477, 1.8638826608657837, 1.91708505153656, 1.5705809593200684, 1.9332958459854126, 1.5954722166061401, 2.2147319316864014, 1.9446831941604614, 1.9136208295822144, 1.7900820970535278]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8380271792411804, 1.262848138809204, 1.156741738319397, 0.8727965950965881, 0.7288748025894165, 1.6654229164123535, 1.2330493927001953, 1.2829608917236328, 0.9183085560798645, 1.0528002977371216, 1.1976348161697388, 0.9182263612747192, 1.0859243869781494, 1.3087258338928223, 1.2980220317840576, 1.0629605054855347]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.4077422618865967, 2.4398858547210693, 2.49882173538208, 2.469627618789673, 2.445333957672119, 2.598177433013916, 2.5121381282806396, 2.3451640605926514, 2.384040355682373, 2.658397674560547, 2.5925843715667725, 2.4825618267059326, 2.4265177249908447, 2.541492462158203, 2.6455817222595215, 2.6495604515075684]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.5873440504074097, 1.3858331441879272, 1.5597738027572632, 1.4706943035125732, 1.6441681385040283, 1.5576891899108887, 1.5947514772415161, 1.4283230304718018, 1.5545132160186768, 1.7075790166854858, 1.3073949813842773, 1.5066651105880737, 1.5837680101394653, 1.519785761833191, 1.55765962600708, 1.7073547840118408]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.2485899925231934, 1.7885229587554932, 1.6959285736083984, 1.6649117469787598, 1.80996572971344, 1.8455923795700073, 1.6725866794586182, 1.8883925676345825, 1.667199730873108, 1.674965739250183, 1.7940775156021118, 1.6851168870925903, 1.741466760635376, 1.2925450801849365, 1.7276294231414795, 1.8329250812530518]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.554046630859375, 2.7547619342803955, 2.428945541381836, 2.410088062286377, 2.657125234603882, 2.7047433853149414, 2.679399013519287, 2.5801947116851807, 2.550138473510742, 2.735898017883301, 2.6623620986938477, 2.537503480911255, 2.5876781940460205, 2.747232675552368, 2.5940403938293457, 2.715383768081665]
Layer: gate_24 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.9112063646316528, 1.931931495666504, 1.813456416130066, 2.1517326831817627, 1.7128894329071045, 1.953662633895874, 1.9084582328796387, 1.9785776138305664, 2.001744508743286, 2.0061657428741455, 1.976448655128479, 2.0439717769622803, 1.9544271230697632, 2.0055503845214844, 1.866530179977417, 2.1496241092681885]
Layer: gate_25 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [1.9746347665786743, 1.8931970596313477, 1.9774945974349976, 2.0772857666015625, 1.9136877059936523, 1.9688122272491455, 1.935317039489746, 1.9285635948181152, 1.9223294258117676, 2.1991171836853027, 2.251668691635132, 1.934633731842041, 2.0478265285491943, 2.099515199661255, 1.8924001455307007, 1.9699366092681885]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.8938839435577393, 1.8057596683502197, 1.8855931758880615, 1.931566834449768, 1.7781726121902466, 1.8749654293060303, 1.9582736492156982, 1.9415850639343262, 1.8472219705581665, 1.9719085693359375, 1.6220611333847046, 1.8724035024642944, 2.03695011138916, 1.9666064977645874, 1.902111291885376, 1.8810381889343262]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.3157525062561035, 3.392436981201172, 3.636669397354126, 3.39096999168396, 3.465379476547241, 3.3475751876831055, 3.433229923248291, 3.482658624649048, 3.2969493865966797, 3.409062623977661, 3.3215019702911377, 3.1770005226135254, 3.3827147483825684, 3.251899480819702, 3.489309787750244, 3.4138965606689453]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.061611175537109, 7.52653169631958, 7.219193458557129, 6.97235631942749, 7.051530838012695, 6.788065433502197, 7.3826003074646, 7.361255168914795, 7.387332916259766, 7.016301155090332, 7.055244445800781, 6.984896183013916, 7.197935581207275, 7.241462707519531, 7.533486366271973, 7.162113189697266]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_30 - Captured router_logits: [4.2439351081848145, 4.334637641906738, 4.112541198730469, 3.6985950469970703, 4.375992774963379, 4.368683338165283, 3.9319801330566406, 4.08880090713501, 4.193023204803467, 4.170827388763428, 4.042317867279053, 3.571434259414673, 3.9396820068359375, 4.348716735839844, 4.221950054168701, 4.171870708465576]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.964559316635132, 2.939689874649048, 2.913513422012329, 2.7960000038146973, 2.8289453983306885, 2.9265074729919434, 2.8339338302612305, 2.774864673614502, 2.824158191680908, 2.8054676055908203, 3.1410810947418213, 2.2063848972320557, 2.8187544345855713, 2.782344341278076, 3.110992431640625, 2.8639297485351562]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.05905461683869362, 0.07353191077709198, 0.07710123807191849, -0.24266517162322998, -0.22193175554275513, -0.0507667101919651, 0.10280762612819672, -0.12885040044784546, 0.060743559151887894, 0.061998508870601654, 0.06672848016023636, 0.04651018604636192, 0.06990710645914078, 0.09711723029613495, -1.0222009420394897, 0.09728024899959564]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.06076202914118767, 0.030794553458690643, 0.01761161908507347, 0.041664376854896545, 0.06493724137544632, 0.021535567939281464, 0.04250403866171837, 0.042775750160217285, 0.0018929572543129325, 0.03962322324514389, -0.19905999302864075, 0.04744460806250572, 0.043276842683553696, -0.03229102864861488, 0.043040480464696884, 0.014356371946632862]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.04098618030548096, 0.02696882002055645, 0.07535985112190247, 0.08101064711809158, 0.07704222202301025, 0.07830779999494553, 0.030840856954455376, -0.12570108473300934, 0.10822375118732452, 0.06823619455099106, 0.01665665954351425, 0.0798916146159172, -0.19420967996120453, 0.009851858019828796, -0.06257756799459457, 0.11567863821983337]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.12440215796232224, 0.12521027028560638, 0.10368471592664719, 0.08657873421907425, 0.08299469202756882, 0.10166957974433899, -0.056871701031923294, 0.16345937550067902, 0.12038185447454453, -0.6303624510765076, 0.030083414167165756, 0.08482284098863602, 0.09520620852708817, -0.34531787037849426, -0.0391492061316967, -0.08543475717306137]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.04570943862199783, 0.07970146089792252, 0.06942126899957657, 0.07417269796133041, 0.11130784451961517, -0.09249166399240494, -0.026513170450925827, -0.01002588402479887, -0.04348323121666908, 0.01039198599755764, -0.20050539076328278, 0.09349485486745834, -0.05615900829434395, -0.18425755202770233, 0.09902965277433395, -0.06637125462293625]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.08239464461803436, 0.17114199697971344, 0.07951648533344269, 0.09785458445549011, -0.06497067213058472, -0.029722141101956367, 0.0996941551566124, 0.055977724492549896, 0.08315745741128922, -0.09889332950115204, 0.002718518488109112, 0.17525345087051392, -0.21735361218452454, 0.19730964303016663, 0.15515312552452087, 0.11840309202671051]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.1863420158624649, -0.07862859219312668, 0.06064559891819954, 0.2910017669200897, 0.1293262392282486, 0.08511719852685928, -0.1327991932630539, -0.12798546254634857, 0.016223929822444916, 0.20247220993041992, -0.5822858810424805, 0.22626683115959167, 0.34373223781585693, -0.3761093020439148, 0.09851977229118347, 0.2020057588815689]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.20447832345962524, -0.09549976140260696, 0.04489821940660477, -0.02527713216841221, -0.9572473168373108, -0.07571291923522949, -0.015163536183536053, -0.5003863573074341, -0.004719587042927742, -0.05371205136179924, -0.3585593104362488, 0.007249412592500448, -0.01640300825238228, 0.14181017875671387, -0.6178578734397888, -0.04505065083503723]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_8 - Captured router_logits: [0.16135330498218536, 0.2079145312309265, -0.2470475286245346, 0.4754470884799957, 0.3172549903392792, 0.34043341875076294, 0.07798808068037033, 0.24034741520881653, -0.04561668261885643, 0.16145886480808258, 0.3748030960559845, -0.03042546659708023, 0.5549131035804749, -0.2674594223499298, -0.24735435843467712, 0.14172029495239258]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.30036550760269165, 0.17079684138298035, 0.1003204733133316, 0.5842882394790649, 0.2035350352525711, 0.22259190678596497, 0.4346846640110016, 0.6151620745658875, 0.02112821862101555, 0.07899977266788483, 0.12843376398086548, 0.3762560486793518, 0.5321183204650879, 0.046498119831085205, 0.7574506402015686, 0.5948296785354614]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.6898944973945618, 0.848684549331665, 0.28096121549606323, 0.5734798312187195, 0.5251145362854004, -0.2893630862236023, 0.44669991731643677, 0.7531701922416687, -0.03377747908234596, -0.3959648609161377, 0.48018237948417664, 0.4815070629119873, 0.32532262802124023, 0.1963525265455246, 0.14425191283226013, 0.30149659514427185]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.3153153657913208, 0.6095919013023376, 0.7624232769012451, 0.5194904804229736, 0.9024041891098022, 0.3990567624568939, 0.7214808464050293, 0.36976736783981323, 0.49187615513801575, 0.9493654370307922, 0.5908607840538025, 0.46916961669921875, 0.2771998643875122, 0.3220754563808441, -0.005171114578843117, 0.5911113619804382]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.048516757786273956, 0.9103550314903259, 0.7768004536628723, 0.447945773601532, 0.26357942819595337, 0.40531161427497864, 0.4696699380874634, 0.4996831715106964, 0.4727700352668762, 0.8469381928443909, 1.7344164848327637, 0.7154708504676819, 0.7274951338768005, 0.6663029193878174, 0.6445106863975525, 0.7774537205696106]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.0135301351547241, 0.9119841456413269, 1.304970383644104, 0.3593990206718445, 0.7719734311103821, 0.4399632215499878, 0.8307982087135315, 0.7889729738235474, 0.7472890019416809, 0.8012712001800537, 0.8023952841758728, 1.3967877626419067, 0.374673068523407, 1.0564446449279785, 0.804365873336792, 0.6926402449607849]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5436840057373047, 0.5957401990890503, 1.2372533082962036, 1.1821539402008057, 1.0308728218078613, 0.6258528828620911, -0.006922988686710596, 0.5904734134674072, 0.5119509696960449, 0.5650770664215088, -0.30191200971603394, 0.5922662615776062, 1.0944510698318481, 1.1307027339935303, 1.3515597581863403, 0.26355770230293274]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.4841635227203369, 1.4803979396820068, 1.1556233167648315, 0.9323674440383911, 1.0758607387542725, 1.177032232284546, 1.2557973861694336, 0.6993550062179565, 0.7450162172317505, 0.983981728553772, 0.7620481252670288, 0.22703434526920319, 1.84930419921875, 0.775672197341919, 0.8268733620643616, 0.7831166982650757]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.3312251567840576, 0.6880074143409729, 1.2490715980529785, 1.3584922552108765, 0.9973491430282593, 0.7256444096565247, -0.37619417905807495, 2.1103315353393555, -0.386252760887146, 2.262782573699951, -0.3499949276447296, 1.1555627584457397, 1.482380747795105, 1.4116706848144531, 1.3474856615066528, -0.06259467452764511]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████             | 181/200 [03:22<00:15,  1.22it/s]Layer: gate_17 - Captured router_logits: [0.931867778301239, 1.922703504562378, 1.9357835054397583, 2.346020460128784, 2.2682104110717773, 1.8442391157150269, 1.6966949701309204, 2.0197787284851074, 1.9009840488433838, 1.9634480476379395, 2.173361301422119, 1.5595251321792603, 1.4394822120666504, 1.7481430768966675, 1.5982190370559692, 2.4759066104888916]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.6043119430541992, 1.1740920543670654, 1.8281065225601196, 2.1167383193969727, 1.4802478551864624, 1.6820862293243408, 1.4195457696914673, 2.2616965770721436, 2.63271164894104, 1.5445289611816406, 1.6774358749389648, 1.4455924034118652, 1.649916648864746, 2.0538249015808105, 1.6965869665145874, 1.5788744688034058]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.16473126411438, 1.935681939125061, 1.620357632637024, 1.8744451999664307, 2.062798500061035, 1.7054996490478516, 2.010671377182007, 1.9647737741470337, 1.79533851146698, 1.7396138906478882, 2.1182494163513184, 1.823610782623291, 2.923138380050659, 2.167893409729004, 2.0766420364379883, 1.883724570274353]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.5706448554992676, 1.3379020690917969, 1.2092413902282715, 0.8033777475357056, 0.3233015239238739, 1.6029890775680542, 1.308179497718811, 1.0474944114685059, 1.0101537704467773, 1.1265407800674438, 1.2944151163101196, 1.0599620342254639, 1.1699950695037842, 1.5060951709747314, 1.2959694862365723, 1.0290535688400269]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.636467695236206, 2.783238649368286, 2.709514617919922, 2.7457573413848877, 2.6614387035369873, 2.8799381256103516, 2.8301122188568115, 2.7205469608306885, 2.5902082920074463, 2.8857133388519287, 2.8455138206481934, 2.5258049964904785, 2.5115766525268555, 2.7208917140960693, 2.933819055557251, 2.8725287914276123]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.8663641214370728, 1.6191147565841675, 1.7243139743804932, 1.6080963611602783, 1.8619214296340942, 1.7794207334518433, 1.8672393560409546, 1.5387190580368042, 1.8308024406433105, 2.1341919898986816, 1.5201507806777954, 1.660280704498291, 1.7310445308685303, 1.6681455373764038, 1.7900917530059814, 2.0615034103393555]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.613086462020874, 2.121565341949463, 1.9383083581924438, 1.9382297992706299, 2.0473527908325195, 2.03900146484375, 1.883541464805603, 2.098986864089966, 2.0047812461853027, 1.8029978275299072, 2.092780828475952, 2.0264363288879395, 2.0341053009033203, 1.2287336587905884, 2.0958046913146973, 2.112863063812256]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.7186973094940186, 2.98063588142395, 2.6988048553466797, 2.5810327529907227, 2.773329973220825, 2.7675793170928955, 2.89609432220459, 2.6168689727783203, 2.7296972274780273, 2.8432815074920654, 2.5976173877716064, 2.7348880767822266, 2.6716814041137695, 2.9278807640075684, 2.8410494327545166, 2.9404056072235107]
Layer: gate_24 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.1504154205322266, 2.0669450759887695, 2.046861171722412, 2.3293914794921875, 1.6769728660583496, 2.029181480407715, 2.072828769683838, 2.227522611618042, 2.1404287815093994, 2.1394689083099365, 2.199150562286377, 2.167933702468872, 2.091737747192383, 2.1363778114318848, 1.885335922241211, 2.310534715652466]
Layer: gate_25 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [2.1970858573913574, 2.0666964054107666, 2.0895814895629883, 2.259662389755249, 2.063279628753662, 2.1892921924591064, 2.0815279483795166, 1.9981536865234375, 2.0629758834838867, 2.364159345626831, 2.4088566303253174, 2.1251039505004883, 2.1718881130218506, 2.3880386352539062, 2.0429844856262207, 2.23140025138855]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.115194797515869, 1.971110463142395, 2.0351057052612305, 2.1052002906799316, 1.9665228128433228, 2.0479049682617188, 2.1039533615112305, 2.1063151359558105, 2.0677249431610107, 2.1629576683044434, 1.704949140548706, 2.0166757106781006, 2.2299275398254395, 2.091921329498291, 2.0038723945617676, 2.0396456718444824]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.465310573577881, 3.512929677963257, 3.952139139175415, 3.6032402515411377, 3.706021308898926, 3.5219149589538574, 3.426468849182129, 3.7025065422058105, 3.444075584411621, 3.626675605773926, 3.5341508388519287, 3.3024771213531494, 3.635312795639038, 3.3520050048828125, 3.6563615798950195, 3.4627747535705566]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.288370609283447, 7.6826677322387695, 7.332510471343994, 7.2038116455078125, 7.076756000518799, 6.929477691650391, 7.670067310333252, 7.546650409698486, 7.611227989196777, 7.2939581871032715, 7.28303861618042, 7.146077632904053, 7.372774124145508, 7.399148941040039, 7.714779376983643, 7.3003387451171875]
Layer: gate_29 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_30 - Captured router_logits: [4.8044538497924805, 4.941397190093994, 4.751158237457275, 4.3314924240112305, 5.026138782501221, 4.831573963165283, 4.565411567687988, 4.75979471206665, 4.7912983894348145, 4.8248372077941895, 4.844378471374512, 4.1668782234191895, 4.630381107330322, 4.991631031036377, 4.836040496826172, 4.591968536376953]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.319387674331665, 3.1418468952178955, 3.175739049911499, 2.990000009536743, 3.126833915710449, 3.217182159423828, 3.1140832901000977, 3.0193378925323486, 3.0892539024353027, 3.134120225906372, 3.311332941055298, 2.4675798416137695, 3.0509026050567627, 3.0638108253479004, 3.311392307281494, 3.239661455154419]
Layer: gate_31 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.05643635243177414, 0.06845850497484207, 0.08071569353342056, -0.23070763051509857, -0.21618318557739258, -0.08348699659109116, 0.09244532883167267, -0.034165188670158386, 0.06664363294839859, 0.053393520414829254, 0.06157958135008812, 0.05621260404586792, 0.07114893198013306, 0.08567313104867935, -0.8992879986763, 0.09464288502931595]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_1 - Captured router_logits: [0.06851837784051895, 0.029459821060299873, 0.028696388006210327, 0.028582042083144188, 0.06031283736228943, 0.016096552833914757, 0.04165082424879074, 0.07234600931406021, 0.03968731686472893, 0.03241942450404167, -0.18320003151893616, 0.03972971439361572, 0.019804049283266068, -0.04829022288322449, 0.020032234489917755, 0.029115019366145134]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05518478900194168, 0.0020842086523771286, 0.0879599079489708, 0.09571592509746552, 0.1069842129945755, 0.0809435248374939, 0.05370428040623665, -0.1263398677110672, 0.0793476551771164, 0.074898362159729, 0.016608836129307747, 0.0865926593542099, -0.2070312798023224, -0.0008177559939213097, -0.029728058725595474, 0.10028976202011108]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.12511201202869415, 0.09941890090703964, 0.07099386304616928, 0.10457292944192886, 0.10043872147798538, 0.10549052059650421, -0.058135613799095154, 0.15936513245105743, 0.11185483634471893, -0.573288083076477, -0.04449076950550079, 0.053131069988012314, 0.2393249273300171, -0.3321996033191681, -0.028864026069641113, -0.14874491095542908]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.0233813114464283, 0.08631327003240585, 0.08717401325702667, 0.13126599788665771, 0.04733971506357193, -0.06702934205532074, -0.005020361393690109, -0.01667056791484356, -0.06448838859796524, 0.051314570009708405, -0.2433929443359375, 0.10635595768690109, 0.012242370285093784, -0.2290387600660324, 0.08899789303541183, -0.043725620955228806]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09153034538030624, 0.16719739139080048, 0.03995981439948082, 0.07666027545928955, -0.046979960054159164, -0.03463204205036163, 0.11425497382879257, 0.06553013622760773, 0.1714143007993698, -0.07076059281826019, 0.07353828102350235, 0.1541372835636139, -0.21127210557460785, 0.1789083480834961, 0.19322437047958374, 0.10757642239332199]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.15045775473117828, -0.0666855126619339, 0.03801857680082321, 0.3110288083553314, 0.14364361763000488, 0.11561474949121475, -0.1199265792965889, -0.08876268565654755, 0.054038166999816895, 0.15569205582141876, -0.5549928545951843, 0.27743980288505554, 0.33257007598876953, -0.330090194940567, 0.23204535245895386, 0.24586214125156403]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.006959953811019659, -0.04371220991015434, 0.054138313978910446, -0.02609492652118206, -1.0137523412704468, -0.006074585486203432, -0.07223720848560333, -0.5165637731552124, 0.1642579436302185, -0.0883718803524971, -0.336920827627182, -0.015302042476832867, 0.027563421055674553, 0.08336417376995087, -0.6431102752685547, -0.04744348302483559]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.20642922818660736, 0.21874386072158813, -0.14942669868469238, 0.46121323108673096, 0.38340896368026733, 0.3555079400539398, 0.13583941757678986, 0.1694495677947998, -0.005048561841249466, 0.4533827602863312, 0.3173063397407532, 0.18781347572803497, 0.5329073071479797, -0.21317902207374573, -0.1598532646894455, 0.22214144468307495]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.359246164560318, 0.2756795883178711, 0.2142677754163742, 0.9609150290489197, 0.2799845337867737, 0.34319257736206055, 0.5569731593132019, 0.7035195827484131, 0.32264962792396545, 0.11255313456058502, 0.20567883551120758, 0.4925423264503479, 0.6853858828544617, 0.22169481217861176, 0.8893879055976868, 0.6602097749710083]
Layer: gate_9 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_10 - Captured router_logits: [1.0064362287521362, 0.9196447134017944, 0.3634037971496582, 0.6941683888435364, 0.5547677874565125, 0.045485567301511765, 0.4696598947048187, 0.7588674426078796, 0.023432252928614616, -0.1433665156364441, 0.5451669096946716, 0.66158127784729, 0.28655099868774414, 0.16077233850955963, 0.47871893644332886, 0.35467052459716797]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.25063931941986084, 0.6069832444190979, 0.4659007787704468, 0.41624778509140015, 0.7316296696662903, 0.1221756562590599, 0.5252140164375305, 0.2834301292896271, 0.4232308268547058, 0.9569433927536011, 0.3790018856525421, 0.17380519211292267, -0.05176413431763649, -0.07159434258937836, -0.391120046377182, 0.2885246276855469]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [-0.0767410546541214, 0.7033089995384216, 0.8341765999794006, 0.6078897714614868, 0.07026082277297974, 0.4342974126338959, 0.6666781306266785, 0.7498262524604797, 0.3156096041202545, 0.6464383006095886, 1.477251410484314, 0.6336736679077148, 0.7430462837219238, 0.6926339268684387, 0.5339427590370178, 0.6859791874885559]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7589209079742432, 1.0159093141555786, 1.0429695844650269, 0.0373164564371109, 0.8566413521766663, 0.06829894334077835, 0.9320507645606995, 1.3072510957717896, 0.8792818188667297, 0.8368468284606934, 1.0171600580215454, 1.399605393409729, -0.05900057032704353, 0.9155080914497375, 1.048294186592102, 0.5435285568237305]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5124237537384033, 0.6732738018035889, 1.2413763999938965, 1.071309208869934, 1.0377781391143799, 0.8665857911109924, -0.046796202659606934, 0.9841307401657104, 0.6179535388946533, 0.43077611923217773, -0.4661075472831726, 0.5803739428520203, 1.021857738494873, 1.0827492475509644, 1.348465085029602, 0.24406062066555023]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.4110354483127594, 1.557718276977539, 1.11283278465271, 0.9660155773162842, 0.9421672224998474, 1.0278664827346802, 1.3954063653945923, 0.7389035820960999, 0.8168047070503235, 0.9428098797798157, 0.6614925861358643, 0.0009025480248965323, 1.634836196899414, 0.710895299911499, 0.7365607023239136, 1.050695776939392]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [0.9536081552505493, 0.409145325422287, 0.976281464099884, 1.1966127157211304, 0.9666659235954285, 0.5752308964729309, -0.06347651034593582, 1.7837769985198975, -0.603126585483551, 2.3875019550323486, -0.4885469973087311, 0.9213413596153259, 1.2172787189483643, 1.1921236515045166, 1.1277358531951904, 0.16506141424179077]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [0.7901868224143982, 1.4273661375045776, 1.7078571319580078, 2.0591394901275635, 2.009603977203369, 1.5978164672851562, 1.5782066583633423, 1.6987558603286743, 1.9109731912612915, 1.8757165670394897, 2.3559539318084717, 1.5766880512237549, 1.504591464996338, 1.7124981880187988, 1.205458402633667, 2.1103131771087646]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_18 - Captured router_logits: [1.5543198585510254, 1.1517568826675415, 1.7778491973876953, 2.1034035682678223, 1.2333754301071167, 1.5630171298980713, 1.9021351337432861, 2.049609661102295, 2.7675280570983887, 1.5602840185165405, 1.5249922275543213, 1.3633813858032227, 1.5332411527633667, 1.714024543762207, 1.6517187356948853, 1.5451866388320923]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.860060453414917, 1.7191617488861084, 1.5711110830307007, 1.556632399559021, 1.7738837003707886, 1.4329315423965454, 1.6943645477294922, 1.6795965433120728, 1.9951757192611694, 1.5507513284683228, 1.8549120426177979, 1.589350700378418, 2.5272483825683594, 1.8229576349258423, 1.7752383947372437, 1.6176717281341553]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.8510243892669678, 1.1956772804260254, 1.1739472150802612, 0.8899418711662292, 0.3936266303062439, 1.4986982345581055, 1.1973410844802856, 1.4321292638778687, 0.7322061657905579, 1.1433656215667725, 1.2703667879104614, 1.0094795227050781, 1.0669710636138916, 1.274574875831604, 1.2375482320785522, 0.9094777703285217]
Running loglikelihood requests:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊          | 185/200 [03:25<00:12,  1.24it/s]Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.4935905933380127, 2.6520514488220215, 2.556377649307251, 2.621555805206299, 2.5022006034851074, 2.743277072906494, 2.6785788536071777, 2.684753894805908, 2.45592999458313, 2.765479326248169, 2.7842724323272705, 2.948903799057007, 2.3887245655059814, 2.605576276779175, 2.783029556274414, 2.700864315032959]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_22 - Captured router_logits: [1.7582447528839111, 1.5429027080535889, 1.9046791791915894, 1.4306862354278564, 1.7748217582702637, 1.7029879093170166, 1.6929233074188232, 1.5884478092193604, 1.7408537864685059, 2.031130790710449, 1.5768628120422363, 1.9619266986846924, 1.6837105751037598, 1.643642783164978, 1.6695274114608765, 2.0292060375213623]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.653907537460327, 2.0749127864837646, 1.681898832321167, 1.821035385131836, 1.9329701662063599, 1.8043763637542725, 1.686662197113037, 1.8366868495941162, 1.8095645904541016, 1.6303274631500244, 1.8998289108276367, 1.7956950664520264, 1.75178861618042, 1.0579860210418701, 1.8658145666122437, 1.8504520654678345]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.8327150344848633, 2.993267059326172, 2.7626795768737793, 2.593585968017578, 3.0401463508605957, 2.804861068725586, 2.912264347076416, 2.72585391998291, 2.7753729820251465, 2.8615918159484863, 3.0763556957244873, 2.7772443294525146, 2.7471425533294678, 2.956141948699951, 2.8616225719451904, 2.9619429111480713]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.038602590560913, 2.003218650817871, 1.9721838235855103, 2.176201343536377, 1.6335331201553345, 2.0311927795410156, 2.0106544494628906, 2.212827205657959, 2.021322727203369, 1.9742258787155151, 2.066882371902466, 1.9912303686141968, 2.0297157764434814, 2.060797929763794, 2.1949548721313477, 2.289147138595581]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [2.1467902660369873, 2.09945011138916, 2.045757293701172, 2.1523237228393555, 1.99953031539917, 2.1055502891540527, 2.0310325622558594, 1.9879348278045654, 2.0155673027038574, 2.2279422283172607, 2.5477187633514404, 2.03236985206604, 2.072072982788086, 2.291560411453247, 2.032472610473633, 2.1406188011169434]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [2.032562255859375, 1.952951192855835, 1.971260666847229, 2.0757246017456055, 1.930167555809021, 1.9600398540496826, 2.1152796745300293, 2.094416379928589, 1.9998844861984253, 2.114229679107666, 1.6633061170578003, 1.9867310523986816, 2.156524658203125, 2.218352794647217, 1.965354084968567, 1.984018325805664]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_28 - Captured router_logits: [3.2381131649017334, 3.339366912841797, 3.654902219772339, 3.3068008422851562, 3.4986023902893066, 3.281087636947632, 3.580686092376709, 3.4703545570373535, 3.235403537750244, 3.3391456604003906, 3.2799253463745117, 3.0589380264282227, 3.3379673957824707, 3.1037378311157227, 3.4672834873199463, 3.2383766174316406]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.011106967926025, 7.600194931030273, 7.178108215332031, 6.985295295715332, 6.903235912322998, 6.763898849487305, 7.2708539962768555, 7.304418563842773, 7.362730979919434, 6.995519638061523, 7.072133541107178, 6.88124942779541, 7.147398471832275, 7.151837348937988, 7.421204566955566, 7.108737468719482]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.291557312011719, 4.340070724487305, 4.117002964019775, 3.8029191493988037, 4.463926792144775, 4.272676944732666, 3.9271771907806396, 4.233737468719482, 4.360832214355469, 4.284575462341309, 4.184782028198242, 3.602578639984131, 3.972709894180298, 4.455260276794434, 4.251324653625488, 4.080134391784668]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.8499579429626465, 2.7617554664611816, 2.7651009559631348, 2.7258801460266113, 2.689446449279785, 2.731079578399658, 2.7817063331604004, 2.5716707706451416, 2.6787025928497314, 2.6620352268218994, 3.2064049243927, 2.0183982849121094, 2.680440664291382, 2.626952648162842, 3.061241626739502, 2.7927041053771973]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.06057315319776535, 0.07625415176153183, 0.0857318714261055, -0.22659778594970703, -0.23907695710659027, -0.053915590047836304, 0.10153698921203613, -0.11731815338134766, 0.07061628252267838, 0.06015119329094887, 0.07626598328351974, 0.07163901627063751, 0.07455067336559296, 0.0911293551325798, -0.9713600873947144, 0.09589651972055435]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.06365346908569336, 0.020924707874655724, 0.024773696437478065, 0.04028715193271637, 0.061588022857904434, 0.018455659970641136, 0.027967354282736778, 0.0520067997276783, -0.0020428935531526804, 0.051950231194496155, -0.17817623913288116, 0.04477879777550697, 0.02761003002524376, -0.020209278911352158, 0.025074057281017303, 0.013365925289690495]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.04904540255665779, 0.017887959256768227, 0.07543109357357025, 0.07506630569696426, 0.08584540337324142, 0.07110544294118881, 0.021608524024486542, -0.1290234923362732, 0.0832870751619339, 0.07010786980390549, 0.023863229900598526, 0.08507423102855682, -0.21525174379348755, 0.01927753910422325, -0.07556150108575821, 0.11729685217142105]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.1174493208527565, 0.10377594828605652, 0.09203280508518219, 0.09212545305490494, 0.08537247776985168, 0.09729297459125519, -0.05830636993050575, 0.12616273760795593, 0.10781804472208023, -0.5446051359176636, 0.01703440397977829, 0.048138465732336044, 0.13962513208389282, -0.33264148235321045, -0.03724559769034386, -0.10606969147920609]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.04698506370186806, 0.07659932225942612, 0.09823855012655258, 0.11025818437337875, 0.0819108784198761, -0.08067093789577484, -0.024411717429757118, -0.016300935298204422, -0.035202570259571075, 0.06776857376098633, -0.2288580685853958, 0.106804259121418, -0.07764530926942825, -0.19725731015205383, 0.09319637715816498, -0.02224458009004593]
Layer: gate_4 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09417318552732468, 0.1648445427417755, 0.061856191605329514, 0.08689802139997482, -0.011448781937360764, -0.039586298167705536, 0.08976329863071442, 0.05851483345031738, 0.11142095178365707, -0.06498510390520096, 0.03725754842162132, 0.16132178902626038, -0.2024710774421692, 0.17894810438156128, 0.15559819340705872, 0.12141525745391846]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.16698595881462097, -0.08632776886224747, 0.058841075748205185, 0.32033073902130127, 0.1656406819820404, 0.10953328013420105, -0.10321994870901108, -0.10195836424827576, 0.07412737607955933, 0.18038056790828705, -0.577957808971405, 0.2543596029281616, 0.36545586585998535, -0.34129026532173157, 0.1764535754919052, 0.21836154162883759]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.102945476770401, -0.07919975370168686, 0.03881535306572914, -0.015761788934469223, -1.014325737953186, -0.02450401522219181, -0.05402478203177452, -0.48823466897010803, 0.05142883211374283, -0.07820714265108109, -0.3541345000267029, 0.022179313004016876, -0.02094550058245659, 0.1213754266500473, -0.6617562174797058, 0.006105847656726837]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_8 - Captured router_logits: [0.18707972764968872, 0.21762830018997192, -0.18781009316444397, 0.44004741311073303, 0.3882189989089966, 0.33775436878204346, 0.11782269924879074, 0.2460288405418396, -0.039807677268981934, 0.3136586844921112, 0.3755943775177002, 0.07339101284742355, 0.5577666759490967, -0.2871611416339874, -0.21371208131313324, 0.17220920324325562]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.3475785553455353, 0.1746625155210495, 0.22112692892551422, 0.7520781755447388, 0.2792428731918335, 0.314292848110199, 0.5305667519569397, 0.7073443531990051, 0.20068104565143585, 0.1329520344734192, 0.12818016111850739, 0.49796009063720703, 0.618590235710144, 0.1183660551905632, 0.8349180221557617, 0.6588481664657593]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.8451240062713623, 0.8925102949142456, 0.3340326249599457, 0.6414263248443604, 0.5786952972412109, -0.11521559208631516, 0.4831264913082123, 0.7763180136680603, -0.1101069524884224, -0.2887882590293884, 0.5156956911087036, 0.5555847883224487, 0.2762731909751892, 0.2378261387348175, 0.3187590539455414, 0.2817637324333191]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.2938520610332489, 0.5046783089637756, 0.6643410325050354, 0.4200759828090668, 0.7630722522735596, 0.2916759252548218, 0.605994462966919, 0.270798921585083, 0.44346868991851807, 0.9233279824256897, 0.47163915634155273, 0.2577884793281555, 0.052374884486198425, 0.15453657507896423, -0.30527427792549133, 0.4445994198322296]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.022460635751485825, 0.8117904663085938, 0.7436091303825378, 0.5011928081512451, 0.16398319602012634, 0.36619898676872253, 0.5942410230636597, 0.5757783055305481, 0.49404922127723694, 0.8699513673782349, 1.6892015933990479, 0.6606172323226929, 0.7272417545318604, 0.5580160617828369, 0.6234042048454285, 0.7413955926895142]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.7988169193267822, 0.7695874571800232, 1.0780105590820312, 0.3067536950111389, 0.7728651762008667, 0.32334497570991516, 0.7181845903396606, 1.0151811838150024, 0.6278846859931946, 0.629746675491333, 0.7282669544219971, 1.2483607530593872, 0.1630818396806717, 0.9243948459625244, 0.639636754989624, 0.5074858069419861]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.6601317524909973, 0.6995396018028259, 1.106158971786499, 1.0843267440795898, 0.9272384643554688, 0.879233717918396, 0.3599371016025543, 0.9821691513061523, 0.6832329630851746, 0.6794165372848511, -0.017569920048117638, 0.5798981785774231, 1.1412577629089355, 1.0279710292816162, 1.2324947118759155, 0.43807512521743774]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.4438740015029907, 1.0932852029800415, 0.9983310699462891, 0.7443909645080566, 0.7468061447143555, 0.9284006953239441, 1.1099165678024292, 0.6020047068595886, 0.5656097531318665, 0.7519916296005249, 0.689224123954773, 0.6603714823722839, 1.5073215961456299, 0.6080913543701172, 0.6497521996498108, 0.9551066160202026]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.0454556941986084, 0.6302306652069092, 0.942876398563385, 1.075798511505127, 0.7618249654769897, 0.7282809019088745, -0.008956258185207844, 1.5075783729553223, 0.07870224118232727, 1.9738744497299194, -0.21796073019504547, 1.0746325254440308, 1.0520117282867432, 1.094327688217163, 1.0267442464828491, 0.27509641647338867]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.0165513753890991, 1.6972814798355103, 1.5442415475845337, 1.8488240242004395, 1.7597980499267578, 1.5074406862258911, 1.5412018299102783, 1.5181130170822144, 1.5371174812316895, 1.7654386758804321, 1.8486676216125488, 1.1958574056625366, 1.0619239807128906, 1.3614776134490967, 1.3937691450119019, 1.950549602508545]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.4102107286453247, 1.0092700719833374, 1.558066964149475, 1.9404311180114746, 1.5006558895111084, 1.604310393333435, 1.5073461532592773, 1.925864338874817, 2.3470423221588135, 1.4279052019119263, 1.4780220985412598, 1.4890923500061035, 1.6150338649749756, 1.757927417755127, 1.640566110610962, 1.5205399990081787]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [1.941922664642334, 1.7916109561920166, 1.5247117280960083, 1.7352380752563477, 1.9438285827636719, 1.6092108488082886, 1.8785815238952637, 1.7873687744140625, 1.8999451398849487, 1.517446517944336, 1.9216971397399902, 1.587193489074707, 2.5367345809936523, 1.9781614542007446, 1.8837844133377075, 1.7571473121643066]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.7119573950767517, 1.318482756614685, 1.1814227104187012, 0.8399223685264587, 0.5828722715377808, 1.6345747709274292, 1.2491263151168823, 1.267383337020874, 1.0077853202819824, 1.1045962572097778, 1.2157063484191895, 0.9548630118370056, 1.239976406097412, 1.4257643222808838, 1.2343136072158813, 1.0955091714859009]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.534426689147949, 2.6534247398376465, 2.5789403915405273, 2.589186668395996, 2.5485336780548096, 2.764467477798462, 2.752293586730957, 2.5646002292633057, 2.5367431640625, 2.7918171882629395, 2.714564085006714, 2.629621744155884, 2.488116502761841, 2.5864388942718506, 2.765719175338745, 2.826643466949463]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_22 - Captured router_logits: [1.7052133083343506, 1.4053689241409302, 1.615427017211914, 1.4950642585754395, 1.6562145948410034, 1.5967036485671997, 1.5951625108718872, 1.3652558326721191, 1.6402939558029175, 1.8343039751052856, 1.397408366203308, 1.6508654356002808, 1.5974884033203125, 1.506560206413269, 1.5854321718215942, 1.754459261894226]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.377007484436035, 1.8862781524658203, 1.7226911783218384, 1.7278448343276978, 1.797566533088684, 1.8731125593185425, 1.749259352684021, 1.8877544403076172, 1.7425693273544312, 1.6725735664367676, 1.8570319414138794, 1.691985845565796, 1.769582748413086, 1.232927680015564, 1.7835807800292969, 1.935322642326355]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.599590539932251, 2.78974986076355, 2.473259687423706, 2.5007567405700684, 2.802180051803589, 2.7068567276000977, 2.7404727935791016, 2.535571813583374, 2.6546874046325684, 2.799382209777832, 2.6211910247802734, 2.614563465118408, 2.6685190200805664, 2.7715156078338623, 2.65578556060791, 2.7565367221832275]
Running loglikelihood requests:  94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 189/200 [03:28<00:08,  1.25it/s]Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [1.9196796417236328, 1.9026720523834229, 1.8613823652267456, 2.1064138412475586, 1.6190193891525269, 1.9031091928482056, 1.8966598510742188, 1.9782238006591797, 1.955680251121521, 2.000145196914673, 1.9908820390701294, 1.9543050527572632, 1.9758881330490112, 1.9968611001968384, 1.8647164106369019, 2.1822640895843506]
Layer: gate_25 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_26 - Captured router_logits: [1.9775797128677368, 1.88997220993042, 1.973894715309143, 2.0480527877807617, 1.933174967765808, 1.9618475437164307, 1.94572114944458, 1.9466489553451538, 1.9163756370544434, 2.200364351272583, 2.2971644401550293, 1.9583579301834106, 2.007554769515991, 2.1253864765167236, 1.8717104196548462, 2.0094540119171143]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.9915553331375122, 1.886742115020752, 1.9661873579025269, 2.0602939128875732, 1.9035135507583618, 1.9127941131591797, 2.0412659645080566, 1.9989206790924072, 1.9953569173812866, 2.074801206588745, 1.7148393392562866, 1.9152874946594238, 2.139315605163574, 2.026381492614746, 1.9977526664733887, 1.9797747135162354]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.4669876098632812, 3.5419859886169434, 3.8678088188171387, 3.625264883041382, 3.67392897605896, 3.4989335536956787, 3.688683032989502, 3.6727066040039062, 3.476555585861206, 3.6098530292510986, 3.572248697280884, 3.352055549621582, 3.60158371925354, 3.381802797317505, 3.6409616470336914, 3.543900489807129]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.216153621673584, 7.666533470153809, 7.203685283660889, 7.1033501625061035, 7.060783863067627, 6.866263389587402, 7.45554780960083, 7.47005033493042, 7.51616907119751, 7.096415042877197, 7.211930274963379, 7.04973840713501, 7.32716178894043, 7.230555057525635, 7.5753912925720215, 7.309597492218018]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.459164619445801, 4.4913835525512695, 4.312143325805664, 3.9136009216308594, 4.775384902954102, 4.675405502319336, 4.129941940307617, 4.324254989624023, 4.424011707305908, 4.391818046569824, 4.403885841369629, 3.754880666732788, 4.161501407623291, 4.7139410972595215, 4.419290542602539, 4.353585243225098]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [2.9691953659057617, 2.9168944358825684, 2.8435254096984863, 2.9107680320739746, 2.7555885314941406, 2.8748912811279297, 2.941131591796875, 2.74269962310791, 2.843848943710327, 2.790558338165283, 3.231545925140381, 2.0499534606933594, 2.8225505352020264, 2.7836315631866455, 3.099308967590332, 2.9199416637420654]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.05754848197102547, 0.07386366277933121, 0.074842669069767, -0.2267107367515564, -0.1903214305639267, -0.0908854752779007, 0.10420794039964676, -0.08683135360479355, 0.07539835572242737, 0.05832304805517197, 0.0669306218624115, 0.0650072768330574, 0.07405899465084076, 0.08352235704660416, -0.927462100982666, 0.09544923156499863]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.06084369495511055, 0.025448167696595192, 0.021138614043593407, 0.029002662748098373, 0.062261130660772324, 0.006615806836634874, 0.0314362496137619, 0.04606049507856369, 0.000294141354970634, 0.03875540941953659, -0.20710362493991852, 0.03278209641575813, 0.04433813318610191, -0.04839419573545456, 0.004315190017223358, 0.0035713815595954657]
Layer: gate_1 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.0481877401471138, 0.024585844948887825, 0.08552900701761246, 0.09895230829715729, 0.0744752436876297, 0.08137147128582001, 0.046972714364528656, -0.12486939877271652, 0.07567406445741653, 0.04024204611778259, 0.007437596097588539, 0.08212828636169434, -0.19915463030338287, -0.0005613514804281294, -0.052827682346105576, 0.08649934083223343]
Layer: gate_2 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_3 - Captured router_logits: [0.11900743097066879, 0.11331824213266373, 0.07561521232128143, 0.13138198852539062, 0.06315165758132935, 0.11542496085166931, -0.024804195389151573, 0.17869511246681213, 0.11366357654333115, -0.5560595989227295, -0.05709434673190117, 0.06410001218318939, 0.1385316103696823, -0.25660446286201477, -0.11350743472576141, -0.1283508837223053]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.04830416291952133, 0.08694223314523697, 0.127365842461586, 0.04759763553738594, 0.044286999851465225, -0.09713380038738251, 0.00822883564978838, -0.01889612339437008, 0.005494504701346159, 0.054811254143714905, -0.17936952412128448, 0.11451461911201477, -0.07898859679698944, -0.21426883339881897, 0.06689567118883133, -0.008803789503872395]
Layer: gate_4 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.09770754724740982, 0.16929715871810913, 0.04627778381109238, 0.007870991714298725, -0.011812163516879082, 0.03255649656057358, 0.12068874388933182, 0.11035934090614319, 0.08602774143218994, -0.0956723764538765, 0.10352620482444763, 0.1405455619096756, -0.18550196290016174, 0.14216135442256927, 0.2119053453207016, 0.08998073637485504]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_6 - Captured router_logits: [0.12079354375600815, -0.0023809352423995733, 0.004636368248611689, 0.34078556299209595, 0.13902343809604645, 0.05573605000972748, -0.08936335146427155, -0.0683109387755394, 0.10807530581951141, 0.1471026986837387, -0.5484728813171387, 0.2928254008293152, 0.29589468240737915, -0.27655860781669617, 0.14081399142742157, 0.21221542358398438]
Layer: gate_6 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.13373470306396484, -0.04653476923704147, 0.09899743646383286, -0.040310557931661606, -0.9913246035575867, -0.017812645062804222, -0.08110044151544571, -0.37894731760025024, 0.008630461990833282, -0.1069110706448555, -0.275614470243454, -0.019120950251817703, 0.10522682219743729, 0.036795809864997864, -0.5318851470947266, 0.08152145147323608]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_8 - Captured router_logits: [0.19739779829978943, 0.17754940688610077, -0.11545850336551666, 0.37833404541015625, 0.3414086699485779, 0.28947997093200684, 0.093965545296669, 0.16468216478824615, -0.03629843890666962, 0.3273378908634186, 0.2986558973789215, 0.13196857273578644, 0.49031710624694824, -0.23572413623332977, -0.15184202790260315, 0.1686721295118332]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.27019187808036804, 0.1827336847782135, 0.11452269554138184, 0.7328154444694519, 0.19425815343856812, 0.3253624737262726, 0.4997963607311249, 0.6272822618484497, 0.1712396740913391, 0.16598419845104218, 0.2628646492958069, 0.4595734477043152, 0.7019678950309753, 0.2504328191280365, 0.7564367055892944, 0.6256779432296753]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.8758097887039185, 0.8679606318473816, 0.2643745541572571, 0.6252877712249756, 0.5316348075866699, -0.0917770117521286, 0.38633522391319275, 0.6856281161308289, -0.04151702672243118, -0.1105535626411438, 0.44070181250572205, 0.5698773264884949, 0.2905209958553314, 0.14642229676246643, 0.21930085122585297, 0.3078949749469757]
Layer: gate_10 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.22306598722934723, 0.511806845664978, 0.4938894510269165, 0.4246908128261566, 0.7051407098770142, 0.24395453929901123, 0.5474311113357544, 0.31129005551338196, 0.4058266878128052, 0.7973471879959106, 0.4324186146259308, 0.19738563895225525, 0.11674033105373383, 0.058432187885046005, -0.31984302401542664, 0.38773560523986816]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.11385800689458847, 0.8015596270561218, 0.7958701848983765, 0.5506994724273682, 0.1790827214717865, 0.4373733103275299, 0.5128171443939209, 0.5544689893722534, 0.44727039337158203, 0.8555734157562256, 1.6572937965393066, 0.6369705200195312, 0.7936227321624756, 0.6293750405311584, 0.5695676803588867, 0.705068051815033]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [0.9031484723091125, 0.9342796206474304, 1.0750553607940674, 0.2012140452861786, 0.6828290820121765, 0.32925325632095337, 0.8390670418739319, 1.060587763786316, 0.6618388295173645, 0.770824134349823, 0.7649456262588501, 1.4846731424331665, 0.12946708500385284, 1.0266627073287964, 0.929702639579773, 0.5724771618843079]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5129092335700989, 0.7269806265830994, 1.1131932735443115, 1.1082905530929565, 0.9514517188072205, 0.7120386958122253, -0.07361838966608047, 0.667284369468689, 0.5272004008293152, 0.4107237756252289, -0.32523247599601746, 0.5320809483528137, 1.0440459251403809, 1.0921111106872559, 1.3190802335739136, 0.28717973828315735]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.47955790162086487, 1.478893518447876, 1.2023489475250244, 0.8591027855873108, 0.9649931192398071, 1.0947266817092896, 1.2191435098648071, 0.7578991055488586, 0.8411353826522827, 0.9356620907783508, 0.734096348285675, 0.14212127029895782, 1.6556048393249512, 0.6891366243362427, 0.8475401997566223, 1.0091737508773804]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.168508768081665, 0.5417098999023438, 1.143222689628601, 1.1755205392837524, 1.0103721618652344, 0.7134304642677307, -0.20908033847808838, 1.9195451736450195, -0.48558351397514343, 2.4593632221221924, -0.3873012065887451, 1.1031506061553955, 1.3563483953475952, 1.199329137802124, 1.2015513181686401, 0.07429088652133942]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.1047738790512085, 1.7777611017227173, 1.837393045425415, 2.3409435749053955, 2.1803200244903564, 1.7584724426269531, 1.7059777975082397, 1.9798437356948853, 1.9331196546554565, 2.137275457382202, 2.2475202083587646, 1.5524754524230957, 1.5573787689208984, 1.7275454998016357, 1.668064832687378, 2.404646158218384]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.4673151969909668, 1.0835627317428589, 1.6430033445358276, 2.0699880123138428, 1.3951603174209595, 1.7006677389144897, 1.4386979341506958, 2.0877015590667725, 2.541602849960327, 1.5204476118087769, 1.6207492351531982, 1.5102838277816772, 1.6571439504623413, 2.0785763263702393, 1.6940630674362183, 1.5721070766448975]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.0064313411712646, 1.8039963245391846, 1.4854552745819092, 1.600066900253296, 1.8643081188201904, 1.565153956413269, 1.8004482984542847, 1.795487880706787, 1.7506767511367798, 1.5259242057800293, 1.9095185995101929, 1.6727991104125977, 2.748610258102417, 1.9314764738082886, 1.8132126331329346, 1.6751387119293213]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.655423104763031, 1.211226224899292, 1.1747066974639893, 0.7863605618476868, 0.48726686835289, 1.4013853073120117, 1.263132929801941, 1.1582403182983398, 0.808351993560791, 1.0435807704925537, 1.3111212253570557, 0.9391863942146301, 1.1034904718399048, 1.3621760606765747, 1.3333078622817993, 1.0718443393707275]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.406559467315674, 2.537727117538452, 2.501953125, 2.5748279094696045, 2.451261520385742, 2.6212387084960938, 2.635148286819458, 2.5949950218200684, 2.3867316246032715, 2.6824164390563965, 2.659574031829834, 2.5480613708496094, 2.3781025409698486, 2.545994758605957, 2.7781877517700195, 2.6708366870880127]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.836671233177185, 1.5770291090011597, 1.7001270055770874, 1.531363844871521, 1.7916077375411987, 1.717490792274475, 1.7971335649490356, 1.4928362369537354, 1.7409604787826538, 2.0970208644866943, 1.5141252279281616, 1.7753629684448242, 1.7112939357757568, 1.6134754419326782, 1.7511482238769531, 2.000422239303589]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.5225305557250977, 2.0567257404327393, 1.8510535955429077, 1.8328945636749268, 1.971000075340271, 1.893344521522522, 1.7941676378250122, 1.9533097743988037, 1.846740484237671, 1.654105305671692, 1.964784860610962, 1.8729935884475708, 1.9109164476394653, 1.1958558559417725, 1.9870948791503906, 1.9591313600540161]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.6771512031555176, 2.887272834777832, 2.7367663383483887, 2.569629192352295, 2.9165282249450684, 2.7684991359710693, 2.933522939682007, 2.6619861125946045, 2.7656657695770264, 2.8655052185058594, 2.7788445949554443, 2.73486590385437, 2.680015802383423, 2.900343894958496, 2.833906888961792, 2.9112660884857178]
Layer: gate_24 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.0129406452178955, 1.9855279922485352, 1.9107738733291626, 2.1734721660614014, 1.5942915678024292, 1.9462810754776, 1.9228689670562744, 2.1159636974334717, 1.9803191423416138, 1.9690781831741333, 2.0086467266082764, 2.008613109588623, 1.9887781143188477, 2.00750470161438, 1.8336259126663208, 2.1716063022613525]
Layer: gate_25 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [2.1069529056549072, 1.9487601518630981, 1.97930109500885, 2.1346657276153564, 1.9696232080459595, 2.0036919116973877, 1.9993714094161987, 1.946954607963562, 1.9503270387649536, 2.274794340133667, 2.3265161514282227, 2.003504991531372, 2.0411217212677, 2.268019914627075, 1.9614938497543335, 2.0431835651397705]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_27 - Captured router_logits: [1.9996819496154785, 1.9207929372787476, 1.9803591966629028, 2.094317674636841, 1.9472239017486572, 1.9851912260055542, 2.0804672241210938, 2.060478448867798, 2.0273303985595703, 2.143259048461914, 1.711037516593933, 1.975779414176941, 2.113417863845825, 2.0203194618225098, 1.9665447473526, 1.9955666065216064]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Running loglikelihood requests:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏    | 193/200 [03:31<00:05,  1.27it/s]Layer: gate_28 - Captured router_logits: [3.408452033996582, 3.457629680633545, 3.8383407592773438, 3.541508436203003, 3.610868215560913, 3.441535711288452, 3.447352886199951, 3.6154115200042725, 3.398756265640259, 3.5595877170562744, 3.473175048828125, 3.2565574645996094, 3.5519280433654785, 3.282860040664673, 3.603250503540039, 3.416900157928467]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.387095928192139, 7.829672336578369, 7.448915958404541, 7.335718154907227, 7.222340106964111, 7.05458927154541, 7.685744762420654, 7.663190841674805, 7.721877574920654, 7.406300067901611, 7.43822717666626, 7.200386047363281, 7.489278316497803, 7.49532413482666, 7.779027938842773, 7.45137882232666]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.690864562988281, 4.75669527053833, 4.627145290374756, 4.2102370262146, 4.892345428466797, 4.689297199249268, 4.471617221832275, 4.595726490020752, 4.691453456878662, 4.712953567504883, 4.652130126953125, 4.135412216186523, 4.578258991241455, 4.881786823272705, 4.713197708129883, 4.4909257888793945]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.3494515419006348, 3.148574113845825, 3.1491146087646484, 2.9881720542907715, 3.098407030105591, 3.224792003631592, 3.1273305416107178, 3.041672945022583, 3.084676504135132, 3.227591037750244, 3.402272939682007, 2.4728360176086426, 3.109443187713623, 3.1362526416778564, 3.3523788452148438, 3.306455373764038]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_0 - Captured router_logits: [0.05626595392823219, 0.06770829111337662, 0.07705158740282059, -0.23546791076660156, -0.20462189614772797, -0.05519760400056839, 0.10131841897964478, -0.10062920302152634, 0.048430390655994415, 0.0613086074590683, 0.0685805082321167, 0.05207742378115654, 0.06912200152873993, 0.08439246565103531, -0.9285269379615784, 0.09785948693752289]
Layer: gate_0 - One-hot representation: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_1 - Captured router_logits: [0.07493662089109421, 0.028773928061127663, 0.025967642664909363, 0.035876840353012085, 0.06008714437484741, 0.03092542104423046, 0.04546785354614258, 0.0505891777575016, 0.004191763233393431, 0.04891396313905716, -0.19970625638961792, 0.04860161244869232, 0.029968438670039177, -0.018060626462101936, 0.022357696667313576, 0.018331827595829964]
Layer: gate_1 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_2 - Captured router_logits: [0.05864213407039642, 0.02056814171373844, 0.08119263499975204, 0.08690580725669861, 0.0863274484872818, 0.09097526967525482, 0.024058885872364044, -0.12658289074897766, 0.08735392242670059, 0.07914022356271744, 0.016001570969820023, 0.08655327558517456, -0.21563728153705597, 0.018334034830331802, -0.07344140857458115, 0.12537820637226105]
Layer: gate_2 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_3 - Captured router_logits: [0.1393275111913681, 0.12453970313072205, 0.10836681723594666, 0.099331334233284, 0.08373299986124039, 0.09663120657205582, -0.08491972833871841, 0.16435885429382324, 0.1074642613530159, -0.5753294229507446, 0.0011019076919183135, 0.06660433113574982, 0.13021838665008545, -0.3405969440937042, -0.028242740780115128, -0.14499832689762115]
Layer: gate_3 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_4 - Captured router_logits: [0.03375033661723137, 0.08372429758310318, 0.07841799408197403, 0.06585139036178589, 0.08015456795692444, -0.07029379904270172, -0.026682881638407707, -0.006865723058581352, -0.044852666556835175, 0.02858944982290268, -0.235609769821167, 0.0993143618106842, -0.0307079516351223, -0.22524291276931763, 0.09091713279485703, -0.06582416594028473]
Layer: gate_4 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_5 - Captured router_logits: [0.102344810962677, 0.1714574545621872, 0.05791674926877022, 0.09077136963605881, -0.05489254742860794, -0.0687975287437439, 0.13581334054470062, 0.07148426026105881, 0.08885210007429123, -0.07885217666625977, 0.05417230352759361, 0.16496674716472626, -0.21488690376281738, 0.1890101283788681, 0.1637510508298874, 0.10577794164419174]
Layer: gate_5 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_6 - Captured router_logits: [0.15508992969989777, -0.11240500211715698, 0.043813154101371765, 0.30783358216285706, 0.1457841396331787, 0.10651019215583801, -0.161264106631279, -0.10844648629426956, -0.041946060955524445, 0.1999666839838028, -0.5875988006591797, 0.23116187751293182, 0.3637186884880066, -0.3587029278278351, 0.14081542193889618, 0.22046354413032532]
Layer: gate_6 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_7 - Captured router_logits: [-0.1716465801000595, -0.0792611837387085, 0.051688794046640396, -0.030749572440981865, -1.0030176639556885, -0.04610554873943329, -0.02493155561387539, -0.5987622737884521, 0.0577983595430851, -0.07891568541526794, -0.32351160049438477, 0.020231127738952637, 0.03678299859166145, 0.12064766138792038, -0.6246200799942017, 0.0166825782507658]
Layer: gate_7 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_8 - Captured router_logits: [0.13518589735031128, 0.21583175659179688, -0.22293055057525635, 0.45127972960472107, 0.34532424807548523, 0.32332953810691833, 0.11266008019447327, 0.2193734049797058, -0.05269167944788933, 0.1839103400707245, 0.3783057630062103, 0.030917726457118988, 0.5582667589187622, -0.2412670999765396, -0.19882597029209137, 0.1498933583498001]
Layer: gate_8 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_9 - Captured router_logits: [0.28465890884399414, 0.11513832956552505, 0.133488267660141, 0.6543729305267334, 0.20367439091205597, 0.3009384572505951, 0.4711693227291107, 0.6326104402542114, 0.07823870331048965, 0.11720599979162216, 0.18808293342590332, 0.4107862710952759, 0.5895262360572815, 0.07493140548467636, 0.761924684047699, 0.6186835765838623]
Layer: gate_9 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_10 - Captured router_logits: [0.7708515524864197, 0.8938659429550171, 0.3188588321208954, 0.6507857441902161, 0.5614392161369324, -0.2350948005914688, 0.48948433995246887, 0.7742687463760376, -0.02094377763569355, -0.3491937220096588, 0.5505738854408264, 0.5341187715530396, 0.4116596579551697, 0.14305637776851654, 0.23893313109874725, 0.35761916637420654]
Layer: gate_10 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_11 - Captured router_logits: [0.27181798219680786, 0.6032635569572449, 0.6355918645858765, 0.4582812786102295, 0.8073503971099854, 0.37955009937286377, 0.7055301666259766, 0.4035227298736572, 0.46172234416007996, 0.915650486946106, 0.5342562198638916, 0.42457857728004456, 0.2093765288591385, 0.2742849886417389, -0.11407287418842316, 0.5780972838401794]
Layer: gate_11 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_12 - Captured router_logits: [0.03327436372637749, 0.8953720927238464, 0.7034022808074951, 0.3820922374725342, 0.20456117391586304, 0.37093767523765564, 0.36980247497558594, 0.4277534782886505, 0.4552139639854431, 0.8259811997413635, 1.6462124586105347, 0.6433423757553101, 0.7056922912597656, 0.6108548045158386, 0.6044690012931824, 0.733279287815094]
Layer: gate_12 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Layer: gate_13 - Captured router_logits: [1.0902528762817383, 0.9960139393806458, 1.3293553590774536, 0.4266553819179535, 0.8153935074806213, 0.5028565526008606, 0.8818631172180176, 0.856782078742981, 0.7943721413612366, 0.9056863784790039, 0.7612637281417847, 1.4423640966415405, 0.4121258854866028, 1.1509596109390259, 0.8367782831192017, 0.7261281609535217]
Running loglikelihood requests:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 197/200 [03:34<00:02,  1.30it/s]Running loglikelihood requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [03:34<00:00,  1.07s/it]
Layer: gate_13 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
Layer: gate_14 - Captured router_logits: [0.5377627611160278, 0.6295292377471924, 1.2145315408706665, 1.1698552370071411, 1.0216927528381348, 0.5632407069206238, -0.12421246618032455, 0.4324198067188263, 0.5508463382720947, 0.5323523283004761, -0.21156150102615356, 0.5646083354949951, 1.0494403839111328, 1.0853086709976196, 1.2557786703109741, 0.2750818431377411]
Layer: gate_14 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_15 - Captured router_logits: [0.5016788840293884, 1.524585485458374, 1.1809353828430176, 0.89934241771698, 1.0444203615188599, 1.2546454668045044, 1.183525562286377, 0.648561954498291, 0.726082444190979, 0.9848008155822754, 0.8003487586975098, 0.21760918200016022, 1.8416497707366943, 0.8111553192138672, 0.8853946924209595, 0.8806455135345459]
Layer: gate_15 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_16 - Captured router_logits: [1.3510316610336304, 0.6490694880485535, 1.2979592084884644, 1.403022289276123, 1.0543211698532104, 0.8294045925140381, -0.4195486009120941, 2.176978588104248, -0.318067342042923, 2.327604055404663, -0.2441418468952179, 1.2973880767822266, 1.5507991313934326, 1.4950735569000244, 1.4119175672531128, -0.01891438290476799]
Layer: gate_16 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_17 - Captured router_logits: [1.0507310628890991, 1.9349746704101562, 1.939523696899414, 2.3605170249938965, 2.2594921588897705, 1.881081223487854, 1.7506589889526367, 2.0461132526397705, 1.9073026180267334, 1.978851079940796, 2.1783485412597656, 1.6376397609710693, 1.5606766939163208, 1.8038489818572998, 1.696227788925171, 2.5217344760894775]
Layer: gate_17 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Layer: gate_18 - Captured router_logits: [1.5632076263427734, 1.1246256828308105, 1.805048942565918, 2.0979561805725098, 1.448973536491394, 1.7228264808654785, 1.3762520551681519, 2.15659236907959, 2.478381633758545, 1.5257623195648193, 1.6291179656982422, 1.520324945449829, 1.5828888416290283, 2.0320870876312256, 1.6739991903305054, 1.5025784969329834]
Layer: gate_18 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_19 - Captured router_logits: [2.195232391357422, 1.9619367122650146, 1.633609652519226, 1.9037007093429565, 2.0993802547454834, 1.7573410272598267, 2.0542185306549072, 1.9812726974487305, 1.8124561309814453, 1.8082696199417114, 2.1552000045776367, 1.8721420764923096, 2.9191462993621826, 2.1866676807403564, 2.091991424560547, 1.9309208393096924]
Layer: gate_19 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_20 - Captured router_logits: [0.5672722458839417, 1.2846521139144897, 1.2053570747375488, 0.7903097867965698, 0.3372933268547058, 1.6015182733535767, 1.256334900856018, 0.9938013553619385, 0.8988459706306458, 1.1265869140625, 1.287213683128357, 1.0369606018066406, 1.2045583724975586, 1.4320712089538574, 1.307031273841858, 1.0571500062942505]
Layer: gate_20 - One-hot representation: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_21 - Captured router_logits: [2.649913787841797, 2.7883858680725098, 2.701545238494873, 2.7790260314941406, 2.703692674636841, 2.9105191230773926, 2.867680788040161, 2.7574899196624756, 2.6178524494171143, 2.879049777984619, 2.8752686977386475, 2.5569450855255127, 2.5152337551116943, 2.755171060562134, 2.9309985637664795, 2.8990492820739746]
Layer: gate_21 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
Layer: gate_22 - Captured router_logits: [1.8786520957946777, 1.6780812740325928, 1.728611707687378, 1.6126028299331665, 1.8544695377349854, 1.8393914699554443, 1.8488293886184692, 1.5705152750015259, 1.8007330894470215, 2.1474132537841797, 1.5470585823059082, 1.6551246643066406, 1.7710468769073486, 1.6893031597137451, 1.8031781911849976, 2.094442844390869]
Layer: gate_22 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
Layer: gate_23 - Captured router_logits: [2.64046573638916, 2.0912837982177734, 1.885165810585022, 1.899082899093628, 2.0156991481781006, 2.0186173915863037, 1.876396656036377, 2.0208981037139893, 1.9688018560409546, 1.7442808151245117, 2.0772082805633545, 1.9740206003189087, 1.9528356790542603, 1.1942017078399658, 1.9870929718017578, 2.0381181240081787]
Layer: gate_23 - One-hot representation: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_24 - Captured router_logits: [2.6470346450805664, 2.9028232097625732, 2.697314500808716, 2.5625088214874268, 2.765625476837158, 2.710857629776001, 2.847299814224243, 2.5977060794830322, 2.7080090045928955, 2.8130035400390625, 2.659745216369629, 2.6738336086273193, 2.660325050354004, 2.8755831718444824, 2.811980724334717, 2.851649284362793]
Layer: gate_24 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_25 - Captured router_logits: [2.094505548477173, 2.0239341259002686, 2.040517568588257, 2.314121723175049, 1.6603493690490723, 2.0283071994781494, 2.041306734085083, 2.201021432876587, 2.127655267715454, 2.0882363319396973, 2.1399905681610107, 2.1124136447906494, 2.074721336364746, 2.126758575439453, 1.8454310894012451, 2.2693467140197754]
Layer: gate_25 - One-hot representation: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_26 - Captured router_logits: [2.1621222496032715, 2.0444467067718506, 2.0867197513580322, 2.232501983642578, 2.0304176807403564, 2.1488406658172607, 2.071868896484375, 1.9954115152359009, 2.0448591709136963, 2.3575353622436523, 2.3747096061706543, 2.08878231048584, 2.1236705780029297, 2.3873496055603027, 2.061292886734009, 2.181934356689453]
Layer: gate_26 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
Layer: gate_27 - Captured router_logits: [2.08300518989563, 1.9583886861801147, 2.01064133644104, 2.096689224243164, 1.9288272857666016, 2.0047919750213623, 2.0873751640319824, 2.0729260444641113, 2.0431160926818848, 2.1264636516571045, 1.669726014137268, 1.9996063709259033, 2.2113406658172607, 2.0944302082061768, 1.9943263530731201, 2.043354034423828]
Layer: gate_27 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
Layer: gate_28 - Captured router_logits: [3.4773037433624268, 3.514200210571289, 3.9203264713287354, 3.6020052433013916, 3.748251438140869, 3.516366720199585, 3.4295902252197266, 3.6946985721588135, 3.460035800933838, 3.612330913543701, 3.547842264175415, 3.2701427936553955, 3.6282882690429688, 3.35601544380188, 3.670078754425049, 3.488917589187622]
Layer: gate_28 - One-hot representation: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_29 - Captured router_logits: [7.304813861846924, 7.715285778045654, 7.369841575622559, 7.212684631347656, 7.143791675567627, 6.962907314300537, 7.596540927886963, 7.512186050415039, 7.646036148071289, 7.284803867340088, 7.3031487464904785, 7.180819988250732, 7.384498596191406, 7.411742210388184, 7.711132049560547, 7.274904251098633]
Layer: gate_29 - One-hot representation: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_30 - Captured router_logits: [4.596133232116699, 4.731557369232178, 4.560890197753906, 4.089343547821045, 4.837363243103027, 4.661707401275635, 4.345487594604492, 4.567189693450928, 4.606224536895752, 4.601398468017578, 4.588649749755859, 3.920060396194458, 4.382857322692871, 4.783336639404297, 4.632022380828857, 4.400498867034912]
Layer: gate_30 - One-hot representation: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Layer: gate_31 - Captured router_logits: [3.264747381210327, 3.1473636627197266, 3.161100387573242, 2.9803903102874756, 3.0929949283599854, 3.1622955799102783, 3.0808653831481934, 2.9688196182250977, 3.079296588897705, 3.051072359085083, 3.2845232486724854, 2.3635926246643066, 3.0427658557891846, 3.035076141357422, 3.3008055686950684, 3.2010529041290283]
Layer: gate_31 - One-hot representation: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
{'gate_0': [4.165223117917776, 4.903217859566212, 4.998150773346424, -11.94170731306076, -10.573446445167065, -5.568808440119028, 5.869652986526489, -7.262374389916658, 3.7452662102878094, 4.176385842263699, 4.5688487105071545, 3.7492126412689686, 4.378517106175423, 5.101776674389839, -51.28515034914017, 5.698865093290806], 'gate_1': [3.646941374987364, 2.0416884692385793, 1.5123493103310466, 2.137838652357459, 3.6499320045113564, 1.1924229281721637, 2.1152785290032625, 3.1545738205313683, 0.6674821707165393, 2.5438559986650944, -9.130887731909752, 2.3140741884708405, 0.7826749218511395, -1.3655048343061935, 0.5927466171560809, 1.0602473226899747], 'gate_2': [3.0773639380931854, 1.8191906940191984, 4.408806681632996, 3.2711236905306578, 4.752002947032452, 4.954486794769764, 2.7609894294291735, -5.741784498095512, 3.6675879284739494, 3.621517810970545, 0.35983446837053634, 4.482074707746506, -9.406088538467884, 0.8171885107367416, -2.469373421743512, 5.257494606077671], 'gate_3': [6.269234597682953, 6.287332616746426, 4.511176388710737, 5.834159299731255, 4.906364776194096, 5.14301797747612, -0.7062472386751324, 7.5913146659731865, 6.574914366006851, -28.15697494149208, -1.9417584090260789, 3.362425995990634, 6.911971293389797, -14.590317711234093, -3.6457803286612034, -6.48028177767992], 'gate_4': [1.9467104068025947, 3.822709336876869, 4.114703722298145, 4.339130527339876, 3.067823026329279, -4.5947687439620495, 0.698119266773574, -0.3877150645421352, -1.2439722581766546, 1.6256427760235965, -10.297446936368942, 5.510203458368778, -3.4252912551164627, -10.918963365256786, 4.4550992622971535, -1.1118477009731578], 'gate_5': [5.0520850121974945, 7.019459716975689, 2.2332398369908333, 3.5240755227714544, -1.3115419913083315, -1.893617972265929, 2.892172252293676, 3.8283037077635527, 5.2102753119543195, -5.058674655854702, -4.541862764395773, 7.515317022800446, -9.827019140124321, 7.711448535323143, 8.0647262185812, 4.271163687109947], 'gate_6': [7.091288197785616, -1.6402902427362278, 1.9421401442959905, 14.15963626652956, 7.870428277179599, 4.232853896915913, -5.69459995161742, -3.5847926698625088, 2.864987251814455, 8.16589592024684, -27.91550722718239, 10.980105623602867, 17.34622636437416, -14.01533878594637, 7.982379646040499, 10.41546038351953], 'gate_7': [-6.925399248953909, -1.5209236863302067, 4.413814125582576, 0.3599135368713178, -50.64497911930084, -7.316509595606476, -6.006541102193296, -21.557052791118622, -1.6389168854802847, -7.926917262375355, -12.595784895122051, -1.3018091655394528, 2.7099238273222, 0.6114763543009758, -27.924202024936676, -2.2404807336861268], 'gate_8': [8.662526607513428, 11.546041652560234, -6.006272215396166, 20.94634298980236, 16.26121085882187, 16.003693230450153, 7.762395277619362, 10.539252806454897, -0.2669663694105111, 14.086618285626173, 14.266522783786058, 2.621964585967362, 24.23781231045723, -10.366131227463484, -6.405350544227986, 9.466541267931461], 'gate_9': [16.73405572026968, 12.393970776349306, 6.939150959253311, 36.865563094615936, 14.548877656459808, 17.105379968881607, 30.106827825307846, 35.01879319548607, 12.739051051437855, 5.11396880086977, 13.898727368563414, 25.12309294939041, 32.04829317331314, 14.96853032335639, 41.44833654165268, 32.53848800063133], 'gate_10': [40.47305524349213, 41.30566078424454, 15.94519217312336, 31.261430203914642, 30.095149993896484, -4.79379328363575, 21.059758096933365, 36.512256145477295, 0.43161314353346825, -10.303820830769837, 23.682787716388702, 29.549444168806076, 11.393490507267416, 9.97484614700079, 13.68891204893589, 18.483382254838943], 'gate_11': [17.273686930537224, 29.976861089468002, 33.920582950115204, 24.998627871274948, 36.98093006014824, 15.258363427550648, 33.37085235118866, 18.262460730969906, 26.37381073832512, 47.119271874427795, 26.974390774965286, 18.845540337264538, 8.339517807587981, 7.884486151393503, -12.390829211100936, 27.718796506524086], 'gate_12': [6.406132593750954, 45.350637286901474, 41.32072076201439, 26.83903557062149, 10.348197516985238, 24.624822467565536, 24.968079090118408, 29.675509199500084, 23.922662690281868, 33.21574354916811, 79.75051891803741, 34.882335901260376, 40.81314042210579, 35.80497372150421, 30.81677657365799, 36.19845786690712], 'gate_13': [54.031270422041416, 54.48279172182083, 65.0812269449234, 21.84569925069809, 46.903878808021545, 18.20748057961464, 49.80757158994675, 53.599910229444504, 46.120411694049835, 48.24352118372917, 48.625618785619736, 71.43708956241608, 17.134900527307764, 60.03494676947594, 51.40146452188492, 38.860892936587334], 'gate_14': [29.61492907255888, 38.076971262693405, 62.081167101860046, 59.203900039196014, 49.9510940015316, 49.06598097085953, -1.4105506357736886, 33.08615022152662, 33.7673876285553, 25.569991439580917, -15.716098977252841, 26.53959995508194, 50.619127705693245, 51.49344927072525, 62.16173279285431, 17.492144972085953], 'gate_15': [29.00225020945072, 68.34906351566315, 57.98108506202698, 47.24707156419754, 49.103072583675385, 58.62271273136139, 65.79966247081757, 36.574871480464935, 40.32614368200302, 48.199621856212616, 40.7192168533802, 12.561684036219958, 83.00455617904663, 41.875843703746796, 43.18088746070862, 46.68519023805857], 'gate_16': [54.42135637998581, 30.325585827231407, 54.652179479599, 58.22467517852783, 45.30194741487503, 37.46833544969559, -11.691800721921027, 90.67968201637268, -16.011463685892522, 104.29453110694885, -20.615032116882503, 54.07505679130554, 65.66022402048111, 63.459901452064514, 59.3244314789772, 0.8984755869023502], 'gate_17': [47.880319356918335, 83.18662250041962, 86.84120309352875, 104.40629529953003, 101.4170618057251, 84.84528493881226, 82.85538876056671, 93.8400571346283, 88.49023485183716, 95.7814165353775, 104.04790711402893, 75.57062005996704, 73.1613187789917, 83.21821355819702, 74.8727148771286, 108.86974084377289], 'gate_18': [75.28056347370148, 56.729157745838165, 84.41648459434509, 100.5016907453537, 70.93590831756592, 84.58135843276978, 75.19738531112671, 102.61863160133362, 121.55599665641785, 76.736696600914, 76.89330554008484, 71.84806895256042, 79.14621555805206, 87.76533627510071, 84.24329257011414, 78.44675767421722], 'gate_19': [102.76866936683655, 93.82682704925537, 79.86006045341492, 87.28662312030792, 97.18898677825928, 83.84575951099396, 93.43172037601471, 93.2740033864975, 95.01399576663971, 81.49387550354004, 99.6973204612732, 87.03918218612671, 133.836909532547, 99.90172100067139, 96.34576046466827, 91.00852453708649], 'gate_20': [35.67608207464218, 64.75278770923615, 61.85902953147888, 41.85546696186066, 24.329932564869523, 75.65827488899231, 63.28644669055939, 62.543797731399536, 44.748473167419434, 56.27394890785217, 61.48971748352051, 50.796081602573395, 56.05597424507141, 67.50006175041199, 64.66636598110199, 50.368186354637146], 'gate_21': [125.53522682189941, 130.56715631484985, 127.9893262386322, 131.43722677230835, 126.87353491783142, 136.90928530693054, 136.12557244300842, 134.21387338638306, 125.38845729827881, 137.5653941631317, 137.5087971687317, 133.21114206314087, 125.16489267349243, 131.5242419242859, 143.23306274414062, 137.75738263130188], 'gate_22': [90.36955559253693, 78.34277737140656, 88.01080214977264, 77.91742539405823, 91.02045118808746, 87.36431014537811, 89.02478682994843, 77.2750678062439, 89.55313956737518, 99.88534533977509, 76.45091533660889, 88.8231680393219, 86.51419973373413, 84.43653666973114, 86.91497361660004, 101.00143194198608], 'gate_23': [123.00890779495239, 102.84963119029999, 92.84301364421844, 92.04434645175934, 100.3887083530426, 96.50390493869781, 92.38917553424835, 100.24322617053986, 95.09789824485779, 88.03251683712006, 99.65135490894318, 93.8042243719101, 96.83065938949585, 63.06127905845642, 98.54950082302094, 99.8146800994873], 'gate_24': [136.1176416873932, 147.12537503242493, 134.92946338653564, 131.43329858779907, 144.97366547584534, 141.81731414794922, 143.9698133468628, 135.14545798301697, 138.7826337814331, 142.7708923816681, 141.28519558906555, 139.31152725219727, 136.49132871627808, 146.90207314491272, 142.56536269187927, 144.48125982284546], 'gate_25': [104.66680157184601, 101.47859930992126, 98.76390886306763, 110.50186896324158, 84.96505284309387, 100.5670919418335, 100.62798488140106, 106.36536252498627, 103.32787787914276, 102.31868767738342, 104.6248551607132, 102.39874029159546, 102.99109208583832, 104.0678858757019, 97.52393448352814, 112.81840252876282], 'gate_26': [106.90219819545746, 100.84048891067505, 100.70825254917145, 109.42911911010742, 100.85377275943756, 103.66364967823029, 102.99240934848785, 100.25303757190704, 100.16044580936432, 112.8411910533905, 120.50536799430847, 103.5866003036499, 105.50047707557678, 110.91049897670746, 99.90401196479797, 105.7243504524231], 'gate_27': [102.5559446811676, 97.87711787223816, 100.48866653442383, 104.85759317874908, 97.82435154914856, 100.35920023918152, 104.36584997177124, 104.82875514030457, 101.36953020095825, 105.82545018196106, 85.68692183494568, 99.84253227710724, 107.75280058383942, 104.0710117816925, 99.76735866069794, 100.21573829650879], 'gate_28': [169.98225831985474, 171.64207315444946, 186.73149061203003, 174.08856987953186, 180.03749871253967, 171.1093454360962, 174.9309470653534, 178.71137595176697, 168.24230647087097, 175.1627926826477, 171.7402582168579, 161.67968082427979, 175.1382303237915, 162.75381755828857, 177.48249578475952, 169.77290725708008], 'gate_29': [359.8832149505615, 382.22825717926025, 365.3405065536499, 357.9226312637329, 353.56929445266724, 344.0913324356079, 374.8433747291565, 371.07986879348755, 374.1026244163513, 358.3661918640137, 361.4743375778198, 353.2241487503052, 363.5912256240845, 367.7450637817383, 380.03146028518677, 362.56099128723145], 'gate_30': [223.34495306015015, 229.11574268341064, 220.71294903755188, 202.76262521743774, 235.44576454162598, 226.78387689590454, 212.51438426971436, 220.55387544631958, 225.85583019256592, 223.89955401420593, 221.91967844963074, 194.78486442565918, 215.0873942375183, 234.31109476089478, 226.10028219223022, 215.66327667236328], 'gate_31': [157.9989607334137, 150.54597544670105, 151.04405403137207, 144.2949025630951, 148.73309469223022, 153.18455290794373, 150.37626552581787, 144.03645491600037, 147.76592087745667, 149.95307397842407, 163.035742521286, 117.8697315454483, 148.49589204788208, 148.70047903060913, 161.9284267425537, 154.7256417274475]}
{'gate_0': [0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 2, 0, 9], 'gate_1': [18, 0, 0, 0, 22, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_2': [0, 0, 1, 1, 11, 19, 0, 0, 0, 2, 0, 0, 0, 0, 0, 16], 'gate_3': [0, 2, 0, 1, 0, 0, 0, 28, 1, 0, 0, 0, 18, 0, 0, 0], 'gate_4': [0, 0, 3, 16, 3, 0, 0, 0, 0, 0, 0, 27, 0, 0, 1, 0], 'gate_5': [0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 9, 0, 11, 25, 0], 'gate_6': [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 48, 0, 0, 0], 'gate_7': [0, 0, 15, 1, 0, 1, 0, 0, 9, 0, 0, 0, 15, 9, 0, 0], 'gate_8': [0, 0, 0, 7, 0, 0, 0, 0, 0, 1, 0, 0, 42, 0, 0, 0], 'gate_9': [0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 0], 'gate_10': [15, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_11': [0, 0, 1, 0, 0, 0, 3, 0, 0, 46, 0, 0, 0, 0, 0, 0], 'gate_12': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0], 'gate_13': [0, 0, 6, 0, 0, 0, 0, 2, 0, 0, 0, 33, 0, 8, 1, 0], 'gate_14': [0, 0, 21, 3, 0, 6, 0, 0, 0, 0, 0, 0, 3, 0, 17, 0], 'gate_15': [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 48, 0, 0, 0], 'gate_16': [0, 0, 0, 0, 0, 0, 0, 8, 0, 42, 0, 0, 0, 0, 0, 0], 'gate_17': [0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 32], 'gate_18': [0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0, 0, 0], 'gate_19': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 49, 0, 0, 0], 'gate_20': [0, 0, 0, 0, 0, 42, 0, 1, 0, 0, 0, 0, 0, 6, 1, 0], 'gate_21': [0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 1, 4, 0, 0, 38, 3], 'gate_22': [0, 0, 2, 0, 0, 0, 0, 0, 0, 21, 0, 1, 0, 0, 0, 26], 'gate_23': [50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_24': [0, 9, 0, 0, 12, 3, 3, 0, 0, 3, 6, 0, 0, 12, 1, 1], 'gate_25': [0, 0, 0, 12, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 32], 'gate_26': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 0, 0, 1, 0, 0], 'gate_27': [0, 0, 0, 0, 0, 0, 4, 1, 0, 6, 0, 0, 33, 6, 0, 0], 'gate_28': [0, 0, 48, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'gate_29': [0, 38, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 7, 0], 'gate_30': [0, 0, 0, 0, 31, 0, 0, 0, 0, 0, 0, 0, 0, 19, 0, 0], 'gate_31': [6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 26, 0, 0, 0, 18, 0]}
{'mmlu_formal_logic': {'alias': 'formal_logic', 'acc,none': 0.24, 'acc_stderr,none': 0.06101187572589322}}
