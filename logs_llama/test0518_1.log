nohup: ignoring input
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:0'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}
网络架构：{'150': ['247', '82', '144', '146', '12', '57', '160', '253', '83', '156', '3', '84', '248', '100', '115', '179', '163', '136', '220', '14', '172', '126', '211', '175', '6', '5', '103', '69', '27', '243', '21'], '27': ['243', '21', '57', '172', '136', '115', '126', '3', '163', '248', '146', '100', '156', '69', '211', '144', '14', '103', '82', '83', '179', '12', '247', '160', '150', '220', '84', '175', '6', '5', '253'], '253': ['69', '144', '172', '136', '6', '14', '248', '103', '243', '21', '82', '27', '115', '83', '12', '3', '220', '84', '100', '163', '150', '179', '146', '156', '126', '160', '57', '247', '175', '211', '5'], '115': ['57', '248', '247', '27', '172', '150', '160', '211', '6', '21', '220', '5', '82', '126', '156', '83', '14', '253', '3', '84', '163', '146', '103', '243', '12', '144', '136', '175', '69', '179', '100'], '3': ['84', '248', '103', '163', '5', '243', '150', '175', '6', '100', '14', '27', '247', '253', '21', '211', '83', '126', '69', '160', '156', '136', '220', '115', '57', '172', '12', '82', '179', '144', '146'], '14': ['5', '21', '6', '136', '247', '27', '175', '69', '211', '103', '84', '82', '115', '248', '12', '243', '172', '253', '100', '3', '160', '220', '146', '57', '150', '163', '144', '179', '83', '156', '126'], '82': ['6', '146', '100', '248', '163', '253', '14', '150', '175', '69', '12', '160', '243', '172', '220', '136', '57', '126', '27', '156', '103', '84', '211', '5', '144', '21', '247', '179', '3', '83', '115'], '103': ['84', '144', '126', '3', '248', '172', '146', '6', '12', '57', '179', '156', '82', '150', '100', '160', '175', '21', '247', '163', '5', '211', '14', '69', '253', '220', '243', '83', '27', '136', '115'], '100': ['115', '179', '160', '6', '247', '5', '82', '83', '14', '220', '3', '243', '253', '27', '146', '150', '163', '248', '144', '69', '175', '57', '156', '84', '172', '211', '21', '103', '136', '126', '12'], '126': ['14', '253', '243', '156', '12', '150', '5', '83', '136', '146', '179', '84', '100', '57', '144', '163', '82', '172', '220', '211', '21', '103', '175', '6', '3', '27', '160', '248', '247', '69', '115'], '21': ['5', '100', '14', '160', '253', '27', '163', '126', '211', '12', '3', '144', '172', '248', '103', '175', '146', '6', '243', '150', '136', '84', '179', '83', '115', '247', '82', '69', '220', '156', '57'], '179': ['248', '172', '14', '6', '3', '126', '150', '243', '253', '211', '156', '27', '163', '144', '12', '21', '5', '136', '175', '69', '160', '84', '247', '83', '100', '82', '115', '57', '103', '220', '146'], '220': ['156', '247', '69', '27', '163', '211', '146', '248', '100', '103', '12', '243', '175', '3', '21', '150', '57', '5', '253', '144', '83', '115', '82', '6', '172', '179', '126', '84', '136', '160', '14'], '243': ['6', '82', '136', '100', '103', '220', '160', '150', '27', '69', '83', '12', '172', '126', '5', '179', '21', '14', '211', '253', '175', '115', '156', '57', '84', '163', '247', '144', '248', '146', '3'], '172': ['82', '126', '100', '84', '179', '144', '103', '83', '27', '115', '220', '160', '69', '3', '146', '247', '6', '21', '12', '150', '163', '156', '243', '136', '14', '5', '57', '253', '211', '175', '248'], '163': ['57', '103', '253', '150', '69', '160', '82', '5', '146', '248', '243', '156', '144', '14', '84', '3', '27', '6', '100', '175', '83', '126', '211', '179', '220', '12', '172', '21', '136', '247', '115'], '211': ['172', '69', '100', '82', '126', '21', '248', '57', '175', '220', '103', '179', '150', '84', '12', '14', '163', '156', '83', '3', '5', '6', '146', '253', '160', '144', '115', '247', '27', '243', '136'], '146': ['247', '3', '150', '175', '27', '12', '115', '253', '179', '82', '57', '5', '21', '211', '126', '163', '100', '103', '248', '14', '160', '6', '84', '69', '156', '136', '220', '243', '172', '144', '83'], '5': ['144', '69', '175', '247', '82', '160', '248', '136', '100', '21', '211', '27', '83', '14', '126', '156', '150', '57', '146', '12', '84', '172', '220', '6', '253', '103', '179', '3', '243', '115', '163'], '175': ['211', '253', '136', '12', '27', '115', '247', '172', '126', '14', '163', '103', '156', '146', '82', '21', '69', '144', '83', '57', '3', '243', '220', '179', '84', '248', '6', '160', '5', '100', '150'], '248': ['253', '247', '163', '243', '179', '150', '6', '5', '211', '126', '14', '82', '146', '175', '220', '84', '21', '12', '103', '136', '69', '83', '100', '156', '115', '160', '27', '172', '3', '57', '144'], '69': ['179', '21', '211', '14', '220', '156', '57', '163', '103', '3', '253', '82', '175', '248', '160', '5', '172', '247', '83', '150', '100', '243', '27', '115', '146', '136', '144', '84', '6', '12', '126'], '57': ['253', '136', '146', '211', '172', '103', '84', '82', '248', '220', '5', '21', '3', '156', '126', '247', '14', '115', '179', '12', '27', '163', '69', '175', '150', '6', '243', '83', '144', '160', '100'], '136': ['156', '115', '179', '150', '27', '247', '126', '57', '5', '82', '84', '243', '175', '172', '100', '3', '83', '220', '14', '103', '146', '248', '144', '211', '69', '253', '163', '21', '6', '160', '12'], '160': ['146', '179', '100', '220', '82', '83', '84', '243', '126', '211', '248', '69', '12', '27', '14', '103', '247', '163', '175', '21', '156', '6', '5', '150', '3', '57', '115', '253', '172', '144', '136'], '247': ['12', '14', '5', '103', '146', '163', '156', '115', '57', '6', '211', '220', '175', '144', '27', '160', '21', '100', '253', '248', '179', '3', '84', '172', '136', '126', '150', '82', '243', '83', '69'], '83': ['220', '103', '247', '136', '163', '21', '253', '248', '100', '172', '57', '144', '146', '156', '3', '12', '5', '160', '14', '243', '6', '69', '179', '150', '211', '175', '82', '115', '126', '27', '84'], '144': ['211', '21', '247', '163', '6', '27', '12', '126', '83', '115', '248', '14', '5', '103', '146', '220', '150', '179', '84', '100', '172', '253', '57', '160', '175', '69', '156', '82', '243', '3', '136'], '12': ['136', '163', '3', '243', '115', '150', '211', '253', '100', '172', '144', '179', '146', '175', '156', '220', '84', '126', '14', '5', '69', '27', '82', '248', '160', '6', '83', '57', '103', '21', '247'], '156': ['160', '21', '3', '163', '126', '150', '211', '82', '146', '14', '27', '57', '172', '5', '144', '248', '136', '103', '247', '243', '6', '175', '179', '253', '84', '220', '69', '115', '83', '100', '12'], '84': ['69', '5', '27', '57', '248', '247', '172', '163', '126', '211', '179', '243', '156', '150', '3', '6', '175', '115', '83', '100', '103', '146', '160', '144', '136', '14', '12', '82', '21', '220', '253'], '6': ['83', '163', '5', '211', '3', '248', '115', '126', '175', '84', '156', '160', '57', '69', '27', '144', '14', '247', '150', '82', '243', '100', '103', '179', '12', '172', '136', '146', '220', '253', '21']}
150
cuda:0
wnli
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 27.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.89s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 111
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c?recursive=False&expand=False HTTP/1.1" 307 136
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c?recursive=False&expand=False HTTP/1.1" 200 530
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/ax?recursive=False&expand=False HTTP/1.1" 307 139
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/ax?recursive=False&expand=False HTTP/1.1" 200 231
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 111
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/wnli?recursive=False&expand=False HTTP/1.1" 307 141
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/wnli?recursive=False&expand=False HTTP/1.1" 200 352
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140547542310720 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140547542310720 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140547542310720 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140547542310720 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140547545184784 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140547545184784 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140547545184784 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140547545184784 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2649.05it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:12,  3.07s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:03<02:28,  1.07s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:04<01:36,  1.42it/s]Running loglikelihood requests:   5%|▍         | 7/142 [00:05<01:15,  1.78it/s]Running loglikelihood requests:   6%|▋         | 9/142 [00:05<01:04,  2.07it/s]Running loglikelihood requests:   8%|▊         | 11/142 [00:06<00:56,  2.31it/s]Running loglikelihood requests:   9%|▉         | 13/142 [00:07<00:51,  2.49it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:08<00:49,  2.58it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:08<00:45,  2.73it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:09<00:42,  2.88it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:09<00:40,  2.99it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:10<00:38,  3.10it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:11<00:36,  3.18it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:11<00:35,  3.26it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:12<00:34,  3.31it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:12<00:33,  3.36it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:13<00:32,  3.39it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:13<00:31,  3.42it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:14<00:30,  3.43it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:15<00:29,  3.45it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:15<00:29,  3.47it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:16<00:28,  3.48it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:16<00:27,  3.52it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:17<00:26,  3.56it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:17<00:25,  3.60it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:18<00:24,  3.65it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:18<00:24,  3.68it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:19<00:23,  3.71it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:20<00:23,  3.68it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:20<00:22,  3.64it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:21<00:22,  3.61it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:21<00:21,  3.59it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:22<00:21,  3.59it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:22<00:20,  3.59it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:23<00:19,  3.66it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:23<00:19,  3.72it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:24<00:18,  3.77it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:24<00:17,  3.80it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:25<00:16,  3.83it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [00:25<00:16,  3.85it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [00:26<00:15,  3.87it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [00:26<00:15,  3.89it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [00:27<00:14,  3.89it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [00:27<00:14,  3.91it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [00:28<00:13,  3.92it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [00:29<00:12,  3.93it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [00:29<00:12,  3.95it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [00:30<00:11,  3.96it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [00:30<00:11,  3.97it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [00:31<00:10,  3.98it/s]Running loglikelihood requests:  71%|███████   | 101/142 [00:31<00:10,  3.99it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [00:31<00:09,  4.01it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [00:32<00:09,  3.87it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [00:33<00:08,  3.92it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [00:33<00:08,  3.97it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [00:34<00:07,  4.01it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [00:34<00:07,  4.04it/s]Running loglikelihood requests:  81%|████████  | 115/142 [00:34<00:06,  4.07it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [00:35<00:06,  4.08it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [00:35<00:05,  4.11it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [00:36<00:05,  4.12it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [00:36<00:04,  4.14it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [00:37<00:04,  4.16it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [00:37<00:03,  4.17it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [00:38<00:03,  4.19it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [00:38<00:02,  4.20it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [00:39<00:02,  4.22it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [00:39<00:01,  4.23it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [00:40<00:01,  4.27it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [00:40<00:00,  4.30it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [00:41<00:00,  4.35it/s]Running loglikelihood requests: 100%|██████████| 142/142 [00:41<00:00,  3.45it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:1'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:1'}
full model:
{'wnli': {'alias': 'wnli', 'acc,none': 0.5352112676056338, 'acc_stderr,none': 0.0596130578497224}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.31190044950254875
0.7306280553786256
0.6985925052465822
0.5843516732063875
0.8124235844589114
0.820438203921003
0.6253838264581936
0.7600037006947045
0.8225872978677496
0.708398461303215
0.8646089269479391
0.8239362351853257
0.7608099850331435
0.6657423857513638
0.7943257460202938
0.7511476003698512
0.9073696655228775
0.8741838353767599
0.7945799099309127
0.9323691001541556
0.865243808509542
0.8176606226311932
0.6785099625983169
0.9579534328203848
0.788928884938056
0.9833718962298513
0.5933012307657521
0.7829988799240639
0.7823073743206628
0.31190044950254875
0.7306280553786256
0.6985925052465822
0.5843516732063875
0.8124235844589114
0.820438203921003
0.6253838264581936
0.7600037006947045
0.8225872978677496
0.708398461303215
0.8646089269479391
0.8239362351853257
0.7608099850331435
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[7, 4, 5, 1, 6, 3, 2, 0]
tensor([7, 4, 5, 1, 6, 3, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 4, 5, 6, 1, 0, 3]
tensor([7, 2, 4, 5, 6, 1, 0, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 5, 4, 1, 2, 0]
tensor([6, 3, 7, 5, 4, 1, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 6, 4, 7, 1, 2, 0]
tensor([5, 3, 6, 4, 7, 1, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 3, 6, 5, 7, 2, 1, 0]
tensor([4, 3, 6, 5, 7, 2, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 2, 3, 2, 3, 0]
tensor([0, 1, 1, 2, 3, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 2, 2, 3, 0, 3, 1]
tensor([0, 1, 2, 2, 3, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1, 1.0, 1.0, 1.0, 1.0]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 5 to 15
done!
Normal merging for layer 16
tensor([0, 7])
tensor(0)
tensor([1, 2])
tensor(1)
tensor([3, 5])
tensor(3)
tensor([4, 6])
tensor(4)
done!
Normal merging for layer 17
tensor([0, 5])
tensor(0)
tensor([1, 7])
tensor(1)
tensor([2, 3])
tensor(2)
tensor([4, 6])
tensor(4)
done!
Cross-layer merge completed for layers 18 to 30
done!
Normal merging for layer 31
tensor([0, 1])
tensor(0)
tensor([2, 3, 4, 5, 6, 7])
tensor(2)
done!
all done!
Model size: 12.2608 GB
27
cuda:1
wikitext
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:46<00:46, 46.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 27.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.22s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
WARNING:lm_eval.api.task:[Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
WARNING:lm_eval.api.task:[Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
WARNING:lm_eval.api.task:[Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
WARNING:lm_eval.api.task:[Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
WARNING:lm_eval.api.task:[Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/wikitext_document_level HTTP/1.1" 200 1012
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/EleutherAI/wikitext_document_level/EleutherAI/wikitext_document_level.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/wikitext_document_level HTTP/1.1" 200 1012
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/wikitext_document_level/resolve/647234772b9554e208af6c826f23b99e3cac88c8/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/wikitext_document_level/revision/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 1012
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/wikitext_document_level/tree/647234772b9554e208af6c826f23b99e3cac88c8?recursive=False&expand=False HTTP/1.1" 200 357
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 299
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/wikitext_document_level/tree/647234772b9554e208af6c826f23b99e3cac88c8/wikitext-103-raw-v1?recursive=False&expand=False HTTP/1.1" 200 488
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 299
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/wikitext_document_level/revision/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 1012
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 299
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 299
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 299
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 299
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/wikitext_document_level/resolve/647234772b9554e208af6c826f23b99e3cac88c8/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 293
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/wikitext_document_level/tree/647234772b9554e208af6c826f23b99e3cac88c8/wikitext-2-raw-v1?recursive=False&expand=False HTTP/1.1" 200 487
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 293
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 293
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 293
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 293
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 293
DEBUG:filelock:Attempting to acquire lock 140547544781536 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___wikitext_document_level_wikitext-2-raw-v1_0.0.0_647234772b9554e208af6c826f23b99e3cac88c8.lock
DEBUG:filelock:Lock 140547544781536 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___wikitext_document_level_wikitext-2-raw-v1_0.0.0_647234772b9554e208af6c826f23b99e3cac88c8.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___wikitext_document_level/wikitext-2-raw-v1/0.0.0/647234772b9554e208af6c826f23b99e3cac88c8/dataset_info.json
DEBUG:filelock:Attempting to release lock 140547544781536 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___wikitext_document_level_wikitext-2-raw-v1_0.0.0_647234772b9554e208af6c826f23b99e3cac88c8.lock
DEBUG:filelock:Lock 140547544781536 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___wikitext_document_level_wikitext-2-raw-v1_0.0.0_647234772b9554e208af6c826f23b99e3cac88c8.lock
DEBUG:filelock:Attempting to acquire lock 140546767928064 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___wikitext_document_level/wikitext-2-raw-v1/0.0.0/647234772b9554e208af6c826f23b99e3cac88c8_builder.lock
DEBUG:filelock:Lock 140546767928064 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___wikitext_document_level/wikitext-2-raw-v1/0.0.0/647234772b9554e208af6c826f23b99e3cac88c8_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___wikitext_document_level/wikitext-2-raw-v1/0.0.0/647234772b9554e208af6c826f23b99e3cac88c8/dataset_info.json
DEBUG:filelock:Attempting to release lock 140546767928064 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___wikitext_document_level/wikitext-2-raw-v1/0.0.0/647234772b9554e208af6c826f23b99e3cac88c8_builder.lock
DEBUG:filelock:Lock 140546767928064 released on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___wikitext_document_level/wikitext-2-raw-v1/0.0.0/647234772b9554e208af6c826f23b99e3cac88c8_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wikitext from None to 0
INFO:lm_eval.api.task:Building contexts for wikitext on rank 0...
  0%|          | 0/62 [00:00<?, ?it/s]100%|██████████| 62/62 [00:00<00:00, 732.38it/s]
DEBUG:lm_eval.evaluator:Task: wikitext; number of requests on this rank: 62
INFO:lm_eval.evaluator:Running loglikelihood_rolling requests
  0%|          | 0/62 [00:00<?, ?it/s] 18%|█▊        | 11/62 [00:00<00:00, 105.04it/s] 40%|████      | 25/62 [00:00<00:00, 122.15it/s] 61%|██████▏   | 38/62 [00:00<00:00, 108.45it/s] 82%|████████▏ | 51/62 [00:00<00:00, 115.96it/s]100%|██████████| 62/62 [00:00<00:00, 122.76it/s]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:07<00:00,  7.43s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:07<00:00,  7.43s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.45s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.45s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.23s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.23s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:13<00:00, 13.46s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:13<00:00, 13.46s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.22s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.22s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.13s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.13s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.14s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.14s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:11<00:00, 11.34s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:11<00:00, 11.34s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:13<00:00, 13.61s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:13<00:00, 13.61s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.15s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.15s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.16s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.16s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.14s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.14s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.14s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.14s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:13<00:00, 13.19s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:13<00:00, 13.19s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.15s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.16s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.13s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.13s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.14s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.14s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.15s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.15s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:09<00:00,  9.01s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:09<00:00,  9.01s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:15<00:00, 15.85s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:15<00:00, 15.85s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.17s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.17s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.16s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.16s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.17s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.17s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.23s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.23s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:12<00:00, 12.30s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:12<00:00, 12.30s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:03<00:00,  3.05s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:03<00:00,  3.05s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.17s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.18s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.17s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.17s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.15s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.15s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.88s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.89s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.18s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.18s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.16s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.16s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.17s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.17s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.18s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.18s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.18s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.18s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.15s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.15s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.96s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.96s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.17s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.18s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.17s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.17s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.18s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.18s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.17s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.17s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.18s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.18s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.20s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.20s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:13<00:00, 13.60s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:13<00:00, 13.60s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.63s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.64s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:19<00:00, 19.10s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:19<00:00, 19.10s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.70s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.70s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.29s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.29s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.66s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.67s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.87s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.87s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.48s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.49s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.31s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.31s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.02it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.02it/s]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.56s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.56s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.27s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.27s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.30s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.30s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:08<00:00,  8.52s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:08<00:00,  8.52s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.28s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.28s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.38s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.38s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:10<00:00, 10.78s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:10<00:00, 10.78s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.45s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.45s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.40s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.40s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.39s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.39s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.36s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.36s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.38s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.38s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.38s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.38s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:08<00:00,  8.74s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:08<00:00,  8.74s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.38s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.38s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.37s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.37s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.37s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.37s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.38s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.38s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.39s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.39s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.19s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.19s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.26s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.26s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.24s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.24s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.22s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.22s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.20s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.20s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.22s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.22s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.21s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.21s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.24s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.24s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.23s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.23s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.22s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.22s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:13<00:00, 13.28s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:13<00:00, 13.28s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.99s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.99s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.22s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.22s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.22s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.22s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:12<00:00, 12.63s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:12<00:00, 12.63s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.23s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.23s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:07<00:00,  7.01s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:07<00:00,  7.01s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.53s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.53s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.50s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.50s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.52s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.52s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:07<00:00,  7.64s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:07<00:00,  7.64s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:08<00:00,  8.27s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:08<00:00,  8.27s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:15<00:00, 15.90s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:15<00:00, 15.90s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:15<00:00, 15.50s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:15<00:00, 15.50s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.56s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.56s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.39s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.39s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.52s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.52s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.51s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.51s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.51s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.51s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:03<00:00,  3.14s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:03<00:00,  3.14s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.51s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.51s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.50s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.50s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.41s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.41s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.63s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.63s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.59s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.59s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.48s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.48s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.31s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.31s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.29s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:17<00:00, 17.29s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:2'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:2'}
full model:
{'wikitext': {'alias': 'wikitext', 'word_perplexity,none': 10.41810626477151, 'word_perplexity_stderr,none': 'N/A', 'byte_perplexity,none': 1.5499996141598633, 'byte_perplexity_stderr,none': 'N/A', 'bits_per_byte,none': 0.6322678563706606, 'bits_per_byte_stderr,none': 'N/A'}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.45530714642366393
0.6593661321025917
0.45166835110772663
0.3470526807104175
0.3369212652555373
0.5686768652815603
0.5380667106744194
0.1994061645434335
0.7857081855506413
0.7236392518096334
0.6975091963753621
0.7192457446547811
0.6820752222558486
0.2553530000000423
0.9092011229700152
0.8474392887638426
0.29700403366343364
0.3112335629956447
0.316268174131358
0.5041948206367306
0.3067038688170426
0.21198641237235732
0.23379101174921613
0.4411951857077822
0.3636022184812837
0.22411151956337602
0.0659668585894146
0.5103891010860186
0.5291148039479469
0.45530714642366393
0.6593661321025917
0.45166835110772663
0.3470526807104175
0.3369212652555373
0.5686768652815603
0.5380667106744194
0.1994061645434335
0.7857081855506413
0.7236392518096334
0.6975091963753621
0.7192457446547811
0.6820752222558486
0.2553530000000423
0.9092011229700152
0.8474392887638426
0.29700403366343364
0.3112335629956447
0.316268174131358
0.5041948206367306
0.3067038688170426
0.21198641237235732
0.23379101174921613
0.4411951857077822
0.3636022184812837
0.22411151956337602
0.0659668585894146
0.5103891010860186
0.5291148039479469
0.45530714642366393
0.6593661321025917
0.45166835110772663
0.3470526807104175
0.3369212652555373
0.5686768652815603
0.5380667106744194
0.1994061645434335
0.7857081855506413
0.7236392518096334
0.6975091963753621
0.7192457446547811
0.6820752222558486
0.2553530000000423
0.9092011229700152
0.8474392887638426
0.29700403366343364
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 3, 7, 0, 5, 1, 4, 2]
tensor([6, 3, 7, 0, 5, 1, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 5, 0, 4, 3, 6, 7, 1]
tensor([2, 5, 0, 4, 3, 6, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 7, 5, 2, 0, 3, 1, 4]
tensor([6, 7, 5, 2, 0, 3, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 1, 0, 7, 2, 3, 5, 6]
tensor([4, 1, 0, 7, 2, 3, 5, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 3, 2, 4, 0, 1, 1, 0]
tensor([5, 3, 2, 4, 0, 1, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[3, 2, 0, 1, 5, 0, 4, 1]
tensor([3, 2, 0, 1, 5, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 0, 1, 5, 2, 0, 4, 3]
tensor([1, 0, 1, 5, 2, 0, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 4
done!
Normal merging for layer 5
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
done!
Normal merging for layer 6
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
done!
Cross-layer merge completed for layers 7 to 8
done!
Normal merging for layer 9
tensor([4, 7])
tensor(4)
tensor([5, 6])
tensor(5)
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
done!
Normal merging for layer 10
tensor([2, 5])
tensor(2)
tensor([3, 7])
tensor(3)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 11 to 12
done!
Normal merging for layer 13
tensor([1, 5])
tensor(1)
tensor([0, 2])
tensor(0)
tensor([4])
tensor(4)
tensor([7])
tensor(7)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
done!
Cross-layer merge completed for layers 14 to 31
done!
all done!
Model size: 12.3867 GB
253
cuda:2
boolq
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:44<00:44, 44.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:57<00:00, 26.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:57<00:00, 28.95s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/revision/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/revision/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646?recursive=False&expand=False HTTP/1.1" 307 138
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646?recursive=False&expand=False HTTP/1.1" 200 501
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/axb?recursive=False&expand=False HTTP/1.1" 307 142
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/axb?recursive=False&expand=False HTTP/1.1" 200 232
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/revision/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/revision/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/boolq?recursive=False&expand=False HTTP/1.1" 307 144
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/boolq?recursive=False&expand=False HTTP/1.1" 200 355
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:filelock:Attempting to acquire lock 140546767925136 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140546767925136 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140546767925136 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140546767925136 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140546767925088 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140546767925088 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140546767925088 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140546767925088 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of boolq from None to 0
INFO:lm_eval.api.task:Building contexts for boolq on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2606.57it/s]
DEBUG:lm_eval.evaluator:Task: boolq; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:04<14:43,  4.44s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:06<05:52,  1.79s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:07<04:15,  1.31s/it]Running loglikelihood requests:   4%|▎         | 7/200 [00:09<03:33,  1.11s/it]Running loglikelihood requests:   4%|▍         | 9/200 [00:11<03:08,  1.01it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:12<02:52,  1.10it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:14<02:40,  1.17it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:15<02:31,  1.22it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:17<02:23,  1.27it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:18<02:17,  1.32it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:19<02:12,  1.35it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:21<02:08,  1.38it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:22<02:04,  1.41it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:23<02:01,  1.43it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:25<01:58,  1.44it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:26<01:55,  1.46it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:27<01:53,  1.47it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:29<01:51,  1.48it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:30<01:48,  1.50it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:31<01:46,  1.51it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:33<01:44,  1.52it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:34<01:43,  1.52it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:35<01:40,  1.54it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:37<01:38,  1.56it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:38<01:35,  1.58it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:39<01:33,  1.59it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:40<01:31,  1.60it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:41<01:29,  1.61it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:43<01:27,  1.63it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:44<01:25,  1.65it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:45<01:23,  1.66it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:46<01:21,  1.68it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:47<01:19,  1.69it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:48<01:18,  1.70it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:50<01:16,  1.71it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:51<01:14,  1.74it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:52<01:12,  1.76it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:53<01:10,  1.78it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:54<01:08,  1.81it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:55<01:06,  1.82it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:56<01:04,  1.84it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:57<01:03,  1.85it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:58<01:01,  1.86it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:59<01:00,  1.86it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [01:00<00:59,  1.87it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [01:01<00:58,  1.88it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [01:03<00:56,  1.88it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [01:04<00:55,  1.89it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [01:05<00:54,  1.90it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [01:06<00:52,  1.91it/s]Running loglikelihood requests:  50%|█████     | 101/200 [01:07<00:51,  1.93it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [01:08<00:50,  1.94it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [01:09<00:48,  1.95it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [01:10<00:47,  1.96it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [01:11<00:46,  1.97it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [01:12<00:45,  1.98it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [01:13<00:43,  1.98it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [01:14<00:42,  1.99it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [01:15<00:41,  2.00it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [01:16<00:40,  2.02it/s]Running loglikelihood requests:  60%|██████    | 121/200 [01:17<00:38,  2.04it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [01:18<00:37,  2.07it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [01:18<00:35,  2.09it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [01:19<00:34,  2.11it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [01:20<00:33,  2.14it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [01:21<00:31,  2.18it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [01:22<00:30,  2.20it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [01:23<00:29,  2.22it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [01:24<00:28,  2.24it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:25<00:27,  2.25it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:26<00:26,  2.27it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:26<00:24,  2.30it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:27<00:23,  2.32it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:28<00:22,  2.35it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:29<00:21,  2.37it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:30<00:20,  2.38it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:31<00:19,  2.39it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:31<00:18,  2.41it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:32<00:17,  2.44it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:33<00:16,  2.46it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:34<00:15,  2.48it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:35<00:14,  2.50it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:35<00:13,  2.53it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:36<00:12,  2.55it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:37<00:12,  2.57it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:38<00:11,  2.61it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:38<00:10,  2.65it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:39<00:09,  2.68it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:40<00:08,  2.71it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:41<00:07,  2.65it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:41<00:07,  2.63it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:42<00:06,  2.62it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:43<00:05,  2.64it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:44<00:04,  2.66it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:44<00:04,  2.75it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:45<00:03,  2.82it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:46<00:02,  2.87it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:46<00:01,  2.95it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:47<00:00,  3.08it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:47<00:00,  3.25it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:47<00:00,  1.85it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:3'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:3'}
full model:
{'boolq': {'alias': 'boolq', 'acc,none': 0.67, 'acc_stderr,none': 0.04725815626252609}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9782993007407264
0.4824830207482711
0.6583529368570694
0.8822475410601947
0.3059812833421515
0.7645942683746021
0.5411564675926485
0.6399758236302138
0.752913626333875
0.9170504110771489
0.8755679311709376
0.9130694495301263
0.6399088029710222
0.5907791688883935
0.8704540476128766
0.484807489124531
0.7579322019225017
0.8465026175931075
0.8104840653949666
0.671147278193032
0.7709951349967222
0.532915988335396
0.6066099270395096
0.5511989097245372
0.4671998655475952
0.6078287002452507
0.3992240879306912
0.5299030614769079
0.5709371890677749
0.9782993007407264
0.4824830207482711
0.6583529368570694
0.8822475410601947
0.3059812833421515
0.7645942683746021
0.5411564675926485
0.6399758236302138
0.752913626333875
0.9170504110771489
0.8755679311709376
0.9130694495301263
0.6399088029710222
0.5907791688883935
0.8704540476128766
0.484807489124531
0.7579322019225017
0.8465026175931075
0.8104840653949666
0.671147278193032
0.7709951349967222
0.532915988335396
0.6066099270395096
0.5511989097245372
0.4671998655475952
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 7, 0, 5, 3, 4, 1]
tensor([6, 2, 7, 0, 5, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 3, 0, 6, 1, 4, 5]
tensor([7, 2, 3, 0, 6, 1, 4, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 1, 7, 0, 6, 4, 2, 3]
tensor([5, 1, 7, 0, 6, 4, 2, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 3, 5, 1, 7, 2, 6, 0]
tensor([4, 3, 5, 1, 7, 2, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 6, 7, 3, 4, 5, 0, 2]
tensor([1, 6, 7, 3, 4, 5, 0, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 4, 1, 7, 2, 5, 0]
tensor([6, 3, 4, 1, 7, 2, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 6 to 31
done!
all done!
Model size: 12.0718 GB
115
cuda:3
qqp
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 24.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.51s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: qqp] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: qqp] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
WARNING:lm_eval.api.task:[Task: qqp] metric f1 is defined, but aggregation is not. using default aggregation=f1
WARNING:lm_eval.api.task:[Task: qqp] metric f1 is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 111
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/revision/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/qqp?recursive=False&expand=False HTTP/1.1" 307 140
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/qqp?recursive=False&expand=False HTTP/1.1" 200 355
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140545763216608 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140545763216608 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545763216608 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140545763216608 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_qqp_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140545763726768 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140545763726768 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545763726768 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140545763726768 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of qqp from None to 0
INFO:lm_eval.api.task:Building contexts for qqp on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2346.41it/s]
DEBUG:lm_eval.evaluator:Task: qqp; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<05:26,  1.64s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:33,  1.28it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<01:51,  1.75it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:34,  2.05it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:04<01:23,  2.29it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:05<01:17,  2.45it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:06<01:12,  2.56it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:06<01:09,  2.65it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:07<01:07,  2.71it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:08<01:05,  2.77it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:08<01:03,  2.83it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:09<01:01,  2.87it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:10<00:59,  2.92it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:10<00:58,  2.96it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:11<00:57,  2.99it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:12<00:56,  3.00it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:12<00:55,  3.01it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:13<00:54,  3.03it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:14<00:53,  3.05it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:14<00:52,  3.07it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:15<00:51,  3.07it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:16<00:50,  3.08it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:16<00:49,  3.10it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:17<00:50,  3.03it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:18<00:50,  2.97it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:18<00:50,  2.97it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:19<00:49,  2.97it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:20<00:48,  2.97it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:20<00:47,  2.98it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:21<00:47,  2.99it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:22<00:45,  3.03it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:22<00:44,  3.10it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:23<00:42,  3.16it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:24<00:41,  3.20it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:24<00:40,  3.23it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:25<00:39,  3.24it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:25<00:38,  3.26it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:26<00:38,  3.27it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:27<00:37,  3.29it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:27<00:36,  3.31it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:28<00:35,  3.32it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:28<00:35,  3.32it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:29<00:34,  3.33it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:30<00:33,  3.33it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:30<00:33,  3.35it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:31<00:32,  3.35it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:31<00:31,  3.37it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:32<00:31,  3.38it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:32<00:30,  3.39it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:33<00:29,  3.41it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:34<00:29,  3.41it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:34<00:28,  3.42it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:35<00:27,  3.42it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:35<00:26,  3.45it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:36<00:26,  3.46it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:37<00:25,  3.48it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:37<00:24,  3.49it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:38<00:24,  3.50it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:38<00:23,  3.51it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:39<00:23,  3.52it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:39<00:22,  3.53it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:40<00:21,  3.53it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:40<00:21,  3.53it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:41<00:20,  3.54it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:42<00:20,  3.55it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:42<00:19,  3.56it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:43<00:18,  3.58it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:43<00:18,  3.58it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:44<00:17,  3.59it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:44<00:16,  3.59it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:45<00:16,  3.60it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:45<00:15,  3.61it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:46<00:15,  3.62it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:47<00:14,  3.63it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:47<00:14,  3.64it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:48<00:13,  3.66it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:48<00:12,  3.67it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:49<00:12,  3.68it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:49<00:11,  3.69it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:50<00:11,  3.70it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:50<00:10,  3.71it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [00:51<00:09,  3.72it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [00:51<00:09,  3.72it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [00:52<00:08,  3.73it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [00:52<00:08,  3.74it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [00:53<00:07,  3.75it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [00:54<00:07,  3.75it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [00:54<00:06,  3.76it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [00:55<00:06,  3.77it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [00:55<00:05,  3.78it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [00:56<00:05,  3.78it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [00:56<00:04,  3.78it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [00:57<00:03,  3.78it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [00:57<00:03,  3.77it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [00:58<00:02,  3.79it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [00:58<00:02,  3.81it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [00:59<00:01,  3.83it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [00:59<00:01,  3.86it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:00<00:00,  3.88it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:00<00:00,  3.92it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:00<00:00,  3.29it/s]
bootstrapping for stddev (sequential): f1_score
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<01:58,  1.20s/it]  2%|▏         | 2/100 [00:02<01:57,  1.20s/it]  3%|▎         | 3/100 [00:03<01:56,  1.20s/it]  4%|▍         | 4/100 [00:04<01:54,  1.20s/it]  5%|▌         | 5/100 [00:05<01:53,  1.19s/it]  6%|▌         | 6/100 [00:07<01:52,  1.19s/it]  7%|▋         | 7/100 [00:08<01:51,  1.19s/it]  8%|▊         | 8/100 [00:09<01:49,  1.19s/it]  9%|▉         | 9/100 [00:10<01:48,  1.19s/it] 10%|█         | 10/100 [00:11<01:47,  1.19s/it] 11%|█         | 11/100 [00:13<01:46,  1.19s/it] 12%|█▏        | 12/100 [00:14<01:44,  1.19s/it] 13%|█▎        | 13/100 [00:15<01:43,  1.19s/it] 14%|█▍        | 14/100 [00:16<01:42,  1.19s/it] 15%|█▌        | 15/100 [00:17<01:41,  1.19s/it] 16%|█▌        | 16/100 [00:19<01:40,  1.19s/it] 17%|█▋        | 17/100 [00:20<01:39,  1.19s/it] 18%|█▊        | 18/100 [00:21<01:37,  1.19s/it] 19%|█▉        | 19/100 [00:22<01:36,  1.19s/it] 20%|██        | 20/100 [00:23<01:35,  1.19s/it] 21%|██        | 21/100 [00:25<01:34,  1.19s/it] 22%|██▏       | 22/100 [00:26<01:33,  1.19s/it] 23%|██▎       | 23/100 [00:27<01:31,  1.19s/it] 24%|██▍       | 24/100 [00:28<01:30,  1.19s/it] 25%|██▌       | 25/100 [00:29<01:29,  1.19s/it] 26%|██▌       | 26/100 [00:31<01:28,  1.19s/it] 27%|██▋       | 27/100 [00:32<01:27,  1.19s/it] 28%|██▊       | 28/100 [00:33<01:25,  1.19s/it] 29%|██▉       | 29/100 [00:34<01:24,  1.19s/it] 30%|███       | 30/100 [00:35<01:23,  1.19s/it] 31%|███       | 31/100 [00:36<01:22,  1.19s/it] 32%|███▏      | 32/100 [00:38<01:21,  1.19s/it] 33%|███▎      | 33/100 [00:39<01:19,  1.19s/it] 34%|███▍      | 34/100 [00:40<01:18,  1.19s/it] 35%|███▌      | 35/100 [00:41<01:17,  1.19s/it] 36%|███▌      | 36/100 [00:42<01:16,  1.19s/it] 37%|███▋      | 37/100 [00:44<01:15,  1.19s/it] 38%|███▊      | 38/100 [00:45<01:13,  1.19s/it] 39%|███▉      | 39/100 [00:46<01:12,  1.19s/it] 40%|████      | 40/100 [00:47<01:11,  1.19s/it] 41%|████      | 41/100 [00:48<01:10,  1.19s/it] 42%|████▏     | 42/100 [00:50<01:09,  1.19s/it] 43%|████▎     | 43/100 [00:51<01:08,  1.19s/it] 44%|████▍     | 44/100 [00:52<01:06,  1.19s/it] 45%|████▌     | 45/100 [00:53<01:05,  1.19s/it] 46%|████▌     | 46/100 [00:54<01:04,  1.19s/it] 47%|████▋     | 47/100 [00:56<01:03,  1.19s/it] 48%|████▊     | 48/100 [00:57<01:02,  1.20s/it] 49%|████▉     | 49/100 [00:58<01:00,  1.19s/it] 50%|█████     | 50/100 [00:59<00:59,  1.19s/it] 51%|█████     | 51/100 [01:00<00:58,  1.19s/it] 52%|█████▏    | 52/100 [01:02<00:57,  1.19s/it] 53%|█████▎    | 53/100 [01:03<00:56,  1.19s/it] 54%|█████▍    | 54/100 [01:04<00:54,  1.19s/it] 55%|█████▌    | 55/100 [01:05<00:53,  1.19s/it] 56%|█████▌    | 56/100 [01:06<00:52,  1.19s/it] 57%|█████▋    | 57/100 [01:08<00:51,  1.19s/it] 58%|█████▊    | 58/100 [01:09<00:50,  1.20s/it] 59%|█████▉    | 59/100 [01:10<00:49,  1.20s/it] 60%|██████    | 60/100 [01:11<00:48,  1.21s/it] 61%|██████    | 61/100 [01:12<00:46,  1.20s/it] 62%|██████▏   | 62/100 [01:14<00:45,  1.20s/it] 63%|██████▎   | 63/100 [01:15<00:44,  1.20s/it] 64%|██████▍   | 64/100 [01:16<00:43,  1.20s/it] 65%|██████▌   | 65/100 [01:17<00:41,  1.20s/it] 66%|██████▌   | 66/100 [01:18<00:40,  1.20s/it] 67%|██████▋   | 67/100 [01:20<00:39,  1.20s/it] 68%|██████▊   | 68/100 [01:21<00:38,  1.20s/it] 69%|██████▉   | 69/100 [01:22<00:37,  1.20s/it] 70%|███████   | 70/100 [01:23<00:35,  1.20s/it] 71%|███████   | 71/100 [01:24<00:34,  1.20s/it] 72%|███████▏  | 72/100 [01:26<00:33,  1.20s/it] 73%|███████▎  | 73/100 [01:27<00:32,  1.21s/it] 74%|███████▍  | 74/100 [01:28<00:31,  1.21s/it] 75%|███████▌  | 75/100 [01:29<00:30,  1.21s/it] 76%|███████▌  | 76/100 [01:30<00:29,  1.21s/it] 77%|███████▋  | 77/100 [01:32<00:27,  1.21s/it] 78%|███████▊  | 78/100 [01:33<00:26,  1.21s/it] 79%|███████▉  | 79/100 [01:34<00:25,  1.21s/it] 80%|████████  | 80/100 [01:35<00:24,  1.20s/it] 81%|████████  | 81/100 [01:36<00:22,  1.21s/it] 82%|████████▏ | 82/100 [01:38<00:21,  1.21s/it] 83%|████████▎ | 83/100 [01:39<00:20,  1.20s/it] 84%|████████▍ | 84/100 [01:40<00:19,  1.20s/it] 85%|████████▌ | 85/100 [01:41<00:17,  1.20s/it] 86%|████████▌ | 86/100 [01:42<00:16,  1.20s/it] 87%|████████▋ | 87/100 [01:44<00:15,  1.20s/it] 88%|████████▊ | 88/100 [01:45<00:14,  1.19s/it] 89%|████████▉ | 89/100 [01:46<00:13,  1.19s/it] 90%|█████████ | 90/100 [01:47<00:11,  1.19s/it] 91%|█████████ | 91/100 [01:48<00:10,  1.19s/it] 92%|█████████▏| 92/100 [01:50<00:09,  1.19s/it] 93%|█████████▎| 93/100 [01:51<00:08,  1.19s/it] 94%|█████████▍| 94/100 [01:52<00:07,  1.19s/it] 95%|█████████▌| 95/100 [01:53<00:05,  1.19s/it] 96%|█████████▌| 96/100 [01:54<00:04,  1.19s/it] 97%|█████████▋| 97/100 [01:56<00:03,  1.19s/it] 98%|█████████▊| 98/100 [01:57<00:02,  1.19s/it] 99%|█████████▉| 99/100 [01:58<00:01,  1.19s/it]100%|██████████| 100/100 [01:59<00:00,  1.19s/it]100%|██████████| 100/100 [01:59<00:00,  1.20s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:4'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:4'}
full model:
{'qqp': {'alias': 'qqp', 'acc,none': 0.3, 'acc_stderr,none': 0.04605661864718382, 'f1,none': np.float64(0.46153846153846156), 'f1_stderr,none': 0.05409397997448054}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7956670939612598
0.9434665619842647
0.7711450328862919
0.8003111158987146
0.8660685706927677
0.6448594355779524
0.8450704433196731
0.7858702319299253
0.44571740342828275
0.8279983705061201
0.5649013775668057
0.5764430622159564
0.7600312367627909
0.4667905570890965
0.6441885742762831
0.5956490746171949
0.7561037067659027
0.6832036593513438
0.703034342774608
0.741950763385666
0.9094352412525417
0.3828457019607829
0.592762753179094
0.5983336442242687
0.5414593978662989
0.3299447012562784
0.4799857918750434
0.8524920468933548
0.713165518996795
0.7956670939612598
0.9434665619842647
0.7711450328862919
0.8003111158987146
0.8660685706927677
0.6448594355779524
0.8450704433196731
0.7858702319299253
0.44571740342828275
0.8279983705061201
0.5649013775668057
0.5764430622159564
0.7600312367627909
0.4667905570890965
0.6441885742762831
0.5956490746171949
0.7561037067659027
0.6832036593513438
0.703034342774608
0.741950763385666
0.9094352412525417
0.3828457019607829
0.592762753179094
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 4, 6, 2, 7, 1, 3, 0]
tensor([5, 4, 6, 2, 7, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 4, 5, 2, 6, 1, 3, 0]
tensor([7, 4, 5, 2, 6, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 2, 7, 1, 3, 0]
tensor([5, 4, 6, 2, 7, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 4, 5, 2, 7, 3, 0, 1]
tensor([6, 4, 5, 2, 7, 3, 0, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 7, 4, 2, 5, 0, 1, 3]
tensor([6, 7, 4, 2, 5, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 6, 4, 3, 7, 1, 0, 2]
tensor([5, 6, 4, 3, 7, 1, 0, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 6
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
done!
Normal merging for layer 7
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 8 to 31
done!
all done!
Model size: 12.1348 GB
3
cuda:4
mnli
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 25.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.64s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: mnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: mnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mnli?recursive=False&expand=False HTTP/1.1" 307 141
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mnli?recursive=False&expand=False HTTP/1.1" 200 512
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140547563851360 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140547563851360 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140547563851360 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140547563851360 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140545359717936 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140545359717936 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545359717936 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140545359717936 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mnli from None to 0
INFO:lm_eval.api.task:Building contexts for mnli on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 122532.98it/s]
DEBUG:lm_eval.evaluator:Task: mnli; number of requests on this rank: 300
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/300 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/300 [00:02<09:59,  2.00s/it]Running loglikelihood requests:   1%|          | 2/300 [00:03<07:45,  1.56s/it]Running loglikelihood requests:   1%|▏         | 4/300 [00:04<04:23,  1.12it/s]Running loglikelihood requests:   2%|▏         | 5/300 [00:05<04:29,  1.09it/s]Running loglikelihood requests:   2%|▏         | 7/300 [00:06<03:25,  1.43it/s]Running loglikelihood requests:   3%|▎         | 8/300 [00:07<03:39,  1.33it/s]Running loglikelihood requests:   3%|▎         | 10/300 [00:07<02:58,  1.62it/s]Running loglikelihood requests:   4%|▎         | 11/300 [00:08<03:13,  1.50it/s]Running loglikelihood requests:   4%|▍         | 13/300 [00:09<02:42,  1.77it/s]Running loglikelihood requests:   5%|▍         | 14/300 [00:10<02:57,  1.61it/s]Running loglikelihood requests:   5%|▌         | 16/300 [00:11<02:30,  1.89it/s]Running loglikelihood requests:   6%|▌         | 17/300 [00:11<02:44,  1.72it/s]Running loglikelihood requests:   6%|▌         | 18/300 [00:12<02:55,  1.60it/s]Running loglikelihood requests:   7%|▋         | 20/300 [00:13<02:27,  1.90it/s]Running loglikelihood requests:   7%|▋         | 21/300 [00:14<02:40,  1.74it/s]Running loglikelihood requests:   8%|▊         | 23/300 [00:14<02:16,  2.03it/s]Running loglikelihood requests:   8%|▊         | 25/300 [00:15<02:02,  2.24it/s]Running loglikelihood requests:   9%|▊         | 26/300 [00:16<02:17,  1.99it/s]Running loglikelihood requests:   9%|▉         | 28/300 [00:17<02:02,  2.22it/s]Running loglikelihood requests:  10%|▉         | 29/300 [00:17<02:16,  1.98it/s]Running loglikelihood requests:  10%|█         | 30/300 [00:18<02:29,  1.81it/s]Running loglikelihood requests:  10%|█         | 31/300 [00:19<02:39,  1.69it/s]Running loglikelihood requests:  11%|█         | 32/300 [00:19<02:47,  1.60it/s]Running loglikelihood requests:  11%|█▏        | 34/300 [00:20<02:15,  1.97it/s]Running loglikelihood requests:  12%|█▏        | 36/300 [00:21<01:59,  2.22it/s]Running loglikelihood requests:  13%|█▎        | 38/300 [00:22<01:49,  2.40it/s]Running loglikelihood requests:  13%|█▎        | 40/300 [00:22<01:42,  2.55it/s]Running loglikelihood requests:  14%|█▎        | 41/300 [00:23<01:56,  2.22it/s]Running loglikelihood requests:  14%|█▍        | 42/300 [00:24<02:09,  1.99it/s]Running loglikelihood requests:  14%|█▍        | 43/300 [00:24<02:20,  1.83it/s]Running loglikelihood requests:  15%|█▌        | 45/300 [00:25<01:58,  2.15it/s]Running loglikelihood requests:  16%|█▌        | 47/300 [00:26<01:46,  2.38it/s]Running loglikelihood requests:  16%|█▌        | 48/300 [00:26<01:59,  2.10it/s]Running loglikelihood requests:  17%|█▋        | 50/300 [00:27<01:46,  2.35it/s]Running loglikelihood requests:  17%|█▋        | 52/300 [00:28<01:38,  2.53it/s]Running loglikelihood requests:  18%|█▊        | 53/300 [00:28<01:51,  2.22it/s]Running loglikelihood requests:  18%|█▊        | 54/300 [00:29<02:02,  2.00it/s]Running loglikelihood requests:  18%|█▊        | 55/300 [00:30<02:12,  1.85it/s]Running loglikelihood requests:  19%|█▉        | 57/300 [00:30<01:51,  2.19it/s]Running loglikelihood requests:  20%|█▉        | 59/300 [00:31<01:39,  2.42it/s]Running loglikelihood requests:  20%|██        | 61/300 [00:32<01:32,  2.60it/s]Running loglikelihood requests:  21%|██        | 62/300 [00:32<01:44,  2.27it/s]Running loglikelihood requests:  21%|██▏       | 64/300 [00:33<01:34,  2.49it/s]Running loglikelihood requests:  22%|██▏       | 65/300 [00:34<01:46,  2.20it/s]Running loglikelihood requests:  22%|██▏       | 66/300 [00:34<01:57,  2.00it/s]Running loglikelihood requests:  22%|██▏       | 67/300 [00:35<02:05,  1.86it/s]Running loglikelihood requests:  23%|██▎       | 69/300 [00:36<01:44,  2.21it/s]Running loglikelihood requests:  24%|██▎       | 71/300 [00:36<01:33,  2.46it/s]Running loglikelihood requests:  24%|██▍       | 72/300 [00:37<01:44,  2.18it/s]Running loglikelihood requests:  24%|██▍       | 73/300 [00:38<01:54,  1.99it/s]Running loglikelihood requests:  25%|██▌       | 75/300 [00:38<01:37,  2.31it/s]Running loglikelihood requests:  26%|██▌       | 77/300 [00:39<01:27,  2.54it/s]Running loglikelihood requests:  26%|██▋       | 79/300 [00:40<01:21,  2.71it/s]Running loglikelihood requests:  27%|██▋       | 80/300 [00:40<01:33,  2.36it/s]Running loglikelihood requests:  27%|██▋       | 81/300 [00:41<01:43,  2.12it/s]Running loglikelihood requests:  27%|██▋       | 82/300 [00:42<01:51,  1.95it/s]Running loglikelihood requests:  28%|██▊       | 83/300 [00:42<01:58,  1.83it/s]Running loglikelihood requests:  28%|██▊       | 85/300 [00:43<01:36,  2.22it/s]Running loglikelihood requests:  29%|██▊       | 86/300 [00:43<01:45,  2.02it/s]Running loglikelihood requests:  29%|██▉       | 88/300 [00:44<01:30,  2.35it/s]Running loglikelihood requests:  30%|██▉       | 89/300 [00:45<01:39,  2.11it/s]Running loglikelihood requests:  30%|███       | 91/300 [00:45<01:26,  2.42it/s]Running loglikelihood requests:  31%|███       | 93/300 [00:46<01:18,  2.64it/s]Running loglikelihood requests:  32%|███▏      | 95/300 [00:47<01:13,  2.79it/s]Running loglikelihood requests:  32%|███▏      | 96/300 [00:47<01:24,  2.43it/s]Running loglikelihood requests:  33%|███▎      | 98/300 [00:48<01:16,  2.65it/s]Running loglikelihood requests:  33%|███▎      | 99/300 [00:49<01:26,  2.33it/s]Running loglikelihood requests:  34%|███▎      | 101/300 [00:49<01:17,  2.58it/s]Running loglikelihood requests:  34%|███▍      | 102/300 [00:50<01:26,  2.28it/s]Running loglikelihood requests:  34%|███▍      | 103/300 [00:50<01:34,  2.07it/s]Running loglikelihood requests:  35%|███▍      | 104/300 [00:51<01:41,  1.93it/s]Running loglikelihood requests:  35%|███▌      | 105/300 [00:52<01:46,  1.83it/s]Running loglikelihood requests:  36%|███▌      | 107/300 [00:52<01:26,  2.23it/s]Running loglikelihood requests:  36%|███▌      | 108/300 [00:53<01:33,  2.04it/s]Running loglikelihood requests:  36%|███▋      | 109/300 [00:54<01:42,  1.87it/s]Running loglikelihood requests:  37%|███▋      | 111/300 [00:54<01:23,  2.26it/s]Running loglikelihood requests:  38%|███▊      | 113/300 [00:55<01:13,  2.54it/s]Running loglikelihood requests:  38%|███▊      | 115/300 [00:56<01:07,  2.74it/s]Running loglikelihood requests:  39%|███▉      | 117/300 [00:56<01:03,  2.89it/s]Running loglikelihood requests:  40%|███▉      | 119/300 [00:57<01:00,  2.99it/s]Running loglikelihood requests:  40%|████      | 121/300 [00:57<00:58,  3.07it/s]Running loglikelihood requests:  41%|████      | 122/300 [00:58<01:07,  2.64it/s]Running loglikelihood requests:  41%|████▏     | 124/300 [00:59<01:02,  2.84it/s]Running loglikelihood requests:  42%|████▏     | 125/300 [00:59<01:10,  2.48it/s]Running loglikelihood requests:  42%|████▏     | 126/300 [01:00<01:17,  2.23it/s]Running loglikelihood requests:  42%|████▏     | 127/300 [01:00<01:23,  2.06it/s]Running loglikelihood requests:  43%|████▎     | 129/300 [01:01<01:10,  2.43it/s]Running loglikelihood requests:  44%|████▎     | 131/300 [01:02<01:02,  2.70it/s]Running loglikelihood requests:  44%|████▍     | 133/300 [01:02<00:57,  2.90it/s]Running loglikelihood requests:  45%|████▍     | 134/300 [01:03<01:05,  2.55it/s]Running loglikelihood requests:  45%|████▌     | 135/300 [01:03<01:11,  2.30it/s]Running loglikelihood requests:  45%|████▌     | 136/300 [01:04<01:17,  2.13it/s]Running loglikelihood requests:  46%|████▌     | 137/300 [01:05<01:21,  2.00it/s]Running loglikelihood requests:  46%|████▌     | 138/300 [01:05<01:24,  1.92it/s]Running loglikelihood requests:  47%|████▋     | 140/300 [01:06<01:07,  2.37it/s]Running loglikelihood requests:  47%|████▋     | 142/300 [01:06<00:58,  2.69it/s]Running loglikelihood requests:  48%|████▊     | 143/300 [01:07<01:05,  2.40it/s]Running loglikelihood requests:  48%|████▊     | 145/300 [01:07<00:56,  2.72it/s]Running loglikelihood requests:  49%|████▊     | 146/300 [01:08<01:03,  2.42it/s]Running loglikelihood requests:  49%|████▉     | 147/300 [01:09<01:09,  2.21it/s]Running loglikelihood requests:  49%|████▉     | 148/300 [01:09<01:13,  2.07it/s]Running loglikelihood requests:  50%|█████     | 150/300 [01:10<01:00,  2.48it/s]Running loglikelihood requests:  51%|█████     | 152/300 [01:10<00:53,  2.77it/s]Running loglikelihood requests:  51%|█████▏    | 154/300 [01:11<00:49,  2.98it/s]Running loglikelihood requests:  52%|█████▏    | 155/300 [01:11<00:55,  2.60it/s]Running loglikelihood requests:  52%|█████▏    | 157/300 [01:12<00:49,  2.86it/s]Running loglikelihood requests:  53%|█████▎    | 159/300 [01:13<00:46,  3.04it/s]Running loglikelihood requests:  54%|█████▎    | 161/300 [01:13<00:43,  3.18it/s]Running loglikelihood requests:  54%|█████▍    | 162/300 [01:14<00:50,  2.75it/s]Running loglikelihood requests:  55%|█████▍    | 164/300 [01:14<00:45,  2.97it/s]Running loglikelihood requests:  55%|█████▌    | 165/300 [01:15<00:51,  2.62it/s]Running loglikelihood requests:  56%|█████▌    | 167/300 [01:15<00:46,  2.89it/s]Running loglikelihood requests:  56%|█████▋    | 169/300 [01:16<00:42,  3.08it/s]Running loglikelihood requests:  57%|█████▋    | 170/300 [01:17<00:48,  2.69it/s]Running loglikelihood requests:  57%|█████▋    | 171/300 [01:17<00:53,  2.42it/s]Running loglikelihood requests:  57%|█████▋    | 172/300 [01:18<00:57,  2.23it/s]Running loglikelihood requests:  58%|█████▊    | 173/300 [01:18<01:00,  2.10it/s]Running loglikelihood requests:  58%|█████▊    | 175/300 [01:19<00:49,  2.53it/s]Running loglikelihood requests:  59%|█████▉    | 177/300 [01:19<00:43,  2.85it/s]Running loglikelihood requests:  60%|█████▉    | 179/300 [01:20<00:39,  3.07it/s]Running loglikelihood requests:  60%|██████    | 180/300 [01:21<00:44,  2.68it/s]Running loglikelihood requests:  60%|██████    | 181/300 [01:21<00:49,  2.42it/s]Running loglikelihood requests:  61%|██████    | 183/300 [01:22<00:42,  2.77it/s]Running loglikelihood requests:  62%|██████▏   | 185/300 [01:22<00:37,  3.03it/s]Running loglikelihood requests:  62%|██████▏   | 186/300 [01:23<00:42,  2.66it/s]Running loglikelihood requests:  63%|██████▎   | 188/300 [01:23<00:37,  2.96it/s]Running loglikelihood requests:  63%|██████▎   | 190/300 [01:24<00:34,  3.17it/s]Running loglikelihood requests:  64%|██████▎   | 191/300 [01:24<00:39,  2.76it/s]Running loglikelihood requests:  64%|██████▍   | 192/300 [01:25<00:43,  2.49it/s]Running loglikelihood requests:  65%|██████▍   | 194/300 [01:26<00:37,  2.84it/s]Running loglikelihood requests:  65%|██████▌   | 195/300 [01:26<00:41,  2.54it/s]Running loglikelihood requests:  65%|██████▌   | 196/300 [01:27<00:44,  2.32it/s]Running loglikelihood requests:  66%|██████▌   | 198/300 [01:27<00:37,  2.73it/s]Running loglikelihood requests:  66%|██████▋   | 199/300 [01:28<00:40,  2.47it/s]Running loglikelihood requests:  67%|██████▋   | 200/300 [01:28<00:43,  2.29it/s]Running loglikelihood requests:  67%|██████▋   | 201/300 [01:29<00:45,  2.16it/s]Running loglikelihood requests:  68%|██████▊   | 203/300 [01:29<00:37,  2.62it/s]Running loglikelihood requests:  68%|██████▊   | 205/300 [01:30<00:32,  2.95it/s]Running loglikelihood requests:  69%|██████▊   | 206/300 [01:30<00:35,  2.63it/s]Running loglikelihood requests:  69%|██████▉   | 208/300 [01:31<00:31,  2.96it/s]Running loglikelihood requests:  70%|███████   | 210/300 [01:31<00:28,  3.19it/s]Running loglikelihood requests:  71%|███████   | 212/300 [01:32<00:26,  3.35it/s]Running loglikelihood requests:  71%|███████   | 213/300 [01:33<00:29,  2.92it/s]Running loglikelihood requests:  72%|███████▏  | 215/300 [01:33<00:26,  3.17it/s]Running loglikelihood requests:  72%|███████▏  | 216/300 [01:34<00:30,  2.79it/s]Running loglikelihood requests:  72%|███████▏  | 217/300 [01:34<00:32,  2.52it/s]Running loglikelihood requests:  73%|███████▎  | 219/300 [01:35<00:27,  2.90it/s]Running loglikelihood requests:  74%|███████▎  | 221/300 [01:35<00:24,  3.17it/s]Running loglikelihood requests:  74%|███████▍  | 222/300 [01:36<00:27,  2.79it/s]Running loglikelihood requests:  75%|███████▍  | 224/300 [01:36<00:24,  3.09it/s]Running loglikelihood requests:  75%|███████▌  | 225/300 [01:37<00:27,  2.74it/s]Running loglikelihood requests:  76%|███████▌  | 227/300 [01:37<00:23,  3.07it/s]Running loglikelihood requests:  76%|███████▌  | 228/300 [01:38<00:26,  2.73it/s]Running loglikelihood requests:  76%|███████▋  | 229/300 [01:38<00:28,  2.49it/s]Running loglikelihood requests:  77%|███████▋  | 231/300 [01:39<00:23,  2.90it/s]Running loglikelihood requests:  77%|███████▋  | 232/300 [01:39<00:26,  2.61it/s]Running loglikelihood requests:  78%|███████▊  | 233/300 [01:40<00:27,  2.41it/s]Running loglikelihood requests:  78%|███████▊  | 234/300 [01:40<00:29,  2.27it/s]Running loglikelihood requests:  78%|███████▊  | 235/300 [01:41<00:29,  2.17it/s]Running loglikelihood requests:  79%|███████▊  | 236/300 [01:41<00:30,  2.10it/s]Running loglikelihood requests:  79%|███████▉  | 237/300 [01:42<00:30,  2.05it/s]Running loglikelihood requests:  79%|███████▉  | 238/300 [01:42<00:30,  2.02it/s]Running loglikelihood requests:  80%|████████  | 240/300 [01:43<00:23,  2.58it/s]Running loglikelihood requests:  81%|████████  | 242/300 [01:43<00:19,  2.97it/s]Running loglikelihood requests:  81%|████████▏ | 244/300 [01:44<00:17,  3.25it/s]Running loglikelihood requests:  82%|████████▏ | 246/300 [01:44<00:15,  3.44it/s]Running loglikelihood requests:  83%|████████▎ | 248/300 [01:45<00:14,  3.57it/s]Running loglikelihood requests:  83%|████████▎ | 250/300 [01:45<00:13,  3.67it/s]Running loglikelihood requests:  84%|████████▍ | 252/300 [01:46<00:12,  3.74it/s]Running loglikelihood requests:  85%|████████▍ | 254/300 [01:47<00:12,  3.79it/s]Running loglikelihood requests:  85%|████████▌ | 256/300 [01:47<00:11,  3.83it/s]Running loglikelihood requests:  86%|████████▌ | 257/300 [01:48<00:13,  3.29it/s]Running loglikelihood requests:  86%|████████▌ | 258/300 [01:48<00:14,  2.90it/s]Running loglikelihood requests:  87%|████████▋ | 260/300 [01:49<00:12,  3.23it/s]Running loglikelihood requests:  87%|████████▋ | 261/300 [01:49<00:13,  2.87it/s]Running loglikelihood requests:  88%|████████▊ | 263/300 [01:50<00:11,  3.21it/s]Running loglikelihood requests:  88%|████████▊ | 264/300 [01:50<00:12,  2.85it/s]Running loglikelihood requests:  88%|████████▊ | 265/300 [01:51<00:13,  2.60it/s]Running loglikelihood requests:  89%|████████▊ | 266/300 [01:51<00:14,  2.43it/s]Running loglikelihood requests:  89%|████████▉ | 268/300 [01:52<00:11,  2.91it/s]Running loglikelihood requests:  90%|█████████ | 270/300 [01:52<00:09,  3.25it/s]Running loglikelihood requests:  91%|█████████ | 272/300 [01:53<00:08,  3.49it/s]Running loglikelihood requests:  91%|█████████▏| 274/300 [01:53<00:07,  3.68it/s]Running loglikelihood requests:  92%|█████████▏| 275/300 [01:53<00:07,  3.21it/s]Running loglikelihood requests:  92%|█████████▏| 277/300 [01:54<00:06,  3.50it/s]Running loglikelihood requests:  93%|█████████▎| 278/300 [01:54<00:07,  3.08it/s]Running loglikelihood requests:  93%|█████████▎| 279/300 [01:55<00:07,  2.79it/s]Running loglikelihood requests:  94%|█████████▎| 281/300 [01:55<00:05,  3.21it/s]Running loglikelihood requests:  94%|█████████▍| 282/300 [01:56<00:06,  2.88it/s]Running loglikelihood requests:  94%|█████████▍| 283/300 [01:56<00:06,  2.65it/s]Running loglikelihood requests:  95%|█████████▍| 284/300 [01:57<00:06,  2.48it/s]Running loglikelihood requests:  95%|█████████▌| 286/300 [01:57<00:04,  3.00it/s]Running loglikelihood requests:  96%|█████████▌| 288/300 [01:58<00:03,  3.37it/s]Running loglikelihood requests:  97%|█████████▋| 290/300 [01:58<00:02,  3.64it/s]Running loglikelihood requests:  97%|█████████▋| 292/300 [01:59<00:02,  3.85it/s]Running loglikelihood requests:  98%|█████████▊| 293/300 [01:59<00:02,  3.36it/s]Running loglikelihood requests:  98%|█████████▊| 294/300 [02:00<00:01,  3.01it/s]Running loglikelihood requests:  99%|█████████▊| 296/300 [02:00<00:01,  3.43it/s]Running loglikelihood requests:  99%|█████████▉| 298/300 [02:00<00:00,  3.75it/s]Running loglikelihood requests: 100%|█████████▉| 299/300 [02:01<00:00,  3.32it/s]Running loglikelihood requests: 100%|██████████| 300/300 [02:01<00:00,  2.47it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:5'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:5'}
full model:
{'mnli': {'alias': 'mnli', 'acc,none': 0.4, 'acc_stderr,none': 0.0492365963917331}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8171409999433525
0.8739893758684915
0.8333413990263023
0.8782828649633461
0.7602933913725962
0.7047335470608914
0.9813668198386444
0.7850949920891616
0.748983595161589
0.6557873189175887
0.7043210867322975
0.9420132312730142
0.9548348739467002
0.8459964595909506
0.6932219150662173
0.8804855383939617
0.868566987197908
0.8437399449827557
0.9153450822718411
0.8433082326287293
0.910397090611448
0.7626694638419581
0.752151649412453
0.8346986395359313
0.9530736745854924
0.8635918585764104
0.8647192950921815
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[4, 5, 6, 1, 7, 2, 3, 0]
tensor([4, 5, 6, 1, 7, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 4, 6, 2, 5, 1, 3, 0]
tensor([7, 4, 6, 2, 5, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 7, 3, 5, 1, 4, 0]
tensor([6, 2, 7, 3, 5, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 5, 4, 6, 1, 3, 0]
tensor([7, 2, 5, 4, 6, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 5, 3, 1, 2, 4, 0]
tensor([0, 1, 5, 3, 1, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 1, 2, 2, 3, 0]
tensor([0, 3, 1, 1, 2, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 2, 3, 1, 2, 0, 3, 1]
tensor([0, 2, 3, 1, 2, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 3, 1, 2, 2, 1, 3, 0]
tensor([0, 3, 1, 2, 2, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 4 to 8
done!
Normal merging for layer 9
tensor([0, 7])
tensor(0)
tensor([1, 4])
tensor(1)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 10 to 20
done!
Normal merging for layer 21
tensor([0, 7])
tensor(0)
tensor([2, 3])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([1, 6])
tensor(1)
done!
Normal merging for layer 22
tensor([0, 5])
tensor(0)
tensor([3, 7])
tensor(3)
tensor([1, 4])
tensor(1)
tensor([2, 6])
tensor(2)
done!
Normal merging for layer 23
tensor([0, 7])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3, 4])
tensor(3)
tensor([1, 6])
tensor(1)
done!
Cross-layer merge completed for layers 24 to 25
done!
Normal merging for layer 26
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 27 to 31
done!
all done!
Model size: 12.5127 GB
14
cuda:5
mrpc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 25.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.43s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: mrpc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: mrpc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
WARNING:lm_eval.api.task:[Task: mrpc] metric f1 is defined, but aggregation is not. using default aggregation=f1
WARNING:lm_eval.api.task:[Task: mrpc] metric f1 is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc?recursive=False&expand=False HTTP/1.1" 307 141
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/mrpc?recursive=False&expand=False HTTP/1.1" 200 356
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140545763950368 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140545763950368 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545763950368 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140545763950368 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140516066143856 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140516066143856 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516066143856 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140516066143856 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mrpc from None to 0
INFO:lm_eval.api.task:Building contexts for mrpc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2479.15it/s]
DEBUG:lm_eval.evaluator:Task: mrpc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<05:48,  1.75s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:34,  1.28it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<01:58,  1.64it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:43,  1.87it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:05<01:35,  2.01it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:06<01:29,  2.11it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:06<01:25,  2.18it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:07<01:22,  2.23it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:08<01:20,  2.27it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:09<01:18,  2.30it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:10<01:17,  2.32it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:11<01:15,  2.34it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:12<01:14,  2.36it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:12<01:13,  2.37it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:13<01:11,  2.38it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:14<01:10,  2.39it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:15<01:09,  2.41it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:16<01:08,  2.42it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:16<01:07,  2.43it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:17<01:06,  2.43it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:18<01:05,  2.45it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:19<01:03,  2.46it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:20<01:02,  2.46it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:21<01:01,  2.47it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:21<01:00,  2.48it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:22<00:59,  2.49it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:23<00:58,  2.50it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:24<00:57,  2.51it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:24<00:56,  2.52it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:25<00:55,  2.53it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:26<00:54,  2.54it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:27<00:53,  2.55it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:28<00:52,  2.55it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:28<00:51,  2.56it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:29<00:51,  2.56it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:30<00:49,  2.58it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:31<00:48,  2.59it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:31<00:48,  2.60it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:32<00:47,  2.61it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:33<00:46,  2.62it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:34<00:45,  2.63it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:34<00:44,  2.64it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:35<00:43,  2.64it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:36<00:42,  2.65it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:37<00:41,  2.66it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:37<00:40,  2.66it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:38<00:40,  2.67it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:39<00:39,  2.68it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:40<00:38,  2.69it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:40<00:37,  2.70it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:41<00:36,  2.71it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:42<00:35,  2.72it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:43<00:34,  2.72it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:43<00:34,  2.72it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:44<00:33,  2.73it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:45<00:32,  2.73it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:46<00:31,  2.74it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:46<00:31,  2.74it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:47<00:30,  2.74it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:48<00:29,  2.75it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:48<00:28,  2.75it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:49<00:27,  2.76it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:50<00:27,  2.77it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:51<00:26,  2.77it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:51<00:25,  2.78it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:52<00:24,  2.79it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:53<00:23,  2.80it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:53<00:23,  2.80it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:54<00:22,  2.81it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:55<00:21,  2.81it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:56<00:21,  2.78it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:56<00:20,  2.78it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:57<00:19,  2.79it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:58<00:19,  2.78it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:58<00:18,  2.80it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:59<00:17,  2.82it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:00<00:16,  2.83it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:01<00:15,  2.84it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:01<00:15,  2.86it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:02<00:14,  2.87it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:03<00:13,  2.88it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:03<00:12,  2.90it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:04<00:11,  2.92it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:05<00:11,  2.94it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:05<00:10,  2.95it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:06<00:09,  2.97it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:07<00:09,  2.99it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:07<00:08,  3.00it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:08<00:07,  3.02it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:09<00:06,  3.05it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:09<00:06,  3.07it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:10<00:05,  3.10it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:11<00:04,  3.12it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:11<00:04,  3.15it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:12<00:03,  3.17it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:12<00:02,  3.18it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:13<00:02,  3.20it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:14<00:01,  3.22it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:14<00:00,  3.24it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:15<00:00,  3.29it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:15<00:00,  2.66it/s]
bootstrapping for stddev (sequential): f1_score
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:03<06:13,  3.77s/it]  2%|▏         | 2/100 [00:04<03:41,  2.26s/it]  3%|▎         | 3/100 [00:06<02:52,  1.78s/it]  4%|▍         | 4/100 [00:07<02:28,  1.55s/it]  5%|▌         | 5/100 [00:08<02:15,  1.42s/it]  6%|▌         | 6/100 [00:09<02:06,  1.35s/it]  7%|▋         | 7/100 [00:10<02:01,  1.30s/it]  8%|▊         | 8/100 [00:12<01:56,  1.27s/it]  9%|▉         | 9/100 [00:13<01:53,  1.25s/it] 10%|█         | 10/100 [00:14<01:51,  1.24s/it] 11%|█         | 11/100 [00:15<01:49,  1.23s/it] 12%|█▏        | 12/100 [00:17<01:47,  1.22s/it] 13%|█▎        | 13/100 [00:18<01:45,  1.21s/it] 14%|█▍        | 14/100 [00:19<01:43,  1.21s/it] 15%|█▌        | 15/100 [00:20<01:42,  1.20s/it] 16%|█▌        | 16/100 [00:21<01:40,  1.20s/it] 17%|█▋        | 17/100 [00:22<01:39,  1.20s/it] 18%|█▊        | 18/100 [00:24<01:38,  1.20s/it] 19%|█▉        | 19/100 [00:25<01:37,  1.20s/it] 20%|██        | 20/100 [00:26<01:35,  1.20s/it] 21%|██        | 21/100 [00:27<01:34,  1.20s/it] 22%|██▏       | 22/100 [00:28<01:33,  1.20s/it] 23%|██▎       | 23/100 [00:30<01:32,  1.20s/it] 24%|██▍       | 24/100 [00:31<01:30,  1.19s/it] 25%|██▌       | 25/100 [00:32<01:29,  1.20s/it] 26%|██▌       | 26/100 [00:33<01:28,  1.19s/it] 27%|██▋       | 27/100 [00:34<01:27,  1.20s/it] 28%|██▊       | 28/100 [00:36<01:26,  1.19s/it] 29%|██▉       | 29/100 [00:37<01:24,  1.19s/it] 30%|███       | 30/100 [00:38<01:23,  1.19s/it] 31%|███       | 31/100 [00:39<01:23,  1.21s/it] 32%|███▏      | 32/100 [00:40<01:21,  1.20s/it] 33%|███▎      | 33/100 [00:42<01:20,  1.20s/it] 34%|███▍      | 34/100 [00:43<01:19,  1.20s/it] 35%|███▌      | 35/100 [00:44<01:18,  1.20s/it] 36%|███▌      | 36/100 [00:45<01:16,  1.20s/it] 37%|███▋      | 37/100 [00:46<01:15,  1.20s/it] 38%|███▊      | 38/100 [00:48<01:14,  1.20s/it] 39%|███▉      | 39/100 [00:49<01:13,  1.20s/it] 40%|████      | 40/100 [00:50<01:12,  1.20s/it] 41%|████      | 41/100 [00:51<01:10,  1.20s/it] 42%|████▏     | 42/100 [00:52<01:09,  1.20s/it] 43%|████▎     | 43/100 [00:54<01:08,  1.21s/it] 44%|████▍     | 44/100 [00:55<01:07,  1.21s/it] 45%|████▌     | 45/100 [00:56<01:06,  1.20s/it] 46%|████▌     | 46/100 [00:57<01:05,  1.21s/it] 47%|████▋     | 47/100 [00:59<01:04,  1.21s/it] 48%|████▊     | 48/100 [01:00<01:03,  1.21s/it] 49%|████▉     | 49/100 [01:01<01:01,  1.21s/it] 50%|█████     | 50/100 [01:02<01:00,  1.21s/it] 51%|█████     | 51/100 [01:03<00:59,  1.21s/it] 52%|█████▏    | 52/100 [01:05<00:58,  1.22s/it] 53%|█████▎    | 53/100 [01:06<00:56,  1.21s/it] 54%|█████▍    | 54/100 [01:07<00:55,  1.21s/it] 55%|█████▌    | 55/100 [01:08<00:54,  1.20s/it] 56%|█████▌    | 56/100 [01:09<00:52,  1.20s/it] 57%|█████▋    | 57/100 [01:11<00:51,  1.20s/it] 58%|█████▊    | 58/100 [01:12<00:50,  1.20s/it] 59%|█████▉    | 59/100 [01:13<00:49,  1.20s/it] 60%|██████    | 60/100 [01:14<00:48,  1.21s/it] 61%|██████    | 61/100 [01:15<00:47,  1.21s/it] 62%|██████▏   | 62/100 [01:17<00:45,  1.21s/it] 63%|██████▎   | 63/100 [01:18<00:44,  1.21s/it] 64%|██████▍   | 64/100 [01:19<00:43,  1.21s/it] 65%|██████▌   | 65/100 [01:20<00:42,  1.21s/it] 66%|██████▌   | 66/100 [01:21<00:40,  1.21s/it] 67%|██████▋   | 67/100 [01:23<00:39,  1.20s/it] 68%|██████▊   | 68/100 [01:24<00:38,  1.20s/it] 69%|██████▉   | 69/100 [01:25<00:37,  1.21s/it] 70%|███████   | 70/100 [01:26<00:36,  1.21s/it] 71%|███████   | 71/100 [01:27<00:34,  1.20s/it] 72%|███████▏  | 72/100 [01:29<00:33,  1.20s/it] 73%|███████▎  | 73/100 [01:30<00:32,  1.21s/it] 74%|███████▍  | 74/100 [01:31<00:31,  1.21s/it] 75%|███████▌  | 75/100 [01:32<00:30,  1.21s/it] 76%|███████▌  | 76/100 [01:34<00:28,  1.20s/it] 77%|███████▋  | 77/100 [01:35<00:27,  1.20s/it] 78%|███████▊  | 78/100 [01:36<00:26,  1.20s/it] 79%|███████▉  | 79/100 [01:37<00:25,  1.20s/it] 80%|████████  | 80/100 [01:38<00:24,  1.20s/it] 81%|████████  | 81/100 [01:40<00:22,  1.20s/it] 82%|████████▏ | 82/100 [01:41<00:21,  1.20s/it] 83%|████████▎ | 83/100 [01:42<00:20,  1.20s/it] 84%|████████▍ | 84/100 [01:43<00:19,  1.20s/it] 85%|████████▌ | 85/100 [01:44<00:18,  1.20s/it] 86%|████████▌ | 86/100 [01:46<00:16,  1.20s/it] 87%|████████▋ | 87/100 [01:47<00:15,  1.20s/it] 88%|████████▊ | 88/100 [01:48<00:14,  1.20s/it] 89%|████████▉ | 89/100 [01:49<00:13,  1.20s/it] 90%|█████████ | 90/100 [01:50<00:11,  1.20s/it] 91%|█████████ | 91/100 [01:52<00:10,  1.20s/it] 92%|█████████▏| 92/100 [01:53<00:09,  1.20s/it] 93%|█████████▎| 93/100 [01:54<00:08,  1.20s/it] 94%|█████████▍| 94/100 [01:55<00:07,  1.21s/it] 95%|█████████▌| 95/100 [01:56<00:06,  1.21s/it] 96%|█████████▌| 96/100 [01:58<00:04,  1.21s/it] 97%|█████████▋| 97/100 [01:59<00:03,  1.21s/it] 98%|█████████▊| 98/100 [02:00<00:02,  1.21s/it] 99%|█████████▉| 99/100 [02:01<00:01,  1.21s/it]100%|██████████| 100/100 [02:02<00:00,  1.20s/it]100%|██████████| 100/100 [02:02<00:00,  1.23s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:6'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:6'}
full model:
{'mrpc': {'alias': 'mrpc', 'acc,none': 0.7, 'acc_stderr,none': 0.04605661864718383, 'f1,none': np.float64(0.8214285714285714), 'f1_stderr,none': 0.0323693653386572}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.32375514242269293
0.9664141198776434
0.4263233992087649
0.29258192731768157
0.3497718342079178
0.5968480671674146
0.7870107004084098
0.7068508532201193
0.7836897379939763
0.2579819071737284
0.5774144903645764
0.7819348699339372
0.7404680063618143
0.651835611327285
0.6832243011482585
0.34680079188801705
0.6033476505145444
0.6299526106979766
0.6468468631829007
0.8458578364778718
0.39447753640727073
0.5291234791393744
0.944540808938638
0.8460909093145242
0.4915397062049404
0.5314721523201049
0.8670806018560002
0.6451914961665037
0.7380077491787214
0.32375514242269293
0.9664141198776434
0.4263233992087649
0.29258192731768157
0.3497718342079178
0.5968480671674146
0.7870107004084098
0.7068508532201193
0.7836897379939763
0.2579819071737284
0.5774144903645764
0.7819348699339372
0.7404680063618143
0.651835611327285
0.6832243011482585
0.34680079188801705
0.6033476505145444
0.6299526106979766
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[4, 5, 7, 1, 6, 2, 3, 0]
tensor([4, 5, 7, 1, 6, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 4, 5, 0, 1, 2]
tensor([6, 3, 7, 4, 5, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 1, 7, 2, 6, 0, 3, 5]
tensor([4, 1, 7, 2, 6, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 5, 2, 7, 0, 4, 1]
tensor([6, 3, 5, 2, 7, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 6, 7, 3, 1, 0, 2]
tensor([4, 5, 6, 7, 3, 1, 0, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 2, 4, 1, 0, 5]
tensor([0, 3, 1, 2, 4, 1, 0, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1, 1.0]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 2
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 4 to 5
done!
Normal merging for layer 6
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
done!
Cross-layer merge completed for layers 7 to 9
done!
Normal merging for layer 10
tensor([0, 6])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([7])
tensor(7)
done!
Cross-layer merge completed for layers 11 to 26
done!
Normal merging for layer 27
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 28 to 30
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.3867 GB
82
cuda:6
coqa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 25.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.22s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/coqa HTTP/1.1" 200 846
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/EleutherAI/coqa/EleutherAI/coqa.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/coqa HTTP/1.1" 200 857
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/coqa/resolve/82e11af842af6c1396f5e9a5c7de260107c50cf1/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/coqa/revision/82e11af842af6c1396f5e9a5c7de260107c50cf1 HTTP/1.1" 200 857
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/coqa/tree/82e11af842af6c1396f5e9a5c7de260107c50cf1?recursive=False&expand=False HTTP/1.1" 200 489
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/coqa/tree/82e11af842af6c1396f5e9a5c7de260107c50cf1/data?recursive=False&expand=False HTTP/1.1" 404 79
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/coqa/tree/82e11af842af6c1396f5e9a5c7de260107c50cf1/data?recursive=False&expand=False HTTP/1.1" 404 79
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/coqa/revision/82e11af842af6c1396f5e9a5c7de260107c50cf1 HTTP/1.1" 200 857
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/coqa/resolve/82e11af842af6c1396f5e9a5c7de260107c50cf1/dataset_infos.json HTTP/1.1" 200 0
DEBUG:filelock:Attempting to acquire lock 140516041420112 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___coqa_default_0.0.0_82e11af842af6c1396f5e9a5c7de260107c50cf1.lock
DEBUG:filelock:Lock 140516041420112 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___coqa_default_0.0.0_82e11af842af6c1396f5e9a5c7de260107c50cf1.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___coqa/default/0.0.0/82e11af842af6c1396f5e9a5c7de260107c50cf1/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516041420112 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___coqa_default_0.0.0_82e11af842af6c1396f5e9a5c7de260107c50cf1.lock
DEBUG:filelock:Lock 140516041420112 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___coqa_default_0.0.0_82e11af842af6c1396f5e9a5c7de260107c50cf1.lock
DEBUG:filelock:Attempting to acquire lock 140541532080752 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___coqa/default/0.0.0/82e11af842af6c1396f5e9a5c7de260107c50cf1_builder.lock
DEBUG:filelock:Lock 140541532080752 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___coqa/default/0.0.0/82e11af842af6c1396f5e9a5c7de260107c50cf1_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___coqa/default/0.0.0/82e11af842af6c1396f5e9a5c7de260107c50cf1/dataset_info.json
DEBUG:filelock:Attempting to release lock 140541532080752 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___coqa/default/0.0.0/82e11af842af6c1396f5e9a5c7de260107c50cf1_builder.lock
DEBUG:filelock:Lock 140541532080752 released on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___coqa/default/0.0.0/82e11af842af6c1396f5e9a5c7de260107c50cf1_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
DEBUG:lm_eval.api.task:doc_to_target returned a list. Assuming multiple targets.
INFO:lm_eval.evaluator:coqa: Using gen_kwargs: {'until': ['\nQ:']}
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of coqa from None to 0
INFO:lm_eval.api.task:Building contexts for coqa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 79407.50it/s]
DEBUG:lm_eval.evaluator:Task: coqa; number of requests on this rank: 100
INFO:lm_eval.evaluator:Running generate_until requests
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/100 [00:06<10:25,  6.32s/it]Running generate_until requests:   2%|▏         | 2/100 [00:11<09:10,  5.62s/it]Running generate_until requests:   3%|▎         | 3/100 [00:17<09:28,  5.87s/it]Running generate_until requests:   4%|▍         | 4/100 [00:22<08:45,  5.47s/it]Running generate_until requests:   5%|▌         | 5/100 [00:27<08:25,  5.32s/it]Running generate_until requests:   6%|▌         | 6/100 [00:32<07:57,  5.08s/it]Running generate_until requests:   7%|▋         | 7/100 [00:37<08:06,  5.23s/it]Running generate_until requests:   8%|▊         | 8/100 [00:42<07:43,  5.03s/it]Running generate_until requests:   9%|▉         | 9/100 [00:47<07:35,  5.01s/it]Running generate_until requests:  10%|█         | 10/100 [00:52<07:46,  5.18s/it]Running generate_until requests:  11%|█         | 11/100 [00:57<07:38,  5.15s/it]Running generate_until requests:  12%|█▏        | 12/100 [01:04<08:02,  5.48s/it]Running generate_until requests:  13%|█▎        | 13/100 [01:08<07:30,  5.17s/it]Running generate_until requests:  14%|█▍        | 14/100 [01:13<07:05,  4.95s/it]Running generate_until requests:  15%|█▌        | 15/100 [01:17<06:54,  4.87s/it]Running generate_until requests:  16%|█▌        | 16/100 [01:22<06:36,  4.72s/it]Running generate_until requests:  17%|█▋        | 17/100 [01:26<06:26,  4.66s/it]Running generate_until requests:  18%|█▊        | 18/100 [01:30<06:11,  4.53s/it]Running generate_until requests:  19%|█▉        | 19/100 [01:35<06:06,  4.52s/it]Running generate_until requests:  20%|██        | 20/100 [01:39<05:57,  4.47s/it]Running generate_until requests:  21%|██        | 21/100 [01:44<05:52,  4.46s/it]Running generate_until requests:  22%|██▏       | 22/100 [01:48<05:44,  4.42s/it]Running generate_until requests:  23%|██▎       | 23/100 [01:53<06:01,  4.70s/it]Running generate_until requests:  24%|██▍       | 24/100 [01:59<06:08,  4.85s/it]Running generate_until requests:  25%|██▌       | 25/100 [02:03<05:51,  4.68s/it]Running generate_until requests:  26%|██▌       | 26/100 [02:07<05:45,  4.66s/it]Running generate_until requests:  27%|██▋       | 27/100 [02:12<05:39,  4.65s/it]Running generate_until requests:  28%|██▊       | 28/100 [02:17<05:31,  4.60s/it]Running generate_until requests:  29%|██▉       | 29/100 [02:21<05:25,  4.59s/it]Running generate_until requests:  30%|███       | 30/100 [02:26<05:24,  4.63s/it]Running generate_until requests:  31%|███       | 31/100 [02:30<05:07,  4.45s/it]Running generate_until requests:  32%|███▏      | 32/100 [02:35<05:06,  4.51s/it]Running generate_until requests:  33%|███▎      | 33/100 [02:39<05:07,  4.59s/it]Running generate_until requests:  34%|███▍      | 34/100 [02:44<04:58,  4.52s/it]Running generate_until requests:  35%|███▌      | 35/100 [02:48<04:43,  4.36s/it]Running generate_until requests:  36%|███▌      | 36/100 [02:53<05:00,  4.70s/it]Running generate_until requests:  37%|███▋      | 37/100 [02:57<04:41,  4.47s/it]Running generate_until requests:  38%|███▊      | 38/100 [03:01<04:30,  4.36s/it]Running generate_until requests:  39%|███▉      | 39/100 [03:05<04:23,  4.32s/it]Running generate_until requests:  40%|████      | 40/100 [03:09<04:11,  4.20s/it]Running generate_until requests:  41%|████      | 41/100 [03:14<04:08,  4.21s/it]Running generate_until requests:  42%|████▏     | 42/100 [03:19<04:23,  4.54s/it]Running generate_until requests:  43%|████▎     | 43/100 [03:23<04:11,  4.41s/it]Running generate_until requests:  44%|████▍     | 44/100 [03:27<04:06,  4.40s/it]Running generate_until requests:  45%|████▌     | 45/100 [03:32<03:59,  4.35s/it]Running generate_until requests:  46%|████▌     | 46/100 [03:35<03:46,  4.20s/it]Running generate_until requests:  47%|████▋     | 47/100 [03:39<03:36,  4.08s/it]Running generate_until requests:  48%|████▊     | 48/100 [03:44<03:36,  4.16s/it]Running generate_until requests:  49%|████▉     | 49/100 [03:48<03:42,  4.36s/it]Running generate_until requests:  50%|█████     | 50/100 [03:52<03:29,  4.19s/it]Running generate_until requests:  51%|█████     | 51/100 [03:57<03:34,  4.38s/it]Running generate_until requests:  52%|█████▏    | 52/100 [04:01<03:21,  4.19s/it]Running generate_until requests:  53%|█████▎    | 53/100 [04:05<03:20,  4.26s/it]Running generate_until requests:  54%|█████▍    | 54/100 [04:09<03:08,  4.10s/it]Running generate_until requests:  55%|█████▌    | 55/100 [04:13<03:04,  4.11s/it]Running generate_until requests:  56%|█████▌    | 56/100 [04:18<03:08,  4.29s/it]Running generate_until requests:  57%|█████▋    | 57/100 [04:22<03:04,  4.28s/it]Running generate_until requests:  58%|█████▊    | 58/100 [04:26<02:52,  4.12s/it]Running generate_until requests:  59%|█████▉    | 59/100 [04:31<02:59,  4.38s/it]Running generate_until requests:  60%|██████    | 60/100 [04:34<02:46,  4.17s/it]Running generate_until requests:  61%|██████    | 61/100 [04:39<02:48,  4.33s/it]Running generate_until requests:  62%|██████▏   | 62/100 [04:44<02:48,  4.43s/it]Running generate_until requests:  63%|██████▎   | 63/100 [04:47<02:34,  4.18s/it]Running generate_until requests:  64%|██████▍   | 64/100 [04:51<02:24,  4.01s/it]Running generate_until requests:  65%|██████▌   | 65/100 [04:55<02:15,  3.87s/it]Running generate_until requests:  66%|██████▌   | 66/100 [04:58<02:08,  3.78s/it]Running generate_until requests:  67%|██████▋   | 67/100 [05:02<02:02,  3.70s/it]Running generate_until requests:  68%|██████▊   | 68/100 [05:06<02:02,  3.84s/it]Running generate_until requests:  69%|██████▉   | 69/100 [05:11<02:15,  4.36s/it]Running generate_until requests:  70%|███████   | 70/100 [05:15<02:07,  4.26s/it]Running generate_until requests:  71%|███████   | 71/100 [05:24<02:38,  5.46s/it]Running generate_until requests:  72%|███████▏  | 72/100 [05:27<02:17,  4.89s/it]Running generate_until requests:  73%|███████▎  | 73/100 [05:31<02:01,  4.49s/it]Running generate_until requests:  74%|███████▍  | 74/100 [05:35<01:52,  4.31s/it]Running generate_until requests:  75%|███████▌  | 75/100 [05:38<01:41,  4.07s/it]Running generate_until requests:  76%|███████▌  | 76/100 [05:42<01:33,  3.88s/it]Running generate_until requests:  77%|███████▋  | 77/100 [05:46<01:32,  4.03s/it]Running generate_until requests:  78%|███████▊  | 78/100 [05:50<01:26,  3.92s/it]Running generate_until requests:  79%|███████▉  | 79/100 [05:53<01:19,  3.77s/it]Running generate_until requests:  80%|████████  | 80/100 [05:57<01:15,  3.77s/it]Running generate_until requests:  81%|████████  | 81/100 [06:02<01:21,  4.31s/it]Running generate_until requests:  82%|████████▏ | 82/100 [06:06<01:11,  3.99s/it]Running generate_until requests:  83%|████████▎ | 83/100 [06:09<01:05,  3.87s/it]Running generate_until requests:  84%|████████▍ | 84/100 [06:12<00:58,  3.67s/it]Running generate_until requests:  85%|████████▌ | 85/100 [06:16<00:52,  3.51s/it]Running generate_until requests:  86%|████████▌ | 86/100 [06:19<00:48,  3.47s/it]Running generate_until requests:  87%|████████▋ | 87/100 [06:23<00:48,  3.71s/it]Running generate_until requests:  88%|████████▊ | 88/100 [06:27<00:43,  3.63s/it]Running generate_until requests:  89%|████████▉ | 89/100 [06:30<00:37,  3.42s/it]Running generate_until requests:  90%|█████████ | 90/100 [06:32<00:32,  3.25s/it]Running generate_until requests:  91%|█████████ | 91/100 [06:35<00:27,  3.09s/it]Running generate_until requests:  92%|█████████▏| 92/100 [06:38<00:23,  3.00s/it]Running generate_until requests:  93%|█████████▎| 93/100 [06:41<00:20,  2.96s/it]Running generate_until requests:  94%|█████████▍| 94/100 [06:44<00:17,  2.96s/it]Running generate_until requests:  95%|█████████▌| 95/100 [06:47<00:15,  3.03s/it]Running generate_until requests:  96%|█████████▌| 96/100 [06:50<00:12,  3.04s/it]Running generate_until requests:  97%|█████████▋| 97/100 [06:52<00:08,  2.87s/it]Running generate_until requests:  98%|█████████▊| 98/100 [06:55<00:05,  2.75s/it]Running generate_until requests:  99%|█████████▉| 99/100 [06:58<00:02,  2.74s/it]Running generate_until requests: 100%|██████████| 100/100 [07:00<00:00,  2.75s/it]Running generate_until requests: 100%|██████████| 100/100 [07:00<00:00,  4.21s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:7'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:7'}
full model:
{'coqa': {'alias': 'coqa', 'em,none': 0.595, 'em_stderr,none': 0.044774970461162564, 'f1,none': 0.7211574141733987, 'f1_stderr,none': 0.037128235455690536}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6057853926542468
0.4195568875297668
0.5244744113321889
0.5202703028806769
0.6150870974034927
0.5299634576457063
0.9336763524510373
0.23649940737178063
0.388911200696845
0.6478041116722705
0.5517233675449297
0.6723258763091353
0.7175526480521238
0.8411089149883405
0.7404554224148189
0.26376935916880817
0.9373006475493478
0.5360566853939598
0.38729358133282565
0.4541602442018795
0.8623573205888978
0.7318340566806717
0.6643209906079897
0.8122565195147101
0.4707270481319977
0.9785001455445378
0.17075087907531752
0.489625917805058
0.7595051272431785
0.6057853926542468
0.4195568875297668
0.5244744113321889
0.5202703028806769
0.6150870974034927
0.5299634576457063
0.9336763524510373
0.23649940737178063
0.388911200696845
0.6478041116722705
0.5517233675449297
0.6723258763091353
0.7175526480521238
0.8411089149883405
0.7404554224148189
0.26376935916880817
0.9373006475493478
0.5360566853939598
0.38729358133282565
0.4541602442018795
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[5, 4, 3, 2, 0, 1, 7, 6]
tensor([5, 4, 3, 2, 0, 1, 7, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 4, 0, 2, 1, 7, 6]
tensor([5, 3, 4, 0, 2, 1, 7, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 1, 7, 0, 6, 2, 3, 4]
tensor([5, 1, 7, 0, 6, 2, 3, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 3, 6, 0, 5, 2, 4, 1]
tensor([7, 3, 6, 0, 5, 2, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 5, 7, 2, 4, 0, 3, 1]
tensor([6, 5, 7, 2, 4, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 0, 0, 1, 2, 1, 3]
tensor([4, 5, 0, 0, 1, 2, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 2, 2, 0, 3, 3]
tensor([0, 1, 1, 2, 2, 0, 3, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([6])
tensor(6)
done!
Normal merging for layer 2
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Normal merging for layer 4
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 5 to 8
done!
Normal merging for layer 9
tensor([2, 3])
tensor(2)
tensor([4, 6])
tensor(4)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
done!
Cross-layer merge completed for layers 10 to 22
done!
Normal merging for layer 23
tensor([0, 5])
tensor(0)
tensor([1, 2])
tensor(1)
tensor([3, 4])
tensor(3)
tensor([6, 7])
tensor(6)
done!
Cross-layer merge completed for layers 24 to 31
done!
all done!
Model size: 12.3238 GB
103
cuda:7
wic
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 25.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.22s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wic] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wic] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/wic?recursive=False&expand=False HTTP/1.1" 307 142
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/wic?recursive=False&expand=False HTTP/1.1" 200 356
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140545362492400 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140545362492400 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545362492400 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140545362492400 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140547536704752 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140547536704752 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140547536704752 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140547536704752 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wic from None to 0
INFO:lm_eval.api.task:Building contexts for wic on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1566.66it/s]
DEBUG:lm_eval.evaluator:Task: wic; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<04:33,  1.37s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:00,  1.63it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:02<01:32,  2.11it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:03<01:20,  2.39it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:04<01:14,  2.58it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:04<01:09,  2.71it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:05<01:06,  2.81it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:06<01:04,  2.88it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:06<01:02,  2.93it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:07<01:00,  2.97it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:08<00:59,  3.01it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:08<00:58,  3.03it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:09<00:57,  3.04it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:10<00:56,  3.06it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:10<00:55,  3.07it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:11<00:54,  3.09it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:11<00:53,  3.10it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:12<00:52,  3.12it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:13<00:51,  3.14it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:13<00:50,  3.16it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:14<00:50,  3.18it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:15<00:49,  3.19it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:15<00:48,  3.20it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:16<00:47,  3.20it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:16<00:47,  3.20it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:17<00:46,  3.21it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:18<00:45,  3.22it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:18<00:44,  3.22it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:19<00:44,  3.23it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:20<00:43,  3.23it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:20<00:42,  3.24it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:21<00:42,  3.24it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:21<00:41,  3.24it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:22<00:41,  3.24it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:23<00:40,  3.25it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:23<00:39,  3.25it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:24<00:38,  3.26it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:24<00:38,  3.27it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:25<00:37,  3.28it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:26<00:36,  3.28it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:26<00:36,  3.29it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:27<00:35,  3.29it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:27<00:34,  3.30it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:28<00:34,  3.30it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:29<00:33,  3.30it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:29<00:33,  3.30it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:30<00:32,  3.30it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:30<00:31,  3.30it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:31<00:31,  3.30it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:32<00:30,  3.30it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:32<00:29,  3.30it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:33<00:29,  3.30it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:34<00:28,  3.31it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:34<00:28,  3.31it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:35<00:27,  3.32it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:35<00:26,  3.32it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:36<00:26,  3.33it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:37<00:25,  3.32it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:37<00:24,  3.33it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:38<00:24,  3.33it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:38<00:23,  3.33it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:39<00:23,  3.33it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:40<00:22,  3.33it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:40<00:21,  3.34it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:41<00:21,  3.34it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:41<00:20,  3.35it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:42<00:19,  3.35it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:42<00:19,  3.35it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:43<00:18,  3.36it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:44<00:18,  3.37it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:44<00:17,  3.37it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:45<00:16,  3.38it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:45<00:16,  3.39it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:46<00:15,  3.40it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:47<00:14,  3.40it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:47<00:14,  3.41it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:48<00:13,  3.41it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:48<00:13,  3.41it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:49<00:12,  3.42it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:50<00:11,  3.43it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:50<00:11,  3.43it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [00:51<00:10,  3.43it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [00:51<00:10,  3.43it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [00:52<00:09,  3.44it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [00:52<00:09,  3.44it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [00:53<00:08,  3.45it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [00:54<00:07,  3.46it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [00:54<00:07,  3.47it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [00:55<00:06,  3.48it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [00:55<00:06,  3.50it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [00:56<00:05,  3.50it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [00:56<00:04,  3.51it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [00:57<00:04,  3.51it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [00:58<00:03,  3.52it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [00:58<00:03,  3.53it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [00:59<00:02,  3.54it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [00:59<00:01,  3.55it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:00<00:01,  3.56it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:00<00:00,  3.57it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:01<00:00,  3.58it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:01<00:00,  3.26it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:0'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}
full model:
{'wic': {'alias': 'wic', 'acc,none': 0.47, 'acc_stderr,none': 0.05016135580465919}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7015569150227223
0.5302361010977743
0.6131123609930033
0.8131827739550247
0.5456264918897312
0.5653128506125247
0.9024119585362896
0.8122852497904204
0.9072724946141106
0.866764102055741
0.8260299199157425
0.7472915500213457
0.8874866998217976
0.7441602305581367
0.22948143665096393
0.6763976434023368
0.5909756859477309
0.6775915070630182
0.8311737665735953
0.5882947608660276
0.7888779075700829
0.9530393862783458
0.7563942945196994
0.7021129984434293
0.9133573687405422
0.8864659884483975
0.43949477197814607
0.49530015739760547
0.9835705252160515
0.7015569150227223
0.5302361010977743
0.6131123609930033
0.8131827739550247
0.5456264918897312
0.5653128506125247
0.9024119585362896
0.8122852497904204
0.9072724946141106
0.866764102055741
0.8260299199157425
0.7472915500213457
0.8874866998217976
0.7441602305581367
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[6, 5, 2, 3, 7, 0, 4, 1]
tensor([6, 5, 2, 3, 7, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 6, 1, 5, 4, 2, 3, 0]
tensor([7, 6, 1, 5, 4, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 6, 2, 4, 5, 0, 3, 1]
tensor([7, 6, 2, 4, 5, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 0, 5, 1, 1, 3, 2]
tensor([4, 0, 0, 5, 1, 1, 3, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 0, 2, 5, 3, 4, 1]
tensor([0, 1, 0, 2, 5, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 0, 1, 4, 1, 2, 3, 0]
tensor([5, 0, 1, 4, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 1, 1, 3, 4, 2, 5, 0]
tensor([0, 1, 1, 3, 4, 2, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 1, 0, 2, 3, 1, 2, 3]
tensor([0, 1, 0, 2, 3, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 2 to 3
done!
Normal merging for layer 4
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 7
done!
Normal merging for layer 8
tensor([1, 2])
tensor(1)
tensor([4, 5])
tensor(4)
tensor([7])
tensor(7)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
done!
Cross-layer merge completed for layers 9 to 10
done!
Normal merging for layer 11
tensor([0, 2])
tensor(0)
tensor([1, 7])
tensor(1)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 12 to 13
done!
Normal merging for layer 14
tensor([1, 7])
tensor(1)
tensor([2, 4])
tensor(2)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
done!
Normal merging for layer 15
tensor([0, 7])
tensor(0)
tensor([1, 2])
tensor(1)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
done!
Normal merging for layer 16
tensor([0, 2])
tensor(0)
tensor([1, 5])
tensor(1)
tensor([3, 6])
tensor(3)
tensor([4, 7])
tensor(4)
done!
Cross-layer merge completed for layers 17 to 31
done!
all done!
Model size: 12.5757 GB
100
cuda:0
sciq
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 25.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.68s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq HTTP/1.1" 200 1238
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/sciq/sciq.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq HTTP/1.1" 200 1238
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq/revision/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 111
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq/revision/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 1238
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq/tree/2c94ad3e1aafab77146f384e23536f97a4849815?recursive=False&expand=False HTTP/1.1" 307 136
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq/tree/2c94ad3e1aafab77146f384e23536f97a4849815?recursive=False&expand=False HTTP/1.1" 200 291
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq/tree/2c94ad3e1aafab77146f384e23536f97a4849815/data?recursive=False&expand=False HTTP/1.1" 307 141
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq/tree/2c94ad3e1aafab77146f384e23536f97a4849815/data?recursive=False&expand=False HTTP/1.1" 200 358
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/sciq/revision/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 111
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/sciq/revision/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 1238
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/sciq/resolve/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/sciq/paths-info/2c94ad3e1aafab77146f384e23536f97a4849815 HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140541531556560 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:filelock:Lock 140541531556560 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_info.json
DEBUG:filelock:Attempting to release lock 140541531556560 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:filelock:Lock 140541531556560 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_sciq_default_0.0.0_2c94ad3e1aafab77146f384e23536f97a4849815.lock
DEBUG:filelock:Attempting to acquire lock 140545763781536 on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:filelock:Lock 140545763781536 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545763781536 on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:filelock:Lock 140545763781536 released on /public/home/zouyifei001/.cache/huggingface/datasets/sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of sciq from None to 0
INFO:lm_eval.api.task:Building contexts for sciq on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s] 93%|█████████▎| 93/100 [00:00<00:00, 928.50it/s]100%|██████████| 100/100 [00:00<00:00, 928.61it/s]
DEBUG:lm_eval.evaluator:Task: sciq; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:03<25:31,  3.84s/it]Running loglikelihood requests:   0%|          | 2/400 [00:07<23:46,  3.58s/it]Running loglikelihood requests:   1%|          | 3/400 [00:10<22:50,  3.45s/it]Running loglikelihood requests:   1%|          | 4/400 [00:13<22:22,  3.39s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:16<21:37,  3.29s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:20<21:08,  3.22s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:23<20:48,  3.18s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:26<20:31,  3.14s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:28<19:39,  3.02s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:31<19:02,  2.93s/it]Running loglikelihood requests:   3%|▎         | 11/400 [00:34<18:39,  2.88s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:37<18:18,  2.83s/it]Running loglikelihood requests:   3%|▎         | 13/400 [00:39<18:08,  2.81s/it]Running loglikelihood requests:   4%|▎         | 14/400 [00:42<17:33,  2.73s/it]Running loglikelihood requests:   4%|▍         | 15/400 [00:44<17:06,  2.67s/it]Running loglikelihood requests:   4%|▍         | 16/400 [00:47<16:47,  2.62s/it]Running loglikelihood requests:   4%|▍         | 17/400 [00:49<16:27,  2.58s/it]Running loglikelihood requests:   4%|▍         | 18/400 [00:52<16:08,  2.54s/it]Running loglikelihood requests:   5%|▍         | 19/400 [00:55<16:29,  2.60s/it]Running loglikelihood requests:   5%|▌         | 20/400 [00:57<16:07,  2.55s/it]Running loglikelihood requests:   5%|▌         | 21/400 [00:59<15:49,  2.50s/it]Running loglikelihood requests:   6%|▌         | 22/400 [01:02<15:36,  2.48s/it]Running loglikelihood requests:   6%|▌         | 23/400 [01:04<15:26,  2.46s/it]Running loglikelihood requests:   6%|▌         | 24/400 [01:07<15:21,  2.45s/it]Running loglikelihood requests:   6%|▋         | 25/400 [01:09<14:56,  2.39s/it]Running loglikelihood requests:   6%|▋         | 26/400 [01:11<14:36,  2.34s/it]Running loglikelihood requests:   7%|▋         | 27/400 [01:13<14:20,  2.31s/it]Running loglikelihood requests:   7%|▋         | 28/400 [01:16<14:08,  2.28s/it]Running loglikelihood requests:   7%|▋         | 29/400 [01:18<13:37,  2.20s/it]Running loglikelihood requests:   8%|▊         | 30/400 [01:20<13:15,  2.15s/it]Running loglikelihood requests:   8%|▊         | 31/400 [01:22<12:59,  2.11s/it]Running loglikelihood requests:   8%|▊         | 32/400 [01:24<12:45,  2.08s/it]Running loglikelihood requests:   8%|▊         | 33/400 [01:26<12:23,  2.03s/it]Running loglikelihood requests:   8%|▊         | 34/400 [01:28<12:07,  1.99s/it]Running loglikelihood requests:   9%|▉         | 35/400 [01:29<11:54,  1.96s/it]Running loglikelihood requests:   9%|▉         | 36/400 [01:31<11:45,  1.94s/it]Running loglikelihood requests:   9%|▉         | 37/400 [01:33<11:38,  1.92s/it]Running loglikelihood requests:  10%|▉         | 38/400 [01:35<11:33,  1.92s/it]Running loglikelihood requests:  10%|█         | 40/400 [01:37<08:48,  1.47s/it]Running loglikelihood requests:  10%|█         | 41/400 [01:39<09:11,  1.54s/it]Running loglikelihood requests:  10%|█         | 42/400 [01:40<09:27,  1.59s/it]Running loglikelihood requests:  11%|█         | 43/400 [01:42<09:40,  1.63s/it]Running loglikelihood requests:  11%|█         | 44/400 [01:44<09:48,  1.65s/it]Running loglikelihood requests:  11%|█▏        | 45/400 [01:46<09:47,  1.65s/it]Running loglikelihood requests:  12%|█▏        | 46/400 [01:47<09:46,  1.66s/it]Running loglikelihood requests:  12%|█▏        | 47/400 [01:49<09:44,  1.66s/it]Running loglikelihood requests:  12%|█▏        | 48/400 [01:51<09:42,  1.65s/it]Running loglikelihood requests:  12%|█▏        | 49/400 [01:52<09:40,  1.65s/it]Running loglikelihood requests:  12%|█▎        | 50/400 [01:54<09:38,  1.65s/it]Running loglikelihood requests:  13%|█▎        | 51/400 [01:56<09:36,  1.65s/it]Running loglikelihood requests:  13%|█▎        | 52/400 [01:57<09:34,  1.65s/it]Running loglikelihood requests:  13%|█▎        | 53/400 [01:59<09:15,  1.60s/it]Running loglikelihood requests:  14%|█▎        | 54/400 [02:00<09:01,  1.57s/it]Running loglikelihood requests:  14%|█▍        | 55/400 [02:02<08:50,  1.54s/it]Running loglikelihood requests:  14%|█▍        | 57/400 [02:03<06:36,  1.16s/it]Running loglikelihood requests:  14%|█▍        | 58/400 [02:04<06:57,  1.22s/it]Running loglikelihood requests:  15%|█▍        | 59/400 [02:06<07:12,  1.27s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [02:07<04:45,  1.18it/s]Running loglikelihood requests:  16%|█▌        | 63/400 [02:09<05:23,  1.04it/s]Running loglikelihood requests:  16%|█▌        | 64/400 [02:10<05:55,  1.06s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [02:11<06:18,  1.13s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [02:13<06:36,  1.19s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [02:14<06:50,  1.23s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [02:15<07:00,  1.27s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [02:17<07:05,  1.28s/it]Running loglikelihood requests:  18%|█▊        | 70/400 [02:18<07:08,  1.30s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [02:19<07:10,  1.31s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [02:21<07:22,  1.35s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [02:22<07:18,  1.34s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [02:24<07:14,  1.33s/it]Running loglikelihood requests:  19%|█▉        | 77/400 [02:25<04:27,  1.21it/s]Running loglikelihood requests:  20%|█▉        | 78/400 [02:26<04:57,  1.08it/s]Running loglikelihood requests:  20%|█▉        | 79/400 [02:27<05:23,  1.01s/it]Running loglikelihood requests:  20%|██        | 81/400 [02:29<04:27,  1.19it/s]Running loglikelihood requests:  20%|██        | 82/400 [02:30<04:49,  1.10it/s]Running loglikelihood requests:  21%|██        | 83/400 [02:31<05:08,  1.03it/s]Running loglikelihood requests:  21%|██        | 84/400 [02:32<05:22,  1.02s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [02:33<05:32,  1.06s/it]Running loglikelihood requests:  22%|██▏       | 86/400 [02:34<05:39,  1.08s/it]Running loglikelihood requests:  22%|██▏       | 89/400 [02:36<03:38,  1.43it/s]Running loglikelihood requests:  22%|██▎       | 90/400 [02:37<04:06,  1.26it/s]Running loglikelihood requests:  23%|██▎       | 91/400 [02:38<04:29,  1.15it/s]Running loglikelihood requests:  23%|██▎       | 92/400 [02:39<04:47,  1.07it/s]Running loglikelihood requests:  23%|██▎       | 93/400 [02:40<05:02,  1.02it/s]Running loglikelihood requests:  24%|██▍       | 97/400 [02:41<02:50,  1.78it/s]Running loglikelihood requests:  24%|██▍       | 98/400 [02:42<03:19,  1.51it/s]Running loglikelihood requests:  25%|██▍       | 99/400 [02:43<03:45,  1.33it/s]Running loglikelihood requests:  25%|██▌       | 100/400 [02:45<04:09,  1.20it/s]Running loglikelihood requests:  25%|██▌       | 101/400 [02:46<04:28,  1.11it/s]Running loglikelihood requests:  26%|██▌       | 102/400 [02:47<04:43,  1.05it/s]Running loglikelihood requests:  26%|██▌       | 103/400 [02:48<04:54,  1.01it/s]Running loglikelihood requests:  26%|██▌       | 104/400 [02:49<05:02,  1.02s/it]Running loglikelihood requests:  26%|██▋       | 105/400 [02:50<05:07,  1.04s/it]Running loglikelihood requests:  26%|██▋       | 106/400 [02:51<05:10,  1.05s/it]Running loglikelihood requests:  27%|██▋       | 107/400 [02:52<05:11,  1.06s/it]Running loglikelihood requests:  27%|██▋       | 108/400 [02:53<05:12,  1.07s/it]Running loglikelihood requests:  27%|██▋       | 109/400 [02:54<05:11,  1.07s/it]Running loglikelihood requests:  28%|██▊       | 110/400 [02:55<05:09,  1.07s/it]Running loglikelihood requests:  28%|██▊       | 111/400 [02:56<05:08,  1.07s/it]Running loglikelihood requests:  28%|██▊       | 112/400 [02:58<05:07,  1.07s/it]Running loglikelihood requests:  28%|██▊       | 113/400 [02:59<05:05,  1.06s/it]Running loglikelihood requests:  28%|██▊       | 114/400 [03:00<05:03,  1.06s/it]Running loglikelihood requests:  29%|██▉       | 115/400 [03:01<05:01,  1.06s/it]Running loglikelihood requests:  29%|██▉       | 116/400 [03:02<04:59,  1.05s/it]Running loglikelihood requests:  29%|██▉       | 117/400 [03:03<04:56,  1.05s/it]Running loglikelihood requests:  30%|██▉       | 118/400 [03:04<04:52,  1.04s/it]Running loglikelihood requests:  30%|██▉       | 119/400 [03:05<04:50,  1.03s/it]Running loglikelihood requests:  30%|███       | 120/400 [03:06<04:48,  1.03s/it]Running loglikelihood requests:  30%|███       | 121/400 [03:07<04:46,  1.03s/it]Running loglikelihood requests:  30%|███       | 122/400 [03:08<04:45,  1.03s/it]Running loglikelihood requests:  31%|███       | 123/400 [03:09<04:43,  1.02s/it]Running loglikelihood requests:  31%|███       | 124/400 [03:10<04:42,  1.03s/it]Running loglikelihood requests:  31%|███▏      | 125/400 [03:11<04:40,  1.02s/it]Running loglikelihood requests:  32%|███▏      | 126/400 [03:12<04:37,  1.01s/it]Running loglikelihood requests:  32%|███▏      | 127/400 [03:13<04:34,  1.01s/it]Running loglikelihood requests:  32%|███▏      | 128/400 [03:14<04:33,  1.00s/it]Running loglikelihood requests:  32%|███▏      | 129/400 [03:15<04:31,  1.00s/it]Running loglikelihood requests:  32%|███▎      | 130/400 [03:16<04:29,  1.00it/s]Running loglikelihood requests:  33%|███▎      | 131/400 [03:17<04:27,  1.01it/s]Running loglikelihood requests:  34%|███▎      | 134/400 [03:18<02:44,  1.62it/s]Running loglikelihood requests:  34%|███▍      | 135/400 [03:19<03:04,  1.44it/s]Running loglikelihood requests:  34%|███▍      | 136/400 [03:20<03:20,  1.31it/s]Running loglikelihood requests:  34%|███▍      | 137/400 [03:21<03:34,  1.23it/s]Running loglikelihood requests:  34%|███▍      | 138/400 [03:22<03:44,  1.17it/s]Running loglikelihood requests:  35%|███▍      | 139/400 [03:23<03:51,  1.13it/s]Running loglikelihood requests:  36%|███▌      | 142/400 [03:24<02:30,  1.71it/s]Running loglikelihood requests:  36%|███▌      | 143/400 [03:25<02:50,  1.51it/s]Running loglikelihood requests:  36%|███▌      | 144/400 [03:26<03:07,  1.36it/s]Running loglikelihood requests:  36%|███▋      | 145/400 [03:27<03:21,  1.26it/s]Running loglikelihood requests:  36%|███▋      | 146/400 [03:28<03:32,  1.20it/s]Running loglikelihood requests:  37%|███▋      | 147/400 [03:29<03:40,  1.15it/s]Running loglikelihood requests:  37%|███▋      | 148/400 [03:30<03:45,  1.12it/s]Running loglikelihood requests:  37%|███▋      | 149/400 [03:31<03:49,  1.09it/s]Running loglikelihood requests:  38%|███▊      | 150/400 [03:31<03:51,  1.08it/s]Running loglikelihood requests:  38%|███▊      | 151/400 [03:32<03:52,  1.07it/s]Running loglikelihood requests:  38%|███▊      | 152/400 [03:33<03:53,  1.06it/s]Running loglikelihood requests:  38%|███▊      | 153/400 [03:34<03:53,  1.06it/s]Running loglikelihood requests:  38%|███▊      | 154/400 [03:35<03:52,  1.06it/s]Running loglikelihood requests:  39%|███▉      | 155/400 [03:36<03:52,  1.06it/s]Running loglikelihood requests:  39%|███▉      | 156/400 [03:37<03:51,  1.05it/s]Running loglikelihood requests:  39%|███▉      | 157/400 [03:38<03:50,  1.06it/s]Running loglikelihood requests:  40%|███▉      | 158/400 [03:39<03:49,  1.06it/s]Running loglikelihood requests:  40%|███▉      | 159/400 [03:40<03:47,  1.06it/s]Running loglikelihood requests:  40%|████      | 160/400 [03:41<03:46,  1.06it/s]Running loglikelihood requests:  40%|████      | 161/400 [03:42<03:45,  1.06it/s]Running loglikelihood requests:  40%|████      | 162/400 [03:43<03:43,  1.06it/s]Running loglikelihood requests:  41%|████      | 163/400 [03:44<03:42,  1.06it/s]Running loglikelihood requests:  41%|████      | 164/400 [03:45<03:41,  1.07it/s]Running loglikelihood requests:  41%|████▏     | 165/400 [03:46<03:38,  1.07it/s]Running loglikelihood requests:  42%|████▏     | 166/400 [03:47<03:37,  1.08it/s]Running loglikelihood requests:  42%|████▏     | 167/400 [03:47<03:35,  1.08it/s]Running loglikelihood requests:  42%|████▏     | 168/400 [03:48<03:34,  1.08it/s]Running loglikelihood requests:  42%|████▏     | 169/400 [03:49<03:32,  1.09it/s]Running loglikelihood requests:  42%|████▎     | 170/400 [03:50<03:30,  1.09it/s]Running loglikelihood requests:  43%|████▎     | 172/400 [03:51<02:40,  1.42it/s]Running loglikelihood requests:  43%|████▎     | 173/400 [03:52<02:49,  1.34it/s]Running loglikelihood requests:  44%|████▎     | 174/400 [03:53<02:56,  1.28it/s]Running loglikelihood requests:  44%|████▍     | 177/400 [03:54<01:56,  1.91it/s]Running loglikelihood requests:  44%|████▍     | 178/400 [03:55<02:12,  1.68it/s]Running loglikelihood requests:  45%|████▍     | 179/400 [03:55<02:25,  1.52it/s]Running loglikelihood requests:  45%|████▌     | 180/400 [03:56<02:36,  1.41it/s]Running loglikelihood requests:  45%|████▌     | 181/400 [03:57<02:44,  1.33it/s]Running loglikelihood requests:  46%|████▌     | 182/400 [03:58<02:49,  1.28it/s]Running loglikelihood requests:  46%|████▌     | 183/400 [03:59<02:53,  1.25it/s]Running loglikelihood requests:  46%|████▌     | 184/400 [04:00<02:56,  1.23it/s]Running loglikelihood requests:  46%|████▋     | 185/400 [04:01<02:57,  1.21it/s]Running loglikelihood requests:  46%|████▋     | 186/400 [04:02<02:58,  1.20it/s]Running loglikelihood requests:  47%|████▋     | 187/400 [04:02<02:58,  1.19it/s]Running loglikelihood requests:  47%|████▋     | 188/400 [04:03<02:58,  1.19it/s]Running loglikelihood requests:  47%|████▋     | 189/400 [04:04<02:57,  1.19it/s]Running loglikelihood requests:  48%|████▊     | 190/400 [04:05<02:56,  1.19it/s]Running loglikelihood requests:  48%|████▊     | 191/400 [04:06<02:55,  1.19it/s]Running loglikelihood requests:  48%|████▊     | 192/400 [04:07<02:53,  1.20it/s]Running loglikelihood requests:  48%|████▊     | 193/400 [04:07<02:52,  1.20it/s]Running loglikelihood requests:  48%|████▊     | 194/400 [04:08<02:50,  1.21it/s]Running loglikelihood requests:  49%|████▉     | 195/400 [04:09<02:49,  1.21it/s]Running loglikelihood requests:  49%|████▉     | 196/400 [04:10<02:48,  1.21it/s]Running loglikelihood requests:  49%|████▉     | 197/400 [04:11<02:46,  1.22it/s]Running loglikelihood requests:  50%|████▉     | 198/400 [04:11<02:45,  1.22it/s]Running loglikelihood requests:  50%|████▉     | 199/400 [04:12<02:43,  1.23it/s]Running loglikelihood requests:  50%|█████     | 200/400 [04:13<02:42,  1.23it/s]Running loglikelihood requests:  50%|█████     | 201/400 [04:14<02:41,  1.23it/s]Running loglikelihood requests:  50%|█████     | 202/400 [04:15<02:40,  1.24it/s]Running loglikelihood requests:  51%|█████     | 203/400 [04:15<02:39,  1.24it/s]Running loglikelihood requests:  51%|█████     | 204/400 [04:16<02:38,  1.24it/s]Running loglikelihood requests:  51%|█████▏    | 205/400 [04:17<02:37,  1.24it/s]Running loglikelihood requests:  52%|█████▏    | 206/400 [04:18<02:36,  1.24it/s]Running loglikelihood requests:  52%|█████▏    | 207/400 [04:19<02:35,  1.24it/s]Running loglikelihood requests:  52%|█████▏    | 208/400 [04:20<02:34,  1.24it/s]Running loglikelihood requests:  52%|█████▏    | 209/400 [04:20<02:33,  1.24it/s]Running loglikelihood requests:  53%|█████▎    | 212/400 [04:21<01:34,  1.99it/s]Running loglikelihood requests:  53%|█████▎    | 213/400 [04:22<01:45,  1.78it/s]Running loglikelihood requests:  54%|█████▎    | 214/400 [04:23<01:54,  1.63it/s]Running loglikelihood requests:  54%|█████▍    | 215/400 [04:23<02:01,  1.53it/s]Running loglikelihood requests:  54%|█████▍    | 216/400 [04:24<02:06,  1.45it/s]Running loglikelihood requests:  54%|█████▍    | 217/400 [04:25<02:10,  1.40it/s]Running loglikelihood requests:  55%|█████▍    | 218/400 [04:26<02:13,  1.37it/s]Running loglikelihood requests:  55%|█████▍    | 219/400 [04:27<02:14,  1.34it/s]Running loglikelihood requests:  55%|█████▌    | 220/400 [04:27<02:16,  1.32it/s]Running loglikelihood requests:  55%|█████▌    | 221/400 [04:28<02:16,  1.31it/s]Running loglikelihood requests:  56%|█████▌    | 222/400 [04:29<02:16,  1.31it/s]Running loglikelihood requests:  56%|█████▌    | 223/400 [04:30<02:15,  1.31it/s]Running loglikelihood requests:  56%|█████▌    | 224/400 [04:30<02:14,  1.31it/s]Running loglikelihood requests:  56%|█████▋    | 225/400 [04:31<02:14,  1.31it/s]Running loglikelihood requests:  56%|█████▋    | 226/400 [04:32<02:13,  1.31it/s]Running loglikelihood requests:  57%|█████▋    | 227/400 [04:33<02:12,  1.31it/s]Running loglikelihood requests:  57%|█████▋    | 228/400 [04:34<02:11,  1.31it/s]Running loglikelihood requests:  57%|█████▋    | 229/400 [04:34<02:10,  1.31it/s]Running loglikelihood requests:  57%|█████▊    | 230/400 [04:35<02:10,  1.31it/s]Running loglikelihood requests:  58%|█████▊    | 231/400 [04:36<02:09,  1.31it/s]Running loglikelihood requests:  58%|█████▊    | 232/400 [04:37<02:08,  1.31it/s]Running loglikelihood requests:  58%|█████▊    | 233/400 [04:37<02:07,  1.31it/s]Running loglikelihood requests:  58%|█████▊    | 234/400 [04:38<02:06,  1.31it/s]Running loglikelihood requests:  59%|█████▉    | 235/400 [04:39<02:05,  1.31it/s]Running loglikelihood requests:  59%|█████▉    | 236/400 [04:40<02:04,  1.32it/s]Running loglikelihood requests:  59%|█████▉    | 237/400 [04:40<02:03,  1.32it/s]Running loglikelihood requests:  60%|██████    | 240/400 [04:41<01:15,  2.11it/s]Running loglikelihood requests:  60%|██████    | 241/400 [04:42<01:24,  1.89it/s]Running loglikelihood requests:  60%|██████    | 242/400 [04:43<01:31,  1.73it/s]Running loglikelihood requests:  61%|██████    | 243/400 [04:43<01:37,  1.62it/s]Running loglikelihood requests:  61%|██████    | 244/400 [04:44<01:41,  1.54it/s]Running loglikelihood requests:  61%|██████▏   | 245/400 [04:45<01:44,  1.49it/s]Running loglikelihood requests:  62%|██████▏   | 246/400 [04:46<01:46,  1.45it/s]Running loglikelihood requests:  62%|██████▏   | 247/400 [04:46<01:47,  1.43it/s]Running loglikelihood requests:  62%|██████▏   | 248/400 [04:47<01:47,  1.41it/s]Running loglikelihood requests:  62%|██████▏   | 249/400 [04:48<01:47,  1.40it/s]Running loglikelihood requests:  62%|██████▎   | 250/400 [04:48<01:47,  1.39it/s]Running loglikelihood requests:  63%|██████▎   | 251/400 [04:49<01:47,  1.39it/s]Running loglikelihood requests:  63%|██████▎   | 252/400 [04:50<01:46,  1.39it/s]Running loglikelihood requests:  63%|██████▎   | 253/400 [04:51<01:45,  1.39it/s]Running loglikelihood requests:  64%|██████▎   | 254/400 [04:51<01:45,  1.39it/s]Running loglikelihood requests:  64%|██████▍   | 255/400 [04:52<01:45,  1.38it/s]Running loglikelihood requests:  64%|██████▍   | 256/400 [04:53<01:44,  1.38it/s]Running loglikelihood requests:  64%|██████▍   | 257/400 [04:54<01:43,  1.39it/s]Running loglikelihood requests:  64%|██████▍   | 258/400 [04:54<01:41,  1.39it/s]Running loglikelihood requests:  65%|██████▍   | 259/400 [04:55<01:40,  1.40it/s]Running loglikelihood requests:  65%|██████▌   | 260/400 [04:56<01:39,  1.40it/s]Running loglikelihood requests:  65%|██████▌   | 261/400 [04:56<01:37,  1.43it/s]Running loglikelihood requests:  66%|██████▌   | 262/400 [04:57<01:35,  1.45it/s]Running loglikelihood requests:  66%|██████▌   | 263/400 [04:58<01:33,  1.46it/s]Running loglikelihood requests:  66%|██████▌   | 264/400 [04:58<01:32,  1.47it/s]Running loglikelihood requests:  66%|██████▋   | 265/400 [04:59<01:29,  1.51it/s]Running loglikelihood requests:  66%|██████▋   | 266/400 [05:00<01:27,  1.53it/s]Running loglikelihood requests:  67%|██████▋   | 267/400 [05:00<01:25,  1.55it/s]Running loglikelihood requests:  67%|██████▋   | 268/400 [05:01<01:24,  1.56it/s]Running loglikelihood requests:  67%|██████▋   | 269/400 [05:01<01:23,  1.58it/s]Running loglikelihood requests:  68%|██████▊   | 270/400 [05:02<01:21,  1.59it/s]Running loglikelihood requests:  68%|██████▊   | 271/400 [05:03<01:20,  1.61it/s]Running loglikelihood requests:  68%|██████▊   | 272/400 [05:03<01:19,  1.61it/s]Running loglikelihood requests:  68%|██████▊   | 273/400 [05:04<01:17,  1.64it/s]Running loglikelihood requests:  68%|██████▊   | 274/400 [05:05<01:17,  1.62it/s]Running loglikelihood requests:  69%|██████▉   | 275/400 [05:05<01:17,  1.61it/s]Running loglikelihood requests:  69%|██████▉   | 276/400 [05:06<01:17,  1.61it/s]Running loglikelihood requests:  69%|██████▉   | 277/400 [05:06<01:16,  1.61it/s]Running loglikelihood requests:  70%|██████▉   | 278/400 [05:07<01:15,  1.61it/s]Running loglikelihood requests:  70%|██████▉   | 279/400 [05:08<01:15,  1.60it/s]Running loglikelihood requests:  70%|███████   | 280/400 [05:08<01:14,  1.61it/s]Running loglikelihood requests:  70%|███████   | 281/400 [05:09<01:13,  1.61it/s]Running loglikelihood requests:  70%|███████   | 282/400 [05:09<01:12,  1.62it/s]Running loglikelihood requests:  71%|███████   | 283/400 [05:10<01:11,  1.63it/s]Running loglikelihood requests:  71%|███████   | 284/400 [05:11<01:09,  1.66it/s]Running loglikelihood requests:  71%|███████▏  | 285/400 [05:11<01:08,  1.69it/s]Running loglikelihood requests:  72%|███████▏  | 286/400 [05:12<01:06,  1.72it/s]Running loglikelihood requests:  72%|███████▏  | 287/400 [05:12<01:05,  1.74it/s]Running loglikelihood requests:  72%|███████▏  | 288/400 [05:13<01:04,  1.75it/s]Running loglikelihood requests:  72%|███████▏  | 289/400 [05:13<01:03,  1.76it/s]Running loglikelihood requests:  72%|███████▎  | 290/400 [05:14<01:02,  1.77it/s]Running loglikelihood requests:  73%|███████▎  | 291/400 [05:15<01:01,  1.77it/s]Running loglikelihood requests:  73%|███████▎  | 292/400 [05:15<01:00,  1.77it/s]Running loglikelihood requests:  73%|███████▎  | 293/400 [05:16<01:00,  1.78it/s]Running loglikelihood requests:  74%|███████▎  | 294/400 [05:16<00:59,  1.78it/s]Running loglikelihood requests:  74%|███████▍  | 295/400 [05:17<00:58,  1.79it/s]Running loglikelihood requests:  74%|███████▍  | 296/400 [05:17<00:57,  1.80it/s]Running loglikelihood requests:  74%|███████▍  | 297/400 [05:18<00:57,  1.80it/s]Running loglikelihood requests:  74%|███████▍  | 298/400 [05:18<00:56,  1.81it/s]Running loglikelihood requests:  75%|███████▍  | 299/400 [05:19<00:55,  1.81it/s]Running loglikelihood requests:  75%|███████▌  | 300/400 [05:20<00:55,  1.81it/s]Running loglikelihood requests:  75%|███████▌  | 301/400 [05:20<00:54,  1.81it/s]Running loglikelihood requests:  76%|███████▌  | 302/400 [05:21<00:54,  1.81it/s]Running loglikelihood requests:  76%|███████▌  | 303/400 [05:21<00:53,  1.82it/s]Running loglikelihood requests:  76%|███████▌  | 304/400 [05:22<00:52,  1.82it/s]Running loglikelihood requests:  76%|███████▋  | 305/400 [05:22<00:52,  1.82it/s]Running loglikelihood requests:  76%|███████▋  | 306/400 [05:23<00:51,  1.82it/s]Running loglikelihood requests:  77%|███████▋  | 307/400 [05:23<00:50,  1.82it/s]Running loglikelihood requests:  78%|███████▊  | 310/400 [05:24<00:30,  2.93it/s]Running loglikelihood requests:  78%|███████▊  | 311/400 [05:25<00:34,  2.60it/s]Running loglikelihood requests:  78%|███████▊  | 312/400 [05:25<00:37,  2.38it/s]Running loglikelihood requests:  78%|███████▊  | 313/400 [05:26<00:39,  2.21it/s]Running loglikelihood requests:  78%|███████▊  | 314/400 [05:26<00:40,  2.10it/s]Running loglikelihood requests:  79%|███████▉  | 315/400 [05:27<00:42,  2.02it/s]Running loglikelihood requests:  79%|███████▉  | 316/400 [05:27<00:42,  1.97it/s]Running loglikelihood requests:  79%|███████▉  | 317/400 [05:28<00:42,  1.95it/s]Running loglikelihood requests:  80%|███████▉  | 318/400 [05:28<00:42,  1.92it/s]Running loglikelihood requests:  80%|███████▉  | 319/400 [05:29<00:42,  1.92it/s]Running loglikelihood requests:  80%|████████  | 321/400 [05:29<00:31,  2.49it/s]Running loglikelihood requests:  80%|████████  | 322/400 [05:30<00:33,  2.33it/s]Running loglikelihood requests:  81%|████████  | 323/400 [05:30<00:34,  2.21it/s]Running loglikelihood requests:  81%|████████  | 324/400 [05:31<00:35,  2.13it/s]Running loglikelihood requests:  81%|████████▏ | 325/400 [05:31<00:36,  2.07it/s]Running loglikelihood requests:  82%|████████▏ | 326/400 [05:32<00:36,  2.03it/s]Running loglikelihood requests:  82%|████████▏ | 327/400 [05:32<00:36,  2.01it/s]Running loglikelihood requests:  82%|████████▏ | 328/400 [05:33<00:36,  1.99it/s]Running loglikelihood requests:  82%|████████▏ | 329/400 [05:33<00:35,  1.98it/s]Running loglikelihood requests:  82%|████████▎ | 330/400 [05:34<00:35,  1.98it/s]Running loglikelihood requests:  83%|████████▎ | 331/400 [05:34<00:34,  1.97it/s]Running loglikelihood requests:  83%|████████▎ | 332/400 [05:35<00:34,  1.97it/s]Running loglikelihood requests:  83%|████████▎ | 333/400 [05:35<00:33,  1.98it/s]Running loglikelihood requests:  84%|████████▎ | 334/400 [05:36<00:33,  2.00it/s]Running loglikelihood requests:  84%|████████▍ | 335/400 [05:36<00:32,  2.01it/s]Running loglikelihood requests:  84%|████████▍ | 336/400 [05:37<00:31,  2.01it/s]Running loglikelihood requests:  84%|████████▍ | 337/400 [05:37<00:31,  2.02it/s]Running loglikelihood requests:  84%|████████▍ | 338/400 [05:38<00:30,  2.03it/s]Running loglikelihood requests:  85%|████████▍ | 339/400 [05:38<00:29,  2.03it/s]Running loglikelihood requests:  85%|████████▌ | 340/400 [05:39<00:29,  2.04it/s]Running loglikelihood requests:  85%|████████▌ | 341/400 [05:39<00:28,  2.04it/s]Running loglikelihood requests:  86%|████████▌ | 342/400 [05:40<00:28,  2.04it/s]Running loglikelihood requests:  86%|████████▌ | 343/400 [05:40<00:27,  2.05it/s]Running loglikelihood requests:  86%|████████▌ | 344/400 [05:41<00:27,  2.05it/s]Running loglikelihood requests:  86%|████████▋ | 345/400 [05:41<00:26,  2.06it/s]Running loglikelihood requests:  86%|████████▋ | 346/400 [05:42<00:26,  2.06it/s]Running loglikelihood requests:  87%|████████▋ | 347/400 [05:42<00:25,  2.06it/s]Running loglikelihood requests:  87%|████████▋ | 348/400 [05:43<00:25,  2.07it/s]Running loglikelihood requests:  87%|████████▋ | 349/400 [05:43<00:24,  2.07it/s]Running loglikelihood requests:  88%|████████▊ | 350/400 [05:44<00:24,  2.07it/s]Running loglikelihood requests:  88%|████████▊ | 351/400 [05:44<00:23,  2.07it/s]Running loglikelihood requests:  88%|████████▊ | 352/400 [05:45<00:23,  2.07it/s]Running loglikelihood requests:  88%|████████▊ | 353/400 [05:45<00:22,  2.08it/s]Running loglikelihood requests:  88%|████████▊ | 354/400 [05:46<00:22,  2.08it/s]Running loglikelihood requests:  89%|████████▉ | 355/400 [05:46<00:21,  2.09it/s]Running loglikelihood requests:  89%|████████▉ | 356/400 [05:47<00:21,  2.09it/s]Running loglikelihood requests:  89%|████████▉ | 357/400 [05:47<00:20,  2.10it/s]Running loglikelihood requests:  90%|████████▉ | 358/400 [05:48<00:19,  2.10it/s]Running loglikelihood requests:  90%|████████▉ | 359/400 [05:48<00:19,  2.11it/s]Running loglikelihood requests:  90%|█████████ | 360/400 [05:49<00:18,  2.11it/s]Running loglikelihood requests:  90%|█████████ | 361/400 [05:49<00:18,  2.11it/s]Running loglikelihood requests:  90%|█████████ | 362/400 [05:49<00:17,  2.11it/s]Running loglikelihood requests:  91%|█████████ | 363/400 [05:50<00:17,  2.12it/s]Running loglikelihood requests:  91%|█████████ | 364/400 [05:50<00:16,  2.13it/s]Running loglikelihood requests:  91%|█████████▏| 365/400 [05:51<00:16,  2.13it/s]Running loglikelihood requests:  92%|█████████▏| 366/400 [05:51<00:15,  2.13it/s]Running loglikelihood requests:  92%|█████████▏| 368/400 [05:52<00:11,  2.77it/s]Running loglikelihood requests:  92%|█████████▏| 369/400 [05:52<00:12,  2.58it/s]Running loglikelihood requests:  92%|█████████▎| 370/400 [05:53<00:12,  2.45it/s]Running loglikelihood requests:  93%|█████████▎| 371/400 [05:53<00:12,  2.35it/s]Running loglikelihood requests:  93%|█████████▎| 373/400 [05:54<00:09,  2.93it/s]Running loglikelihood requests:  94%|█████████▎| 374/400 [05:54<00:09,  2.70it/s]Running loglikelihood requests:  94%|█████████▍| 375/400 [05:55<00:09,  2.54it/s]Running loglikelihood requests:  94%|█████████▍| 376/400 [05:55<00:09,  2.43it/s]Running loglikelihood requests:  94%|█████████▍| 377/400 [05:56<00:09,  2.36it/s]Running loglikelihood requests:  94%|█████████▍| 378/400 [05:56<00:09,  2.31it/s]Running loglikelihood requests:  95%|█████████▍| 379/400 [05:56<00:09,  2.28it/s]Running loglikelihood requests:  95%|█████████▌| 380/400 [05:57<00:08,  2.26it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [05:57<00:08,  2.24it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [05:58<00:08,  2.24it/s]Running loglikelihood requests:  96%|█████████▌| 383/400 [05:58<00:07,  2.24it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [05:59<00:07,  2.25it/s]Running loglikelihood requests:  96%|█████████▋| 385/400 [05:59<00:06,  2.25it/s]Running loglikelihood requests:  96%|█████████▋| 386/400 [06:00<00:06,  2.25it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [06:00<00:05,  2.26it/s]Running loglikelihood requests:  97%|█████████▋| 388/400 [06:00<00:05,  2.26it/s]Running loglikelihood requests:  97%|█████████▋| 389/400 [06:01<00:04,  2.27it/s]Running loglikelihood requests:  98%|█████████▊| 390/400 [06:01<00:04,  2.27it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [06:02<00:03,  2.27it/s]Running loglikelihood requests:  98%|█████████▊| 392/400 [06:02<00:03,  2.28it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [06:03<00:03,  2.29it/s]Running loglikelihood requests:  98%|█████████▊| 394/400 [06:03<00:02,  2.29it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [06:04<00:02,  2.30it/s]Running loglikelihood requests:  99%|█████████▉| 396/400 [06:04<00:01,  2.30it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [06:04<00:01,  2.31it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [06:05<00:00,  2.31it/s]Running loglikelihood requests: 100%|█████████▉| 399/400 [06:05<00:00,  2.32it/s]Running loglikelihood requests: 100%|██████████| 400/400 [06:06<00:00,  2.33it/s]Running loglikelihood requests: 100%|██████████| 400/400 [06:06<00:00,  1.09it/s]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:1'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:1'}
full model:
{'sciq': {'alias': 'sciq', 'acc,none': 0.94, 'acc_stderr,none': 0.023868325657594204, 'acc_norm,none': 0.91, 'acc_norm_stderr,none': 0.028762349126466136}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.969062788859705
0.9024924890572922
0.7706109127217512
0.8221264026535647
0.9190490061886575
0.9866654579796295
0.6586322754204971
0.7962110384246164
0.8195614021629236
0.7124178311176441
0.787697814339696
0.7034455022322618
0.8136386046534271
0.8174990104652458
0.6784276389594894
0.8698440245672888
0.8886492811850213
0.6541737276411673
0.6560861559753316
0.8139845219953913
0.6714741870309046
0.6164364868717988
0.8331581872497299
0.9065420049234512
0.9246185715568276
0.7477515960551026
0.574165362968651
0.8586446364199891
0.8889771415746612
Total groups 70 exceeded the threshold, stopping comparison.
The group tensor is
[7, 3, 4, 2, 6, 1, 5, 0]
tensor([7, 3, 4, 2, 6, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 5, 3, 4, 0, 7, 1]
tensor([6, 2, 5, 3, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 3, 6, 2, 7, 1, 4, 0]
tensor([5, 3, 6, 2, 7, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 4, 2, 1, 3, 5, 1]
tensor([0, 0, 4, 2, 1, 3, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 3, 4, 5, 0, 1, 1]
tensor([0, 2, 3, 4, 5, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 0, 2, 2, 3, 1]
tensor([0, 3, 1, 0, 2, 2, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 3, 1, 1, 2, 2, 3, 0]
tensor([0, 3, 1, 1, 2, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1.0, 1.0, 1.0, 1.0, 1]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 4
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 6 to 9
done!
Normal merging for layer 10
tensor([0, 1])
tensor(0)
tensor([4, 7])
tensor(4)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 11 to 12
done!
Normal merging for layer 13
tensor([0, 5])
tensor(0)
tensor([6, 7])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 14 to 19
done!
Normal merging for layer 20
tensor([0, 3])
tensor(0)
tensor([2, 7])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([1, 6])
tensor(1)
done!
Normal merging for layer 21
tensor([0, 7])
tensor(0)
tensor([2, 3])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([1, 6])
tensor(1)
done!
Cross-layer merge completed for layers 22 to 23
done!
Normal merging for layer 24
tensor([0, 1])
tensor(0)
tensor([2, 3, 4, 5, 6, 7])
tensor(2)
done!
Cross-layer merge completed for layers 25 to 31
done!
all done!
Model size: 12.2608 GB
126
cuda:1
wsc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 25.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.86s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wsc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wsc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/wsc.fixed?recursive=False&expand=False HTTP/1.1" 307 148
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/wsc.fixed?recursive=False&expand=False HTTP/1.1" 200 356
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:filelock:Attempting to acquire lock 140545763873600 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140545763873600 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545763873600 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140545763873600 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140545763878064 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140545763878064 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545763878064 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140545763878064 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wsc from None to 0
INFO:lm_eval.api.task:Building contexts for wsc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 83651.85it/s]
DEBUG:lm_eval.evaluator:Task: wsc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<04:30,  1.36s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:10,  1.51it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:03<01:08,  2.82it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:03<01:11,  2.66it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:04<01:13,  2.57it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:05<01:14,  2.51it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:06<01:14,  2.47it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:07<01:14,  2.45it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:08<01:13,  2.45it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:08<01:13,  2.45it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:09<01:12,  2.45it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:10<01:11,  2.45it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:11<01:10,  2.45it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:12<01:09,  2.45it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:12<01:08,  2.46it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:13<01:06,  2.50it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:14<01:05,  2.51it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:15<01:04,  2.53it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:16<01:03,  2.55it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:16<01:02,  2.56it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:17<01:00,  2.57it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:18<01:01,  2.52it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:19<01:00,  2.55it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:19<00:58,  2.58it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:20<00:57,  2.60it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:21<00:56,  2.62it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:22<00:55,  2.63it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:22<00:53,  2.65it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:23<00:52,  2.67it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:24<00:51,  2.68it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:25<00:50,  2.69it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:25<00:49,  2.71it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:26<00:48,  2.72it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:27<00:47,  2.73it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:28<00:47,  2.74it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:28<00:46,  2.75it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:29<00:45,  2.76it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:30<00:44,  2.77it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:30<00:43,  2.78it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:31<00:42,  2.79it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:32<00:41,  2.82it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:32<00:40,  2.85it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:33<00:39,  2.87it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:34<00:38,  2.88it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:35<00:37,  2.88it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:35<00:36,  2.90it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:36<00:36,  2.91it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:37<00:34,  2.95it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:37<00:33,  2.98it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:38<00:32,  3.00it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:39<00:32,  3.02it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:39<00:31,  3.03it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:40<00:30,  3.04it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:41<00:29,  3.05it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:41<00:29,  3.05it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:42<00:28,  3.09it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:42<00:20,  4.06it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:43<00:16,  4.76it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:44<00:17,  4.30it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:44<00:18,  3.98it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:45<00:19,  3.77it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:45<00:19,  3.63it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:46<00:19,  3.53it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:47<00:19,  3.48it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:47<00:18,  3.43it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:48<00:18,  3.40it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:48<00:17,  3.39it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:49<00:17,  3.38it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:50<00:16,  3.38it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:50<00:16,  3.40it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:51<00:15,  3.41it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:51<00:14,  3.42it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:52<00:14,  3.43it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:53<00:13,  3.45it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:53<00:13,  3.46it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:54<00:12,  3.46it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:54<00:11,  3.48it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:55<00:11,  3.48it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [00:55<00:10,  3.49it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [00:56<00:10,  3.49it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [00:57<00:09,  3.50it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [00:57<00:08,  3.50it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [00:58<00:08,  3.50it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [00:58<00:07,  3.51it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [00:59<00:07,  3.49it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [00:59<00:06,  3.53it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:00<00:05,  3.57it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:01<00:05,  3.61it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:01<00:04,  3.65it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:02<00:04,  3.67it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:02<00:03,  3.68it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:03<00:02,  3.70it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:03<00:02,  3.73it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:04<00:01,  3.74it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:04<00:01,  3.76it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:05<00:00,  3.79it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:05<00:00,  3.80it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:05<00:00,  3.04it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:2'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:2'}
full model:
{'wsc': {'alias': 'wsc', 'acc,none': 0.39, 'acc_stderr,none': 0.04902071300001973}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8279363410631274
0.19299873358047231
0.24697427628971416
0.9498246079459217
0.9208790959024347
0.5433529265767616
0.7541489406957872
0.8642404861438043
0.9451768410410958
0.6364904470004392
0.8605038470072015
0.8806563523485148
0.7320941418431636
0.7167995199876819
0.9558598958577776
0.7974395317006763
0.8001507378395173
0.8689133172877098
0.6142386786363212
0.6039861720637983
0.8068446902119512
0.6154342295548068
0.8415314242927469
0.42042589459680124
0.7443787948515065
0.7081725962998161
0.7223774755032815
0.9338202429729527
0.7700364698774241
0.8279363410631274
0.19299873358047231
0.24697427628971416
0.9498246079459217
0.9208790959024347
0.5433529265767616
0.7541489406957872
0.8642404861438043
0.9451768410410958
0.6364904470004392
0.8605038470072015
0.8806563523485148
0.7320941418431636
0.7167995199876819
0.9558598958577776
0.7974395317006763
0.8001507378395173
0.8689133172877098
0.6142386786363212
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 7, 2, 4, 5, 1, 3, 0]
tensor([6, 7, 2, 4, 5, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 7, 3, 6, 2, 5, 4, 0]
tensor([1, 7, 3, 6, 2, 5, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 5, 6, 3, 7, 4, 2, 0]
tensor([1, 5, 6, 3, 7, 4, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 5, 3, 6, 7, 4, 2, 0]
tensor([1, 5, 3, 6, 7, 4, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 7, 5, 6, 1, 2, 3, 0]
tensor([4, 7, 5, 6, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 7, 1, 5, 6, 3, 2, 0]
tensor([4, 7, 1, 5, 6, 3, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1, 1.0, 0, 1.0]
tensor([0, 1, 1, 1, 1, 1, 0, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
done!
Normal merging for layer 6
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
done!
Normal merging for layer 7
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
done!
Cross-layer merge completed for layers 8 to 28
done!
Normal merging for layer 29
tensor([0, 6])
tensor(0)
tensor([1, 2, 3, 4, 5, 7])
tensor(1)
done!
Cross-layer merge completed for layers 30 to 31
done!
all done!
Model size: 12.3238 GB
21
cuda:2
mrpc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:40<00:40, 40.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.74s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: mrpc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: mrpc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
WARNING:lm_eval.api.task:[Task: mrpc] metric f1 is defined, but aggregation is not. using default aggregation=f1
WARNING:lm_eval.api.task:[Task: mrpc] metric f1 is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140546770417136 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140546770417136 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140546770417136 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140546770417136 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mrpc_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140545359718224 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140545359718224 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545359718224 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140545359718224 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mrpc from None to 0
INFO:lm_eval.api.task:Building contexts for mrpc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2476.02it/s]
DEBUG:lm_eval.evaluator:Task: mrpc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<04:52,  1.47s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:19,  1.42it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<01:53,  1.71it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:42,  1.88it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:05<01:36,  1.98it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:05<01:29,  2.11it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:06<01:25,  2.20it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:07<01:21,  2.26it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:08<01:19,  2.30it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:09<01:17,  2.34it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:10<01:15,  2.37it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:10<01:14,  2.39it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:11<01:12,  2.41it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:12<01:11,  2.43it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:13<01:10,  2.44it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:14<01:08,  2.46it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:14<01:07,  2.47it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:15<01:06,  2.49it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:16<01:05,  2.50it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:17<01:04,  2.50it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:18<01:03,  2.51it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:18<01:02,  2.52it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:19<01:01,  2.53it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:20<01:00,  2.54it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:21<00:59,  2.55it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:22<00:58,  2.56it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:22<00:57,  2.57it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:23<00:56,  2.58it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:24<00:55,  2.59it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:25<00:54,  2.60it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:25<00:53,  2.60it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:26<00:52,  2.61it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:27<00:51,  2.62it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:28<00:50,  2.62it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:28<00:49,  2.62it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:29<00:48,  2.64it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:30<00:47,  2.66it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:31<00:46,  2.66it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:31<00:46,  2.67it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:32<00:45,  2.68it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:33<00:44,  2.69it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:34<00:43,  2.69it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:34<00:42,  2.70it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:35<00:41,  2.70it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:36<00:40,  2.71it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:37<00:40,  2.72it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:37<00:39,  2.74it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:38<00:38,  2.75it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:39<00:37,  2.76it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:39<00:36,  2.77it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:40<00:35,  2.77it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:41<00:34,  2.78it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:42<00:34,  2.79it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:42<00:33,  2.79it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:43<00:32,  2.79it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:44<00:31,  2.80it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:44<00:31,  2.81it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:45<00:30,  2.81it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:46<00:29,  2.81it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:47<00:28,  2.82it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:47<00:27,  2.82it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:48<00:27,  2.83it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:49<00:26,  2.84it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:49<00:25,  2.84it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:50<00:24,  2.85it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:51<00:24,  2.86it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:51<00:23,  2.86it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:52<00:22,  2.87it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:53<00:21,  2.88it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:54<00:21,  2.88it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:54<00:20,  2.89it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:55<00:19,  2.90it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:56<00:18,  2.92it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:56<00:18,  2.92it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:57<00:17,  2.93it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:58<00:16,  2.94it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:58<00:15,  2.94it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:59<00:15,  2.94it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:00<00:14,  2.96it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:00<00:13,  2.97it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:01<00:13,  2.97it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:02<00:12,  2.98it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:02<00:11,  3.00it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:03<00:10,  3.02it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:04<00:10,  3.03it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:04<00:09,  3.04it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:05<00:08,  3.06it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:06<00:08,  3.08it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:06<00:07,  3.10it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:07<00:06,  3.13it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:07<00:06,  3.15it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:08<00:05,  3.17it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:09<00:04,  3.19it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:09<00:04,  3.22it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:10<00:03,  3.24it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:10<00:02,  3.26it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:11<00:02,  3.28it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:12<00:01,  3.30it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:12<00:00,  3.32it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:13<00:00,  3.37it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:13<00:00,  2.73it/s]
bootstrapping for stddev (sequential): f1_score
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<01:58,  1.19s/it]  2%|▏         | 2/100 [00:02<01:56,  1.19s/it]  3%|▎         | 3/100 [00:03<01:55,  1.19s/it]  4%|▍         | 4/100 [00:04<01:54,  1.19s/it]  5%|▌         | 5/100 [00:05<01:53,  1.19s/it]  6%|▌         | 6/100 [00:07<01:52,  1.19s/it]  7%|▋         | 7/100 [00:08<01:51,  1.19s/it]  8%|▊         | 8/100 [00:09<01:49,  1.19s/it]  9%|▉         | 9/100 [00:10<01:48,  1.19s/it] 10%|█         | 10/100 [00:11<01:47,  1.19s/it] 11%|█         | 11/100 [00:13<01:46,  1.19s/it] 12%|█▏        | 12/100 [00:14<01:45,  1.19s/it] 13%|█▎        | 13/100 [00:15<01:43,  1.19s/it] 14%|█▍        | 14/100 [00:16<01:42,  1.20s/it] 15%|█▌        | 15/100 [00:17<01:41,  1.20s/it] 16%|█▌        | 16/100 [00:19<01:41,  1.20s/it] 17%|█▋        | 17/100 [00:20<01:40,  1.21s/it] 18%|█▊        | 18/100 [00:21<01:38,  1.21s/it] 19%|█▉        | 19/100 [00:22<01:37,  1.20s/it] 20%|██        | 20/100 [00:23<01:35,  1.20s/it] 21%|██        | 21/100 [00:25<01:34,  1.20s/it] 22%|██▏       | 22/100 [00:26<01:33,  1.20s/it] 23%|██▎       | 23/100 [00:27<01:32,  1.19s/it] 24%|██▍       | 24/100 [00:28<01:30,  1.19s/it] 25%|██▌       | 25/100 [00:29<01:29,  1.20s/it] 26%|██▌       | 26/100 [00:31<01:28,  1.19s/it] 27%|██▋       | 27/100 [00:32<01:27,  1.19s/it] 28%|██▊       | 28/100 [00:33<01:25,  1.19s/it] 29%|██▉       | 29/100 [00:34<01:24,  1.20s/it] 30%|███       | 30/100 [00:35<01:23,  1.20s/it] 31%|███       | 31/100 [00:37<01:22,  1.20s/it] 32%|███▏      | 32/100 [00:38<01:21,  1.20s/it] 33%|███▎      | 33/100 [00:39<01:20,  1.20s/it] 34%|███▍      | 34/100 [00:40<01:19,  1.20s/it] 35%|███▌      | 35/100 [00:41<01:17,  1.20s/it] 36%|███▌      | 36/100 [00:43<01:16,  1.19s/it] 37%|███▋      | 37/100 [00:44<01:15,  1.19s/it] 38%|███▊      | 38/100 [00:45<01:14,  1.20s/it] 39%|███▉      | 39/100 [00:46<01:12,  1.19s/it] 40%|████      | 40/100 [00:47<01:11,  1.19s/it] 41%|████      | 41/100 [00:49<01:10,  1.19s/it] 42%|████▏     | 42/100 [00:50<01:09,  1.20s/it] 43%|████▎     | 43/100 [00:51<01:08,  1.20s/it] 44%|████▍     | 44/100 [00:52<01:06,  1.20s/it] 45%|████▌     | 45/100 [00:53<01:05,  1.20s/it] 46%|████▌     | 46/100 [00:55<01:04,  1.20s/it] 47%|████▋     | 47/100 [00:56<01:03,  1.19s/it] 48%|████▊     | 48/100 [00:57<01:02,  1.19s/it] 49%|████▉     | 49/100 [00:58<01:00,  1.19s/it] 50%|█████     | 50/100 [00:59<00:59,  1.20s/it] 51%|█████     | 51/100 [01:00<00:58,  1.20s/it] 52%|█████▏    | 52/100 [01:02<00:57,  1.20s/it] 53%|█████▎    | 53/100 [01:03<00:56,  1.20s/it] 54%|█████▍    | 54/100 [01:04<00:54,  1.19s/it] 55%|█████▌    | 55/100 [01:05<00:53,  1.20s/it] 56%|█████▌    | 56/100 [01:06<00:52,  1.20s/it] 57%|█████▋    | 57/100 [01:08<00:51,  1.20s/it] 58%|█████▊    | 58/100 [01:09<00:50,  1.19s/it] 59%|█████▉    | 59/100 [01:10<00:49,  1.20s/it] 60%|██████    | 60/100 [01:11<00:47,  1.19s/it] 61%|██████    | 61/100 [01:12<00:46,  1.19s/it] 62%|██████▏   | 62/100 [01:14<00:45,  1.19s/it] 63%|██████▎   | 63/100 [01:15<00:44,  1.20s/it] 64%|██████▍   | 64/100 [01:16<00:43,  1.20s/it] 65%|██████▌   | 65/100 [01:17<00:41,  1.19s/it] 66%|██████▌   | 66/100 [01:18<00:40,  1.19s/it] 67%|██████▋   | 67/100 [01:20<00:39,  1.20s/it] 68%|██████▊   | 68/100 [01:21<00:38,  1.20s/it] 69%|██████▉   | 69/100 [01:22<00:37,  1.20s/it] 70%|███████   | 70/100 [01:23<00:35,  1.20s/it] 71%|███████   | 71/100 [01:24<00:34,  1.20s/it] 72%|███████▏  | 72/100 [01:26<00:33,  1.19s/it] 73%|███████▎  | 73/100 [01:27<00:32,  1.19s/it] 74%|███████▍  | 74/100 [01:28<00:31,  1.19s/it] 75%|███████▌  | 75/100 [01:29<00:29,  1.20s/it] 76%|███████▌  | 76/100 [01:30<00:28,  1.19s/it] 77%|███████▋  | 77/100 [01:32<00:27,  1.19s/it] 78%|███████▊  | 78/100 [01:33<00:26,  1.19s/it] 79%|███████▉  | 79/100 [01:34<00:25,  1.19s/it] 80%|████████  | 80/100 [01:35<00:23,  1.20s/it] 81%|████████  | 81/100 [01:36<00:22,  1.20s/it] 82%|████████▏ | 82/100 [01:38<00:21,  1.19s/it] 83%|████████▎ | 83/100 [01:39<00:20,  1.19s/it] 84%|████████▍ | 84/100 [01:40<00:19,  1.19s/it] 85%|████████▌ | 85/100 [01:41<00:17,  1.19s/it] 86%|████████▌ | 86/100 [01:42<00:16,  1.19s/it] 87%|████████▋ | 87/100 [01:44<00:15,  1.19s/it] 88%|████████▊ | 88/100 [01:45<00:14,  1.20s/it] 89%|████████▉ | 89/100 [01:46<00:13,  1.20s/it] 90%|█████████ | 90/100 [01:47<00:11,  1.19s/it] 91%|█████████ | 91/100 [01:48<00:10,  1.19s/it] 92%|█████████▏| 92/100 [01:49<00:09,  1.20s/it] 93%|█████████▎| 93/100 [01:51<00:08,  1.20s/it] 94%|█████████▍| 94/100 [01:52<00:07,  1.19s/it] 95%|█████████▌| 95/100 [01:53<00:05,  1.19s/it] 96%|█████████▌| 96/100 [01:54<00:04,  1.20s/it] 97%|█████████▋| 97/100 [01:55<00:03,  1.19s/it] 98%|█████████▊| 98/100 [01:57<00:02,  1.19s/it] 99%|█████████▉| 99/100 [01:58<00:01,  1.19s/it]100%|██████████| 100/100 [01:59<00:00,  1.19s/it]100%|██████████| 100/100 [01:59<00:00,  1.20s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:3'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:3'}
full model:
{'mrpc': {'alias': 'mrpc', 'acc,none': 0.7, 'acc_stderr,none': 0.04605661864718383, 'f1,none': np.float64(0.8214285714285714), 'f1_stderr,none': 0.0323693653386572}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.32375514242269293
0.9664141198776434
0.4263233992087649
0.29258192731768157
0.3497718342079178
0.5968480671674146
0.7870107004084098
0.7068508532201193
0.7836897379939763
0.2579819071737284
0.5774144903645764
0.7819348699339372
0.7404680063618143
0.651835611327285
0.6832243011482585
0.34680079188801705
0.6033476505145444
0.6299526106979766
0.6468468631829007
0.8458578364778718
0.39447753640727073
0.5291234791393744
0.944540808938638
0.8460909093145242
0.4915397062049404
0.5314721523201049
0.8670806018560002
0.6451914961665037
0.7380077491787214
0.32375514242269293
0.9664141198776434
0.4263233992087649
0.29258192731768157
0.3497718342079178
0.5968480671674146
0.7870107004084098
0.7068508532201193
0.7836897379939763
0.2579819071737284
0.5774144903645764
0.7819348699339372
0.7404680063618143
0.651835611327285
0.6832243011482585
0.34680079188801705
0.6033476505145444
0.6299526106979766
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[4, 5, 7, 1, 6, 2, 3, 0]
tensor([4, 5, 7, 1, 6, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 4, 5, 0, 1, 2]
tensor([6, 3, 7, 4, 5, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 1, 7, 2, 6, 0, 3, 5]
tensor([4, 1, 7, 2, 6, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 5, 2, 7, 0, 4, 1]
tensor([6, 3, 5, 2, 7, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 6, 7, 3, 1, 0, 2]
tensor([4, 5, 6, 7, 3, 1, 0, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 2, 4, 1, 0, 5]
tensor([0, 3, 1, 2, 4, 1, 0, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1, 1.0]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 2
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 4 to 5
done!
Normal merging for layer 6
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
done!
Cross-layer merge completed for layers 7 to 9
done!
Normal merging for layer 10
tensor([0, 6])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([7])
tensor(7)
done!
Cross-layer merge completed for layers 11 to 26
done!
Normal merging for layer 27
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 28 to 30
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.3867 GB
179
cuda:3
rte
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.81s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: rte] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte?recursive=False&expand=False HTTP/1.1" 307 140
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue/tree/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte?recursive=False&expand=False HTTP/1.1" 200 354
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140542941720560 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140542941720560 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140542941720560 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140542941720560 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_rte_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140546770417136 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140546770417136 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140546770417136 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140546770417136 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of rte from None to 0
INFO:lm_eval.api.task:Building contexts for rte on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2573.92it/s]
DEBUG:lm_eval.evaluator:Task: rte; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:02<06:51,  2.07s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:03<03:39,  1.12s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:04<02:54,  1.12it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:06<02:30,  1.28it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:07<02:15,  1.41it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:08<02:05,  1.50it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:09<01:59,  1.57it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:10<01:54,  1.62it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:12<01:49,  1.66it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:13<01:46,  1.70it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:14<01:42,  1.74it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:15<01:39,  1.78it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:16<01:37,  1.80it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:17<01:33,  1.85it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:18<01:31,  1.88it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:19<01:28,  1.91it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:20<01:26,  1.93it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:21<01:24,  1.95it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:22<01:23,  1.96it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:23<01:21,  1.98it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:24<01:18,  2.02it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:25<01:16,  2.06it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:26<01:13,  2.10it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:27<01:11,  2.14it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:28<01:09,  2.19it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:28<01:06,  2.24it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:29<01:03,  2.30it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:30<01:01,  2.35it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:31<00:59,  2.41it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:32<00:57,  2.46it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:32<00:55,  2.49it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:33<00:54,  2.53it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:34<00:52,  2.55it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:35<00:51,  2.57it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:35<00:50,  2.60it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:36<00:49,  2.62it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:37<00:48,  2.64it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:38<00:46,  2.66it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:38<00:45,  2.69it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:39<00:44,  2.71it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:40<00:43,  2.74it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:40<00:42,  2.76it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:41<00:41,  2.78it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:42<00:40,  2.78it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:43<00:39,  2.81it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:43<00:38,  2.83it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:44<00:37,  2.85it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:45<00:36,  2.87it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:45<00:35,  2.88it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:46<00:34,  2.90it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:47<00:33,  2.91it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:47<00:33,  2.92it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:48<00:32,  2.94it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:49<00:31,  2.96it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:49<00:30,  2.98it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:50<00:29,  3.00it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:51<00:28,  3.01it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:51<00:28,  3.01it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:52<00:27,  3.02it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:53<00:26,  3.03it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:53<00:26,  3.04it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:54<00:25,  3.05it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:55<00:24,  3.06it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:55<00:23,  3.06it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:56<00:23,  3.07it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:57<00:22,  3.08it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:57<00:21,  3.10it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:58<00:20,  3.13it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:58<00:20,  3.14it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:59<00:19,  3.16it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:00<00:18,  3.17it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:00<00:17,  3.19it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:01<00:17,  3.20it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:02<00:16,  3.21it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:02<00:15,  3.22it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:03<00:15,  3.24it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:03<00:14,  3.24it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:04<00:13,  3.26it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:05<00:13,  3.27it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:05<00:12,  3.30it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:06<00:11,  3.32it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:06<00:11,  3.33it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:07<00:10,  3.34it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:08<00:09,  3.36it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:08<00:09,  3.38it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:09<00:08,  3.41it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:09<00:07,  3.43it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:10<00:07,  3.44it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:10<00:06,  3.46it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:11<00:06,  3.47it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:12<00:05,  3.48it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:12<00:04,  3.49it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:13<00:04,  3.49it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:13<00:03,  3.51it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:14<00:03,  3.53it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:14<00:02,  3.55it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:15<00:01,  3.56it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:16<00:01,  3.59it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:16<00:00,  3.67it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:17<00:00,  3.73it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:17<00:00,  2.60it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:4'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:4'}
full model:
{'rte': {'alias': 'rte', 'acc,none': 0.5, 'acc_stderr,none': 0.050251890762960605}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
0.5728915095234594
0.1682455054057436
0.93212414632396
0.9148362604533635
0.8268537756297094
0.7592245907029287
0.7256008379011685
0.7109756105942956
0.34161229456626735
0.905233410256777
0.5205040718697735
0.4121994254524892
0.7398116665887099
0.6225415196831932
0.7923970242263771
0.7353888240887675
0.6535613357308766
0.7757058271862038
0.734046359122903
0.4471799126982846
0.773619360921301
0.7955347039939479
0.8672068064531693
0.8652880343596522
0.3302235467760883
0.6789268064017625
0.6072221471952108
0.9194446824778495
0.4812004589187253
Total groups 73 exceeded the threshold, stopping comparison.
The group tensor is
[5, 2, 7, 1, 6, 4, 3, 0]
tensor([5, 2, 7, 1, 6, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 6, 0, 7, 3, 4, 1]
tensor([5, 2, 6, 0, 7, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 5, 4, 3, 0]
tensor([6, 1, 7, 2, 5, 4, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 2, 4, 1, 5, 0]
tensor([6, 3, 7, 2, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 5, 6, 2, 3, 1, 4, 0]
tensor([7, 5, 6, 2, 3, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 5, 4, 1, 0, 1, 3]
tensor([0, 2, 5, 4, 1, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 8
done!
Normal merging for layer 9
tensor([0, 5])
tensor(0)
tensor([4, 6])
tensor(4)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 10 to 30
done!
Normal merging for layer 31
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
all done!
Model size: 12.1348 GB
220
cuda:4
multirc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.98s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: multirc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: multirc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/multirc?recursive=False&expand=False HTTP/1.1" 307 146
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/multirc?recursive=False&expand=False HTTP/1.1" 200 365
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:filelock:Attempting to acquire lock 140514984887888 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140514984887888 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514984887888 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140514984887888 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140545763943936 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140545763943936 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545763943936 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140545763943936 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of multirc from None to 0
INFO:lm_eval.api.task:Building contexts for multirc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s] 62%|██████▏   | 62/100 [00:01<00:00, 40.78it/s]100%|██████████| 100/100 [00:01<00:00, 64.48it/s]
DEBUG:lm_eval.evaluator:Task: multirc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:03<10:46,  3.25s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:05<05:47,  1.76s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:08<04:50,  1.49s/it]Running loglikelihood requests:   4%|▎         | 7/200 [00:10<04:26,  1.38s/it]Running loglikelihood requests:   4%|▍         | 9/200 [00:13<04:13,  1.33s/it]Running loglikelihood requests:   6%|▌         | 11/200 [00:15<04:03,  1.29s/it]Running loglikelihood requests:   6%|▋         | 13/200 [00:18<03:56,  1.27s/it]Running loglikelihood requests:   8%|▊         | 15/200 [00:20<03:51,  1.25s/it]Running loglikelihood requests:   8%|▊         | 17/200 [00:22<03:47,  1.24s/it]Running loglikelihood requests:  10%|▉         | 19/200 [00:25<03:43,  1.23s/it]Running loglikelihood requests:  10%|█         | 21/200 [00:27<03:39,  1.23s/it]Running loglikelihood requests:  12%|█▏        | 23/200 [00:30<03:36,  1.22s/it]Running loglikelihood requests:  12%|█▎        | 25/200 [00:32<03:33,  1.22s/it]Running loglikelihood requests:  14%|█▎        | 27/200 [00:35<03:30,  1.22s/it]Running loglikelihood requests:  14%|█▍        | 29/200 [00:37<03:28,  1.22s/it]Running loglikelihood requests:  16%|█▌        | 31/200 [00:39<03:25,  1.22s/it]Running loglikelihood requests:  16%|█▋        | 33/200 [00:42<03:22,  1.22s/it]Running loglikelihood requests:  18%|█▊        | 35/200 [00:44<03:20,  1.21s/it]Running loglikelihood requests:  18%|█▊        | 37/200 [00:47<03:17,  1.21s/it]Running loglikelihood requests:  20%|█▉        | 39/200 [00:49<03:15,  1.21s/it]Running loglikelihood requests:  20%|██        | 41/200 [00:52<03:12,  1.21s/it]Running loglikelihood requests:  22%|██▏       | 43/200 [00:54<03:10,  1.21s/it]Running loglikelihood requests:  22%|██▎       | 45/200 [00:56<03:07,  1.21s/it]Running loglikelihood requests:  24%|██▎       | 47/200 [00:59<03:04,  1.21s/it]Running loglikelihood requests:  24%|██▍       | 49/200 [01:01<03:02,  1.21s/it]Running loglikelihood requests:  26%|██▌       | 51/200 [01:04<02:59,  1.21s/it]Running loglikelihood requests:  26%|██▋       | 53/200 [01:06<02:56,  1.20s/it]Running loglikelihood requests:  28%|██▊       | 55/200 [01:08<02:54,  1.20s/it]Running loglikelihood requests:  28%|██▊       | 57/200 [01:11<02:53,  1.21s/it]Running loglikelihood requests:  30%|██▉       | 59/200 [01:13<02:51,  1.21s/it]Running loglikelihood requests:  30%|███       | 61/200 [01:16<02:48,  1.21s/it]Running loglikelihood requests:  32%|███▏      | 63/200 [01:18<02:45,  1.21s/it]Running loglikelihood requests:  32%|███▎      | 65/200 [01:21<02:43,  1.21s/it]Running loglikelihood requests:  34%|███▎      | 67/200 [01:23<02:40,  1.21s/it]Running loglikelihood requests:  34%|███▍      | 69/200 [01:26<02:41,  1.23s/it]Running loglikelihood requests:  36%|███▌      | 71/200 [01:28<02:38,  1.23s/it]Running loglikelihood requests:  36%|███▋      | 73/200 [01:30<02:35,  1.23s/it]Running loglikelihood requests:  38%|███▊      | 75/200 [01:33<02:32,  1.22s/it]Running loglikelihood requests:  38%|███▊      | 77/200 [01:35<02:29,  1.22s/it]Running loglikelihood requests:  40%|███▉      | 79/200 [01:38<02:28,  1.23s/it]Running loglikelihood requests:  40%|████      | 81/200 [01:40<02:26,  1.23s/it]Running loglikelihood requests:  42%|████▏     | 83/200 [01:43<02:23,  1.23s/it]Running loglikelihood requests:  42%|████▎     | 85/200 [01:45<02:20,  1.22s/it]Running loglikelihood requests:  44%|████▎     | 87/200 [01:47<02:17,  1.21s/it]Running loglikelihood requests:  44%|████▍     | 89/200 [01:50<02:14,  1.21s/it]Running loglikelihood requests:  46%|████▌     | 91/200 [01:52<02:11,  1.21s/it]Running loglikelihood requests:  46%|████▋     | 93/200 [01:55<02:09,  1.21s/it]Running loglikelihood requests:  48%|████▊     | 95/200 [01:57<02:06,  1.21s/it]Running loglikelihood requests:  48%|████▊     | 97/200 [02:00<02:04,  1.21s/it]Running loglikelihood requests:  50%|████▉     | 99/200 [02:02<02:01,  1.20s/it]Running loglikelihood requests:  50%|█████     | 101/200 [02:04<01:59,  1.20s/it]Running loglikelihood requests:  52%|█████▏    | 103/200 [02:07<01:56,  1.20s/it]Running loglikelihood requests:  52%|█████▎    | 105/200 [02:09<01:53,  1.20s/it]Running loglikelihood requests:  54%|█████▎    | 107/200 [02:12<01:51,  1.20s/it]Running loglikelihood requests:  55%|█████▍    | 109/200 [02:14<01:48,  1.20s/it]Running loglikelihood requests:  56%|█████▌    | 111/200 [02:16<01:46,  1.20s/it]Running loglikelihood requests:  56%|█████▋    | 113/200 [02:19<01:43,  1.19s/it]Running loglikelihood requests:  57%|█████▊    | 115/200 [02:21<01:41,  1.19s/it]Running loglikelihood requests:  58%|█████▊    | 117/200 [02:23<01:38,  1.19s/it]Running loglikelihood requests:  60%|█████▉    | 119/200 [02:26<01:36,  1.19s/it]Running loglikelihood requests:  60%|██████    | 121/200 [02:28<01:34,  1.19s/it]Running loglikelihood requests:  62%|██████▏   | 123/200 [02:31<01:31,  1.19s/it]Running loglikelihood requests:  62%|██████▎   | 125/200 [02:33<01:29,  1.19s/it]Running loglikelihood requests:  64%|██████▎   | 127/200 [02:35<01:26,  1.19s/it]Running loglikelihood requests:  64%|██████▍   | 129/200 [02:38<01:24,  1.19s/it]Running loglikelihood requests:  66%|██████▌   | 131/200 [02:40<01:21,  1.18s/it]Running loglikelihood requests:  66%|██████▋   | 133/200 [02:42<01:19,  1.18s/it]Running loglikelihood requests:  68%|██████▊   | 135/200 [02:44<01:09,  1.07s/it]Running loglikelihood requests:  68%|██████▊   | 137/200 [02:45<01:01,  1.02it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [02:47<00:55,  1.09it/s]Running loglikelihood requests:  70%|███████   | 141/200 [02:49<00:51,  1.15it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [02:50<00:47,  1.20it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [02:52<00:44,  1.24it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [02:53<00:41,  1.27it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [02:55<00:39,  1.29it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [02:56<00:37,  1.31it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [02:57<00:35,  1.33it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [02:59<00:33,  1.34it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [03:00<00:31,  1.35it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [03:02<00:30,  1.36it/s]Running loglikelihood requests:  80%|████████  | 161/200 [03:03<00:28,  1.38it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [03:05<00:26,  1.41it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [03:06<00:24,  1.44it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [03:07<00:23,  1.43it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [03:09<00:21,  1.43it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [03:10<00:20,  1.44it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [03:11<00:18,  1.46it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [03:13<00:16,  1.48it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [03:14<00:15,  1.49it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [03:15<00:13,  1.51it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [03:17<00:12,  1.51it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [03:18<00:11,  1.53it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [03:19<00:09,  1.52it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [03:21<00:08,  1.51it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [03:22<00:07,  1.52it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [03:23<00:05,  1.54it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [03:24<00:04,  1.56it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [03:26<00:03,  1.57it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [03:27<00:01,  1.57it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [03:28<00:00,  1.58it/s]Running loglikelihood requests: 100%|██████████| 200/200 [03:28<00:00,  1.04s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:5'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:5'}
full model:
{'multirc': {'alias': 'multirc', 'acc,none': 0.54, 'acc_stderr,none': 0.05009082659620331}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
0.28954340172694804
0.7956189982678256
0.789982900191189
0.3644425339397572
0.37373995160413354
0.19965826892219962
0.3752330861186013
0.6279352732581062
0.3474029418882723
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
0.28954340172694804
0.7956189982678256
0.789982900191189
0.3644425339397572
0.37373995160413354
0.19965826892219962
0.3752330861186013
0.6279352732581062
0.3474029418882723
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 0, 6, 2, 4, 1]
tensor([5, 3, 7, 0, 6, 2, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 7, 4, 2, 1, 3, 6, 0]
tensor([5, 7, 4, 2, 1, 3, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 1, 3, 6, 0, 5, 7, 4]
tensor([2, 1, 3, 6, 0, 5, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 4, 1, 6, 0, 2, 3, 5]
tensor([7, 4, 1, 6, 0, 2, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 6, 3, 2, 5, 1, 7, 0]
tensor([4, 6, 3, 2, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 0, 2, 1, 4, 7, 3, 6]
tensor([5, 0, 2, 1, 4, 7, 3, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1, 1.0, 1.0, 1.0]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
done!
Normal merging for layer 2
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
done!
Normal merging for layer 6
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
done!
Normal merging for layer 7
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
done!
Cross-layer merge completed for layers 8 to 25
done!
Normal merging for layer 26
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 27 to 31
done!
all done!
Model size: 12.3238 GB
243
cuda:5
multirc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 25.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.10s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: multirc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: multirc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:filelock:Attempting to acquire lock 140545763773472 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140545763773472 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545763773472 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140545763773472 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140545763779616 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140545763779616 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545763779616 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140545763779616 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of multirc from None to 0
INFO:lm_eval.api.task:Building contexts for multirc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1248.15it/s]
DEBUG:lm_eval.evaluator:Task: multirc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:03<10:39,  3.21s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:05<05:51,  1.78s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:08<04:55,  1.51s/it]Running loglikelihood requests:   4%|▎         | 7/200 [00:10<04:31,  1.41s/it]Running loglikelihood requests:   4%|▍         | 9/200 [00:13<04:18,  1.35s/it]Running loglikelihood requests:   6%|▌         | 11/200 [00:15<04:09,  1.32s/it]Running loglikelihood requests:   6%|▋         | 13/200 [00:18<04:01,  1.29s/it]Running loglikelihood requests:   8%|▊         | 15/200 [00:20<03:55,  1.27s/it]Running loglikelihood requests:   8%|▊         | 17/200 [00:23<03:50,  1.26s/it]Running loglikelihood requests:  10%|▉         | 19/200 [00:25<03:45,  1.25s/it]Running loglikelihood requests:  10%|█         | 21/200 [00:28<03:41,  1.24s/it]Running loglikelihood requests:  12%|█▏        | 23/200 [00:30<03:38,  1.23s/it]Running loglikelihood requests:  12%|█▎        | 25/200 [00:33<03:35,  1.23s/it]Running loglikelihood requests:  14%|█▎        | 27/200 [00:35<03:31,  1.22s/it]Running loglikelihood requests:  14%|█▍        | 29/200 [00:37<03:28,  1.22s/it]Running loglikelihood requests:  16%|█▌        | 31/200 [00:40<03:26,  1.22s/it]Running loglikelihood requests:  16%|█▋        | 33/200 [00:42<03:23,  1.22s/it]Running loglikelihood requests:  18%|█▊        | 35/200 [00:45<03:20,  1.22s/it]Running loglikelihood requests:  18%|█▊        | 37/200 [00:47<03:17,  1.21s/it]Running loglikelihood requests:  20%|█▉        | 39/200 [00:49<03:15,  1.21s/it]Running loglikelihood requests:  20%|██        | 41/200 [00:52<03:12,  1.21s/it]Running loglikelihood requests:  22%|██▏       | 43/200 [00:54<03:09,  1.21s/it]Running loglikelihood requests:  22%|██▎       | 45/200 [00:57<03:07,  1.21s/it]Running loglikelihood requests:  24%|██▎       | 47/200 [00:59<03:04,  1.21s/it]Running loglikelihood requests:  24%|██▍       | 49/200 [01:02<03:02,  1.21s/it]Running loglikelihood requests:  26%|██▌       | 51/200 [01:04<02:59,  1.21s/it]Running loglikelihood requests:  26%|██▋       | 53/200 [01:06<02:57,  1.20s/it]Running loglikelihood requests:  28%|██▊       | 55/200 [01:09<02:54,  1.20s/it]Running loglikelihood requests:  28%|██▊       | 57/200 [01:11<02:51,  1.20s/it]Running loglikelihood requests:  30%|██▉       | 59/200 [01:14<02:51,  1.21s/it]Running loglikelihood requests:  30%|███       | 61/200 [01:16<02:48,  1.21s/it]Running loglikelihood requests:  32%|███▏      | 63/200 [01:18<02:45,  1.21s/it]Running loglikelihood requests:  32%|███▎      | 65/200 [01:21<02:42,  1.21s/it]Running loglikelihood requests:  34%|███▎      | 67/200 [01:23<02:40,  1.21s/it]Running loglikelihood requests:  34%|███▍      | 69/200 [01:26<02:37,  1.20s/it]Running loglikelihood requests:  36%|███▌      | 71/200 [01:28<02:35,  1.20s/it]Running loglikelihood requests:  36%|███▋      | 73/200 [01:30<02:32,  1.20s/it]Running loglikelihood requests:  38%|███▊      | 75/200 [01:33<02:30,  1.20s/it]Running loglikelihood requests:  38%|███▊      | 77/200 [01:35<02:28,  1.20s/it]Running loglikelihood requests:  40%|███▉      | 79/200 [01:38<02:26,  1.21s/it]Running loglikelihood requests:  40%|████      | 81/200 [01:40<02:25,  1.22s/it]Running loglikelihood requests:  42%|████▏     | 83/200 [01:43<02:24,  1.23s/it]Running loglikelihood requests:  42%|████▎     | 85/200 [01:45<02:22,  1.24s/it]Running loglikelihood requests:  44%|████▎     | 87/200 [01:48<02:20,  1.24s/it]Running loglikelihood requests:  44%|████▍     | 89/200 [01:50<02:16,  1.23s/it]Running loglikelihood requests:  46%|████▌     | 91/200 [01:53<02:13,  1.23s/it]Running loglikelihood requests:  46%|████▋     | 93/200 [01:55<02:11,  1.23s/it]Running loglikelihood requests:  48%|████▊     | 95/200 [01:57<02:08,  1.22s/it]Running loglikelihood requests:  48%|████▊     | 97/200 [02:00<02:05,  1.22s/it]Running loglikelihood requests:  50%|████▉     | 99/200 [02:02<02:03,  1.22s/it]Running loglikelihood requests:  50%|█████     | 101/200 [02:05<02:00,  1.22s/it]Running loglikelihood requests:  52%|█████▏    | 103/200 [02:07<01:58,  1.22s/it]Running loglikelihood requests:  52%|█████▎    | 105/200 [02:10<01:55,  1.22s/it]Running loglikelihood requests:  54%|█████▎    | 107/200 [02:12<01:53,  1.22s/it]Running loglikelihood requests:  55%|█████▍    | 109/200 [02:15<01:50,  1.22s/it]Running loglikelihood requests:  56%|█████▌    | 111/200 [02:17<01:47,  1.21s/it]Running loglikelihood requests:  56%|█████▋    | 113/200 [02:19<01:45,  1.21s/it]Running loglikelihood requests:  57%|█████▊    | 115/200 [02:22<01:42,  1.21s/it]Running loglikelihood requests:  58%|█████▊    | 117/200 [02:24<01:40,  1.20s/it]Running loglikelihood requests:  60%|█████▉    | 119/200 [02:27<01:37,  1.20s/it]Running loglikelihood requests:  60%|██████    | 121/200 [02:29<01:34,  1.20s/it]Running loglikelihood requests:  62%|██████▏   | 123/200 [02:31<01:32,  1.20s/it]Running loglikelihood requests:  62%|██████▎   | 125/200 [02:34<01:29,  1.20s/it]Running loglikelihood requests:  64%|██████▎   | 127/200 [02:36<01:27,  1.20s/it]Running loglikelihood requests:  64%|██████▍   | 129/200 [02:38<01:25,  1.20s/it]Running loglikelihood requests:  66%|██████▌   | 131/200 [02:41<01:22,  1.20s/it]Running loglikelihood requests:  66%|██████▋   | 133/200 [02:43<01:20,  1.20s/it]Running loglikelihood requests:  68%|██████▊   | 135/200 [02:45<01:10,  1.08s/it]Running loglikelihood requests:  68%|██████▊   | 137/200 [02:46<01:02,  1.01it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [02:48<00:56,  1.08it/s]Running loglikelihood requests:  70%|███████   | 141/200 [02:50<00:51,  1.14it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [02:51<00:48,  1.19it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [02:53<00:45,  1.22it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [02:54<00:42,  1.25it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [02:56<00:40,  1.27it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [02:57<00:37,  1.29it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [02:59<00:35,  1.31it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [03:00<00:33,  1.33it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [03:02<00:32,  1.34it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [03:03<00:30,  1.34it/s]Running loglikelihood requests:  80%|████████  | 161/200 [03:04<00:28,  1.36it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [03:06<00:26,  1.39it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [03:07<00:24,  1.42it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [03:08<00:22,  1.44it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [03:10<00:21,  1.44it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [03:11<00:19,  1.45it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [03:12<00:18,  1.48it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [03:14<00:16,  1.50it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [03:15<00:15,  1.51it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [03:16<00:13,  1.52it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [03:18<00:12,  1.53it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [03:19<00:11,  1.54it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [03:20<00:09,  1.55it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [03:21<00:08,  1.56it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [03:23<00:07,  1.57it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [03:24<00:05,  1.57it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [03:25<00:04,  1.57it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [03:27<00:03,  1.58it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [03:28<00:01,  1.58it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [03:29<00:00,  1.58it/s]Running loglikelihood requests: 100%|██████████| 200/200 [03:29<00:00,  1.05s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:6'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:6'}
full model:
{'multirc': {'alias': 'multirc', 'acc,none': 0.54, 'acc_stderr,none': 0.05009082659620331}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
0.28954340172694804
0.7956189982678256
0.789982900191189
0.3644425339397572
0.37373995160413354
0.19965826892219962
0.3752330861186013
0.6279352732581062
0.3474029418882723
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
0.28954340172694804
0.7956189982678256
0.789982900191189
0.3644425339397572
0.37373995160413354
0.19965826892219962
0.3752330861186013
0.6279352732581062
0.3474029418882723
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 0, 6, 2, 4, 1]
tensor([5, 3, 7, 0, 6, 2, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 7, 4, 2, 1, 3, 6, 0]
tensor([5, 7, 4, 2, 1, 3, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 1, 3, 6, 0, 5, 7, 4]
tensor([2, 1, 3, 6, 0, 5, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 4, 1, 6, 0, 2, 3, 5]
tensor([7, 4, 1, 6, 0, 2, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 6, 3, 2, 5, 1, 7, 0]
tensor([4, 6, 3, 2, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 0, 2, 1, 4, 7, 3, 6]
tensor([5, 0, 2, 1, 4, 7, 3, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1, 1.0, 1.0, 1.0]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
done!
Normal merging for layer 2
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
done!
Normal merging for layer 6
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
done!
Normal merging for layer 7
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
done!
Cross-layer merge completed for layers 8 to 25
done!
Normal merging for layer 26
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 27 to 31
done!
all done!
Model size: 12.3238 GB
172
cuda:6
logiqa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:40<00:40, 40.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.63s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/logiqa HTTP/1.1" 200 743
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/EleutherAI/logiqa/EleutherAI/logiqa.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): datasets-server.hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://datasets-server.hf-mirror.com:443 "GET /parquet?dataset=EleutherAI/logiqa HTTP/1.1" 302 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET / HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/EleutherAI/logiqa/EleutherAI/logiqa.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/logiqa/resolve/main/logiqa.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/logiqa/resolve/main/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/logiqa/resolve/main/README.md HTTP/1.1" 200 0
DEBUG:filelock:Attempting to acquire lock 140545763947104 on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Lock 140545763947104 acquired on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Attempting to release lock 140545763947104 on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Lock 140545763947104 released on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Attempting to acquire lock 140522205063424 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:filelock:Lock 140522205063424 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81/dataset_info.json
DEBUG:filelock:Attempting to release lock 140522205063424 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:filelock:Lock 140522205063424 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:filelock:Attempting to acquire lock 140555779278656 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:filelock:Lock 140555779278656 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81/dataset_info.json
DEBUG:filelock:Attempting to release lock 140555779278656 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:filelock:Lock 140555779278656 released on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of logiqa from None to 0
INFO:lm_eval.api.task:Building contexts for logiqa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 3114.23it/s]
DEBUG:lm_eval.evaluator:Task: logiqa; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:02<18:54,  2.84s/it]Running loglikelihood requests:   0%|          | 2/400 [00:05<16:44,  2.52s/it]Running loglikelihood requests:   1%|          | 3/400 [00:07<15:53,  2.40s/it]Running loglikelihood requests:   1%|          | 4/400 [00:09<15:23,  2.33s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:11<15:01,  2.28s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:14<14:45,  2.25s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:16<14:35,  2.23s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:18<14:26,  2.21s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:20<14:18,  2.20s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:22<14:08,  2.18s/it]Running loglikelihood requests:   3%|▎         | 11/400 [00:24<13:56,  2.15s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:26<13:43,  2.12s/it]Running loglikelihood requests:   3%|▎         | 13/400 [00:28<13:34,  2.10s/it]Running loglikelihood requests:   4%|▎         | 14/400 [00:30<13:26,  2.09s/it]Running loglikelihood requests:   4%|▍         | 15/400 [00:32<13:20,  2.08s/it]Running loglikelihood requests:   4%|▍         | 16/400 [00:35<13:15,  2.07s/it]Running loglikelihood requests:   4%|▍         | 17/400 [00:37<13:10,  2.06s/it]Running loglikelihood requests:   4%|▍         | 18/400 [00:39<13:05,  2.06s/it]Running loglikelihood requests:   5%|▍         | 19/400 [00:41<12:59,  2.05s/it]Running loglikelihood requests:   5%|▌         | 20/400 [00:43<12:54,  2.04s/it]Running loglikelihood requests:   5%|▌         | 21/400 [00:45<12:48,  2.03s/it]Running loglikelihood requests:   6%|▌         | 22/400 [00:47<12:43,  2.02s/it]Running loglikelihood requests:   6%|▌         | 23/400 [00:49<12:38,  2.01s/it]Running loglikelihood requests:   6%|▌         | 24/400 [00:51<12:34,  2.01s/it]Running loglikelihood requests:   6%|▋         | 25/400 [00:53<12:28,  2.00s/it]Running loglikelihood requests:   6%|▋         | 26/400 [00:55<12:22,  1.99s/it]Running loglikelihood requests:   7%|▋         | 27/400 [00:57<12:16,  1.97s/it]Running loglikelihood requests:   7%|▋         | 28/400 [00:58<12:11,  1.97s/it]Running loglikelihood requests:   7%|▋         | 29/400 [01:00<12:05,  1.96s/it]Running loglikelihood requests:   8%|▊         | 30/400 [01:02<12:00,  1.95s/it]Running loglikelihood requests:   8%|▊         | 31/400 [01:04<11:56,  1.94s/it]Running loglikelihood requests:   8%|▊         | 32/400 [01:06<11:52,  1.94s/it]Running loglikelihood requests:   8%|▊         | 33/400 [01:08<11:49,  1.93s/it]Running loglikelihood requests:   8%|▊         | 34/400 [01:10<11:46,  1.93s/it]Running loglikelihood requests:   9%|▉         | 35/400 [01:12<11:42,  1.92s/it]Running loglikelihood requests:   9%|▉         | 36/400 [01:14<11:40,  1.92s/it]Running loglikelihood requests:   9%|▉         | 37/400 [01:16<11:34,  1.91s/it]Running loglikelihood requests:  10%|▉         | 38/400 [01:18<11:28,  1.90s/it]Running loglikelihood requests:  10%|▉         | 39/400 [01:20<11:23,  1.89s/it]Running loglikelihood requests:  10%|█         | 40/400 [01:21<11:18,  1.89s/it]Running loglikelihood requests:  10%|█         | 41/400 [01:23<11:14,  1.88s/it]Running loglikelihood requests:  10%|█         | 42/400 [01:25<11:10,  1.87s/it]Running loglikelihood requests:  11%|█         | 43/400 [01:27<11:16,  1.89s/it]Running loglikelihood requests:  11%|█         | 44/400 [01:29<11:17,  1.90s/it]Running loglikelihood requests:  11%|█▏        | 45/400 [01:31<11:17,  1.91s/it]Running loglikelihood requests:  12%|█▏        | 46/400 [01:33<11:08,  1.89s/it]Running loglikelihood requests:  12%|█▏        | 47/400 [01:35<10:59,  1.87s/it]Running loglikelihood requests:  12%|█▏        | 48/400 [01:36<10:53,  1.86s/it]Running loglikelihood requests:  12%|█▏        | 49/400 [01:38<10:48,  1.85s/it]Running loglikelihood requests:  12%|█▎        | 50/400 [01:40<10:43,  1.84s/it]Running loglikelihood requests:  13%|█▎        | 51/400 [01:42<10:40,  1.84s/it]Running loglikelihood requests:  13%|█▎        | 52/400 [01:44<10:37,  1.83s/it]Running loglikelihood requests:  13%|█▎        | 53/400 [01:45<10:34,  1.83s/it]Running loglikelihood requests:  14%|█▎        | 54/400 [01:47<10:31,  1.82s/it]Running loglikelihood requests:  14%|█▍        | 55/400 [01:49<10:27,  1.82s/it]Running loglikelihood requests:  14%|█▍        | 56/400 [01:51<10:23,  1.81s/it]Running loglikelihood requests:  14%|█▍        | 57/400 [01:53<10:16,  1.80s/it]Running loglikelihood requests:  14%|█▍        | 58/400 [01:54<10:11,  1.79s/it]Running loglikelihood requests:  15%|█▍        | 59/400 [01:56<10:07,  1.78s/it]Running loglikelihood requests:  15%|█▌        | 60/400 [01:58<10:03,  1.77s/it]Running loglikelihood requests:  15%|█▌        | 61/400 [02:00<09:57,  1.76s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [02:01<09:51,  1.75s/it]Running loglikelihood requests:  16%|█▌        | 63/400 [02:03<09:46,  1.74s/it]Running loglikelihood requests:  16%|█▌        | 64/400 [02:05<09:44,  1.74s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [02:07<09:38,  1.73s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [02:08<09:34,  1.72s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [02:10<09:28,  1.71s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [02:12<09:24,  1.70s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [02:13<09:19,  1.69s/it]Running loglikelihood requests:  18%|█▊        | 70/400 [02:15<09:16,  1.68s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [02:17<09:12,  1.68s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [02:18<09:09,  1.67s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [02:20<09:04,  1.67s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [02:22<09:00,  1.66s/it]Running loglikelihood requests:  19%|█▉        | 75/400 [02:23<08:56,  1.65s/it]Running loglikelihood requests:  19%|█▉        | 76/400 [02:25<08:52,  1.64s/it]Running loglikelihood requests:  19%|█▉        | 77/400 [02:26<08:46,  1.63s/it]Running loglikelihood requests:  20%|█▉        | 78/400 [02:28<08:43,  1.62s/it]Running loglikelihood requests:  20%|█▉        | 79/400 [02:30<08:38,  1.62s/it]Running loglikelihood requests:  20%|██        | 80/400 [02:31<08:34,  1.61s/it]Running loglikelihood requests:  20%|██        | 81/400 [02:33<08:29,  1.60s/it]Running loglikelihood requests:  20%|██        | 82/400 [02:34<08:25,  1.59s/it]Running loglikelihood requests:  21%|██        | 83/400 [02:36<08:21,  1.58s/it]Running loglikelihood requests:  21%|██        | 84/400 [02:38<08:18,  1.58s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [02:39<08:15,  1.57s/it]Running loglikelihood requests:  22%|██▏       | 86/400 [02:41<08:11,  1.57s/it]Running loglikelihood requests:  22%|██▏       | 87/400 [02:42<08:08,  1.56s/it]Running loglikelihood requests:  22%|██▏       | 88/400 [02:44<08:06,  1.56s/it]Running loglikelihood requests:  22%|██▏       | 89/400 [02:45<08:04,  1.56s/it]Running loglikelihood requests:  22%|██▎       | 90/400 [02:47<08:02,  1.56s/it]Running loglikelihood requests:  23%|██▎       | 91/400 [02:48<08:00,  1.55s/it]Running loglikelihood requests:  23%|██▎       | 92/400 [02:50<07:58,  1.55s/it]Running loglikelihood requests:  23%|██▎       | 93/400 [02:51<07:55,  1.55s/it]Running loglikelihood requests:  24%|██▎       | 94/400 [02:53<07:55,  1.55s/it]Running loglikelihood requests:  24%|██▍       | 95/400 [02:55<08:00,  1.58s/it]Running loglikelihood requests:  24%|██▍       | 96/400 [02:56<07:59,  1.58s/it]Running loglikelihood requests:  24%|██▍       | 97/400 [02:58<07:54,  1.57s/it]Running loglikelihood requests:  24%|██▍       | 98/400 [02:59<07:50,  1.56s/it]Running loglikelihood requests:  25%|██▍       | 99/400 [03:01<07:47,  1.55s/it]Running loglikelihood requests:  25%|██▌       | 100/400 [03:02<07:44,  1.55s/it]Running loglikelihood requests:  25%|██▌       | 101/400 [03:04<07:41,  1.54s/it]Running loglikelihood requests:  26%|██▌       | 102/400 [03:05<07:39,  1.54s/it]Running loglikelihood requests:  26%|██▌       | 103/400 [03:07<07:37,  1.54s/it]Running loglikelihood requests:  26%|██▌       | 104/400 [03:09<07:34,  1.54s/it]Running loglikelihood requests:  26%|██▋       | 105/400 [03:10<07:32,  1.53s/it]Running loglikelihood requests:  26%|██▋       | 106/400 [03:12<07:30,  1.53s/it]Running loglikelihood requests:  27%|██▋       | 107/400 [03:13<07:27,  1.53s/it]Running loglikelihood requests:  27%|██▋       | 108/400 [03:15<07:24,  1.52s/it]Running loglikelihood requests:  27%|██▋       | 109/400 [03:16<07:22,  1.52s/it]Running loglikelihood requests:  28%|██▊       | 110/400 [03:18<07:20,  1.52s/it]Running loglikelihood requests:  28%|██▊       | 111/400 [03:19<07:17,  1.51s/it]Running loglikelihood requests:  28%|██▊       | 112/400 [03:21<07:15,  1.51s/it]Running loglikelihood requests:  28%|██▊       | 113/400 [03:22<07:12,  1.51s/it]Running loglikelihood requests:  28%|██▊       | 114/400 [03:24<07:10,  1.50s/it]Running loglikelihood requests:  29%|██▉       | 115/400 [03:25<07:07,  1.50s/it]Running loglikelihood requests:  29%|██▉       | 116/400 [03:27<07:05,  1.50s/it]Running loglikelihood requests:  29%|██▉       | 117/400 [03:28<07:03,  1.50s/it]Running loglikelihood requests:  30%|██▉       | 118/400 [03:30<07:00,  1.49s/it]Running loglikelihood requests:  30%|██▉       | 119/400 [03:31<06:58,  1.49s/it]Running loglikelihood requests:  30%|███       | 120/400 [03:33<06:55,  1.48s/it]Running loglikelihood requests:  30%|███       | 121/400 [03:34<06:54,  1.48s/it]Running loglikelihood requests:  30%|███       | 122/400 [03:36<06:51,  1.48s/it]Running loglikelihood requests:  31%|███       | 123/400 [03:37<06:50,  1.48s/it]Running loglikelihood requests:  31%|███       | 124/400 [03:39<06:48,  1.48s/it]Running loglikelihood requests:  31%|███▏      | 125/400 [03:40<06:46,  1.48s/it]Running loglikelihood requests:  32%|███▏      | 126/400 [03:41<06:44,  1.48s/it]Running loglikelihood requests:  32%|███▏      | 127/400 [03:43<06:42,  1.47s/it]Running loglikelihood requests:  32%|███▏      | 128/400 [03:44<06:40,  1.47s/it]Running loglikelihood requests:  32%|███▏      | 129/400 [03:46<06:37,  1.47s/it]Running loglikelihood requests:  32%|███▎      | 130/400 [03:47<06:35,  1.46s/it]Running loglikelihood requests:  33%|███▎      | 131/400 [03:49<06:33,  1.46s/it]Running loglikelihood requests:  33%|███▎      | 132/400 [03:50<06:31,  1.46s/it]Running loglikelihood requests:  33%|███▎      | 133/400 [03:52<06:29,  1.46s/it]Running loglikelihood requests:  34%|███▎      | 134/400 [03:53<06:27,  1.46s/it]Running loglikelihood requests:  34%|███▍      | 135/400 [03:55<06:25,  1.46s/it]Running loglikelihood requests:  34%|███▍      | 136/400 [03:56<06:24,  1.46s/it]Running loglikelihood requests:  34%|███▍      | 137/400 [03:57<06:22,  1.45s/it]Running loglikelihood requests:  34%|███▍      | 138/400 [03:59<06:20,  1.45s/it]Running loglikelihood requests:  35%|███▍      | 139/400 [04:00<06:19,  1.45s/it]Running loglikelihood requests:  35%|███▌      | 140/400 [04:02<06:23,  1.48s/it]Running loglikelihood requests:  36%|███▌      | 142/400 [04:03<04:54,  1.14s/it]Running loglikelihood requests:  36%|███▌      | 143/400 [04:05<05:14,  1.22s/it]Running loglikelihood requests:  36%|███▌      | 144/400 [04:06<05:29,  1.29s/it]Running loglikelihood requests:  36%|███▋      | 145/400 [04:08<05:40,  1.33s/it]Running loglikelihood requests:  36%|███▋      | 146/400 [04:09<05:48,  1.37s/it]Running loglikelihood requests:  37%|███▋      | 147/400 [04:11<05:53,  1.40s/it]Running loglikelihood requests:  37%|███▋      | 148/400 [04:12<05:56,  1.42s/it]Running loglikelihood requests:  37%|███▋      | 149/400 [04:14<05:58,  1.43s/it]Running loglikelihood requests:  38%|███▊      | 150/400 [04:15<05:59,  1.44s/it]Running loglikelihood requests:  38%|███▊      | 151/400 [04:17<05:59,  1.44s/it]Running loglikelihood requests:  38%|███▊      | 152/400 [04:18<05:59,  1.45s/it]Running loglikelihood requests:  38%|███▊      | 153/400 [04:19<05:58,  1.45s/it]Running loglikelihood requests:  38%|███▊      | 154/400 [04:21<05:57,  1.45s/it]Running loglikelihood requests:  39%|███▉      | 155/400 [04:22<05:56,  1.46s/it]Running loglikelihood requests:  39%|███▉      | 156/400 [04:24<05:55,  1.46s/it]Running loglikelihood requests:  39%|███▉      | 157/400 [04:25<05:52,  1.45s/it]Running loglikelihood requests:  40%|███▉      | 158/400 [04:27<05:49,  1.44s/it]Running loglikelihood requests:  40%|███▉      | 159/400 [04:28<05:46,  1.44s/it]Running loglikelihood requests:  40%|████      | 160/400 [04:30<05:44,  1.43s/it]Running loglikelihood requests:  40%|████      | 161/400 [04:31<05:42,  1.43s/it]Running loglikelihood requests:  40%|████      | 162/400 [04:32<05:40,  1.43s/it]Running loglikelihood requests:  41%|████      | 163/400 [04:34<05:38,  1.43s/it]Running loglikelihood requests:  41%|████      | 164/400 [04:35<05:43,  1.46s/it]Running loglikelihood requests:  41%|████▏     | 165/400 [04:37<05:46,  1.47s/it]Running loglikelihood requests:  42%|████▏     | 166/400 [04:38<05:44,  1.47s/it]Running loglikelihood requests:  42%|████▏     | 167/400 [04:40<05:41,  1.46s/it]Running loglikelihood requests:  42%|████▏     | 168/400 [04:41<05:35,  1.44s/it]Running loglikelihood requests:  42%|████▏     | 169/400 [04:43<05:30,  1.43s/it]Running loglikelihood requests:  42%|████▎     | 170/400 [04:44<05:26,  1.42s/it]Running loglikelihood requests:  43%|████▎     | 171/400 [04:45<05:23,  1.41s/it]Running loglikelihood requests:  43%|████▎     | 172/400 [04:47<05:20,  1.40s/it]Running loglikelihood requests:  43%|████▎     | 173/400 [04:48<05:17,  1.40s/it]Running loglikelihood requests:  44%|████▎     | 174/400 [04:50<05:15,  1.40s/it]Running loglikelihood requests:  44%|████▍     | 175/400 [04:51<05:13,  1.39s/it]Running loglikelihood requests:  44%|████▍     | 176/400 [04:52<05:11,  1.39s/it]Running loglikelihood requests:  44%|████▍     | 177/400 [04:54<05:09,  1.39s/it]Running loglikelihood requests:  44%|████▍     | 178/400 [04:55<05:07,  1.39s/it]Running loglikelihood requests:  45%|████▍     | 179/400 [04:56<05:06,  1.39s/it]Running loglikelihood requests:  45%|████▌     | 180/400 [04:58<05:03,  1.38s/it]Running loglikelihood requests:  45%|████▌     | 181/400 [04:59<05:01,  1.38s/it]Running loglikelihood requests:  46%|████▌     | 182/400 [05:01<04:59,  1.38s/it]Running loglikelihood requests:  46%|████▌     | 183/400 [05:02<04:57,  1.37s/it]Running loglikelihood requests:  46%|████▌     | 184/400 [05:03<04:55,  1.37s/it]Running loglikelihood requests:  46%|████▋     | 185/400 [05:05<04:53,  1.37s/it]Running loglikelihood requests:  46%|████▋     | 186/400 [05:06<04:51,  1.36s/it]Running loglikelihood requests:  47%|████▋     | 187/400 [05:07<04:50,  1.36s/it]Running loglikelihood requests:  47%|████▋     | 188/400 [05:09<04:48,  1.36s/it]Running loglikelihood requests:  47%|████▋     | 189/400 [05:10<04:46,  1.36s/it]Running loglikelihood requests:  48%|████▊     | 190/400 [05:11<04:45,  1.36s/it]Running loglikelihood requests:  48%|████▊     | 191/400 [05:13<04:43,  1.36s/it]Running loglikelihood requests:  48%|████▊     | 192/400 [05:14<04:41,  1.35s/it]Running loglikelihood requests:  48%|████▊     | 193/400 [05:15<04:39,  1.35s/it]Running loglikelihood requests:  48%|████▊     | 194/400 [05:17<04:37,  1.35s/it]Running loglikelihood requests:  49%|████▉     | 195/400 [05:18<04:35,  1.34s/it]Running loglikelihood requests:  49%|████▉     | 196/400 [05:20<04:33,  1.34s/it]Running loglikelihood requests:  49%|████▉     | 197/400 [05:21<04:31,  1.34s/it]Running loglikelihood requests:  50%|████▉     | 198/400 [05:22<04:30,  1.34s/it]Running loglikelihood requests:  50%|████▉     | 199/400 [05:24<04:28,  1.33s/it]Running loglikelihood requests:  50%|█████     | 200/400 [05:25<04:26,  1.33s/it]Running loglikelihood requests:  50%|█████     | 201/400 [05:26<04:24,  1.33s/it]Running loglikelihood requests:  50%|█████     | 202/400 [05:27<04:22,  1.33s/it]Running loglikelihood requests:  51%|█████     | 203/400 [05:29<04:21,  1.33s/it]Running loglikelihood requests:  51%|█████     | 204/400 [05:30<04:19,  1.33s/it]Running loglikelihood requests:  51%|█████▏    | 205/400 [05:31<04:18,  1.33s/it]Running loglikelihood requests:  52%|█████▏    | 206/400 [05:33<04:17,  1.33s/it]Running loglikelihood requests:  52%|█████▏    | 207/400 [05:34<04:15,  1.32s/it]Running loglikelihood requests:  52%|█████▏    | 208/400 [05:35<04:17,  1.34s/it]Running loglikelihood requests:  52%|█████▏    | 209/400 [05:37<04:16,  1.34s/it]Running loglikelihood requests:  52%|█████▎    | 210/400 [05:38<04:14,  1.34s/it]Running loglikelihood requests:  53%|█████▎    | 211/400 [05:40<04:13,  1.34s/it]Running loglikelihood requests:  53%|█████▎    | 212/400 [05:41<04:12,  1.34s/it]Running loglikelihood requests:  53%|█████▎    | 213/400 [05:42<04:10,  1.34s/it]Running loglikelihood requests:  54%|█████▎    | 214/400 [05:44<04:08,  1.34s/it]Running loglikelihood requests:  54%|█████▍    | 215/400 [05:45<04:07,  1.34s/it]Running loglikelihood requests:  54%|█████▍    | 216/400 [05:46<04:05,  1.34s/it]Running loglikelihood requests:  54%|█████▍    | 217/400 [05:48<04:04,  1.34s/it]Running loglikelihood requests:  55%|█████▍    | 218/400 [05:49<04:02,  1.33s/it]Running loglikelihood requests:  55%|█████▍    | 219/400 [05:50<04:01,  1.33s/it]Running loglikelihood requests:  55%|█████▌    | 220/400 [05:52<03:59,  1.33s/it]Running loglikelihood requests:  55%|█████▌    | 221/400 [05:53<03:57,  1.33s/it]Running loglikelihood requests:  56%|█████▌    | 222/400 [05:54<03:56,  1.33s/it]Running loglikelihood requests:  56%|█████▌    | 223/400 [05:55<03:54,  1.33s/it]Running loglikelihood requests:  56%|█████▌    | 224/400 [05:57<03:53,  1.33s/it]Running loglikelihood requests:  56%|█████▋    | 225/400 [05:58<03:51,  1.33s/it]Running loglikelihood requests:  56%|█████▋    | 226/400 [05:59<03:50,  1.32s/it]Running loglikelihood requests:  57%|█████▋    | 227/400 [06:01<03:48,  1.32s/it]Running loglikelihood requests:  57%|█████▋    | 228/400 [06:02<03:47,  1.32s/it]Running loglikelihood requests:  57%|█████▋    | 229/400 [06:03<03:45,  1.32s/it]Running loglikelihood requests:  57%|█████▊    | 230/400 [06:05<03:44,  1.32s/it]Running loglikelihood requests:  58%|█████▊    | 231/400 [06:06<03:42,  1.32s/it]Running loglikelihood requests:  58%|█████▊    | 232/400 [06:07<03:41,  1.32s/it]Running loglikelihood requests:  58%|█████▊    | 233/400 [06:09<03:39,  1.32s/it]Running loglikelihood requests:  58%|█████▊    | 234/400 [06:10<03:38,  1.32s/it]Running loglikelihood requests:  59%|█████▉    | 235/400 [06:11<03:37,  1.32s/it]Running loglikelihood requests:  59%|█████▉    | 236/400 [06:13<03:35,  1.31s/it]Running loglikelihood requests:  59%|█████▉    | 237/400 [06:14<03:33,  1.31s/it]Running loglikelihood requests:  60%|█████▉    | 238/400 [06:15<03:31,  1.31s/it]Running loglikelihood requests:  60%|█████▉    | 239/400 [06:17<03:30,  1.31s/it]Running loglikelihood requests:  60%|██████    | 240/400 [06:18<03:31,  1.32s/it]Running loglikelihood requests:  60%|██████    | 241/400 [06:19<03:30,  1.32s/it]Running loglikelihood requests:  60%|██████    | 242/400 [06:20<03:28,  1.32s/it]Running loglikelihood requests:  61%|██████    | 243/400 [06:22<03:26,  1.31s/it]Running loglikelihood requests:  61%|██████    | 244/400 [06:23<03:24,  1.31s/it]Running loglikelihood requests:  61%|██████▏   | 245/400 [06:24<03:23,  1.31s/it]Running loglikelihood requests:  62%|██████▏   | 246/400 [06:26<03:22,  1.31s/it]Running loglikelihood requests:  62%|██████▏   | 247/400 [06:27<03:20,  1.31s/it]Running loglikelihood requests:  62%|██████▏   | 248/400 [06:28<03:18,  1.31s/it]Running loglikelihood requests:  62%|██████▏   | 249/400 [06:30<03:17,  1.31s/it]Running loglikelihood requests:  62%|██████▎   | 250/400 [06:31<03:15,  1.31s/it]Running loglikelihood requests:  63%|██████▎   | 251/400 [06:32<03:14,  1.30s/it]Running loglikelihood requests:  63%|██████▎   | 252/400 [06:34<03:12,  1.30s/it]Running loglikelihood requests:  63%|██████▎   | 253/400 [06:35<03:11,  1.30s/it]Running loglikelihood requests:  64%|██████▎   | 254/400 [06:36<03:09,  1.30s/it]Running loglikelihood requests:  64%|██████▍   | 255/400 [06:37<03:08,  1.30s/it]Running loglikelihood requests:  64%|██████▍   | 256/400 [06:39<03:06,  1.30s/it]Running loglikelihood requests:  64%|██████▍   | 257/400 [06:40<03:05,  1.29s/it]Running loglikelihood requests:  64%|██████▍   | 258/400 [06:41<03:03,  1.29s/it]Running loglikelihood requests:  65%|██████▍   | 259/400 [06:43<03:02,  1.29s/it]Running loglikelihood requests:  65%|██████▌   | 260/400 [06:44<03:01,  1.29s/it]Running loglikelihood requests:  65%|██████▌   | 261/400 [06:45<02:59,  1.29s/it]Running loglikelihood requests:  66%|██████▌   | 262/400 [06:46<02:58,  1.29s/it]Running loglikelihood requests:  66%|██████▌   | 263/400 [06:48<02:56,  1.29s/it]Running loglikelihood requests:  66%|██████▌   | 264/400 [06:49<02:54,  1.28s/it]Running loglikelihood requests:  66%|██████▋   | 265/400 [06:50<02:53,  1.28s/it]Running loglikelihood requests:  66%|██████▋   | 266/400 [06:52<02:51,  1.28s/it]Running loglikelihood requests:  67%|██████▋   | 267/400 [06:53<02:49,  1.28s/it]Running loglikelihood requests:  67%|██████▋   | 268/400 [06:54<02:48,  1.28s/it]Running loglikelihood requests:  67%|██████▋   | 269/400 [06:55<02:46,  1.27s/it]Running loglikelihood requests:  68%|██████▊   | 270/400 [06:57<02:45,  1.27s/it]Running loglikelihood requests:  68%|██████▊   | 271/400 [06:58<02:43,  1.27s/it]Running loglikelihood requests:  68%|██████▊   | 272/400 [06:59<02:42,  1.27s/it]Running loglikelihood requests:  68%|██████▊   | 273/400 [07:00<02:40,  1.27s/it]Running loglikelihood requests:  68%|██████▊   | 274/400 [07:02<02:39,  1.26s/it]Running loglikelihood requests:  69%|██████▉   | 275/400 [07:03<02:37,  1.26s/it]Running loglikelihood requests:  69%|██████▉   | 276/400 [07:04<02:36,  1.26s/it]Running loglikelihood requests:  69%|██████▉   | 277/400 [07:05<02:34,  1.26s/it]Running loglikelihood requests:  70%|██████▉   | 278/400 [07:07<02:33,  1.26s/it]Running loglikelihood requests:  70%|███████   | 280/400 [07:08<01:55,  1.04it/s]Running loglikelihood requests:  70%|███████   | 281/400 [07:09<02:03,  1.03s/it]Running loglikelihood requests:  70%|███████   | 282/400 [07:10<02:08,  1.09s/it]Running loglikelihood requests:  71%|███████   | 283/400 [07:12<02:12,  1.13s/it]Running loglikelihood requests:  71%|███████   | 284/400 [07:13<02:14,  1.16s/it]Running loglikelihood requests:  71%|███████▏  | 285/400 [07:14<02:15,  1.18s/it]Running loglikelihood requests:  72%|███████▏  | 286/400 [07:15<02:16,  1.20s/it]Running loglikelihood requests:  72%|███████▏  | 287/400 [07:17<02:16,  1.21s/it]Running loglikelihood requests:  72%|███████▏  | 288/400 [07:18<02:15,  1.21s/it]Running loglikelihood requests:  72%|███████▏  | 289/400 [07:19<02:14,  1.21s/it]Running loglikelihood requests:  72%|███████▎  | 290/400 [07:20<02:13,  1.21s/it]Running loglikelihood requests:  73%|███████▎  | 291/400 [07:21<02:12,  1.21s/it]Running loglikelihood requests:  73%|███████▎  | 292/400 [07:23<02:10,  1.21s/it]Running loglikelihood requests:  73%|███████▎  | 293/400 [07:24<02:09,  1.21s/it]Running loglikelihood requests:  74%|███████▎  | 294/400 [07:25<02:08,  1.21s/it]Running loglikelihood requests:  74%|███████▍  | 295/400 [07:26<02:07,  1.21s/it]Running loglikelihood requests:  74%|███████▍  | 296/400 [07:28<02:05,  1.21s/it]Running loglikelihood requests:  74%|███████▍  | 297/400 [07:29<02:04,  1.21s/it]Running loglikelihood requests:  74%|███████▍  | 298/400 [07:30<02:02,  1.20s/it]Running loglikelihood requests:  75%|███████▍  | 299/400 [07:31<02:01,  1.20s/it]Running loglikelihood requests:  75%|███████▌  | 300/400 [07:32<01:59,  1.20s/it]Running loglikelihood requests:  75%|███████▌  | 301/400 [07:34<01:58,  1.20s/it]Running loglikelihood requests:  76%|███████▌  | 302/400 [07:35<01:57,  1.19s/it]Running loglikelihood requests:  76%|███████▌  | 303/400 [07:36<01:55,  1.19s/it]Running loglikelihood requests:  76%|███████▌  | 304/400 [07:37<01:54,  1.19s/it]Running loglikelihood requests:  76%|███████▋  | 305/400 [07:38<01:52,  1.19s/it]Running loglikelihood requests:  76%|███████▋  | 306/400 [07:39<01:51,  1.19s/it]Running loglikelihood requests:  77%|███████▋  | 307/400 [07:41<01:50,  1.18s/it]Running loglikelihood requests:  77%|███████▋  | 308/400 [07:42<01:48,  1.18s/it]Running loglikelihood requests:  77%|███████▋  | 309/400 [07:43<01:47,  1.18s/it]Running loglikelihood requests:  78%|███████▊  | 310/400 [07:44<01:46,  1.18s/it]Running loglikelihood requests:  78%|███████▊  | 311/400 [07:45<01:44,  1.18s/it]Running loglikelihood requests:  78%|███████▊  | 312/400 [07:46<01:43,  1.17s/it]Running loglikelihood requests:  78%|███████▊  | 313/400 [07:48<01:42,  1.17s/it]Running loglikelihood requests:  78%|███████▊  | 314/400 [07:49<01:40,  1.17s/it]Running loglikelihood requests:  79%|███████▉  | 315/400 [07:50<01:39,  1.17s/it]Running loglikelihood requests:  79%|███████▉  | 316/400 [07:51<01:38,  1.17s/it]Running loglikelihood requests:  79%|███████▉  | 317/400 [07:52<01:37,  1.17s/it]Running loglikelihood requests:  80%|███████▉  | 318/400 [07:54<01:36,  1.18s/it]Running loglikelihood requests:  80%|███████▉  | 319/400 [07:55<01:36,  1.19s/it]Running loglikelihood requests:  80%|████████  | 320/400 [07:56<01:34,  1.19s/it]Running loglikelihood requests:  80%|████████  | 321/400 [07:57<01:33,  1.19s/it]Running loglikelihood requests:  80%|████████  | 322/400 [07:58<01:32,  1.19s/it]Running loglikelihood requests:  81%|████████  | 323/400 [08:00<01:31,  1.19s/it]Running loglikelihood requests:  81%|████████  | 324/400 [08:01<01:30,  1.19s/it]Running loglikelihood requests:  81%|████████▏ | 325/400 [08:02<01:29,  1.19s/it]Running loglikelihood requests:  82%|████████▏ | 326/400 [08:03<01:27,  1.19s/it]Running loglikelihood requests:  82%|████████▏ | 327/400 [08:04<01:25,  1.18s/it]Running loglikelihood requests:  82%|████████▏ | 328/400 [08:05<01:24,  1.17s/it]Running loglikelihood requests:  82%|████████▏ | 329/400 [08:07<01:22,  1.16s/it]Running loglikelihood requests:  82%|████████▎ | 330/400 [08:08<01:20,  1.16s/it]Running loglikelihood requests:  83%|████████▎ | 331/400 [08:09<01:19,  1.15s/it]Running loglikelihood requests:  83%|████████▎ | 332/400 [08:10<01:18,  1.15s/it]Running loglikelihood requests:  83%|████████▎ | 333/400 [08:11<01:16,  1.14s/it]Running loglikelihood requests:  84%|████████▎ | 334/400 [08:12<01:15,  1.14s/it]Running loglikelihood requests:  84%|████████▍ | 335/400 [08:13<01:13,  1.14s/it]Running loglikelihood requests:  84%|████████▍ | 336/400 [08:14<01:12,  1.13s/it]Running loglikelihood requests:  84%|████████▍ | 337/400 [08:16<01:11,  1.13s/it]Running loglikelihood requests:  84%|████████▍ | 338/400 [08:17<01:10,  1.13s/it]Running loglikelihood requests:  85%|████████▍ | 339/400 [08:18<01:08,  1.13s/it]Running loglikelihood requests:  85%|████████▌ | 340/400 [08:19<01:07,  1.13s/it]Running loglikelihood requests:  85%|████████▌ | 341/400 [08:20<01:06,  1.12s/it]Running loglikelihood requests:  86%|████████▌ | 342/400 [08:21<01:04,  1.12s/it]Running loglikelihood requests:  86%|████████▌ | 343/400 [08:22<01:03,  1.11s/it]Running loglikelihood requests:  86%|████████▌ | 344/400 [08:23<01:02,  1.11s/it]Running loglikelihood requests:  86%|████████▋ | 345/400 [08:25<01:01,  1.11s/it]Running loglikelihood requests:  86%|████████▋ | 346/400 [08:26<00:59,  1.11s/it]Running loglikelihood requests:  87%|████████▋ | 347/400 [08:27<00:58,  1.11s/it]Running loglikelihood requests:  87%|████████▋ | 348/400 [08:28<00:57,  1.10s/it]Running loglikelihood requests:  87%|████████▋ | 349/400 [08:29<00:56,  1.10s/it]Running loglikelihood requests:  88%|████████▊ | 350/400 [08:30<00:55,  1.10s/it]Running loglikelihood requests:  88%|████████▊ | 351/400 [08:31<00:53,  1.10s/it]Running loglikelihood requests:  88%|████████▊ | 352/400 [08:32<00:52,  1.10s/it]Running loglikelihood requests:  88%|████████▊ | 353/400 [08:33<00:51,  1.09s/it]Running loglikelihood requests:  88%|████████▊ | 354/400 [08:34<00:50,  1.09s/it]Running loglikelihood requests:  89%|████████▉ | 355/400 [08:35<00:49,  1.09s/it]Running loglikelihood requests:  89%|████████▉ | 356/400 [08:37<00:47,  1.09s/it]Running loglikelihood requests:  89%|████████▉ | 357/400 [08:38<00:46,  1.09s/it]Running loglikelihood requests:  90%|████████▉ | 358/400 [08:39<00:45,  1.09s/it]Running loglikelihood requests:  90%|████████▉ | 359/400 [08:40<00:44,  1.08s/it]Running loglikelihood requests:  90%|█████████ | 360/400 [08:41<00:43,  1.08s/it]Running loglikelihood requests:  90%|█████████ | 361/400 [08:42<00:42,  1.08s/it]Running loglikelihood requests:  90%|█████████ | 362/400 [08:43<00:40,  1.08s/it]Running loglikelihood requests:  91%|█████████ | 363/400 [08:44<00:39,  1.08s/it]Running loglikelihood requests:  91%|█████████ | 364/400 [08:45<00:38,  1.08s/it]Running loglikelihood requests:  91%|█████████▏| 365/400 [08:46<00:37,  1.08s/it]Running loglikelihood requests:  92%|█████████▏| 366/400 [08:47<00:36,  1.07s/it]Running loglikelihood requests:  92%|█████████▏| 367/400 [08:48<00:35,  1.07s/it]Running loglikelihood requests:  92%|█████████▏| 368/400 [08:49<00:34,  1.07s/it]Running loglikelihood requests:  92%|█████████▏| 369/400 [08:51<00:33,  1.07s/it]Running loglikelihood requests:  92%|█████████▎| 370/400 [08:52<00:31,  1.07s/it]Running loglikelihood requests:  93%|█████████▎| 371/400 [08:53<00:30,  1.06s/it]Running loglikelihood requests:  93%|█████████▎| 372/400 [08:54<00:29,  1.07s/it]Running loglikelihood requests:  93%|█████████▎| 373/400 [08:55<00:28,  1.07s/it]Running loglikelihood requests:  94%|█████████▎| 374/400 [08:56<00:27,  1.06s/it]Running loglikelihood requests:  94%|█████████▍| 375/400 [08:57<00:26,  1.05s/it]Running loglikelihood requests:  94%|█████████▍| 376/400 [08:58<00:25,  1.05s/it]Running loglikelihood requests:  94%|█████████▍| 377/400 [08:59<00:23,  1.04s/it]Running loglikelihood requests:  94%|█████████▍| 378/400 [09:00<00:22,  1.03s/it]Running loglikelihood requests:  95%|█████████▍| 379/400 [09:01<00:21,  1.02s/it]Running loglikelihood requests:  95%|█████████▌| 380/400 [09:02<00:20,  1.01s/it]Running loglikelihood requests:  95%|█████████▌| 381/400 [09:03<00:19,  1.00s/it]Running loglikelihood requests:  96%|█████████▌| 382/400 [09:04<00:17,  1.01it/s]Running loglikelihood requests:  96%|█████████▌| 383/400 [09:05<00:16,  1.01it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [09:06<00:16,  1.00s/it]Running loglikelihood requests:  96%|█████████▋| 385/400 [09:07<00:15,  1.00s/it]Running loglikelihood requests:  96%|█████████▋| 386/400 [09:08<00:14,  1.01s/it]Running loglikelihood requests:  97%|█████████▋| 387/400 [09:09<00:13,  1.01s/it]Running loglikelihood requests:  97%|█████████▋| 389/400 [09:10<00:08,  1.30it/s]Running loglikelihood requests:  98%|█████████▊| 390/400 [09:11<00:08,  1.23it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [09:12<00:07,  1.19it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [09:13<00:04,  1.51it/s]Running loglikelihood requests:  98%|█████████▊| 394/400 [09:13<00:04,  1.42it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [09:14<00:03,  1.35it/s]Running loglikelihood requests:  99%|█████████▉| 396/400 [09:15<00:03,  1.30it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [09:16<00:02,  1.26it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [09:17<00:01,  1.22it/s]Running loglikelihood requests: 100%|█████████▉| 399/400 [09:18<00:00,  1.21it/s]Running loglikelihood requests: 100%|██████████| 400/400 [09:19<00:00,  1.20it/s]Running loglikelihood requests: 100%|██████████| 400/400 [09:19<00:00,  1.40s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:7'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:7'}
full model:
{'logiqa': {'alias': 'logiqa', 'acc,none': 0.29, 'acc_stderr,none': 0.045604802157206865, 'acc_norm,none': 0.33, 'acc_norm_stderr,none': 0.04725815626252609}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9179964803140478
0.7817057225882229
0.8413553272072316
0.9274797474668193
0.8768807463293081
0.9494139907523571
0.8960692461846443
0.9131107283061946
0.6329173647892901
0.8375042173336539
0.8817471801904351
0.8172295355829869
0.7824572665005357
0.9227400642857845
0.9246594853497696
0.8075911590072223
0.6900210787422486
0.599615993999193
0.9308030044211123
0.9504015361511146
0.8866807231108503
0.540104242930401
0.6701728801805507
0.9744992661822648
0.8193037468812308
0.840784693447352
0.9052511591891966
Total groups 70 exceeded the threshold, stopping comparison.
The group tensor is
[3, 6, 7, 1, 5, 2, 4, 0]
tensor([3, 6, 7, 1, 5, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 6, 3, 7, 1, 5, 0]
tensor([4, 2, 6, 3, 7, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 6, 1, 7, 2, 4, 0]
tensor([5, 3, 6, 1, 7, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 1, 7, 2, 3, 0]
tensor([5, 4, 6, 1, 7, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 3, 0, 2, 1, 0, 4, 1]
tensor([5, 3, 0, 2, 1, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 5, 0, 1, 2, 3, 1]
tensor([4, 0, 5, 0, 1, 2, 3, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 4 to 8
done!
Normal merging for layer 9
tensor([2, 5])
tensor(2)
tensor([4, 7])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 10 to 13
done!
Normal merging for layer 14
tensor([1, 3])
tensor(1)
tensor([4, 7])
tensor(4)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 15 to 31
done!
all done!
Model size: 11.9458 GB
163
cuda:7
mnli
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 24.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.51s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: mnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: mnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140516066118448 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140516066118448 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516066118448 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140516066118448 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_mnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140545766343072 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140545766343072 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545766343072 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140545766343072 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mnli from None to 0
INFO:lm_eval.api.task:Building contexts for mnli on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 126182.43it/s]
DEBUG:lm_eval.evaluator:Task: mnli; number of requests on this rank: 300
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/300 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/300 [00:01<08:43,  1.75s/it]Running loglikelihood requests:   1%|          | 2/300 [00:02<07:07,  1.43s/it]Running loglikelihood requests:   1%|▏         | 4/300 [00:03<04:05,  1.20it/s]Running loglikelihood requests:   2%|▏         | 5/300 [00:04<04:17,  1.14it/s]Running loglikelihood requests:   2%|▏         | 7/300 [00:05<03:16,  1.49it/s]Running loglikelihood requests:   3%|▎         | 8/300 [00:06<03:29,  1.39it/s]Running loglikelihood requests:   3%|▎         | 10/300 [00:07<02:50,  1.70it/s]Running loglikelihood requests:   4%|▎         | 11/300 [00:08<03:04,  1.57it/s]Running loglikelihood requests:   4%|▍         | 13/300 [00:09<02:34,  1.85it/s]Running loglikelihood requests:   5%|▍         | 14/300 [00:09<02:48,  1.69it/s]Running loglikelihood requests:   5%|▌         | 16/300 [00:10<02:23,  1.98it/s]Running loglikelihood requests:   6%|▌         | 17/300 [00:11<02:36,  1.81it/s]Running loglikelihood requests:   6%|▌         | 18/300 [00:11<02:47,  1.68it/s]Running loglikelihood requests:   7%|▋         | 20/300 [00:12<02:20,  1.99it/s]Running loglikelihood requests:   7%|▋         | 21/300 [00:13<02:33,  1.81it/s]Running loglikelihood requests:   8%|▊         | 23/300 [00:14<02:12,  2.10it/s]Running loglikelihood requests:   8%|▊         | 25/300 [00:14<01:59,  2.31it/s]Running loglikelihood requests:   9%|▊         | 26/300 [00:15<02:14,  2.04it/s]Running loglikelihood requests:   9%|▉         | 28/300 [00:16<01:59,  2.28it/s]Running loglikelihood requests:  10%|▉         | 29/300 [00:16<02:13,  2.02it/s]Running loglikelihood requests:  10%|█         | 30/300 [00:17<02:26,  1.84it/s]Running loglikelihood requests:  10%|█         | 31/300 [00:18<02:36,  1.72it/s]Running loglikelihood requests:  11%|█         | 32/300 [00:19<02:43,  1.63it/s]Running loglikelihood requests:  11%|█▏        | 34/300 [00:19<02:12,  2.00it/s]Running loglikelihood requests:  12%|█▏        | 36/300 [00:20<01:56,  2.26it/s]Running loglikelihood requests:  13%|█▎        | 38/300 [00:21<01:47,  2.44it/s]Running loglikelihood requests:  13%|█▎        | 40/300 [00:21<01:40,  2.59it/s]Running loglikelihood requests:  14%|█▎        | 41/300 [00:22<01:54,  2.26it/s]Running loglikelihood requests:  14%|█▍        | 42/300 [00:23<02:07,  2.02it/s]Running loglikelihood requests:  14%|█▍        | 43/300 [00:23<02:18,  1.86it/s]Running loglikelihood requests:  15%|█▌        | 45/300 [00:24<01:56,  2.19it/s]Running loglikelihood requests:  16%|█▌        | 47/300 [00:25<01:44,  2.42it/s]Running loglikelihood requests:  16%|█▌        | 48/300 [00:25<01:57,  2.14it/s]Running loglikelihood requests:  17%|█▋        | 50/300 [00:26<01:44,  2.39it/s]Running loglikelihood requests:  17%|█▋        | 52/300 [00:27<01:36,  2.57it/s]Running loglikelihood requests:  18%|█▊        | 53/300 [00:27<01:49,  2.25it/s]Running loglikelihood requests:  18%|█▊        | 54/300 [00:28<02:00,  2.03it/s]Running loglikelihood requests:  18%|█▊        | 55/300 [00:29<02:10,  1.88it/s]Running loglikelihood requests:  19%|█▉        | 57/300 [00:29<01:49,  2.22it/s]Running loglikelihood requests:  20%|█▉        | 59/300 [00:30<01:37,  2.46it/s]Running loglikelihood requests:  20%|██        | 61/300 [00:31<01:30,  2.64it/s]Running loglikelihood requests:  21%|██        | 62/300 [00:31<01:42,  2.31it/s]Running loglikelihood requests:  21%|██▏       | 64/300 [00:32<01:32,  2.54it/s]Running loglikelihood requests:  22%|██▏       | 65/300 [00:33<01:44,  2.24it/s]Running loglikelihood requests:  22%|██▏       | 66/300 [00:33<01:54,  2.04it/s]Running loglikelihood requests:  22%|██▏       | 67/300 [00:34<02:03,  1.89it/s]Running loglikelihood requests:  23%|██▎       | 69/300 [00:35<01:42,  2.25it/s]Running loglikelihood requests:  24%|██▎       | 71/300 [00:35<01:31,  2.51it/s]Running loglikelihood requests:  24%|██▍       | 72/300 [00:36<01:42,  2.23it/s]Running loglikelihood requests:  24%|██▍       | 73/300 [00:37<01:52,  2.03it/s]Running loglikelihood requests:  25%|██▌       | 75/300 [00:37<01:35,  2.36it/s]Running loglikelihood requests:  26%|██▌       | 77/300 [00:38<01:26,  2.59it/s]Running loglikelihood requests:  26%|██▋       | 79/300 [00:38<01:20,  2.76it/s]Running loglikelihood requests:  27%|██▋       | 80/300 [00:39<01:31,  2.40it/s]Running loglikelihood requests:  27%|██▋       | 81/300 [00:40<01:41,  2.16it/s]Running loglikelihood requests:  27%|██▋       | 82/300 [00:40<01:49,  1.98it/s]Running loglikelihood requests:  28%|██▊       | 83/300 [00:41<01:56,  1.87it/s]Running loglikelihood requests:  28%|██▊       | 85/300 [00:42<01:35,  2.26it/s]Running loglikelihood requests:  29%|██▊       | 86/300 [00:42<01:44,  2.06it/s]Running loglikelihood requests:  29%|██▉       | 88/300 [00:43<01:28,  2.39it/s]Running loglikelihood requests:  30%|██▉       | 89/300 [00:43<01:38,  2.14it/s]Running loglikelihood requests:  30%|███       | 91/300 [00:44<01:25,  2.45it/s]Running loglikelihood requests:  31%|███       | 93/300 [00:45<01:17,  2.67it/s]Running loglikelihood requests:  32%|███▏      | 95/300 [00:45<01:12,  2.82it/s]Running loglikelihood requests:  32%|███▏      | 96/300 [00:46<01:22,  2.46it/s]Running loglikelihood requests:  33%|███▎      | 98/300 [00:47<01:15,  2.68it/s]Running loglikelihood requests:  33%|███▎      | 99/300 [00:47<01:27,  2.30it/s]Running loglikelihood requests:  34%|███▎      | 101/300 [00:48<01:19,  2.51it/s]Running loglikelihood requests:  34%|███▍      | 102/300 [00:49<01:28,  2.23it/s]Running loglikelihood requests:  34%|███▍      | 103/300 [00:49<01:36,  2.04it/s]Running loglikelihood requests:  35%|███▍      | 104/300 [00:50<01:42,  1.91it/s]Running loglikelihood requests:  35%|███▌      | 105/300 [00:51<01:47,  1.81it/s]Running loglikelihood requests:  36%|███▌      | 107/300 [00:51<01:26,  2.22it/s]Running loglikelihood requests:  36%|███▌      | 108/300 [00:52<01:34,  2.03it/s]Running loglikelihood requests:  36%|███▋      | 109/300 [00:52<01:40,  1.90it/s]Running loglikelihood requests:  37%|███▋      | 111/300 [00:53<01:22,  2.29it/s]Running loglikelihood requests:  38%|███▊      | 113/300 [00:54<01:14,  2.52it/s]Running loglikelihood requests:  38%|███▊      | 115/300 [00:54<01:08,  2.69it/s]Running loglikelihood requests:  39%|███▉      | 117/300 [00:55<01:04,  2.85it/s]Running loglikelihood requests:  40%|███▉      | 119/300 [00:56<01:00,  2.97it/s]Running loglikelihood requests:  40%|████      | 121/300 [00:56<00:58,  3.06it/s]Running loglikelihood requests:  41%|████      | 122/300 [00:57<01:07,  2.64it/s]Running loglikelihood requests:  41%|████▏     | 124/300 [00:57<01:01,  2.84it/s]Running loglikelihood requests:  42%|████▏     | 125/300 [00:58<01:10,  2.49it/s]Running loglikelihood requests:  42%|████▏     | 126/300 [00:59<01:17,  2.24it/s]Running loglikelihood requests:  42%|████▏     | 127/300 [00:59<01:23,  2.07it/s]Running loglikelihood requests:  43%|████▎     | 129/300 [01:00<01:09,  2.45it/s]Running loglikelihood requests:  44%|████▎     | 131/300 [01:00<01:02,  2.71it/s]Running loglikelihood requests:  44%|████▍     | 133/300 [01:01<00:57,  2.92it/s]Running loglikelihood requests:  45%|████▍     | 134/300 [01:02<01:04,  2.57it/s]Running loglikelihood requests:  45%|████▌     | 135/300 [01:02<01:11,  2.32it/s]Running loglikelihood requests:  45%|████▌     | 136/300 [01:03<01:16,  2.14it/s]Running loglikelihood requests:  46%|████▌     | 137/300 [01:03<01:20,  2.01it/s]Running loglikelihood requests:  46%|████▌     | 138/300 [01:04<01:24,  1.93it/s]Running loglikelihood requests:  47%|████▋     | 140/300 [01:04<01:07,  2.38it/s]Running loglikelihood requests:  47%|████▋     | 142/300 [01:05<00:58,  2.71it/s]Running loglikelihood requests:  48%|████▊     | 143/300 [01:06<01:04,  2.42it/s]Running loglikelihood requests:  48%|████▊     | 145/300 [01:06<00:56,  2.73it/s]Running loglikelihood requests:  49%|████▊     | 146/300 [01:07<01:03,  2.43it/s]Running loglikelihood requests:  49%|████▉     | 147/300 [01:07<01:08,  2.22it/s]Running loglikelihood requests:  49%|████▉     | 148/300 [01:08<01:13,  2.08it/s]Running loglikelihood requests:  50%|█████     | 150/300 [01:08<01:00,  2.49it/s]Running loglikelihood requests:  51%|█████     | 152/300 [01:09<00:53,  2.79it/s]Running loglikelihood requests:  51%|█████▏    | 154/300 [01:10<00:48,  3.00it/s]Running loglikelihood requests:  52%|█████▏    | 155/300 [01:10<00:55,  2.63it/s]Running loglikelihood requests:  52%|█████▏    | 157/300 [01:11<00:49,  2.89it/s]Running loglikelihood requests:  53%|█████▎    | 159/300 [01:11<00:45,  3.07it/s]Running loglikelihood requests:  54%|█████▎    | 161/300 [01:12<00:43,  3.21it/s]Running loglikelihood requests:  54%|█████▍    | 162/300 [01:12<00:49,  2.78it/s]Running loglikelihood requests:  55%|█████▍    | 164/300 [01:13<00:45,  3.00it/s]Running loglikelihood requests:  55%|█████▌    | 165/300 [01:14<00:51,  2.64it/s]Running loglikelihood requests:  56%|█████▌    | 167/300 [01:14<00:45,  2.91it/s]Running loglikelihood requests:  56%|█████▋    | 169/300 [01:15<00:42,  3.10it/s]Running loglikelihood requests:  57%|█████▋    | 170/300 [01:15<00:48,  2.71it/s]Running loglikelihood requests:  57%|█████▋    | 171/300 [01:16<00:52,  2.44it/s]Running loglikelihood requests:  57%|█████▋    | 172/300 [01:16<00:57,  2.24it/s]Running loglikelihood requests:  58%|█████▊    | 173/300 [01:17<01:00,  2.11it/s]Running loglikelihood requests:  58%|█████▊    | 175/300 [01:18<00:49,  2.55it/s]Running loglikelihood requests:  59%|█████▉    | 177/300 [01:18<00:42,  2.86it/s]Running loglikelihood requests:  60%|█████▉    | 179/300 [01:19<00:39,  3.08it/s]Running loglikelihood requests:  60%|██████    | 180/300 [01:19<00:44,  2.69it/s]Running loglikelihood requests:  60%|██████    | 181/300 [01:20<00:49,  2.43it/s]Running loglikelihood requests:  61%|██████    | 183/300 [01:20<00:42,  2.78it/s]Running loglikelihood requests:  62%|██████▏   | 185/300 [01:21<00:37,  3.04it/s]Running loglikelihood requests:  62%|██████▏   | 186/300 [01:21<00:42,  2.67it/s]Running loglikelihood requests:  63%|██████▎   | 188/300 [01:22<00:37,  2.96it/s]Running loglikelihood requests:  63%|██████▎   | 190/300 [01:23<00:34,  3.17it/s]Running loglikelihood requests:  64%|██████▎   | 191/300 [01:23<00:39,  2.77it/s]Running loglikelihood requests:  64%|██████▍   | 192/300 [01:24<00:43,  2.50it/s]Running loglikelihood requests:  65%|██████▍   | 194/300 [01:24<00:37,  2.85it/s]Running loglikelihood requests:  65%|██████▌   | 195/300 [01:25<00:41,  2.56it/s]Running loglikelihood requests:  65%|██████▌   | 196/300 [01:25<00:44,  2.34it/s]Running loglikelihood requests:  66%|██████▌   | 198/300 [01:26<00:37,  2.70it/s]Running loglikelihood requests:  66%|██████▋   | 199/300 [01:26<00:41,  2.42it/s]Running loglikelihood requests:  67%|██████▋   | 200/300 [01:27<00:44,  2.23it/s]Running loglikelihood requests:  67%|██████▋   | 201/300 [01:27<00:47,  2.10it/s]Running loglikelihood requests:  68%|██████▊   | 203/300 [01:28<00:38,  2.55it/s]Running loglikelihood requests:  68%|██████▊   | 205/300 [01:29<00:33,  2.86it/s]Running loglikelihood requests:  69%|██████▊   | 206/300 [01:29<00:36,  2.57it/s]Running loglikelihood requests:  69%|██████▉   | 208/300 [01:30<00:31,  2.91it/s]Running loglikelihood requests:  70%|███████   | 210/300 [01:30<00:28,  3.16it/s]Running loglikelihood requests:  71%|███████   | 212/300 [01:31<00:26,  3.33it/s]Running loglikelihood requests:  71%|███████   | 213/300 [01:31<00:29,  2.91it/s]Running loglikelihood requests:  72%|███████▏  | 215/300 [01:32<00:26,  3.16it/s]Running loglikelihood requests:  72%|███████▏  | 216/300 [01:32<00:30,  2.78it/s]Running loglikelihood requests:  72%|███████▏  | 217/300 [01:33<00:32,  2.52it/s]Running loglikelihood requests:  73%|███████▎  | 219/300 [01:33<00:27,  2.90it/s]Running loglikelihood requests:  74%|███████▎  | 221/300 [01:34<00:24,  3.17it/s]Running loglikelihood requests:  74%|███████▍  | 222/300 [01:34<00:27,  2.79it/s]Running loglikelihood requests:  75%|███████▍  | 224/300 [01:35<00:24,  3.10it/s]Running loglikelihood requests:  75%|███████▌  | 225/300 [01:35<00:27,  2.75it/s]Running loglikelihood requests:  76%|███████▌  | 227/300 [01:36<00:23,  3.08it/s]Running loglikelihood requests:  76%|███████▌  | 228/300 [01:36<00:26,  2.73it/s]Running loglikelihood requests:  76%|███████▋  | 229/300 [01:37<00:28,  2.49it/s]Running loglikelihood requests:  77%|███████▋  | 231/300 [01:38<00:23,  2.90it/s]Running loglikelihood requests:  77%|███████▋  | 232/300 [01:38<00:26,  2.61it/s]Running loglikelihood requests:  78%|███████▊  | 233/300 [01:39<00:27,  2.41it/s]Running loglikelihood requests:  78%|███████▊  | 234/300 [01:39<00:29,  2.27it/s]Running loglikelihood requests:  78%|███████▊  | 235/300 [01:40<00:29,  2.17it/s]Running loglikelihood requests:  79%|███████▊  | 236/300 [01:40<00:30,  2.10it/s]Running loglikelihood requests:  79%|███████▉  | 237/300 [01:41<00:30,  2.06it/s]Running loglikelihood requests:  79%|███████▉  | 238/300 [01:41<00:30,  2.02it/s]Running loglikelihood requests:  80%|████████  | 240/300 [01:42<00:23,  2.58it/s]Running loglikelihood requests:  81%|████████  | 242/300 [01:42<00:19,  2.97it/s]Running loglikelihood requests:  81%|████████▏ | 244/300 [01:43<00:17,  3.26it/s]Running loglikelihood requests:  82%|████████▏ | 246/300 [01:43<00:15,  3.45it/s]Running loglikelihood requests:  83%|████████▎ | 248/300 [01:44<00:14,  3.59it/s]Running loglikelihood requests:  83%|████████▎ | 250/300 [01:44<00:13,  3.69it/s]Running loglikelihood requests:  84%|████████▍ | 252/300 [01:45<00:12,  3.76it/s]Running loglikelihood requests:  85%|████████▍ | 254/300 [01:45<00:12,  3.81it/s]Running loglikelihood requests:  85%|████████▌ | 256/300 [01:46<00:11,  3.77it/s]Running loglikelihood requests:  86%|████████▌ | 257/300 [01:46<00:13,  3.22it/s]Running loglikelihood requests:  86%|████████▌ | 258/300 [01:47<00:14,  2.81it/s]Running loglikelihood requests:  87%|████████▋ | 260/300 [01:47<00:12,  3.16it/s]Running loglikelihood requests:  87%|████████▋ | 261/300 [01:48<00:13,  2.82it/s]Running loglikelihood requests:  88%|████████▊ | 263/300 [01:48<00:11,  3.17it/s]Running loglikelihood requests:  88%|████████▊ | 264/300 [01:49<00:12,  2.79it/s]Running loglikelihood requests:  88%|████████▊ | 265/300 [01:49<00:13,  2.55it/s]Running loglikelihood requests:  89%|████████▊ | 266/300 [01:50<00:14,  2.38it/s]Running loglikelihood requests:  89%|████████▉ | 268/300 [01:50<00:11,  2.85it/s]Running loglikelihood requests:  90%|█████████ | 270/300 [01:51<00:09,  3.16it/s]Running loglikelihood requests:  91%|█████████ | 272/300 [01:51<00:08,  3.43it/s]Running loglikelihood requests:  91%|█████████▏| 274/300 [01:52<00:07,  3.64it/s]Running loglikelihood requests:  92%|█████████▏| 275/300 [01:52<00:07,  3.20it/s]Running loglikelihood requests:  92%|█████████▏| 277/300 [01:53<00:06,  3.49it/s]Running loglikelihood requests:  93%|█████████▎| 278/300 [01:53<00:07,  3.07it/s]Running loglikelihood requests:  93%|█████████▎| 279/300 [01:54<00:07,  2.79it/s]Running loglikelihood requests:  94%|█████████▎| 281/300 [01:54<00:05,  3.22it/s]Running loglikelihood requests:  94%|█████████▍| 282/300 [01:55<00:06,  2.89it/s]Running loglikelihood requests:  94%|█████████▍| 283/300 [01:55<00:06,  2.67it/s]Running loglikelihood requests:  95%|█████████▍| 284/300 [01:56<00:06,  2.50it/s]Running loglikelihood requests:  95%|█████████▌| 286/300 [01:56<00:04,  3.03it/s]Running loglikelihood requests:  96%|█████████▌| 288/300 [01:57<00:03,  3.40it/s]Running loglikelihood requests:  97%|█████████▋| 290/300 [01:57<00:02,  3.67it/s]Running loglikelihood requests:  97%|█████████▋| 292/300 [01:57<00:02,  3.88it/s]Running loglikelihood requests:  98%|█████████▊| 293/300 [01:58<00:02,  3.39it/s]Running loglikelihood requests:  98%|█████████▊| 294/300 [01:58<00:01,  3.04it/s]Running loglikelihood requests:  99%|█████████▊| 296/300 [01:59<00:01,  3.46it/s]Running loglikelihood requests:  99%|█████████▉| 298/300 [01:59<00:00,  3.79it/s]Running loglikelihood requests: 100%|█████████▉| 299/300 [02:00<00:00,  3.37it/s]Running loglikelihood requests: 100%|██████████| 300/300 [02:00<00:00,  2.50it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:0'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}
full model:
{'mnli': {'alias': 'mnli', 'acc,none': 0.4, 'acc_stderr,none': 0.0492365963917331}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8171409999433525
0.8739893758684915
0.8333413990263023
0.8782828649633461
0.7602933913725962
0.7047335470608914
0.9813668198386444
0.7850949920891616
0.748983595161589
0.6557873189175887
0.7043210867322975
0.9420132312730142
0.9548348739467002
0.8459964595909506
0.6932219150662173
0.8804855383939617
0.868566987197908
0.8437399449827557
0.9153450822718411
0.8433082326287293
0.910397090611448
0.7626694638419581
0.752151649412453
0.8346986395359313
0.9530736745854924
0.8635918585764104
0.8647192950921815
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[4, 5, 6, 1, 7, 2, 3, 0]
tensor([4, 5, 6, 1, 7, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 4, 6, 2, 5, 1, 3, 0]
tensor([7, 4, 6, 2, 5, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 7, 3, 5, 1, 4, 0]
tensor([6, 2, 7, 3, 5, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 5, 4, 6, 1, 3, 0]
tensor([7, 2, 5, 4, 6, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 5, 3, 1, 2, 4, 0]
tensor([0, 1, 5, 3, 1, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 1, 2, 2, 3, 0]
tensor([0, 3, 1, 1, 2, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 2, 3, 1, 2, 0, 3, 1]
tensor([0, 2, 3, 1, 2, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 3, 1, 2, 2, 1, 3, 0]
tensor([0, 3, 1, 2, 2, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 0, 1.0, 1]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 4 to 8
done!
Normal merging for layer 9
tensor([0, 7])
tensor(0)
tensor([1, 4])
tensor(1)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 10 to 20
done!
Normal merging for layer 21
tensor([0, 7])
tensor(0)
tensor([2, 3])
tensor(2)
tensor([4, 5])
tensor(4)
tensor([1, 6])
tensor(1)
done!
Normal merging for layer 22
tensor([0, 5])
tensor(0)
tensor([3, 7])
tensor(3)
tensor([1, 4])
tensor(1)
tensor([2, 6])
tensor(2)
done!
Normal merging for layer 23
tensor([0, 7])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3, 4])
tensor(3)
tensor([1, 6])
tensor(1)
done!
Cross-layer merge completed for layers 24 to 25
done!
Normal merging for layer 26
tensor([0, 5])
tensor(0)
tensor([1, 2, 3, 4, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 27 to 31
done!
all done!
Model size: 12.5127 GB
211
cuda:0
copa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 25.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.44s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: copa] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: copa] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/copa?recursive=False&expand=False HTTP/1.1" 307 143
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue/tree/3de24cf8022e94f4ee4b9d55a6f539891524d646/copa?recursive=False&expand=False HTTP/1.1" 200 348
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514985494240 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140514985494240 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514985494240 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140514985494240 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_copa_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140545900393536 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140545900393536 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545900393536 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140545900393536 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/copa/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
DEBUG:lm_eval.api.task:Both target_delimiter " " and target choice: " the toilet filled with water." have whitespace
DEBUG:lm_eval.api.task:Both target_delimiter " " and target choice: " water flowed from the spout." have whitespace
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of copa from None to 0
INFO:lm_eval.api.task:Building contexts for copa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 151528.32it/s]
DEBUG:lm_eval.evaluator:Task: copa; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:00<03:09,  1.05it/s]Running loglikelihood requests:   1%|          | 2/200 [00:01<02:13,  1.48it/s]Running loglikelihood requests:   2%|▏         | 3/200 [00:01<01:53,  1.73it/s]Running loglikelihood requests:   2%|▏         | 4/200 [00:02<01:43,  1.89it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:02<01:37,  2.00it/s]Running loglikelihood requests:   3%|▎         | 6/200 [00:03<01:34,  2.06it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:03<01:31,  2.11it/s]Running loglikelihood requests:   4%|▍         | 8/200 [00:04<01:29,  2.15it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:04<01:28,  2.17it/s]Running loglikelihood requests:   5%|▌         | 10/200 [00:05<01:27,  2.17it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:05<01:26,  2.18it/s]Running loglikelihood requests:   6%|▌         | 12/200 [00:05<01:27,  2.15it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:06<01:27,  2.15it/s]Running loglikelihood requests:   7%|▋         | 14/200 [00:06<01:26,  2.15it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:07<01:25,  2.16it/s]Running loglikelihood requests:   8%|▊         | 16/200 [00:07<01:24,  2.17it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:08<01:24,  2.17it/s]Running loglikelihood requests:   9%|▉         | 18/200 [00:08<01:23,  2.18it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:09<01:22,  2.18it/s]Running loglikelihood requests:  10%|█         | 20/200 [00:09<01:22,  2.18it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:10<01:20,  2.21it/s]Running loglikelihood requests:  11%|█         | 22/200 [00:10<01:19,  2.24it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:10<01:18,  2.26it/s]Running loglikelihood requests:  12%|█▏        | 24/200 [00:11<01:17,  2.27it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:11<01:16,  2.29it/s]Running loglikelihood requests:  13%|█▎        | 26/200 [00:12<01:15,  2.29it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:12<01:15,  2.30it/s]Running loglikelihood requests:  14%|█▍        | 28/200 [00:13<01:14,  2.30it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:13<01:14,  2.31it/s]Running loglikelihood requests:  15%|█▌        | 30/200 [00:14<01:13,  2.31it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:14<01:12,  2.32it/s]Running loglikelihood requests:  16%|█▌        | 32/200 [00:14<01:12,  2.33it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:15<01:11,  2.33it/s]Running loglikelihood requests:  17%|█▋        | 34/200 [00:15<01:11,  2.33it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:16<01:10,  2.33it/s]Running loglikelihood requests:  18%|█▊        | 36/200 [00:16<01:10,  2.33it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:16<01:09,  2.34it/s]Running loglikelihood requests:  19%|█▉        | 38/200 [00:17<01:09,  2.34it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:17<01:08,  2.34it/s]Running loglikelihood requests:  20%|██        | 40/200 [00:18<01:08,  2.34it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:18<01:07,  2.34it/s]Running loglikelihood requests:  21%|██        | 42/200 [00:19<01:07,  2.34it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:19<01:06,  2.34it/s]Running loglikelihood requests:  22%|██▏       | 44/200 [00:19<01:06,  2.34it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:20<01:06,  2.35it/s]Running loglikelihood requests:  23%|██▎       | 46/200 [00:20<01:05,  2.35it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:21<01:05,  2.35it/s]Running loglikelihood requests:  24%|██▍       | 48/200 [00:21<01:04,  2.34it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:22<01:04,  2.34it/s]Running loglikelihood requests:  25%|██▌       | 50/200 [00:22<01:04,  2.34it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:22<01:03,  2.34it/s]Running loglikelihood requests:  26%|██▌       | 52/200 [00:23<01:03,  2.34it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:23<01:02,  2.34it/s]Running loglikelihood requests:  27%|██▋       | 54/200 [00:24<01:04,  2.25it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:24<01:03,  2.29it/s]Running loglikelihood requests:  28%|██▊       | 56/200 [00:25<01:02,  2.31it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:25<01:01,  2.33it/s]Running loglikelihood requests:  29%|██▉       | 58/200 [00:25<01:00,  2.34it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:26<01:00,  2.35it/s]Running loglikelihood requests:  30%|███       | 60/200 [00:26<00:59,  2.35it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:27<00:59,  2.35it/s]Running loglikelihood requests:  31%|███       | 62/200 [00:27<00:58,  2.36it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:28<00:58,  2.36it/s]Running loglikelihood requests:  32%|███▏      | 64/200 [00:28<00:57,  2.36it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:28<00:57,  2.36it/s]Running loglikelihood requests:  33%|███▎      | 66/200 [00:29<00:56,  2.36it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:29<00:56,  2.36it/s]Running loglikelihood requests:  34%|███▍      | 68/200 [00:30<00:55,  2.37it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:30<00:55,  2.37it/s]Running loglikelihood requests:  35%|███▌      | 70/200 [00:31<00:54,  2.37it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:31<00:54,  2.37it/s]Running loglikelihood requests:  36%|███▌      | 72/200 [00:31<00:54,  2.37it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:32<00:53,  2.38it/s]Running loglikelihood requests:  37%|███▋      | 74/200 [00:32<00:52,  2.38it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:33<00:52,  2.38it/s]Running loglikelihood requests:  38%|███▊      | 76/200 [00:33<00:51,  2.39it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:34<00:51,  2.39it/s]Running loglikelihood requests:  39%|███▉      | 78/200 [00:34<00:50,  2.40it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:34<00:50,  2.40it/s]Running loglikelihood requests:  40%|████      | 80/200 [00:35<00:50,  2.40it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:35<00:50,  2.33it/s]Running loglikelihood requests:  41%|████      | 82/200 [00:36<00:50,  2.31it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:36<00:50,  2.32it/s]Running loglikelihood requests:  42%|████▏     | 84/200 [00:36<00:49,  2.35it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:37<00:48,  2.36it/s]Running loglikelihood requests:  43%|████▎     | 86/200 [00:37<00:47,  2.38it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:38<00:47,  2.38it/s]Running loglikelihood requests:  44%|████▍     | 88/200 [00:38<00:46,  2.39it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:39<00:46,  2.39it/s]Running loglikelihood requests:  45%|████▌     | 90/200 [00:39<00:46,  2.39it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:39<00:46,  2.34it/s]Running loglikelihood requests:  46%|████▌     | 92/200 [00:40<00:47,  2.30it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:40<00:46,  2.32it/s]Running loglikelihood requests:  47%|████▋     | 94/200 [00:41<00:45,  2.34it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:41<00:44,  2.35it/s]Running loglikelihood requests:  48%|████▊     | 96/200 [00:42<00:44,  2.36it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:42<00:44,  2.32it/s]Running loglikelihood requests:  49%|████▉     | 98/200 [00:42<00:43,  2.35it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:43<00:42,  2.37it/s]Running loglikelihood requests:  50%|█████     | 100/200 [00:43<00:42,  2.38it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:44<00:41,  2.38it/s]Running loglikelihood requests:  51%|█████     | 102/200 [00:44<00:41,  2.38it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:45<00:41,  2.35it/s]Running loglikelihood requests:  52%|█████▏    | 104/200 [00:45<00:41,  2.34it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:45<00:40,  2.33it/s]Running loglikelihood requests:  53%|█████▎    | 106/200 [00:46<00:40,  2.32it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:46<00:39,  2.34it/s]Running loglikelihood requests:  54%|█████▍    | 108/200 [00:47<00:38,  2.36it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:47<00:38,  2.37it/s]Running loglikelihood requests:  55%|█████▌    | 110/200 [00:48<00:37,  2.38it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:48<00:37,  2.39it/s]Running loglikelihood requests:  56%|█████▌    | 112/200 [00:48<00:36,  2.39it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:49<00:36,  2.40it/s]Running loglikelihood requests:  57%|█████▋    | 114/200 [00:49<00:35,  2.42it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:50<00:35,  2.42it/s]Running loglikelihood requests:  58%|█████▊    | 116/200 [00:50<00:34,  2.44it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:50<00:33,  2.45it/s]Running loglikelihood requests:  59%|█████▉    | 118/200 [00:51<00:33,  2.43it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:51<00:33,  2.43it/s]Running loglikelihood requests:  60%|██████    | 120/200 [00:52<00:32,  2.43it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:52<00:32,  2.42it/s]Running loglikelihood requests:  61%|██████    | 122/200 [00:52<00:32,  2.43it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:53<00:31,  2.44it/s]Running loglikelihood requests:  62%|██████▏   | 124/200 [00:53<00:30,  2.45it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:54<00:30,  2.45it/s]Running loglikelihood requests:  63%|██████▎   | 126/200 [00:54<00:30,  2.45it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:54<00:29,  2.46it/s]Running loglikelihood requests:  64%|██████▍   | 128/200 [00:55<00:29,  2.45it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:55<00:28,  2.45it/s]Running loglikelihood requests:  65%|██████▌   | 130/200 [00:56<00:28,  2.46it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:56<00:28,  2.46it/s]Running loglikelihood requests:  66%|██████▌   | 132/200 [00:57<00:27,  2.46it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:57<00:27,  2.45it/s]Running loglikelihood requests:  67%|██████▋   | 134/200 [00:57<00:26,  2.45it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:58<00:26,  2.45it/s]Running loglikelihood requests:  68%|██████▊   | 136/200 [00:58<00:26,  2.45it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:59<00:25,  2.45it/s]Running loglikelihood requests:  69%|██████▉   | 138/200 [00:59<00:25,  2.46it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:59<00:24,  2.46it/s]Running loglikelihood requests:  70%|███████   | 140/200 [01:00<00:24,  2.46it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:00<00:23,  2.46it/s]Running loglikelihood requests:  71%|███████   | 142/200 [01:01<00:23,  2.47it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:01<00:23,  2.47it/s]Running loglikelihood requests:  72%|███████▏  | 144/200 [01:01<00:22,  2.48it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:02<00:22,  2.48it/s]Running loglikelihood requests:  73%|███████▎  | 146/200 [01:02<00:21,  2.48it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:03<00:21,  2.49it/s]Running loglikelihood requests:  74%|███████▍  | 148/200 [01:03<00:20,  2.49it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:03<00:20,  2.49it/s]Running loglikelihood requests:  75%|███████▌  | 150/200 [01:04<00:20,  2.49it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:04<00:19,  2.48it/s]Running loglikelihood requests:  76%|███████▌  | 152/200 [01:05<00:19,  2.48it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:05<00:18,  2.48it/s]Running loglikelihood requests:  77%|███████▋  | 154/200 [01:05<00:18,  2.49it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:06<00:18,  2.49it/s]Running loglikelihood requests:  78%|███████▊  | 156/200 [01:06<00:17,  2.50it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:07<00:17,  2.50it/s]Running loglikelihood requests:  79%|███████▉  | 158/200 [01:07<00:16,  2.49it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:07<00:16,  2.49it/s]Running loglikelihood requests:  80%|████████  | 160/200 [01:08<00:15,  2.50it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:08<00:15,  2.51it/s]Running loglikelihood requests:  81%|████████  | 162/200 [01:09<00:15,  2.51it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:09<00:14,  2.52it/s]Running loglikelihood requests:  82%|████████▏ | 164/200 [01:09<00:14,  2.51it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:10<00:13,  2.51it/s]Running loglikelihood requests:  83%|████████▎ | 166/200 [01:10<00:13,  2.52it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:11<00:13,  2.53it/s]Running loglikelihood requests:  84%|████████▍ | 168/200 [01:11<00:12,  2.53it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:11<00:12,  2.52it/s]Running loglikelihood requests:  85%|████████▌ | 170/200 [01:12<00:11,  2.51it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:12<00:11,  2.51it/s]Running loglikelihood requests:  86%|████████▌ | 172/200 [01:13<00:11,  2.52it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:13<00:10,  2.53it/s]Running loglikelihood requests:  87%|████████▋ | 174/200 [01:13<00:10,  2.53it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:14<00:09,  2.54it/s]Running loglikelihood requests:  88%|████████▊ | 176/200 [01:14<00:09,  2.54it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:15<00:09,  2.55it/s]Running loglikelihood requests:  89%|████████▉ | 178/200 [01:15<00:08,  2.56it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:15<00:08,  2.55it/s]Running loglikelihood requests:  90%|█████████ | 180/200 [01:16<00:07,  2.56it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:16<00:07,  2.55it/s]Running loglikelihood requests:  91%|█████████ | 182/200 [01:16<00:07,  2.54it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:17<00:06,  2.56it/s]Running loglikelihood requests:  92%|█████████▏| 184/200 [01:17<00:06,  2.57it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:18<00:05,  2.57it/s]Running loglikelihood requests:  93%|█████████▎| 186/200 [01:18<00:05,  2.57it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:18<00:05,  2.58it/s]Running loglikelihood requests:  94%|█████████▍| 188/200 [01:19<00:04,  2.58it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:19<00:04,  2.59it/s]Running loglikelihood requests:  95%|█████████▌| 190/200 [01:20<00:03,  2.61it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:20<00:03,  2.60it/s]Running loglikelihood requests:  96%|█████████▌| 192/200 [01:20<00:03,  2.60it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:21<00:02,  2.60it/s]Running loglikelihood requests:  97%|█████████▋| 194/200 [01:21<00:02,  2.60it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:21<00:01,  2.64it/s]Running loglikelihood requests:  98%|█████████▊| 196/200 [01:22<00:01,  2.63it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:22<00:01,  2.65it/s]Running loglikelihood requests:  99%|█████████▉| 198/200 [01:23<00:00,  2.67it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:23<00:00,  2.67it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:23<00:00,  2.69it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:23<00:00,  2.39it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:1'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:1'}
full model:
{'copa': {'alias': 'copa', 'acc,none': 0.82, 'acc_stderr,none': 0.03861229196653691}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7827148355678417
0.8244609672079803
0.649635438690672
0.6067147398962018
0.806517251330611
0.9304450428937726
0.9492767057467371
0.8146777207922447
0.5943920512469448
0.6904992046065734
0.898578027138337
0.9772061768478973
0.9053772479641496
0.8020143483317842
0.5125553000556808
0.7018400400551881
0.8957865573767195
0.6076519427757794
0.975563476608663
0.9639212062070597
0.9296418237714587
0.9150145595079396
0.9197448761365332
0.7066536936990038
0.6639302750778917
0.912306872752127
0.7698141900267782
0.6230420491138425
0.8289959673107662
Total groups 68 exceeded the threshold, stopping comparison.
The group tensor is
[7, 3, 4, 1, 2, 6, 5, 0]
tensor([7, 3, 4, 1, 2, 6, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 4, 1, 0, 5, 7, 2]
tensor([6, 3, 4, 1, 0, 5, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 4, 3, 2, 0, 5, 6, 1]
tensor([7, 4, 3, 2, 0, 5, 6, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 6, 4, 2, 1, 3, 5, 0]
tensor([7, 6, 4, 2, 1, 3, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 0, 1, 2, 3, 2]
tensor([0, 1, 3, 0, 1, 2, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 1, 0, 2, 3, 2]
tensor([0, 1, 3, 1, 0, 2, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1.0, 1.0, 1]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 3
done!
Normal merging for layer 4
tensor([4])
tensor(4)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 6
done!
Normal merging for layer 7
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 8 to 15
done!
Normal merging for layer 16
tensor([0, 3])
tensor(0)
tensor([1, 4])
tensor(1)
tensor([5, 7])
tensor(5)
tensor([2, 6])
tensor(2)
done!
Cross-layer merge completed for layers 17 to 21
done!
Normal merging for layer 22
tensor([0, 4])
tensor(0)
tensor([1, 3])
tensor(1)
tensor([5, 7])
tensor(5)
tensor([2, 6])
tensor(2)
done!
Cross-layer merge completed for layers 23 to 27
done!
Normal merging for layer 28
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 29 to 30
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.1348 GB
146
cuda:1
mastermind_35_easy
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 25.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.06s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_35_mcq_random HTTP/1.1" 200 772
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/flair/mastermind_35_mcq_random/flair/mastermind_35_mcq_random.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_35_mcq_random HTTP/1.1" 200 780
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_35_mcq_random/resolve/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_35_mcq_random/revision/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 780
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_35_mcq_random/tree/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1?recursive=False&expand=False HTTP/1.1" 200 290
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_35_mcq_random/tree/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1/data?recursive=False&expand=False HTTP/1.1" 200 359
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/flair/mastermind_35_mcq_random/revision/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 780
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/flair/mastermind_35_mcq_random/resolve/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/flair/mastermind_35_mcq_random/paths-info/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1 HTTP/1.1" 200 218
DEBUG:filelock:Attempting to acquire lock 140545359708384 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_35_mcq_random_default_0.0.0_15dd5105771e9c8d2d3ea71c8d44fffda374a7a1.lock
DEBUG:filelock:Lock 140545359708384 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_35_mcq_random_default_0.0.0_15dd5105771e9c8d2d3ea71c8d44fffda374a7a1.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_35_mcq_random/default/0.0.0/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545359708384 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_35_mcq_random_default_0.0.0_15dd5105771e9c8d2d3ea71c8d44fffda374a7a1.lock
DEBUG:filelock:Lock 140545359708384 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_flair___mastermind_35_mcq_random_default_0.0.0_15dd5105771e9c8d2d3ea71c8d44fffda374a7a1.lock
DEBUG:filelock:Attempting to acquire lock 140516066045360 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_35_mcq_random/default/0.0.0/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1_builder.lock
DEBUG:filelock:Lock 140516066045360 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_35_mcq_random/default/0.0.0/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_35_mcq_random/default/0.0.0/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516066045360 on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_35_mcq_random/default/0.0.0/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1_builder.lock
DEBUG:filelock:Lock 140516066045360 released on /public/home/zouyifei001/.cache/huggingface/datasets/flair___mastermind_35_mcq_random/default/0.0.0/15dd5105771e9c8d2d3ea71c8d44fffda374a7a1_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of mastermind_35_easy from None to 0
INFO:lm_eval.api.task:Building contexts for mastermind_35_easy on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1507.89it/s]
DEBUG:lm_eval.evaluator:Task: mastermind_35_easy; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<12:10,  1.83s/it]Running loglikelihood requests:   0%|          | 2/400 [00:03<09:37,  1.45s/it]Running loglikelihood requests:   1%|          | 3/400 [00:04<08:46,  1.33s/it]Running loglikelihood requests:   1%|          | 4/400 [00:05<08:21,  1.27s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:06<08:06,  1.23s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:07<07:56,  1.21s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:08<07:49,  1.20s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:10<07:45,  1.19s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:11<07:41,  1.18s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:12<07:38,  1.18s/it]Running loglikelihood requests:   3%|▎         | 11/400 [00:13<07:36,  1.17s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:14<07:33,  1.17s/it]Running loglikelihood requests:   3%|▎         | 13/400 [00:15<07:31,  1.17s/it]Running loglikelihood requests:   4%|▎         | 14/400 [00:17<07:29,  1.16s/it]Running loglikelihood requests:   4%|▍         | 15/400 [00:18<07:27,  1.16s/it]Running loglikelihood requests:   4%|▍         | 16/400 [00:19<07:25,  1.16s/it]Running loglikelihood requests:   4%|▍         | 17/400 [00:20<07:23,  1.16s/it]Running loglikelihood requests:   4%|▍         | 18/400 [00:21<07:21,  1.16s/it]Running loglikelihood requests:   5%|▍         | 19/400 [00:22<07:19,  1.15s/it]Running loglikelihood requests:   5%|▌         | 20/400 [00:23<07:17,  1.15s/it]Running loglikelihood requests:   5%|▌         | 21/400 [00:25<07:16,  1.15s/it]Running loglikelihood requests:   6%|▌         | 23/400 [00:26<05:33,  1.13it/s]Running loglikelihood requests:   6%|▌         | 24/400 [00:27<05:56,  1.05it/s]Running loglikelihood requests:   6%|▋         | 25/400 [00:28<06:14,  1.00it/s]Running loglikelihood requests:   6%|▋         | 26/400 [00:29<06:27,  1.04s/it]Running loglikelihood requests:   7%|▋         | 27/400 [00:30<06:37,  1.06s/it]Running loglikelihood requests:   7%|▋         | 28/400 [00:31<06:44,  1.09s/it]Running loglikelihood requests:   7%|▋         | 29/400 [00:33<06:48,  1.10s/it]Running loglikelihood requests:   8%|▊         | 30/400 [00:34<06:52,  1.11s/it]Running loglikelihood requests:   8%|▊         | 31/400 [00:35<06:54,  1.12s/it]Running loglikelihood requests:   8%|▊         | 32/400 [00:36<06:55,  1.13s/it]Running loglikelihood requests:   8%|▊         | 33/400 [00:37<06:55,  1.13s/it]Running loglikelihood requests:   8%|▊         | 34/400 [00:38<06:54,  1.13s/it]Running loglikelihood requests:   9%|▉         | 35/400 [00:39<06:54,  1.14s/it]Running loglikelihood requests:   9%|▉         | 36/400 [00:41<06:53,  1.13s/it]Running loglikelihood requests:   9%|▉         | 37/400 [00:42<06:52,  1.14s/it]Running loglikelihood requests:  10%|▉         | 38/400 [00:43<06:50,  1.13s/it]Running loglikelihood requests:  10%|▉         | 39/400 [00:44<06:49,  1.14s/it]Running loglikelihood requests:  10%|█         | 40/400 [00:45<06:48,  1.13s/it]Running loglikelihood requests:  10%|█         | 41/400 [00:46<06:46,  1.13s/it]Running loglikelihood requests:  10%|█         | 42/400 [00:47<06:45,  1.13s/it]Running loglikelihood requests:  11%|█         | 43/400 [00:48<06:43,  1.13s/it]Running loglikelihood requests:  11%|█         | 44/400 [00:50<06:42,  1.13s/it]Running loglikelihood requests:  11%|█▏        | 45/400 [00:51<06:41,  1.13s/it]Running loglikelihood requests:  12%|█▏        | 46/400 [00:52<06:40,  1.13s/it]Running loglikelihood requests:  12%|█▏        | 47/400 [00:53<06:39,  1.13s/it]Running loglikelihood requests:  12%|█▏        | 48/400 [00:54<06:39,  1.13s/it]Running loglikelihood requests:  12%|█▏        | 49/400 [00:55<06:38,  1.13s/it]Running loglikelihood requests:  12%|█▎        | 50/400 [00:56<06:36,  1.13s/it]Running loglikelihood requests:  13%|█▎        | 51/400 [00:58<06:35,  1.13s/it]Running loglikelihood requests:  13%|█▎        | 52/400 [00:59<06:33,  1.13s/it]Running loglikelihood requests:  13%|█▎        | 53/400 [01:00<06:32,  1.13s/it]Running loglikelihood requests:  14%|█▎        | 54/400 [01:01<06:30,  1.13s/it]Running loglikelihood requests:  14%|█▍        | 55/400 [01:02<06:29,  1.13s/it]Running loglikelihood requests:  14%|█▍        | 56/400 [01:03<06:28,  1.13s/it]Running loglikelihood requests:  14%|█▍        | 57/400 [01:04<06:27,  1.13s/it]Running loglikelihood requests:  14%|█▍        | 58/400 [01:05<06:26,  1.13s/it]Running loglikelihood requests:  15%|█▍        | 59/400 [01:07<06:25,  1.13s/it]Running loglikelihood requests:  15%|█▌        | 60/400 [01:08<06:24,  1.13s/it]Running loglikelihood requests:  15%|█▌        | 61/400 [01:09<06:23,  1.13s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [01:10<06:22,  1.13s/it]Running loglikelihood requests:  16%|█▌        | 63/400 [01:11<06:20,  1.13s/it]Running loglikelihood requests:  16%|█▌        | 64/400 [01:12<06:18,  1.13s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [01:13<06:17,  1.13s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [01:14<06:15,  1.12s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [01:16<06:14,  1.12s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [01:17<06:12,  1.12s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [01:18<06:11,  1.12s/it]Running loglikelihood requests:  18%|█▊        | 70/400 [01:19<06:23,  1.16s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [01:20<06:33,  1.20s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [01:22<06:40,  1.22s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [01:23<06:44,  1.24s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [01:24<06:48,  1.25s/it]Running loglikelihood requests:  19%|█▉        | 75/400 [01:25<06:50,  1.26s/it]Running loglikelihood requests:  19%|█▉        | 76/400 [01:27<06:50,  1.27s/it]Running loglikelihood requests:  19%|█▉        | 77/400 [01:28<06:35,  1.22s/it]Running loglikelihood requests:  20%|█▉        | 78/400 [01:29<06:24,  1.19s/it]Running loglikelihood requests:  20%|█▉        | 79/400 [01:30<06:19,  1.18s/it]Running loglikelihood requests:  20%|██        | 80/400 [01:31<06:14,  1.17s/it]Running loglikelihood requests:  20%|██        | 81/400 [01:32<06:11,  1.17s/it]Running loglikelihood requests:  20%|██        | 82/400 [01:34<06:07,  1.16s/it]Running loglikelihood requests:  21%|██        | 83/400 [01:35<06:04,  1.15s/it]Running loglikelihood requests:  21%|██        | 84/400 [01:36<06:01,  1.15s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [01:37<05:59,  1.14s/it]Running loglikelihood requests:  22%|██▏       | 86/400 [01:38<05:56,  1.13s/it]Running loglikelihood requests:  22%|██▏       | 87/400 [01:39<05:53,  1.13s/it]Running loglikelihood requests:  22%|██▏       | 88/400 [01:40<05:51,  1.13s/it]Running loglikelihood requests:  22%|██▏       | 89/400 [01:41<05:49,  1.12s/it]Running loglikelihood requests:  22%|██▎       | 90/400 [01:43<05:48,  1.12s/it]Running loglikelihood requests:  23%|██▎       | 91/400 [01:44<05:46,  1.12s/it]Running loglikelihood requests:  23%|██▎       | 92/400 [01:45<05:45,  1.12s/it]Running loglikelihood requests:  23%|██▎       | 93/400 [01:46<05:43,  1.12s/it]Running loglikelihood requests:  24%|██▎       | 94/400 [01:47<05:42,  1.12s/it]Running loglikelihood requests:  24%|██▍       | 95/400 [01:48<05:41,  1.12s/it]Running loglikelihood requests:  24%|██▍       | 96/400 [01:49<05:40,  1.12s/it]Running loglikelihood requests:  24%|██▍       | 97/400 [01:50<05:39,  1.12s/it]Running loglikelihood requests:  24%|██▍       | 98/400 [01:52<05:38,  1.12s/it]Running loglikelihood requests:  25%|██▍       | 99/400 [01:53<05:37,  1.12s/it]Running loglikelihood requests:  25%|██▌       | 100/400 [01:54<05:35,  1.12s/it]Running loglikelihood requests:  25%|██▌       | 101/400 [01:55<05:34,  1.12s/it]Running loglikelihood requests:  26%|██▌       | 102/400 [01:56<05:32,  1.12s/it]Running loglikelihood requests:  26%|██▌       | 103/400 [01:57<05:31,  1.12s/it]Running loglikelihood requests:  26%|██▌       | 104/400 [01:58<05:30,  1.12s/it]Running loglikelihood requests:  26%|██▋       | 105/400 [01:59<05:28,  1.12s/it]Running loglikelihood requests:  26%|██▋       | 106/400 [02:00<05:27,  1.11s/it]Running loglikelihood requests:  27%|██▋       | 107/400 [02:02<05:26,  1.11s/it]Running loglikelihood requests:  27%|██▋       | 108/400 [02:03<05:25,  1.11s/it]Running loglikelihood requests:  27%|██▋       | 109/400 [02:04<05:23,  1.11s/it]Running loglikelihood requests:  28%|██▊       | 110/400 [02:05<05:22,  1.11s/it]Running loglikelihood requests:  28%|██▊       | 112/400 [02:06<04:06,  1.17it/s]Running loglikelihood requests:  28%|██▊       | 113/400 [02:07<04:24,  1.09it/s]Running loglikelihood requests:  28%|██▊       | 114/400 [02:08<04:37,  1.03it/s]Running loglikelihood requests:  29%|██▉       | 115/400 [02:09<04:47,  1.01s/it]Running loglikelihood requests:  29%|██▉       | 116/400 [02:11<04:56,  1.04s/it]Running loglikelihood requests:  29%|██▉       | 117/400 [02:12<05:01,  1.07s/it]Running loglikelihood requests:  30%|██▉       | 118/400 [02:13<05:05,  1.08s/it]Running loglikelihood requests:  30%|██▉       | 119/400 [02:14<05:17,  1.13s/it]Running loglikelihood requests:  30%|███       | 120/400 [02:15<05:19,  1.14s/it]Running loglikelihood requests:  30%|███       | 121/400 [02:16<05:30,  1.18s/it]Running loglikelihood requests:  30%|███       | 122/400 [02:18<05:34,  1.20s/it]Running loglikelihood requests:  31%|███       | 123/400 [02:19<05:31,  1.20s/it]Running loglikelihood requests:  31%|███       | 124/400 [02:20<05:24,  1.17s/it]Running loglikelihood requests:  31%|███▏      | 125/400 [02:21<05:18,  1.16s/it]Running loglikelihood requests:  32%|███▏      | 126/400 [02:22<05:13,  1.14s/it]Running loglikelihood requests:  32%|███▏      | 127/400 [02:23<05:09,  1.13s/it]Running loglikelihood requests:  32%|███▏      | 128/400 [02:24<05:06,  1.13s/it]Running loglikelihood requests:  32%|███▏      | 129/400 [02:26<05:04,  1.12s/it]Running loglikelihood requests:  32%|███▎      | 130/400 [02:27<05:02,  1.12s/it]Running loglikelihood requests:  33%|███▎      | 131/400 [02:28<05:00,  1.12s/it]Running loglikelihood requests:  33%|███▎      | 132/400 [02:29<04:59,  1.12s/it]Running loglikelihood requests:  33%|███▎      | 133/400 [02:30<04:57,  1.11s/it]Running loglikelihood requests:  34%|███▎      | 134/400 [02:31<04:56,  1.11s/it]Running loglikelihood requests:  34%|███▍      | 135/400 [02:32<04:54,  1.11s/it]Running loglikelihood requests:  34%|███▍      | 136/400 [02:33<04:53,  1.11s/it]Running loglikelihood requests:  34%|███▍      | 137/400 [02:34<04:51,  1.11s/it]Running loglikelihood requests:  34%|███▍      | 138/400 [02:36<04:50,  1.11s/it]Running loglikelihood requests:  35%|███▍      | 139/400 [02:37<04:48,  1.11s/it]Running loglikelihood requests:  35%|███▌      | 140/400 [02:38<04:47,  1.11s/it]Running loglikelihood requests:  35%|███▌      | 141/400 [02:39<04:46,  1.11s/it]Running loglikelihood requests:  36%|███▌      | 142/400 [02:40<04:45,  1.11s/it]Running loglikelihood requests:  36%|███▌      | 143/400 [02:41<04:44,  1.11s/it]Running loglikelihood requests:  36%|███▌      | 144/400 [02:42<04:42,  1.10s/it]Running loglikelihood requests:  36%|███▋      | 145/400 [02:43<04:41,  1.10s/it]Running loglikelihood requests:  36%|███▋      | 146/400 [02:44<04:40,  1.10s/it]Running loglikelihood requests:  37%|███▋      | 147/400 [02:45<04:39,  1.10s/it]Running loglikelihood requests:  37%|███▋      | 148/400 [02:47<04:37,  1.10s/it]Running loglikelihood requests:  37%|███▋      | 149/400 [02:48<04:36,  1.10s/it]Running loglikelihood requests:  38%|███▊      | 150/400 [02:49<04:35,  1.10s/it]Running loglikelihood requests:  38%|███▊      | 151/400 [02:50<04:34,  1.10s/it]Running loglikelihood requests:  38%|███▊      | 152/400 [02:51<04:33,  1.10s/it]Running loglikelihood requests:  38%|███▊      | 153/400 [02:52<04:31,  1.10s/it]Running loglikelihood requests:  38%|███▊      | 154/400 [02:53<04:30,  1.10s/it]Running loglikelihood requests:  39%|███▉      | 155/400 [02:54<04:29,  1.10s/it]Running loglikelihood requests:  39%|███▉      | 156/400 [02:55<04:28,  1.10s/it]Running loglikelihood requests:  39%|███▉      | 157/400 [02:56<04:26,  1.10s/it]Running loglikelihood requests:  40%|███▉      | 158/400 [02:58<04:25,  1.10s/it]Running loglikelihood requests:  40%|███▉      | 159/400 [02:59<04:24,  1.10s/it]Running loglikelihood requests:  40%|████      | 160/400 [03:00<04:24,  1.10s/it]Running loglikelihood requests:  40%|████      | 161/400 [03:01<04:22,  1.10s/it]Running loglikelihood requests:  40%|████      | 162/400 [03:02<04:20,  1.10s/it]Running loglikelihood requests:  41%|████      | 163/400 [03:03<04:19,  1.09s/it]Running loglikelihood requests:  41%|████      | 164/400 [03:04<04:18,  1.09s/it]Running loglikelihood requests:  41%|████▏     | 165/400 [03:05<04:16,  1.09s/it]Running loglikelihood requests:  42%|████▏     | 166/400 [03:06<04:15,  1.09s/it]Running loglikelihood requests:  42%|████▏     | 167/400 [03:07<04:14,  1.09s/it]Running loglikelihood requests:  42%|████▏     | 168/400 [03:09<04:13,  1.09s/it]Running loglikelihood requests:  42%|████▏     | 169/400 [03:10<04:12,  1.09s/it]Running loglikelihood requests:  42%|████▎     | 170/400 [03:11<04:10,  1.09s/it]Running loglikelihood requests:  43%|████▎     | 171/400 [03:12<04:09,  1.09s/it]Running loglikelihood requests:  43%|████▎     | 172/400 [03:13<04:07,  1.09s/it]Running loglikelihood requests:  43%|████▎     | 173/400 [03:14<04:06,  1.09s/it]Running loglikelihood requests:  44%|████▎     | 174/400 [03:15<04:05,  1.09s/it]Running loglikelihood requests:  44%|████▍     | 175/400 [03:16<04:04,  1.09s/it]Running loglikelihood requests:  44%|████▍     | 177/400 [03:17<03:06,  1.20it/s]Running loglikelihood requests:  44%|████▍     | 178/400 [03:18<03:19,  1.11it/s]Running loglikelihood requests:  45%|████▍     | 179/400 [03:19<03:29,  1.06it/s]Running loglikelihood requests:  45%|████▌     | 180/400 [03:20<03:36,  1.02it/s]Running loglikelihood requests:  45%|████▌     | 181/400 [03:22<03:41,  1.01s/it]Running loglikelihood requests:  46%|████▌     | 182/400 [03:23<03:45,  1.03s/it]Running loglikelihood requests:  46%|████▌     | 183/400 [03:24<03:47,  1.05s/it]Running loglikelihood requests:  46%|████▌     | 184/400 [03:25<03:48,  1.06s/it]Running loglikelihood requests:  46%|████▋     | 185/400 [03:26<03:49,  1.07s/it]Running loglikelihood requests:  46%|████▋     | 186/400 [03:27<03:49,  1.07s/it]Running loglikelihood requests:  47%|████▋     | 187/400 [03:28<03:49,  1.08s/it]Running loglikelihood requests:  47%|████▋     | 188/400 [03:29<03:48,  1.08s/it]Running loglikelihood requests:  47%|████▋     | 189/400 [03:30<03:47,  1.08s/it]Running loglikelihood requests:  48%|████▊     | 190/400 [03:31<03:46,  1.08s/it]Running loglikelihood requests:  48%|████▊     | 191/400 [03:32<03:45,  1.08s/it]Running loglikelihood requests:  48%|████▊     | 192/400 [03:33<03:45,  1.08s/it]Running loglikelihood requests:  48%|████▊     | 193/400 [03:35<03:43,  1.08s/it]Running loglikelihood requests:  48%|████▊     | 194/400 [03:36<03:42,  1.08s/it]Running loglikelihood requests:  49%|████▉     | 195/400 [03:37<03:41,  1.08s/it]Running loglikelihood requests:  49%|████▉     | 196/400 [03:38<03:40,  1.08s/it]Running loglikelihood requests:  49%|████▉     | 197/400 [03:39<03:39,  1.08s/it]Running loglikelihood requests:  50%|████▉     | 198/400 [03:40<03:38,  1.08s/it]Running loglikelihood requests:  50%|████▉     | 199/400 [03:41<03:37,  1.08s/it]Running loglikelihood requests:  50%|█████     | 200/400 [03:42<03:35,  1.08s/it]Running loglikelihood requests:  50%|█████     | 201/400 [03:43<03:34,  1.08s/it]Running loglikelihood requests:  50%|█████     | 202/400 [03:44<03:33,  1.08s/it]Running loglikelihood requests:  51%|█████     | 203/400 [03:45<03:33,  1.08s/it]Running loglikelihood requests:  51%|█████▏    | 205/400 [03:46<02:42,  1.20it/s]Running loglikelihood requests:  52%|█████▏    | 206/400 [03:48<02:53,  1.12it/s]Running loglikelihood requests:  52%|█████▏    | 207/400 [03:49<03:02,  1.06it/s]Running loglikelihood requests:  52%|█████▏    | 208/400 [03:50<03:08,  1.02it/s]Running loglikelihood requests:  52%|█████▏    | 209/400 [03:51<03:12,  1.01s/it]Running loglikelihood requests:  52%|█████▎    | 210/400 [03:52<03:15,  1.03s/it]Running loglikelihood requests:  53%|█████▎    | 211/400 [03:53<03:17,  1.04s/it]Running loglikelihood requests:  53%|█████▎    | 212/400 [03:54<03:18,  1.06s/it]Running loglikelihood requests:  53%|█████▎    | 213/400 [03:55<03:18,  1.06s/it]Running loglikelihood requests:  54%|█████▎    | 214/400 [03:56<03:18,  1.07s/it]Running loglikelihood requests:  54%|█████▍    | 215/400 [03:57<03:17,  1.07s/it]Running loglikelihood requests:  54%|█████▍    | 216/400 [03:58<03:17,  1.07s/it]Running loglikelihood requests:  54%|█████▍    | 217/400 [03:59<03:16,  1.07s/it]Running loglikelihood requests:  55%|█████▍    | 218/400 [04:00<03:15,  1.07s/it]Running loglikelihood requests:  55%|█████▍    | 219/400 [04:02<03:14,  1.07s/it]Running loglikelihood requests:  55%|█████▌    | 220/400 [04:03<03:13,  1.07s/it]Running loglikelihood requests:  55%|█████▌    | 221/400 [04:04<03:12,  1.07s/it]Running loglikelihood requests:  56%|█████▌    | 222/400 [04:05<03:10,  1.07s/it]Running loglikelihood requests:  56%|█████▌    | 223/400 [04:06<03:09,  1.07s/it]Running loglikelihood requests:  56%|█████▌    | 224/400 [04:07<03:08,  1.07s/it]Running loglikelihood requests:  56%|█████▋    | 225/400 [04:08<03:07,  1.07s/it]Running loglikelihood requests:  56%|█████▋    | 226/400 [04:09<03:06,  1.07s/it]Running loglikelihood requests:  57%|█████▋    | 227/400 [04:10<03:05,  1.07s/it]Running loglikelihood requests:  57%|█████▋    | 228/400 [04:11<03:04,  1.07s/it]Running loglikelihood requests:  57%|█████▋    | 229/400 [04:12<03:03,  1.07s/it]Running loglikelihood requests:  57%|█████▊    | 230/400 [04:13<03:02,  1.07s/it]Running loglikelihood requests:  58%|█████▊    | 231/400 [04:14<03:00,  1.07s/it]Running loglikelihood requests:  58%|█████▊    | 232/400 [04:15<02:59,  1.07s/it]Running loglikelihood requests:  58%|█████▊    | 233/400 [04:17<02:58,  1.07s/it]Running loglikelihood requests:  58%|█████▊    | 234/400 [04:18<02:57,  1.07s/it]Running loglikelihood requests:  59%|█████▉    | 235/400 [04:19<02:56,  1.07s/it]Running loglikelihood requests:  59%|█████▉    | 236/400 [04:20<02:55,  1.07s/it]Running loglikelihood requests:  59%|█████▉    | 237/400 [04:21<02:53,  1.07s/it]Running loglikelihood requests:  60%|█████▉    | 238/400 [04:22<02:52,  1.07s/it]Running loglikelihood requests:  60%|█████▉    | 239/400 [04:23<02:51,  1.07s/it]Running loglikelihood requests:  60%|██████    | 241/400 [04:24<02:10,  1.22it/s]Running loglikelihood requests:  60%|██████    | 242/400 [04:25<02:19,  1.13it/s]Running loglikelihood requests:  61%|██████    | 243/400 [04:26<02:26,  1.07it/s]Running loglikelihood requests:  61%|██████    | 244/400 [04:27<02:31,  1.03it/s]Running loglikelihood requests:  61%|██████▏   | 245/400 [04:28<02:34,  1.00it/s]Running loglikelihood requests:  62%|██████▏   | 246/400 [04:29<02:36,  1.02s/it]Running loglikelihood requests:  62%|██████▏   | 247/400 [04:30<02:37,  1.03s/it]Running loglikelihood requests:  62%|██████▏   | 248/400 [04:31<02:38,  1.04s/it]Running loglikelihood requests:  62%|██████▏   | 249/400 [04:33<02:38,  1.05s/it]Running loglikelihood requests:  62%|██████▎   | 250/400 [04:34<02:38,  1.06s/it]Running loglikelihood requests:  63%|██████▎   | 251/400 [04:35<02:38,  1.06s/it]Running loglikelihood requests:  63%|██████▎   | 252/400 [04:36<02:37,  1.06s/it]Running loglikelihood requests:  63%|██████▎   | 253/400 [04:37<02:36,  1.06s/it]Running loglikelihood requests:  64%|██████▍   | 255/400 [04:38<01:58,  1.22it/s]Running loglikelihood requests:  64%|██████▍   | 256/400 [04:39<02:06,  1.13it/s]Running loglikelihood requests:  64%|██████▍   | 257/400 [04:40<02:12,  1.08it/s]Running loglikelihood requests:  64%|██████▍   | 258/400 [04:41<02:17,  1.04it/s]Running loglikelihood requests:  65%|██████▍   | 259/400 [04:42<02:20,  1.01it/s]Running loglikelihood requests:  65%|██████▌   | 260/400 [04:43<02:22,  1.01s/it]Running loglikelihood requests:  65%|██████▌   | 261/400 [04:44<02:22,  1.03s/it]Running loglikelihood requests:  66%|██████▌   | 262/400 [04:45<02:23,  1.04s/it]Running loglikelihood requests:  66%|██████▌   | 263/400 [04:46<02:23,  1.04s/it]Running loglikelihood requests:  66%|██████▌   | 264/400 [04:47<02:22,  1.05s/it]Running loglikelihood requests:  66%|██████▋   | 265/400 [04:49<02:22,  1.05s/it]Running loglikelihood requests:  66%|██████▋   | 266/400 [04:50<02:21,  1.05s/it]Running loglikelihood requests:  67%|██████▋   | 267/400 [04:51<02:20,  1.06s/it]Running loglikelihood requests:  67%|██████▋   | 268/400 [04:52<02:19,  1.06s/it]Running loglikelihood requests:  68%|██████▊   | 270/400 [04:53<01:45,  1.23it/s]Running loglikelihood requests:  68%|██████▊   | 271/400 [04:54<01:52,  1.15it/s]Running loglikelihood requests:  68%|██████▊   | 272/400 [04:55<01:57,  1.09it/s]Running loglikelihood requests:  68%|██████▊   | 273/400 [04:56<02:01,  1.05it/s]Running loglikelihood requests:  68%|██████▊   | 274/400 [04:57<02:05,  1.01it/s]Running loglikelihood requests:  69%|██████▉   | 275/400 [04:58<02:06,  1.01s/it]Running loglikelihood requests:  69%|██████▉   | 276/400 [04:59<02:06,  1.02s/it]Running loglikelihood requests:  69%|██████▉   | 277/400 [05:00<02:06,  1.03s/it]Running loglikelihood requests:  70%|██████▉   | 278/400 [05:01<02:06,  1.03s/it]Running loglikelihood requests:  70%|██████▉   | 279/400 [05:02<02:05,  1.04s/it]Running loglikelihood requests:  70%|███████   | 280/400 [05:03<02:04,  1.04s/it]Running loglikelihood requests:  70%|███████   | 281/400 [05:04<02:04,  1.04s/it]Running loglikelihood requests:  70%|███████   | 282/400 [05:05<02:03,  1.04s/it]Running loglikelihood requests:  71%|███████   | 283/400 [05:06<02:02,  1.04s/it]Running loglikelihood requests:  71%|███████   | 284/400 [05:08<02:01,  1.04s/it]Running loglikelihood requests:  71%|███████▏  | 285/400 [05:09<02:00,  1.04s/it]Running loglikelihood requests:  72%|███████▏  | 286/400 [05:10<01:58,  1.04s/it]Running loglikelihood requests:  72%|███████▏  | 287/400 [05:11<01:57,  1.04s/it]Running loglikelihood requests:  72%|███████▏  | 288/400 [05:12<01:56,  1.04s/it]Running loglikelihood requests:  72%|███████▏  | 289/400 [05:13<01:55,  1.04s/it]Running loglikelihood requests:  72%|███████▎  | 290/400 [05:14<01:54,  1.04s/it]Running loglikelihood requests:  73%|███████▎  | 291/400 [05:15<01:53,  1.04s/it]Running loglikelihood requests:  73%|███████▎  | 292/400 [05:16<01:52,  1.04s/it]Running loglikelihood requests:  73%|███████▎  | 293/400 [05:17<01:51,  1.04s/it]Running loglikelihood requests:  74%|███████▎  | 294/400 [05:18<01:50,  1.04s/it]Running loglikelihood requests:  74%|███████▍  | 295/400 [05:19<01:49,  1.04s/it]Running loglikelihood requests:  74%|███████▍  | 296/400 [05:20<01:48,  1.04s/it]Running loglikelihood requests:  74%|███████▍  | 297/400 [05:21<01:47,  1.04s/it]Running loglikelihood requests:  74%|███████▍  | 298/400 [05:22<01:46,  1.05s/it]Running loglikelihood requests:  75%|███████▍  | 299/400 [05:23<01:45,  1.05s/it]Running loglikelihood requests:  75%|███████▌  | 300/400 [05:24<01:44,  1.04s/it]Running loglikelihood requests:  75%|███████▌  | 301/400 [05:25<01:43,  1.04s/it]Running loglikelihood requests:  76%|███████▌  | 302/400 [05:26<01:42,  1.04s/it]Running loglikelihood requests:  76%|███████▌  | 304/400 [05:27<01:16,  1.25it/s]Running loglikelihood requests:  76%|███████▋  | 305/400 [05:28<01:21,  1.16it/s]Running loglikelihood requests:  76%|███████▋  | 306/400 [05:29<01:25,  1.10it/s]Running loglikelihood requests:  77%|███████▋  | 307/400 [05:30<01:27,  1.06it/s]Running loglikelihood requests:  77%|███████▋  | 308/400 [05:31<01:28,  1.04it/s]Running loglikelihood requests:  77%|███████▋  | 309/400 [05:32<01:29,  1.02it/s]Running loglikelihood requests:  78%|███████▊  | 310/400 [05:34<01:29,  1.00it/s]Running loglikelihood requests:  78%|███████▊  | 311/400 [05:35<01:29,  1.01s/it]Running loglikelihood requests:  78%|███████▊  | 312/400 [05:36<01:29,  1.01s/it]Running loglikelihood requests:  78%|███████▊  | 313/400 [05:37<01:28,  1.02s/it]Running loglikelihood requests:  78%|███████▊  | 314/400 [05:38<01:27,  1.02s/it]Running loglikelihood requests:  79%|███████▉  | 315/400 [05:39<01:26,  1.02s/it]Running loglikelihood requests:  79%|███████▉  | 316/400 [05:40<01:25,  1.02s/it]Running loglikelihood requests:  79%|███████▉  | 317/400 [05:41<01:25,  1.03s/it]Running loglikelihood requests:  80%|███████▉  | 318/400 [05:42<01:27,  1.06s/it]Running loglikelihood requests:  80%|███████▉  | 319/400 [05:43<01:26,  1.07s/it]Running loglikelihood requests:  80%|████████  | 320/400 [05:44<01:24,  1.05s/it]Running loglikelihood requests:  80%|████████  | 321/400 [05:45<01:22,  1.04s/it]Running loglikelihood requests:  80%|████████  | 322/400 [05:46<01:20,  1.04s/it]Running loglikelihood requests:  81%|████████  | 323/400 [05:47<01:19,  1.03s/it]Running loglikelihood requests:  81%|████████  | 324/400 [05:48<01:18,  1.03s/it]Running loglikelihood requests:  81%|████████▏ | 325/400 [05:49<01:17,  1.03s/it]Running loglikelihood requests:  82%|████████▏ | 326/400 [05:50<01:15,  1.03s/it]Running loglikelihood requests:  82%|████████▏ | 327/400 [05:51<01:14,  1.03s/it]Running loglikelihood requests:  82%|████████▏ | 328/400 [05:52<01:13,  1.02s/it]Running loglikelihood requests:  82%|████████▎ | 330/400 [05:53<00:55,  1.27it/s]Running loglikelihood requests:  83%|████████▎ | 331/400 [05:54<00:58,  1.19it/s]Running loglikelihood requests:  83%|████████▎ | 332/400 [05:55<01:00,  1.12it/s]Running loglikelihood requests:  83%|████████▎ | 333/400 [05:56<01:01,  1.08it/s]Running loglikelihood requests:  84%|████████▎ | 334/400 [05:57<01:02,  1.05it/s]Running loglikelihood requests:  84%|████████▍ | 335/400 [05:58<01:02,  1.03it/s]Running loglikelihood requests:  84%|████████▍ | 336/400 [05:59<01:02,  1.02it/s]Running loglikelihood requests:  84%|████████▍ | 337/400 [06:00<01:02,  1.01it/s]Running loglikelihood requests:  84%|████████▍ | 338/400 [06:01<01:01,  1.00it/s]Running loglikelihood requests:  85%|████████▍ | 339/400 [06:02<01:01,  1.00s/it]Running loglikelihood requests:  85%|████████▌ | 340/400 [06:03<01:00,  1.01s/it]Running loglikelihood requests:  85%|████████▌ | 341/400 [06:04<00:59,  1.01s/it]Running loglikelihood requests:  86%|████████▌ | 342/400 [06:05<00:58,  1.01s/it]Running loglikelihood requests:  86%|████████▌ | 343/400 [06:06<00:57,  1.01s/it]Running loglikelihood requests:  86%|████████▌ | 344/400 [06:07<00:56,  1.01s/it]Running loglikelihood requests:  86%|████████▋ | 345/400 [06:08<00:55,  1.01s/it]Running loglikelihood requests:  86%|████████▋ | 346/400 [06:09<00:54,  1.02s/it]Running loglikelihood requests:  87%|████████▋ | 347/400 [06:10<00:53,  1.02s/it]Running loglikelihood requests:  87%|████████▋ | 348/400 [06:11<00:52,  1.01s/it]Running loglikelihood requests:  87%|████████▋ | 349/400 [06:12<00:51,  1.01s/it]Running loglikelihood requests:  88%|████████▊ | 350/400 [06:13<00:50,  1.01s/it]Running loglikelihood requests:  88%|████████▊ | 351/400 [06:14<00:49,  1.00s/it]Running loglikelihood requests:  88%|████████▊ | 352/400 [06:15<00:47,  1.00it/s]Running loglikelihood requests:  88%|████████▊ | 353/400 [06:16<00:46,  1.00it/s]Running loglikelihood requests:  88%|████████▊ | 354/400 [06:17<00:45,  1.00it/s]Running loglikelihood requests:  89%|████████▉ | 355/400 [06:18<00:44,  1.00it/s]Running loglikelihood requests:  89%|████████▉ | 356/400 [06:19<00:43,  1.00it/s]Running loglikelihood requests:  89%|████████▉ | 357/400 [06:20<00:42,  1.01it/s]Running loglikelihood requests:  90%|████████▉ | 358/400 [06:21<00:41,  1.01it/s]Running loglikelihood requests:  90%|████████▉ | 359/400 [06:22<00:40,  1.01it/s]Running loglikelihood requests:  90%|█████████ | 360/400 [06:23<00:39,  1.01it/s]Running loglikelihood requests:  90%|█████████ | 361/400 [06:24<00:38,  1.01it/s]Running loglikelihood requests:  90%|█████████ | 362/400 [06:25<00:37,  1.02it/s]Running loglikelihood requests:  91%|█████████ | 363/400 [06:26<00:36,  1.02it/s]Running loglikelihood requests:  91%|█████████ | 364/400 [06:27<00:35,  1.02it/s]Running loglikelihood requests:  91%|█████████▏| 365/400 [06:28<00:34,  1.02it/s]Running loglikelihood requests:  92%|█████████▏| 366/400 [06:29<00:33,  1.02it/s]Running loglikelihood requests:  92%|█████████▏| 367/400 [06:30<00:32,  1.02it/s]Running loglikelihood requests:  92%|█████████▏| 368/400 [06:31<00:31,  1.03it/s]Running loglikelihood requests:  92%|█████████▏| 369/400 [06:32<00:30,  1.03it/s]Running loglikelihood requests:  92%|█████████▎| 370/400 [06:33<00:29,  1.03it/s]Running loglikelihood requests:  93%|█████████▎| 371/400 [06:34<00:28,  1.03it/s]Running loglikelihood requests:  93%|█████████▎| 372/400 [06:35<00:27,  1.03it/s]Running loglikelihood requests:  93%|█████████▎| 373/400 [06:36<00:26,  1.03it/s]Running loglikelihood requests:  94%|█████████▎| 374/400 [06:37<00:25,  1.03it/s]Running loglikelihood requests:  94%|█████████▍| 375/400 [06:38<00:24,  1.03it/s]Running loglikelihood requests:  94%|█████████▍| 376/400 [06:39<00:23,  1.04it/s]Running loglikelihood requests:  94%|█████████▍| 377/400 [06:40<00:22,  1.04it/s]Running loglikelihood requests:  94%|█████████▍| 378/400 [06:41<00:21,  1.03it/s]Running loglikelihood requests:  95%|█████████▍| 379/400 [06:42<00:20,  1.02it/s]Running loglikelihood requests:  95%|█████████▌| 380/400 [06:43<00:19,  1.03it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [06:44<00:18,  1.03it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [06:45<00:17,  1.04it/s]Running loglikelihood requests:  96%|█████████▌| 383/400 [06:46<00:16,  1.04it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [06:47<00:15,  1.05it/s]Running loglikelihood requests:  96%|█████████▋| 385/400 [06:48<00:14,  1.05it/s]Running loglikelihood requests:  96%|█████████▋| 386/400 [06:48<00:13,  1.05it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [06:49<00:12,  1.05it/s]Running loglikelihood requests:  97%|█████████▋| 388/400 [06:50<00:11,  1.05it/s]Running loglikelihood requests:  97%|█████████▋| 389/400 [06:51<00:10,  1.05it/s]Running loglikelihood requests:  98%|█████████▊| 390/400 [06:52<00:09,  1.05it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [06:53<00:08,  1.05it/s]Running loglikelihood requests:  98%|█████████▊| 392/400 [06:54<00:07,  1.05it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [06:55<00:06,  1.05it/s]Running loglikelihood requests:  98%|█████████▊| 394/400 [06:56<00:05,  1.05it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [06:57<00:04,  1.05it/s]Running loglikelihood requests:  99%|█████████▉| 396/400 [06:58<00:03,  1.05it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [06:59<00:02,  1.06it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [07:00<00:01,  1.07it/s]Running loglikelihood requests: 100%|█████████▉| 399/400 [07:01<00:00,  1.07it/s]Running loglikelihood requests: 100%|██████████| 400/400 [07:02<00:00,  1.08it/s]Running loglikelihood requests: 100%|██████████| 400/400 [07:02<00:00,  1.06s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:2'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:2'}
full model:
{'mastermind_35_easy': {'alias': 'mastermind_35_easy', 'acc,none': 0.51, 'acc_stderr,none': 0.05024183937956913}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9685093904417202
0.9837389482734848
0.9887891422859466
0.9728424941793791
0.9273096247071001
0.9956884174036557
0.9936991917224709
0.990637728847171
0.984416562505352
0.9357041463019401
0.957486187236088
0.9794440444506461
0.9854000882321717
0.989321120949945
0.9948124947717892
0.9950699411775289
0.9589145403056732
0.9488498047330014
0.979093344902028
0.9861615563222426
0.9944647439314634
0.9974738465425171
0.9932459641322177
0.9612113452387381
0.9566187588663586
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 1, 0, 3, 7, 6, 2, 4]
tensor([5, 1, 0, 3, 7, 6, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 0, 3, 7, 5, 2, 4]
tensor([6, 1, 0, 3, 7, 5, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 1, 0, 3, 7, 6, 2, 4]
tensor([5, 1, 0, 3, 7, 6, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 0, 3, 7, 5, 2, 4]
tensor([6, 1, 0, 3, 7, 5, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 1, 0, 3, 7, 6, 2, 4]
tensor([5, 1, 0, 3, 7, 6, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 0, 3, 7, 5, 2, 4]
tensor([6, 1, 0, 3, 7, 5, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 3
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
done!
Normal merging for layer 5
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 6 to 31
done!
all done!
Model size: 12.0718 GB
5
cuda:2
winogrande
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 25.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.97s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/winogrande HTTP/1.1" 307 67
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/winogrande HTTP/1.1" 200 1036
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/winogrande/winogrande.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): datasets-server.hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://datasets-server.hf-mirror.com:443 "GET /parquet?dataset=winogrande HTTP/1.1" 302 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET / HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/winogrande/winogrande.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/winogrande/resolve/main/winogrande.py HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/winogrande/resolve/main/winogrande.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/winogrande/resolve/main/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/winogrande/resolve/main/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/winogrande/resolve/main/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/winogrande/resolve/main/README.md HTTP/1.1" 200 0
DEBUG:filelock:Attempting to acquire lock 140545763692800 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:filelock:Lock 140545763692800 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545763692800 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:filelock:Lock 140545763692800 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:filelock:Attempting to acquire lock 140516042179552 on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:filelock:Lock 140516042179552 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516042179552 on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:filelock:Lock 140516042179552 released on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
DEBUG:lm_eval.api.task:doc_to_text returned an int. Assuming multiple inputs.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of winogrande from None to 0
INFO:lm_eval.api.task:Building contexts for winogrande on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 141032.41it/s]
DEBUG:lm_eval.evaluator:Task: winogrande; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<03:22,  1.02s/it]Running loglikelihood requests:   1%|          | 2/200 [00:01<02:26,  1.35it/s]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:04,  1.58it/s]Running loglikelihood requests:   2%|▏         | 4/200 [00:02<01:53,  1.73it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<01:47,  1.82it/s]Running loglikelihood requests:   3%|▎         | 6/200 [00:03<01:42,  1.89it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:39,  1.94it/s]Running loglikelihood requests:   4%|▍         | 8/200 [00:04<01:37,  1.97it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:05<01:35,  2.00it/s]Running loglikelihood requests:   5%|▌         | 10/200 [00:05<01:34,  2.02it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:05<01:33,  2.03it/s]Running loglikelihood requests:   6%|▌         | 12/200 [00:06<01:32,  2.04it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:06<01:31,  2.05it/s]Running loglikelihood requests:   7%|▋         | 14/200 [00:07<01:30,  2.05it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:07<01:29,  2.06it/s]Running loglikelihood requests:   8%|▊         | 16/200 [00:08<01:28,  2.07it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:08<01:28,  2.07it/s]Running loglikelihood requests:   9%|▉         | 18/200 [00:09<01:27,  2.07it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:09<01:27,  2.08it/s]Running loglikelihood requests:  10%|█         | 20/200 [00:10<01:26,  2.08it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:10<01:26,  2.08it/s]Running loglikelihood requests:  11%|█         | 22/200 [00:11<01:25,  2.08it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:11<01:24,  2.09it/s]Running loglikelihood requests:  12%|█▏        | 24/200 [00:12<01:24,  2.09it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:12<01:23,  2.09it/s]Running loglikelihood requests:  13%|█▎        | 26/200 [00:13<01:23,  2.10it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:13<01:22,  2.10it/s]Running loglikelihood requests:  14%|█▍        | 28/200 [00:14<01:21,  2.10it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:14<01:21,  2.10it/s]Running loglikelihood requests:  15%|█▌        | 30/200 [00:15<01:20,  2.10it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:15<01:20,  2.11it/s]Running loglikelihood requests:  16%|█▌        | 32/200 [00:16<01:19,  2.12it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:16<01:18,  2.11it/s]Running loglikelihood requests:  17%|█▋        | 34/200 [00:16<01:18,  2.11it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:17<01:17,  2.12it/s]Running loglikelihood requests:  18%|█▊        | 36/200 [00:17<01:17,  2.12it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:18<01:16,  2.13it/s]Running loglikelihood requests:  19%|█▉        | 38/200 [00:18<01:16,  2.13it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:19<01:15,  2.13it/s]Running loglikelihood requests:  20%|██        | 40/200 [00:19<01:14,  2.14it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:20<01:14,  2.14it/s]Running loglikelihood requests:  21%|██        | 42/200 [00:20<01:13,  2.14it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:21<01:13,  2.14it/s]Running loglikelihood requests:  22%|██▏       | 44/200 [00:21<01:12,  2.14it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:22<01:12,  2.14it/s]Running loglikelihood requests:  23%|██▎       | 46/200 [00:22<01:11,  2.14it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:23<01:11,  2.14it/s]Running loglikelihood requests:  24%|██▍       | 48/200 [00:23<01:11,  2.14it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:23<01:10,  2.14it/s]Running loglikelihood requests:  25%|██▌       | 50/200 [00:24<01:10,  2.14it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:24<01:09,  2.14it/s]Running loglikelihood requests:  26%|██▌       | 52/200 [00:25<01:09,  2.14it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:25<01:08,  2.14it/s]Running loglikelihood requests:  27%|██▋       | 54/200 [00:26<01:08,  2.14it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:26<01:07,  2.15it/s]Running loglikelihood requests:  28%|██▊       | 56/200 [00:27<01:07,  2.15it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:27<01:06,  2.15it/s]Running loglikelihood requests:  29%|██▉       | 58/200 [00:28<01:05,  2.16it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:28<01:05,  2.15it/s]Running loglikelihood requests:  30%|███       | 60/200 [00:29<01:05,  2.15it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:29<01:04,  2.15it/s]Running loglikelihood requests:  31%|███       | 62/200 [00:30<01:03,  2.16it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:30<01:03,  2.16it/s]Running loglikelihood requests:  32%|███▏      | 64/200 [00:30<01:02,  2.16it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:31<01:02,  2.16it/s]Running loglikelihood requests:  33%|███▎      | 66/200 [00:31<01:02,  2.15it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:32<01:01,  2.15it/s]Running loglikelihood requests:  34%|███▍      | 68/200 [00:32<01:01,  2.14it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:33<01:01,  2.14it/s]Running loglikelihood requests:  35%|███▌      | 70/200 [00:33<01:01,  2.13it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:34<01:00,  2.12it/s]Running loglikelihood requests:  36%|███▌      | 72/200 [00:34<01:00,  2.12it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:35<00:59,  2.12it/s]Running loglikelihood requests:  37%|███▋      | 74/200 [00:35<00:59,  2.13it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:36<00:58,  2.13it/s]Running loglikelihood requests:  38%|███▊      | 76/200 [00:36<00:57,  2.15it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:37<00:57,  2.16it/s]Running loglikelihood requests:  39%|███▉      | 78/200 [00:37<00:56,  2.16it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:37<00:55,  2.17it/s]Running loglikelihood requests:  40%|████      | 80/200 [00:38<00:55,  2.17it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:38<00:54,  2.17it/s]Running loglikelihood requests:  41%|████      | 82/200 [00:39<00:54,  2.18it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:39<00:53,  2.18it/s]Running loglikelihood requests:  42%|████▏     | 84/200 [00:40<00:53,  2.19it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:40<00:52,  2.19it/s]Running loglikelihood requests:  43%|████▎     | 86/200 [00:41<00:51,  2.20it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:41<00:51,  2.20it/s]Running loglikelihood requests:  44%|████▍     | 88/200 [00:42<00:51,  2.16it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:42<00:51,  2.17it/s]Running loglikelihood requests:  45%|████▌     | 90/200 [00:42<00:50,  2.17it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:43<00:50,  2.18it/s]Running loglikelihood requests:  46%|████▌     | 92/200 [00:43<00:49,  2.18it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:44<00:49,  2.18it/s]Running loglikelihood requests:  47%|████▋     | 94/200 [00:44<00:48,  2.18it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:45<00:48,  2.18it/s]Running loglikelihood requests:  48%|████▊     | 96/200 [00:45<00:47,  2.18it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:46<00:47,  2.18it/s]Running loglikelihood requests:  49%|████▉     | 98/200 [00:46<00:46,  2.19it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:47<00:46,  2.19it/s]Running loglikelihood requests:  50%|█████     | 100/200 [00:47<00:45,  2.19it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:48<00:45,  2.19it/s]Running loglikelihood requests:  51%|█████     | 102/200 [00:48<00:44,  2.20it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:48<00:43,  2.21it/s]Running loglikelihood requests:  52%|█████▏    | 104/200 [00:49<00:43,  2.21it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:49<00:42,  2.21it/s]Running loglikelihood requests:  53%|█████▎    | 106/200 [00:50<00:42,  2.21it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:50<00:41,  2.22it/s]Running loglikelihood requests:  54%|█████▍    | 108/200 [00:51<00:41,  2.22it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:51<00:41,  2.22it/s]Running loglikelihood requests:  55%|█████▌    | 110/200 [00:52<00:40,  2.21it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:52<00:40,  2.21it/s]Running loglikelihood requests:  56%|█████▌    | 112/200 [00:52<00:39,  2.21it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:53<00:39,  2.21it/s]Running loglikelihood requests:  57%|█████▋    | 114/200 [00:53<00:38,  2.22it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:54<00:38,  2.23it/s]Running loglikelihood requests:  58%|█████▊    | 116/200 [00:54<00:37,  2.23it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:55<00:37,  2.23it/s]Running loglikelihood requests:  59%|█████▉    | 118/200 [00:55<00:36,  2.23it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:56<00:36,  2.23it/s]Running loglikelihood requests:  60%|██████    | 120/200 [00:56<00:35,  2.23it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:57<00:35,  2.23it/s]Running loglikelihood requests:  61%|██████    | 122/200 [00:57<00:34,  2.23it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:57<00:34,  2.23it/s]Running loglikelihood requests:  62%|██████▏   | 124/200 [00:58<00:34,  2.23it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:58<00:33,  2.23it/s]Running loglikelihood requests:  63%|██████▎   | 126/200 [00:59<00:33,  2.23it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:59<00:32,  2.24it/s]Running loglikelihood requests:  64%|██████▍   | 128/200 [01:00<00:32,  2.24it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [01:00<00:31,  2.24it/s]Running loglikelihood requests:  65%|██████▌   | 130/200 [01:01<00:31,  2.24it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [01:01<00:30,  2.25it/s]Running loglikelihood requests:  66%|██████▌   | 132/200 [01:01<00:30,  2.25it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [01:02<00:29,  2.25it/s]Running loglikelihood requests:  67%|██████▋   | 134/200 [01:02<00:29,  2.26it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [01:03<00:28,  2.26it/s]Running loglikelihood requests:  68%|██████▊   | 136/200 [01:03<00:28,  2.25it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [01:04<00:27,  2.26it/s]Running loglikelihood requests:  69%|██████▉   | 138/200 [01:04<00:27,  2.26it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:05<00:27,  2.26it/s]Running loglikelihood requests:  70%|███████   | 140/200 [01:05<00:26,  2.25it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:05<00:26,  2.26it/s]Running loglikelihood requests:  71%|███████   | 142/200 [01:06<00:25,  2.26it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:06<00:25,  2.26it/s]Running loglikelihood requests:  72%|███████▏  | 144/200 [01:07<00:24,  2.26it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:07<00:24,  2.26it/s]Running loglikelihood requests:  73%|███████▎  | 146/200 [01:08<00:23,  2.26it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:08<00:23,  2.26it/s]Running loglikelihood requests:  74%|███████▍  | 148/200 [01:09<00:22,  2.26it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:09<00:22,  2.27it/s]Running loglikelihood requests:  75%|███████▌  | 150/200 [01:09<00:21,  2.27it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:10<00:21,  2.28it/s]Running loglikelihood requests:  76%|███████▌  | 152/200 [01:10<00:21,  2.28it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:11<00:20,  2.28it/s]Running loglikelihood requests:  77%|███████▋  | 154/200 [01:11<00:20,  2.29it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:12<00:19,  2.29it/s]Running loglikelihood requests:  78%|███████▊  | 156/200 [01:12<00:19,  2.29it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:12<00:19,  2.23it/s]Running loglikelihood requests:  79%|███████▉  | 158/200 [01:13<00:19,  2.21it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:13<00:18,  2.20it/s]Running loglikelihood requests:  80%|████████  | 160/200 [01:14<00:18,  2.21it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:14<00:17,  2.24it/s]Running loglikelihood requests:  81%|████████  | 162/200 [01:15<00:16,  2.25it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:15<00:16,  2.26it/s]Running loglikelihood requests:  82%|████████▏ | 164/200 [01:16<00:15,  2.28it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:16<00:15,  2.29it/s]Running loglikelihood requests:  83%|████████▎ | 166/200 [01:16<00:14,  2.31it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:17<00:14,  2.31it/s]Running loglikelihood requests:  84%|████████▍ | 168/200 [01:17<00:13,  2.31it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:18<00:13,  2.31it/s]Running loglikelihood requests:  85%|████████▌ | 170/200 [01:18<00:12,  2.31it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:19<00:12,  2.32it/s]Running loglikelihood requests:  86%|████████▌ | 172/200 [01:19<00:12,  2.33it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:19<00:11,  2.33it/s]Running loglikelihood requests:  87%|████████▋ | 174/200 [01:20<00:11,  2.33it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:20<00:10,  2.34it/s]Running loglikelihood requests:  88%|████████▊ | 176/200 [01:21<00:10,  2.34it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:21<00:09,  2.34it/s]Running loglikelihood requests:  89%|████████▉ | 178/200 [01:22<00:09,  2.33it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:22<00:08,  2.35it/s]Running loglikelihood requests:  90%|█████████ | 180/200 [01:22<00:08,  2.36it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:23<00:08,  2.37it/s]Running loglikelihood requests:  91%|█████████ | 182/200 [01:23<00:07,  2.36it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:24<00:07,  2.37it/s]Running loglikelihood requests:  92%|█████████▏| 184/200 [01:24<00:06,  2.37it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:25<00:06,  2.37it/s]Running loglikelihood requests:  93%|█████████▎| 186/200 [01:25<00:05,  2.37it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:25<00:05,  2.37it/s]Running loglikelihood requests:  94%|█████████▍| 188/200 [01:26<00:05,  2.36it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:26<00:04,  2.36it/s]Running loglikelihood requests:  95%|█████████▌| 190/200 [01:27<00:04,  2.38it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:27<00:03,  2.38it/s]Running loglikelihood requests:  96%|█████████▌| 192/200 [01:27<00:03,  2.38it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:28<00:02,  2.38it/s]Running loglikelihood requests:  97%|█████████▋| 194/200 [01:28<00:02,  2.39it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:29<00:02,  2.39it/s]Running loglikelihood requests:  98%|█████████▊| 196/200 [01:29<00:01,  2.37it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:30<00:01,  2.38it/s]Running loglikelihood requests:  99%|█████████▉| 198/200 [01:30<00:00,  2.37it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:30<00:00,  2.38it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:31<00:00,  2.40it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:31<00:00,  2.19it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:3'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:3'}
full model:
{'winogrande': {'alias': 'winogrande', 'acc,none': 0.69, 'acc_stderr,none': 0.046482319871173176}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9565474555896152
0.9256436871263655
0.919777800952949
0.8502516827829548
0.9788217808920475
0.7842184683361061
0.5847175538207384
0.7741211354638314
0.8734705119073022
0.9731317123565076
0.9046875380381674
0.8858535931048256
0.9314298098713261
0.8442403312485581
0.7166029053748131
0.7061450250233585
0.7563059483803782
0.6877563559653158
0.8738989590810852
0.784285727893607
0.8409178900131131
0.8425380927376759
0.6782655591765769
0.7228968829640015
0.8418686487797186
0.9141168065903919
0.8562481461255738
0.6139553840231665
0.934088538459943
Total groups 66 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 4, 3, 5, 1, 7, 0]
tensor([6, 2, 4, 3, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 5, 4, 7, 0]
tensor([6, 2, 3, 1, 5, 4, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 4, 1, 0, 3, 0, 1, 2]
tensor([5, 4, 1, 0, 3, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 3, 1, 4, 5, 1, 0]
tensor([0, 2, 3, 1, 4, 5, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[5, 1, 2, 3, 4, 0, 1, 0]
tensor([5, 1, 2, 3, 4, 0, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 2, 1, 1, 2, 3, 3, 0]
tensor([0, 2, 1, 1, 2, 3, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 1.0, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1.0, 1.0, 1]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 7
done!
Normal merging for layer 8
tensor([3, 5])
tensor(3)
tensor([2, 6])
tensor(2)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 9 to 13
done!
Normal merging for layer 14
tensor([0, 7])
tensor(0)
tensor([3, 6])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
done!
Normal merging for layer 15
tensor([5, 7])
tensor(5)
tensor([1, 6])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 16
tensor([0, 7])
tensor(0)
tensor([2, 3])
tensor(2)
tensor([1, 4])
tensor(1)
tensor([5, 6])
tensor(5)
done!
Cross-layer merge completed for layers 17 to 23
done!
Normal merging for layer 24
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
Normal merging for layer 25
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 26 to 31
done!
all done!
Model size: 11.8828 GB
175
cuda:3
multirc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.31s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: multirc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: multirc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:filelock:Attempting to acquire lock 140514987563376 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140514987563376 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514987563376 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140514987563376 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140514985141520 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140514985141520 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514985141520 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140514985141520 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of multirc from None to 0
INFO:lm_eval.api.task:Building contexts for multirc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1259.25it/s]
DEBUG:lm_eval.evaluator:Task: multirc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:03<10:15,  3.09s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:05<05:45,  1.75s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:08<04:55,  1.52s/it]Running loglikelihood requests:   4%|▎         | 7/200 [00:10<04:31,  1.41s/it]Running loglikelihood requests:   4%|▍         | 9/200 [00:13<04:18,  1.35s/it]Running loglikelihood requests:   6%|▌         | 11/200 [00:15<04:08,  1.31s/it]Running loglikelihood requests:   6%|▋         | 13/200 [00:18<04:01,  1.29s/it]Running loglikelihood requests:   8%|▊         | 15/200 [00:20<03:55,  1.27s/it]Running loglikelihood requests:   8%|▊         | 17/200 [00:23<03:50,  1.26s/it]Running loglikelihood requests:  10%|▉         | 19/200 [00:25<03:50,  1.27s/it]Running loglikelihood requests:  10%|█         | 21/200 [00:28<03:51,  1.29s/it]Running loglikelihood requests:  12%|█▏        | 23/200 [00:31<03:47,  1.29s/it]Running loglikelihood requests:  12%|█▎        | 25/200 [00:33<03:44,  1.28s/it]Running loglikelihood requests:  14%|█▎        | 27/200 [00:36<03:38,  1.26s/it]Running loglikelihood requests:  14%|█▍        | 29/200 [00:38<03:34,  1.25s/it]Running loglikelihood requests:  16%|█▌        | 31/200 [00:40<03:30,  1.24s/it]Running loglikelihood requests:  16%|█▋        | 33/200 [00:43<03:26,  1.24s/it]Running loglikelihood requests:  18%|█▊        | 35/200 [00:45<03:23,  1.23s/it]Running loglikelihood requests:  18%|█▊        | 37/200 [00:48<03:21,  1.24s/it]Running loglikelihood requests:  20%|█▉        | 39/200 [00:50<03:19,  1.24s/it]Running loglikelihood requests:  20%|██        | 41/200 [00:53<03:16,  1.24s/it]Running loglikelihood requests:  22%|██▏       | 43/200 [00:55<03:14,  1.24s/it]Running loglikelihood requests:  22%|██▎       | 45/200 [00:58<03:11,  1.23s/it]Running loglikelihood requests:  24%|██▎       | 47/200 [01:00<03:07,  1.23s/it]Running loglikelihood requests:  24%|██▍       | 49/200 [01:03<03:04,  1.22s/it]Running loglikelihood requests:  26%|██▌       | 51/200 [01:05<03:01,  1.22s/it]Running loglikelihood requests:  26%|██▋       | 53/200 [01:07<02:58,  1.22s/it]Running loglikelihood requests:  28%|██▊       | 55/200 [01:10<02:56,  1.22s/it]Running loglikelihood requests:  28%|██▊       | 57/200 [01:12<02:53,  1.22s/it]Running loglikelihood requests:  30%|██▉       | 59/200 [01:15<02:51,  1.22s/it]Running loglikelihood requests:  30%|███       | 61/200 [01:17<02:49,  1.22s/it]Running loglikelihood requests:  32%|███▏      | 63/200 [01:20<02:46,  1.21s/it]Running loglikelihood requests:  32%|███▎      | 65/200 [01:22<02:43,  1.21s/it]Running loglikelihood requests:  34%|███▎      | 67/200 [01:24<02:40,  1.21s/it]Running loglikelihood requests:  34%|███▍      | 69/200 [01:27<02:38,  1.21s/it]Running loglikelihood requests:  36%|███▌      | 71/200 [01:29<02:36,  1.21s/it]Running loglikelihood requests:  36%|███▋      | 73/200 [01:32<02:33,  1.21s/it]Running loglikelihood requests:  38%|███▊      | 75/200 [01:34<02:31,  1.21s/it]Running loglikelihood requests:  38%|███▊      | 77/200 [01:36<02:29,  1.21s/it]Running loglikelihood requests:  40%|███▉      | 79/200 [01:39<02:26,  1.21s/it]Running loglikelihood requests:  40%|████      | 81/200 [01:41<02:24,  1.22s/it]Running loglikelihood requests:  42%|████▏     | 83/200 [01:44<02:22,  1.22s/it]Running loglikelihood requests:  42%|████▎     | 85/200 [01:46<02:19,  1.22s/it]Running loglikelihood requests:  44%|████▎     | 87/200 [01:49<02:17,  1.21s/it]Running loglikelihood requests:  44%|████▍     | 89/200 [01:51<02:14,  1.21s/it]Running loglikelihood requests:  46%|████▌     | 91/200 [01:53<02:11,  1.21s/it]Running loglikelihood requests:  46%|████▋     | 93/200 [01:56<02:09,  1.21s/it]Running loglikelihood requests:  48%|████▊     | 95/200 [01:58<02:06,  1.21s/it]Running loglikelihood requests:  48%|████▊     | 97/200 [02:01<02:03,  1.20s/it]Running loglikelihood requests:  50%|████▉     | 99/200 [02:03<02:01,  1.20s/it]Running loglikelihood requests:  50%|█████     | 101/200 [02:05<01:59,  1.20s/it]Running loglikelihood requests:  52%|█████▏    | 103/200 [02:08<01:58,  1.22s/it]Running loglikelihood requests:  52%|█████▎    | 105/200 [02:11<01:58,  1.24s/it]Running loglikelihood requests:  54%|█████▎    | 107/200 [02:13<01:55,  1.24s/it]Running loglikelihood requests:  55%|█████▍    | 109/200 [02:15<01:51,  1.23s/it]Running loglikelihood requests:  56%|█████▌    | 111/200 [02:18<01:48,  1.22s/it]Running loglikelihood requests:  56%|█████▋    | 113/200 [02:20<01:45,  1.21s/it]Running loglikelihood requests:  57%|█████▊    | 115/200 [02:23<01:42,  1.20s/it]Running loglikelihood requests:  58%|█████▊    | 117/200 [02:25<01:39,  1.19s/it]Running loglikelihood requests:  60%|█████▉    | 119/200 [02:27<01:36,  1.19s/it]Running loglikelihood requests:  60%|██████    | 121/200 [02:30<01:33,  1.19s/it]Running loglikelihood requests:  62%|██████▏   | 123/200 [02:32<01:31,  1.19s/it]Running loglikelihood requests:  62%|██████▎   | 125/200 [02:34<01:29,  1.19s/it]Running loglikelihood requests:  64%|██████▎   | 127/200 [02:37<01:27,  1.19s/it]Running loglikelihood requests:  64%|██████▍   | 129/200 [02:39<01:24,  1.20s/it]Running loglikelihood requests:  66%|██████▌   | 131/200 [02:42<01:23,  1.22s/it]Running loglikelihood requests:  66%|██████▋   | 133/200 [02:44<01:21,  1.21s/it]Running loglikelihood requests:  68%|██████▊   | 135/200 [02:46<01:10,  1.08s/it]Running loglikelihood requests:  68%|██████▊   | 137/200 [02:47<01:02,  1.01it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [02:49<00:56,  1.09it/s]Running loglikelihood requests:  70%|███████   | 141/200 [02:50<00:51,  1.15it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [02:52<00:47,  1.20it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [02:53<00:44,  1.24it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [02:55<00:41,  1.27it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [02:56<00:39,  1.29it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [02:58<00:37,  1.31it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [02:59<00:35,  1.33it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [03:01<00:33,  1.34it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [03:02<00:31,  1.35it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [03:03<00:30,  1.36it/s]Running loglikelihood requests:  80%|████████  | 161/200 [03:05<00:28,  1.38it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [03:06<00:26,  1.40it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [03:08<00:24,  1.42it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [03:09<00:22,  1.46it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [03:10<00:20,  1.48it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [03:12<00:19,  1.50it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [03:13<00:17,  1.52it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [03:14<00:16,  1.53it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [03:15<00:14,  1.54it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [03:17<00:13,  1.56it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [03:18<00:12,  1.56it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [03:19<00:10,  1.57it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [03:20<00:09,  1.58it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [03:22<00:08,  1.59it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [03:23<00:06,  1.59it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [03:24<00:05,  1.59it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [03:25<00:04,  1.59it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [03:27<00:03,  1.59it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [03:28<00:01,  1.55it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [03:29<00:00,  1.51it/s]Running loglikelihood requests: 100%|██████████| 200/200 [03:29<00:00,  1.05s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:4'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:4'}
full model:
{'multirc': {'alias': 'multirc', 'acc,none': 0.54, 'acc_stderr,none': 0.05009082659620331}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
0.28954340172694804
0.7956189982678256
0.789982900191189
0.3644425339397572
0.37373995160413354
0.19965826892219962
0.3752330861186013
0.6279352732581062
0.3474029418882723
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
0.28954340172694804
0.7956189982678256
0.789982900191189
0.3644425339397572
0.37373995160413354
0.19965826892219962
0.3752330861186013
0.6279352732581062
0.3474029418882723
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 0, 6, 2, 4, 1]
tensor([5, 3, 7, 0, 6, 2, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 7, 4, 2, 1, 3, 6, 0]
tensor([5, 7, 4, 2, 1, 3, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 1, 3, 6, 0, 5, 7, 4]
tensor([2, 1, 3, 6, 0, 5, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 4, 1, 6, 0, 2, 3, 5]
tensor([7, 4, 1, 6, 0, 2, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 6, 3, 2, 5, 1, 7, 0]
tensor([4, 6, 3, 2, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 0, 2, 1, 4, 7, 3, 6]
tensor([5, 0, 2, 1, 4, 7, 3, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1, 1.0, 1.0, 1.0]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
done!
Normal merging for layer 2
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
done!
Normal merging for layer 6
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
done!
Normal merging for layer 7
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
done!
Cross-layer merge completed for layers 8 to 25
done!
Normal merging for layer 26
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 27 to 31
done!
all done!
Model size: 12.3238 GB
248
cuda:4
wikitext
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.98s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
WARNING:lm_eval.api.task:[Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
WARNING:lm_eval.api.task:[Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity
WARNING:lm_eval.api.task:[Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False
WARNING:lm_eval.api.task:[Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte
WARNING:lm_eval.api.task:[Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/wikitext_document_level HTTP/1.1" 200 1012
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/EleutherAI/wikitext_document_level/EleutherAI/wikitext_document_level.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/wikitext_document_level HTTP/1.1" 200 1012
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/wikitext_document_level/resolve/647234772b9554e208af6c826f23b99e3cac88c8/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 299
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 299
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 299
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 299
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 299
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 299
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/wikitext_document_level/resolve/647234772b9554e208af6c826f23b99e3cac88c8/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 293
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 293
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 293
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 293
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 293
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/EleutherAI/wikitext_document_level/paths-info/647234772b9554e208af6c826f23b99e3cac88c8 HTTP/1.1" 200 293
DEBUG:filelock:Attempting to acquire lock 140516032604176 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___wikitext_document_level_wikitext-2-raw-v1_0.0.0_647234772b9554e208af6c826f23b99e3cac88c8.lock
DEBUG:filelock:Lock 140516032604176 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___wikitext_document_level_wikitext-2-raw-v1_0.0.0_647234772b9554e208af6c826f23b99e3cac88c8.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___wikitext_document_level/wikitext-2-raw-v1/0.0.0/647234772b9554e208af6c826f23b99e3cac88c8/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516032604176 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___wikitext_document_level_wikitext-2-raw-v1_0.0.0_647234772b9554e208af6c826f23b99e3cac88c8.lock
DEBUG:filelock:Lock 140516032604176 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___wikitext_document_level_wikitext-2-raw-v1_0.0.0_647234772b9554e208af6c826f23b99e3cac88c8.lock
DEBUG:filelock:Attempting to acquire lock 140514977775104 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___wikitext_document_level/wikitext-2-raw-v1/0.0.0/647234772b9554e208af6c826f23b99e3cac88c8_builder.lock
DEBUG:filelock:Lock 140514977775104 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___wikitext_document_level/wikitext-2-raw-v1/0.0.0/647234772b9554e208af6c826f23b99e3cac88c8_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___wikitext_document_level/wikitext-2-raw-v1/0.0.0/647234772b9554e208af6c826f23b99e3cac88c8/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514977775104 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___wikitext_document_level/wikitext-2-raw-v1/0.0.0/647234772b9554e208af6c826f23b99e3cac88c8_builder.lock
DEBUG:filelock:Lock 140514977775104 released on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___wikitext_document_level/wikitext-2-raw-v1/0.0.0/647234772b9554e208af6c826f23b99e3cac88c8_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wikitext from None to 0
INFO:lm_eval.api.task:Building contexts for wikitext on rank 0...
  0%|          | 0/62 [00:00<?, ?it/s]100%|██████████| 62/62 [00:00<00:00, 736.95it/s]
DEBUG:lm_eval.evaluator:Task: wikitext; number of requests on this rank: 62
INFO:lm_eval.evaluator:Running loglikelihood_rolling requests
  0%|          | 0/62 [00:00<?, ?it/s] 15%|█▍        | 9/62 [00:00<00:00, 89.67it/s] 39%|███▊      | 24/62 [00:00<00:00, 111.58it/s] 58%|█████▊    | 36/62 [00:00<00:00, 111.59it/s] 77%|███████▋  | 48/62 [00:00<00:00, 101.35it/s]100%|██████████| 62/62 [00:00<00:00, 115.23it/s]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:07<00:00,  7.47s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:07<00:00,  7.47s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.45s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.45s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.37s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.37s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:14<00:00, 14.35s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:14<00:00, 14.35s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.35s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.35s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.16s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.16s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.33s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.33s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:12<00:00, 12.01s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:12<00:00, 12.01s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:14<00:00, 14.52s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:14<00:00, 14.52s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.30s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.30s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.58s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.58s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.26s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.26s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.25s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.25s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:14<00:00, 14.09s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:14<00:00, 14.09s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.22s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.22s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.33s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.33s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.37s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.37s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.46s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.46s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:09<00:00,  9.65s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:09<00:00,  9.65s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:16<00:00, 16.91s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:16<00:00, 16.91s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.28s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.28s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.30s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.30s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.28s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.28s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.95s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.95s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:13<00:00, 13.13s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:13<00:00, 13.13s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:03<00:00,  3.30s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:03<00:00,  3.30s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:10<00:00, 10.02s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:10<00:00, 10.02s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.26s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.26s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.24s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.24s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.35s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.35s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.34s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.34s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.70s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.70s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.45s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.45s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.13s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.13s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.30s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.30s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.18s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.18s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.10s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.10s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:07<00:00,  7.41s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:07<00:00,  7.41s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.21s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.21s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.12s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.12s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.46s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.46s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.17s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.17s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.44s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.44s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.60s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.60s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:14<00:00, 14.35s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:14<00:00, 14.35s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.46s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.46s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.55s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.55s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.45s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.45s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.58s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.58s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.51s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.51s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.48s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.49s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.45s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.45s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.78s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.78s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.24s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.24s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.28s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.28s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:09<00:00,  9.01s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:09<00:00,  9.01s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.11s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.11s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.23s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.23s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:09<00:00,  9.89s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:09<00:00,  9.89s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.03s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.03s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.13s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.13s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.17s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.17s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.03s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.03s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.37s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.37s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.26s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.26s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:09<00:00,  9.21s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:09<00:00,  9.21s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.39s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.39s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.30s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.30s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.27s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.27s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.78s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.78s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.40s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.40s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.29s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.29s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.27s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.28s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.24s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.24s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.37s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.37s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.25s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.25s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.18s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.18s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.40s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.40s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.26s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.26s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.32s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.32s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.32s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.32s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:14<00:00, 14.07s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:14<00:00, 14.07s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.49s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.49s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.34s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.34s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.27s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.27s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:12<00:00, 12.47s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:12<00:00, 12.47s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.51s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.51s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:07<00:00,  7.50s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:07<00:00,  7.50s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.22s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.22s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.14s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.14s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.31s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.31s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:07<00:00,  7.98s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:07<00:00,  7.98s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:08<00:00,  8.60s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:08<00:00,  8.60s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:16<00:00, 16.51s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:16<00:00, 16.51s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:16<00:00, 16.23s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:16<00:00, 16.23s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.52s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.52s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.52s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.52s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.90s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.90s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.73s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.73s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.34s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.34s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.51s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.51s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.18s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.18s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:03<00:00,  3.27s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:03<00:00,  3.28s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.13s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.13s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.15s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.15s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.15s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.15s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.12s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.12s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.06s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.06s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.22s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.22s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.13s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.13s/it]
Running loglikelihood requests:   0%|          | 0/1 [00:00<?, ?it/s]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.17s/it]Running loglikelihood requests: 100%|██████████| 1/1 [00:18<00:00, 18.17s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:5'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:5'}
full model:
{'wikitext': {'alias': 'wikitext', 'word_perplexity,none': 10.41810626477151, 'word_perplexity_stderr,none': 'N/A', 'byte_perplexity,none': 1.5499996141598633, 'byte_perplexity_stderr,none': 'N/A', 'bits_per_byte,none': 0.6322678563706606, 'bits_per_byte_stderr,none': 'N/A'}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.45530714642366393
0.6593661321025917
0.45166835110772663
0.3470526807104175
0.3369212652555373
0.5686768652815603
0.5380667106744194
0.1994061645434335
0.7857081855506413
0.7236392518096334
0.6975091963753621
0.7192457446547811
0.6820752222558486
0.2553530000000423
0.9092011229700152
0.8474392887638426
0.29700403366343364
0.3112335629956447
0.316268174131358
0.5041948206367306
0.3067038688170426
0.21198641237235732
0.23379101174921613
0.4411951857077822
0.3636022184812837
0.22411151956337602
0.0659668585894146
0.5103891010860186
0.5291148039479469
0.45530714642366393
0.6593661321025917
0.45166835110772663
0.3470526807104175
0.3369212652555373
0.5686768652815603
0.5380667106744194
0.1994061645434335
0.7857081855506413
0.7236392518096334
0.6975091963753621
0.7192457446547811
0.6820752222558486
0.2553530000000423
0.9092011229700152
0.8474392887638426
0.29700403366343364
0.3112335629956447
0.316268174131358
0.5041948206367306
0.3067038688170426
0.21198641237235732
0.23379101174921613
0.4411951857077822
0.3636022184812837
0.22411151956337602
0.0659668585894146
0.5103891010860186
0.5291148039479469
0.45530714642366393
0.6593661321025917
0.45166835110772663
0.3470526807104175
0.3369212652555373
0.5686768652815603
0.5380667106744194
0.1994061645434335
0.7857081855506413
0.7236392518096334
0.6975091963753621
0.7192457446547811
0.6820752222558486
0.2553530000000423
0.9092011229700152
0.8474392887638426
0.29700403366343364
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 3, 7, 0, 5, 1, 4, 2]
tensor([6, 3, 7, 0, 5, 1, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 5, 0, 4, 3, 6, 7, 1]
tensor([2, 5, 0, 4, 3, 6, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 7, 5, 2, 0, 3, 1, 4]
tensor([6, 7, 5, 2, 0, 3, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 1, 0, 7, 2, 3, 5, 6]
tensor([4, 1, 0, 7, 2, 3, 5, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 3, 2, 4, 0, 1, 1, 0]
tensor([5, 3, 2, 4, 0, 1, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[3, 2, 0, 1, 5, 0, 4, 1]
tensor([3, 2, 0, 1, 5, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 0, 1, 5, 2, 0, 4, 3]
tensor([1, 0, 1, 5, 2, 0, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 4
done!
Normal merging for layer 5
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([1])
tensor(1)
done!
Normal merging for layer 6
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
done!
Cross-layer merge completed for layers 7 to 8
done!
Normal merging for layer 9
tensor([4, 7])
tensor(4)
tensor([5, 6])
tensor(5)
tensor([2])
tensor(2)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
done!
Normal merging for layer 10
tensor([2, 5])
tensor(2)
tensor([3, 7])
tensor(3)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 11 to 12
done!
Normal merging for layer 13
tensor([1, 5])
tensor(1)
tensor([0, 2])
tensor(0)
tensor([4])
tensor(4)
tensor([7])
tensor(7)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
done!
Cross-layer merge completed for layers 14 to 31
done!
all done!
Model size: 12.3867 GB
69
cuda:5
wsc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 25.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.86s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wsc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wsc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:filelock:Attempting to acquire lock 140514984889856 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140514984889856 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514984889856 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140514984889856 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140545763700288 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140545763700288 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545763700288 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140545763700288 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wsc from None to 0
INFO:lm_eval.api.task:Building contexts for wsc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 84699.19it/s]
DEBUG:lm_eval.evaluator:Task: wsc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<04:36,  1.39s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:10,  1.51it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:03<01:07,  2.86it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:03<01:10,  2.73it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:04<01:11,  2.65it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:05<01:11,  2.60it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:06<01:12,  2.56it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:07<01:12,  2.54it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:07<01:11,  2.54it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:08<01:10,  2.54it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:09<01:09,  2.55it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:10<01:08,  2.55it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:10<01:07,  2.55it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:11<01:07,  2.54it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:12<01:06,  2.53it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:13<01:05,  2.54it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:14<01:04,  2.55it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:14<01:03,  2.56it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:15<01:02,  2.57it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:16<01:01,  2.57it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:17<01:01,  2.57it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:17<01:00,  2.58it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:18<00:59,  2.59it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:19<00:57,  2.60it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:20<00:57,  2.61it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:21<00:56,  2.62it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:21<00:55,  2.63it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:22<00:54,  2.65it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:23<00:52,  2.66it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:23<00:52,  2.67it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:24<00:51,  2.68it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:25<00:50,  2.70it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:26<00:49,  2.70it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:26<00:48,  2.72it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:27<00:47,  2.73it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:28<00:46,  2.74it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:29<00:46,  2.68it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:29<00:45,  2.71it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:30<00:44,  2.73it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:31<00:43,  2.75it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:32<00:41,  2.79it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:32<00:40,  2.82it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:33<00:39,  2.84it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:34<00:38,  2.86it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:34<00:38,  2.87it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:35<00:37,  2.88it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:36<00:36,  2.89it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:36<00:35,  2.93it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:37<00:34,  2.96it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:38<00:33,  2.98it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:38<00:32,  3.00it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:39<00:31,  3.01it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:40<00:30,  3.02it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:40<00:30,  3.03it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:41<00:29,  3.04it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:42<00:28,  3.07it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:42<00:20,  4.04it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:43<00:16,  4.74it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:43<00:17,  4.29it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:44<00:18,  3.97it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:45<00:19,  3.76it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:45<00:19,  3.61it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:46<00:19,  3.52it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:46<00:19,  3.46it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:47<00:19,  3.41it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:48<00:18,  3.39it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:48<00:18,  3.38it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:49<00:17,  3.37it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:49<00:16,  3.37it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:50<00:16,  3.38it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:51<00:15,  3.39it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:51<00:15,  3.40it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:52<00:14,  3.41it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:52<00:13,  3.42it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:53<00:13,  3.43it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:54<00:12,  3.44it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:54<00:11,  3.45it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:55<00:11,  3.46it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [00:55<00:10,  3.46it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [00:56<00:10,  3.46it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [00:56<00:09,  3.46it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [00:57<00:08,  3.45it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [00:58<00:08,  3.46it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [00:58<00:07,  3.47it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [00:59<00:07,  3.48it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [00:59<00:06,  3.50it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:00<00:05,  3.52it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:00<00:05,  3.54it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:01<00:04,  3.56it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:02<00:04,  3.57it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:02<00:03,  3.58it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:03<00:03,  3.60it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:03<00:02,  3.62it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:04<00:01,  3.63it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:04<00:01,  3.56it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:05<00:00,  3.56it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:05<00:00,  3.56it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:05<00:00,  3.03it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:6'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:6'}
full model:
{'wsc': {'alias': 'wsc', 'acc,none': 0.39, 'acc_stderr,none': 0.04902071300001973}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8279363410631274
0.19299873358047231
0.24697427628971416
0.9498246079459217
0.9208790959024347
0.5433529265767616
0.7541489406957872
0.8642404861438043
0.9451768410410958
0.6364904470004392
0.8605038470072015
0.8806563523485148
0.7320941418431636
0.7167995199876819
0.9558598958577776
0.7974395317006763
0.8001507378395173
0.8689133172877098
0.6142386786363212
0.6039861720637983
0.8068446902119512
0.6154342295548068
0.8415314242927469
0.42042589459680124
0.7443787948515065
0.7081725962998161
0.7223774755032815
0.9338202429729527
0.7700364698774241
0.8279363410631274
0.19299873358047231
0.24697427628971416
0.9498246079459217
0.9208790959024347
0.5433529265767616
0.7541489406957872
0.8642404861438043
0.9451768410410958
0.6364904470004392
0.8605038470072015
0.8806563523485148
0.7320941418431636
0.7167995199876819
0.9558598958577776
0.7974395317006763
0.8001507378395173
0.8689133172877098
0.6142386786363212
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 7, 2, 4, 5, 1, 3, 0]
tensor([6, 7, 2, 4, 5, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 7, 3, 6, 2, 5, 4, 0]
tensor([1, 7, 3, 6, 2, 5, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 5, 6, 3, 7, 4, 2, 0]
tensor([1, 5, 6, 3, 7, 4, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 5, 3, 6, 7, 4, 2, 0]
tensor([1, 5, 3, 6, 7, 4, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 7, 5, 6, 1, 2, 3, 0]
tensor([4, 7, 5, 6, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 7, 1, 5, 6, 3, 2, 0]
tensor([4, 7, 1, 5, 6, 3, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1, 1.0, 0, 1.0]
tensor([0, 1, 1, 1, 1, 1, 0, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
done!
Normal merging for layer 6
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
done!
Normal merging for layer 7
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
done!
Cross-layer merge completed for layers 8 to 28
done!
Normal merging for layer 29
tensor([0, 6])
tensor(0)
tensor([1, 2, 3, 4, 5, 7])
tensor(1)
done!
Cross-layer merge completed for layers 30 to 31
done!
all done!
Model size: 12.3238 GB
57
cuda:6
openbookqa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.88s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa HTTP/1.1" 307 67
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa HTTP/1.1" 200 1408
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/openbookqa/openbookqa.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa HTTP/1.1" 307 67
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa HTTP/1.1" 200 1408
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa/revision/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 117
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa/revision/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 1408
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454?recursive=False&expand=False HTTP/1.1" 307 142
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454?recursive=False&expand=False HTTP/1.1" 200 390
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454/additional?recursive=False&expand=False HTTP/1.1" 307 153
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454/additional?recursive=False&expand=False HTTP/1.1" 200 363
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa/revision/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 117
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa/revision/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 1408
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/openbookqa/resolve/388097ea7776314e93a529163e0fea805b8a6454/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454/main?recursive=False&expand=False HTTP/1.1" 307 147
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/openbookqa/tree/388097ea7776314e93a529163e0fea805b8a6454/main?recursive=False&expand=False HTTP/1.1" 200 359
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 307 119
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/allenai/openbookqa/paths-info/388097ea7776314e93a529163e0fea805b8a6454 HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140545361180816 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:filelock:Lock 140545361180816 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545361180816 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:filelock:Lock 140545361180816 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_openbookqa_main_0.0.0_388097ea7776314e93a529163e0fea805b8a6454.lock
DEBUG:filelock:Attempting to acquire lock 140545763690256 on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:filelock:Lock 140545763690256 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545763690256 on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:filelock:Lock 140545763690256 released on /public/home/zouyifei001/.cache/huggingface/datasets/openbookqa/main/0.0.0/388097ea7776314e93a529163e0fea805b8a6454_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of openbookqa from None to 0
INFO:lm_eval.api.task:Building contexts for openbookqa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2834.79it/s]
DEBUG:lm_eval.evaluator:Task: openbookqa; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:01<08:03,  1.21s/it]Running loglikelihood requests:   0%|          | 2/400 [00:01<06:15,  1.06it/s]Running loglikelihood requests:   1%|          | 3/400 [00:02<05:31,  1.20it/s]Running loglikelihood requests:   1%|          | 4/400 [00:03<05:09,  1.28it/s]Running loglikelihood requests:   1%|▏         | 5/400 [00:03<04:39,  1.42it/s]Running loglikelihood requests:   2%|▏         | 6/400 [00:04<04:19,  1.52it/s]Running loglikelihood requests:   2%|▏         | 7/400 [00:05<04:05,  1.60it/s]Running loglikelihood requests:   2%|▏         | 8/400 [00:05<03:56,  1.66it/s]Running loglikelihood requests:   2%|▏         | 9/400 [00:06<03:48,  1.71it/s]Running loglikelihood requests:   2%|▎         | 10/400 [00:06<03:43,  1.74it/s]Running loglikelihood requests:   3%|▎         | 11/400 [00:07<03:40,  1.77it/s]Running loglikelihood requests:   3%|▎         | 12/400 [00:07<03:37,  1.79it/s]Running loglikelihood requests:   3%|▎         | 13/400 [00:08<03:34,  1.80it/s]Running loglikelihood requests:   4%|▍         | 15/400 [00:08<02:42,  2.36it/s]Running loglikelihood requests:   4%|▍         | 16/400 [00:09<02:53,  2.21it/s]Running loglikelihood requests:   4%|▍         | 17/400 [00:09<03:01,  2.11it/s]Running loglikelihood requests:   4%|▍         | 18/400 [00:10<03:06,  2.05it/s]Running loglikelihood requests:   5%|▍         | 19/400 [00:11<03:10,  2.00it/s]Running loglikelihood requests:   5%|▌         | 20/400 [00:11<03:12,  1.98it/s]Running loglikelihood requests:   5%|▌         | 21/400 [00:12<03:12,  1.97it/s]Running loglikelihood requests:   6%|▌         | 22/400 [00:12<03:12,  1.96it/s]Running loglikelihood requests:   6%|▌         | 23/400 [00:13<03:12,  1.96it/s]Running loglikelihood requests:   6%|▌         | 24/400 [00:13<03:12,  1.95it/s]Running loglikelihood requests:   6%|▋         | 25/400 [00:14<03:12,  1.95it/s]Running loglikelihood requests:   6%|▋         | 26/400 [00:14<03:11,  1.95it/s]Running loglikelihood requests:   7%|▋         | 27/400 [00:15<03:10,  1.96it/s]Running loglikelihood requests:   7%|▋         | 28/400 [00:15<03:09,  1.96it/s]Running loglikelihood requests:   7%|▋         | 29/400 [00:16<03:08,  1.97it/s]Running loglikelihood requests:   8%|▊         | 33/400 [00:16<01:37,  3.77it/s]Running loglikelihood requests:   8%|▊         | 34/400 [00:17<01:55,  3.17it/s]Running loglikelihood requests:   9%|▉         | 35/400 [00:17<02:07,  2.87it/s]Running loglikelihood requests:   9%|▉         | 36/400 [00:18<02:17,  2.65it/s]Running loglikelihood requests:   9%|▉         | 37/400 [00:18<02:25,  2.50it/s]Running loglikelihood requests:  10%|▉         | 38/400 [00:19<02:31,  2.39it/s]Running loglikelihood requests:  10%|▉         | 39/400 [00:19<02:35,  2.31it/s]Running loglikelihood requests:  10%|█         | 40/400 [00:20<02:38,  2.26it/s]Running loglikelihood requests:  10%|█         | 41/400 [00:20<02:40,  2.24it/s]Running loglikelihood requests:  10%|█         | 42/400 [00:20<02:41,  2.22it/s]Running loglikelihood requests:  11%|█         | 43/400 [00:21<02:42,  2.20it/s]Running loglikelihood requests:  11%|█         | 44/400 [00:21<02:42,  2.19it/s]Running loglikelihood requests:  11%|█▏        | 45/400 [00:22<02:42,  2.18it/s]Running loglikelihood requests:  12%|█▏        | 46/400 [00:22<02:42,  2.18it/s]Running loglikelihood requests:  12%|█▏        | 47/400 [00:23<02:41,  2.18it/s]Running loglikelihood requests:  12%|█▏        | 48/400 [00:23<02:41,  2.18it/s]Running loglikelihood requests:  12%|█▏        | 49/400 [00:24<02:40,  2.18it/s]Running loglikelihood requests:  12%|█▎        | 50/400 [00:24<02:40,  2.18it/s]Running loglikelihood requests:  13%|█▎        | 51/400 [00:25<02:39,  2.18it/s]Running loglikelihood requests:  13%|█▎        | 52/400 [00:25<02:39,  2.18it/s]Running loglikelihood requests:  13%|█▎        | 53/400 [00:25<02:38,  2.19it/s]Running loglikelihood requests:  14%|█▎        | 54/400 [00:26<02:37,  2.20it/s]Running loglikelihood requests:  14%|█▍        | 55/400 [00:26<02:36,  2.20it/s]Running loglikelihood requests:  14%|█▍        | 57/400 [00:27<01:59,  2.87it/s]Running loglikelihood requests:  15%|█▌        | 60/400 [00:27<01:24,  4.00it/s]Running loglikelihood requests:  15%|█▌        | 61/400 [00:28<01:37,  3.48it/s]Running loglikelihood requests:  16%|█▌        | 62/400 [00:28<01:48,  3.11it/s]Running loglikelihood requests:  16%|█▌        | 63/400 [00:29<01:58,  2.84it/s]Running loglikelihood requests:  16%|█▌        | 64/400 [00:29<02:06,  2.66it/s]Running loglikelihood requests:  16%|█▋        | 65/400 [00:30<02:12,  2.54it/s]Running loglikelihood requests:  16%|█▋        | 66/400 [00:30<02:16,  2.45it/s]Running loglikelihood requests:  17%|█▋        | 67/400 [00:30<02:19,  2.38it/s]Running loglikelihood requests:  17%|█▋        | 68/400 [00:31<02:21,  2.35it/s]Running loglikelihood requests:  17%|█▋        | 69/400 [00:31<02:22,  2.32it/s]Running loglikelihood requests:  18%|█▊        | 70/400 [00:32<02:23,  2.31it/s]Running loglikelihood requests:  18%|█▊        | 71/400 [00:32<02:23,  2.29it/s]Running loglikelihood requests:  18%|█▊        | 72/400 [00:33<02:23,  2.28it/s]Running loglikelihood requests:  18%|█▊        | 73/400 [00:33<02:23,  2.28it/s]Running loglikelihood requests:  18%|█▊        | 74/400 [00:34<02:23,  2.28it/s]Running loglikelihood requests:  19%|█▉        | 75/400 [00:34<02:23,  2.27it/s]Running loglikelihood requests:  19%|█▉        | 76/400 [00:34<02:22,  2.27it/s]Running loglikelihood requests:  19%|█▉        | 77/400 [00:35<02:22,  2.26it/s]Running loglikelihood requests:  20%|█▉        | 78/400 [00:35<02:22,  2.26it/s]Running loglikelihood requests:  20%|█▉        | 79/400 [00:36<02:21,  2.27it/s]Running loglikelihood requests:  20%|██        | 81/400 [00:36<01:47,  2.96it/s]Running loglikelihood requests:  20%|██        | 82/400 [00:37<01:55,  2.75it/s]Running loglikelihood requests:  21%|██        | 83/400 [00:37<02:01,  2.61it/s]Running loglikelihood requests:  21%|██        | 84/400 [00:37<02:05,  2.52it/s]Running loglikelihood requests:  22%|██▏       | 86/400 [00:38<01:40,  3.13it/s]Running loglikelihood requests:  22%|██▏       | 87/400 [00:38<01:48,  2.89it/s]Running loglikelihood requests:  22%|██▏       | 88/400 [00:39<01:55,  2.70it/s]Running loglikelihood requests:  22%|██▏       | 89/400 [00:39<02:00,  2.58it/s]Running loglikelihood requests:  22%|██▎       | 90/400 [00:40<02:04,  2.50it/s]Running loglikelihood requests:  23%|██▎       | 91/400 [00:40<02:06,  2.44it/s]Running loglikelihood requests:  23%|██▎       | 92/400 [00:41<02:08,  2.39it/s]Running loglikelihood requests:  23%|██▎       | 93/400 [00:41<02:09,  2.36it/s]Running loglikelihood requests:  24%|██▎       | 94/400 [00:41<02:10,  2.35it/s]Running loglikelihood requests:  24%|██▍       | 95/400 [00:42<02:10,  2.33it/s]Running loglikelihood requests:  24%|██▍       | 96/400 [00:42<02:11,  2.32it/s]Running loglikelihood requests:  24%|██▍       | 97/400 [00:43<02:11,  2.31it/s]Running loglikelihood requests:  24%|██▍       | 98/400 [00:43<02:11,  2.30it/s]Running loglikelihood requests:  25%|██▍       | 99/400 [00:44<02:11,  2.30it/s]Running loglikelihood requests:  25%|██▌       | 100/400 [00:44<02:10,  2.29it/s]Running loglikelihood requests:  25%|██▌       | 101/400 [00:44<02:10,  2.30it/s]Running loglikelihood requests:  26%|██▌       | 102/400 [00:45<02:09,  2.30it/s]Running loglikelihood requests:  26%|██▌       | 103/400 [00:45<02:08,  2.31it/s]Running loglikelihood requests:  26%|██▋       | 106/400 [00:46<01:19,  3.71it/s]Running loglikelihood requests:  27%|██▋       | 107/400 [00:46<01:28,  3.29it/s]Running loglikelihood requests:  27%|██▋       | 108/400 [00:47<01:37,  3.00it/s]Running loglikelihood requests:  27%|██▋       | 109/400 [00:47<01:44,  2.79it/s]Running loglikelihood requests:  28%|██▊       | 110/400 [00:47<01:49,  2.65it/s]Running loglikelihood requests:  28%|██▊       | 111/400 [00:48<01:53,  2.55it/s]Running loglikelihood requests:  28%|██▊       | 112/400 [00:48<01:56,  2.48it/s]Running loglikelihood requests:  28%|██▊       | 113/400 [00:49<01:58,  2.42it/s]Running loglikelihood requests:  28%|██▊       | 114/400 [00:49<01:59,  2.39it/s]Running loglikelihood requests:  29%|██▉       | 115/400 [00:50<01:59,  2.38it/s]Running loglikelihood requests:  29%|██▉       | 116/400 [00:50<01:59,  2.37it/s]Running loglikelihood requests:  29%|██▉       | 117/400 [00:50<01:59,  2.37it/s]Running loglikelihood requests:  30%|██▉       | 118/400 [00:51<01:59,  2.37it/s]Running loglikelihood requests:  30%|██▉       | 119/400 [00:51<01:58,  2.36it/s]Running loglikelihood requests:  30%|███       | 120/400 [00:52<01:59,  2.35it/s]Running loglikelihood requests:  30%|███       | 121/400 [00:52<01:58,  2.35it/s]Running loglikelihood requests:  30%|███       | 122/400 [00:53<01:58,  2.35it/s]Running loglikelihood requests:  31%|███       | 123/400 [00:53<01:58,  2.35it/s]Running loglikelihood requests:  31%|███       | 124/400 [00:53<01:57,  2.34it/s]Running loglikelihood requests:  31%|███▏      | 125/400 [00:54<01:57,  2.34it/s]Running loglikelihood requests:  32%|███▏      | 126/400 [00:54<01:57,  2.34it/s]Running loglikelihood requests:  32%|███▏      | 127/400 [00:55<01:56,  2.35it/s]Running loglikelihood requests:  32%|███▏      | 128/400 [00:55<01:56,  2.34it/s]Running loglikelihood requests:  32%|███▏      | 129/400 [00:56<01:55,  2.34it/s]Running loglikelihood requests:  32%|███▎      | 130/400 [00:56<01:54,  2.35it/s]Running loglikelihood requests:  33%|███▎      | 131/400 [00:56<01:54,  2.36it/s]Running loglikelihood requests:  33%|███▎      | 132/400 [00:57<01:53,  2.36it/s]Running loglikelihood requests:  33%|███▎      | 133/400 [00:57<01:53,  2.36it/s]Running loglikelihood requests:  34%|███▎      | 134/400 [00:58<01:52,  2.36it/s]Running loglikelihood requests:  34%|███▍      | 138/400 [00:58<00:58,  4.48it/s]Running loglikelihood requests:  35%|███▍      | 139/400 [00:59<01:07,  3.84it/s]Running loglikelihood requests:  35%|███▌      | 140/400 [00:59<01:16,  3.39it/s]Running loglikelihood requests:  35%|███▌      | 141/400 [00:59<01:24,  3.08it/s]Running loglikelihood requests:  36%|███▌      | 142/400 [01:00<01:30,  2.86it/s]Running loglikelihood requests:  36%|███▌      | 143/400 [01:00<01:34,  2.72it/s]Running loglikelihood requests:  36%|███▌      | 144/400 [01:01<01:37,  2.61it/s]Running loglikelihood requests:  36%|███▋      | 145/400 [01:01<01:40,  2.54it/s]Running loglikelihood requests:  37%|███▋      | 147/400 [01:02<01:19,  3.20it/s]Running loglikelihood requests:  37%|███▋      | 148/400 [01:02<01:25,  2.96it/s]Running loglikelihood requests:  37%|███▋      | 149/400 [01:02<01:30,  2.78it/s]Running loglikelihood requests:  38%|███▊      | 151/400 [01:03<01:13,  3.38it/s]Running loglikelihood requests:  38%|███▊      | 152/400 [01:03<01:20,  3.08it/s]Running loglikelihood requests:  38%|███▊      | 153/400 [01:04<01:25,  2.87it/s]Running loglikelihood requests:  38%|███▊      | 154/400 [01:04<01:30,  2.73it/s]Running loglikelihood requests:  39%|███▉      | 155/400 [01:04<01:33,  2.63it/s]Running loglikelihood requests:  39%|███▉      | 156/400 [01:05<01:35,  2.56it/s]Running loglikelihood requests:  39%|███▉      | 157/400 [01:05<01:36,  2.51it/s]Running loglikelihood requests:  40%|███▉      | 158/400 [01:06<01:38,  2.47it/s]Running loglikelihood requests:  40%|███▉      | 159/400 [01:06<01:38,  2.44it/s]Running loglikelihood requests:  40%|████      | 160/400 [01:07<01:38,  2.42it/s]Running loglikelihood requests:  40%|████      | 161/400 [01:07<01:39,  2.41it/s]Running loglikelihood requests:  40%|████      | 162/400 [01:07<01:39,  2.40it/s]Running loglikelihood requests:  41%|████      | 163/400 [01:08<01:38,  2.40it/s]Running loglikelihood requests:  42%|████▏     | 166/400 [01:08<01:00,  3.84it/s]Running loglikelihood requests:  42%|████▏     | 167/400 [01:09<01:08,  3.41it/s]Running loglikelihood requests:  43%|████▎     | 171/400 [01:09<00:43,  5.27it/s]Running loglikelihood requests:  43%|████▎     | 172/400 [01:09<00:51,  4.41it/s]Running loglikelihood requests:  43%|████▎     | 173/400 [01:10<00:59,  3.80it/s]Running loglikelihood requests:  44%|████▎     | 174/400 [01:10<01:06,  3.38it/s]Running loglikelihood requests:  44%|████▍     | 175/400 [01:11<01:12,  3.09it/s]Running loglikelihood requests:  44%|████▍     | 176/400 [01:11<01:17,  2.89it/s]Running loglikelihood requests:  44%|████▍     | 177/400 [01:12<01:20,  2.75it/s]Running loglikelihood requests:  44%|████▍     | 178/400 [01:12<01:23,  2.67it/s]Running loglikelihood requests:  45%|████▍     | 179/400 [01:12<01:25,  2.59it/s]Running loglikelihood requests:  45%|████▌     | 180/400 [01:13<01:26,  2.54it/s]Running loglikelihood requests:  45%|████▌     | 181/400 [01:13<01:27,  2.50it/s]Running loglikelihood requests:  46%|████▌     | 182/400 [01:14<01:28,  2.47it/s]Running loglikelihood requests:  46%|████▌     | 183/400 [01:14<01:28,  2.45it/s]Running loglikelihood requests:  46%|████▌     | 184/400 [01:14<01:27,  2.45it/s]Running loglikelihood requests:  46%|████▋     | 185/400 [01:15<01:27,  2.45it/s]Running loglikelihood requests:  46%|████▋     | 186/400 [01:15<01:27,  2.45it/s]Running loglikelihood requests:  47%|████▋     | 187/400 [01:16<01:27,  2.45it/s]Running loglikelihood requests:  47%|████▋     | 188/400 [01:16<01:26,  2.45it/s]Running loglikelihood requests:  48%|████▊     | 190/400 [01:16<01:05,  3.19it/s]Running loglikelihood requests:  48%|████▊     | 192/400 [01:17<00:56,  3.70it/s]Running loglikelihood requests:  48%|████▊     | 193/400 [01:17<01:02,  3.32it/s]Running loglikelihood requests:  48%|████▊     | 194/400 [01:18<01:07,  3.06it/s]Running loglikelihood requests:  49%|████▉     | 195/400 [01:18<01:11,  2.88it/s]Running loglikelihood requests:  49%|████▉     | 196/400 [01:19<01:14,  2.75it/s]Running loglikelihood requests:  49%|████▉     | 197/400 [01:19<01:16,  2.64it/s]Running loglikelihood requests:  50%|████▉     | 198/400 [01:19<01:18,  2.57it/s]Running loglikelihood requests:  50%|█████     | 202/400 [01:20<00:41,  4.74it/s]Running loglikelihood requests:  51%|█████     | 203/400 [01:20<00:48,  4.05it/s]Running loglikelihood requests:  51%|█████     | 204/400 [01:21<00:54,  3.58it/s]Running loglikelihood requests:  51%|█████▏    | 205/400 [01:21<00:59,  3.26it/s]Running loglikelihood requests:  52%|█████▏    | 206/400 [01:21<01:04,  3.02it/s]Running loglikelihood requests:  52%|█████▏    | 207/400 [01:22<01:08,  2.84it/s]Running loglikelihood requests:  52%|█████▏    | 209/400 [01:22<00:54,  3.48it/s]Running loglikelihood requests:  52%|█████▎    | 210/400 [01:23<00:59,  3.18it/s]Running loglikelihood requests:  53%|█████▎    | 211/400 [01:23<01:03,  2.98it/s]Running loglikelihood requests:  53%|█████▎    | 212/400 [01:23<01:06,  2.82it/s]Running loglikelihood requests:  53%|█████▎    | 213/400 [01:24<01:09,  2.71it/s]Running loglikelihood requests:  54%|█████▎    | 214/400 [01:24<01:10,  2.64it/s]Running loglikelihood requests:  54%|█████▍    | 215/400 [01:25<01:11,  2.59it/s]Running loglikelihood requests:  54%|█████▍    | 216/400 [01:25<01:11,  2.57it/s]Running loglikelihood requests:  54%|█████▍    | 217/400 [01:25<01:11,  2.55it/s]Running loglikelihood requests:  55%|█████▍    | 218/400 [01:26<01:11,  2.54it/s]Running loglikelihood requests:  55%|█████▍    | 219/400 [01:26<01:11,  2.52it/s]Running loglikelihood requests:  55%|█████▌    | 220/400 [01:27<01:11,  2.51it/s]Running loglikelihood requests:  55%|█████▌    | 221/400 [01:27<01:11,  2.51it/s]Running loglikelihood requests:  56%|█████▌    | 222/400 [01:27<01:10,  2.51it/s]Running loglikelihood requests:  56%|█████▌    | 223/400 [01:28<01:10,  2.51it/s]Running loglikelihood requests:  56%|█████▌    | 224/400 [01:28<01:09,  2.53it/s]Running loglikelihood requests:  56%|█████▋    | 225/400 [01:29<01:09,  2.52it/s]Running loglikelihood requests:  56%|█████▋    | 226/400 [01:29<01:08,  2.53it/s]Running loglikelihood requests:  57%|█████▋    | 227/400 [01:29<01:08,  2.53it/s]Running loglikelihood requests:  57%|█████▋    | 228/400 [01:30<01:07,  2.54it/s]Running loglikelihood requests:  57%|█████▋    | 229/400 [01:30<01:07,  2.55it/s]Running loglikelihood requests:  57%|█████▊    | 230/400 [01:31<01:06,  2.54it/s]Running loglikelihood requests:  58%|█████▊    | 231/400 [01:31<01:06,  2.53it/s]Running loglikelihood requests:  58%|█████▊    | 232/400 [01:31<01:06,  2.54it/s]Running loglikelihood requests:  58%|█████▊    | 233/400 [01:32<01:05,  2.55it/s]Running loglikelihood requests:  58%|█████▊    | 234/400 [01:32<01:05,  2.55it/s]Running loglikelihood requests:  59%|█████▉    | 235/400 [01:33<01:04,  2.56it/s]Running loglikelihood requests:  59%|█████▉    | 237/400 [01:33<00:49,  3.32it/s]Running loglikelihood requests:  60%|█████▉    | 239/400 [01:33<00:41,  3.88it/s]Running loglikelihood requests:  60%|██████    | 240/400 [01:34<00:46,  3.47it/s]Running loglikelihood requests:  60%|██████    | 241/400 [01:34<00:49,  3.20it/s]Running loglikelihood requests:  60%|██████    | 242/400 [01:34<00:52,  3.01it/s]Running loglikelihood requests:  61%|██████    | 243/400 [01:35<00:54,  2.87it/s]Running loglikelihood requests:  61%|██████    | 244/400 [01:35<00:56,  2.77it/s]Running loglikelihood requests:  61%|██████▏   | 245/400 [01:36<00:57,  2.70it/s]Running loglikelihood requests:  62%|██████▏   | 246/400 [01:36<00:58,  2.65it/s]Running loglikelihood requests:  62%|██████▏   | 247/400 [01:36<00:58,  2.61it/s]Running loglikelihood requests:  62%|██████▏   | 248/400 [01:37<00:58,  2.59it/s]Running loglikelihood requests:  62%|██████▏   | 249/400 [01:37<00:58,  2.58it/s]Running loglikelihood requests:  62%|██████▎   | 250/400 [01:38<00:58,  2.57it/s]Running loglikelihood requests:  63%|██████▎   | 251/400 [01:38<00:58,  2.55it/s]Running loglikelihood requests:  63%|██████▎   | 252/400 [01:38<00:58,  2.55it/s]Running loglikelihood requests:  63%|██████▎   | 253/400 [01:39<00:57,  2.54it/s]Running loglikelihood requests:  64%|██████▎   | 254/400 [01:39<00:57,  2.56it/s]Running loglikelihood requests:  64%|██████▍   | 255/400 [01:40<00:56,  2.57it/s]Running loglikelihood requests:  64%|██████▍   | 256/400 [01:40<00:55,  2.59it/s]Running loglikelihood requests:  64%|██████▍   | 257/400 [01:40<00:55,  2.59it/s]Running loglikelihood requests:  64%|██████▍   | 258/400 [01:41<00:54,  2.58it/s]Running loglikelihood requests:  65%|██████▍   | 259/400 [01:41<00:54,  2.58it/s]Running loglikelihood requests:  65%|██████▌   | 260/400 [01:42<00:54,  2.58it/s]Running loglikelihood requests:  65%|██████▌   | 261/400 [01:42<00:53,  2.57it/s]Running loglikelihood requests:  66%|██████▌   | 262/400 [01:42<00:53,  2.57it/s]Running loglikelihood requests:  66%|██████▌   | 263/400 [01:43<00:52,  2.60it/s]Running loglikelihood requests:  66%|██████▋   | 265/400 [01:43<00:39,  3.39it/s]Running loglikelihood requests:  66%|██████▋   | 266/400 [01:43<00:42,  3.17it/s]Running loglikelihood requests:  67%|██████▋   | 267/400 [01:44<00:44,  2.99it/s]Running loglikelihood requests:  67%|██████▋   | 268/400 [01:44<00:45,  2.87it/s]Running loglikelihood requests:  67%|██████▋   | 269/400 [01:45<00:46,  2.80it/s]Running loglikelihood requests:  68%|██████▊   | 270/400 [01:45<00:46,  2.77it/s]Running loglikelihood requests:  68%|██████▊   | 271/400 [01:45<00:47,  2.73it/s]Running loglikelihood requests:  68%|██████▊   | 272/400 [01:46<00:47,  2.69it/s]Running loglikelihood requests:  68%|██████▊   | 273/400 [01:46<00:47,  2.69it/s]Running loglikelihood requests:  68%|██████▊   | 274/400 [01:46<00:47,  2.65it/s]Running loglikelihood requests:  69%|██████▉   | 275/400 [01:47<00:47,  2.63it/s]Running loglikelihood requests:  69%|██████▉   | 276/400 [01:47<00:47,  2.61it/s]Running loglikelihood requests:  69%|██████▉   | 277/400 [01:48<00:47,  2.61it/s]Running loglikelihood requests:  70%|██████▉   | 278/400 [01:48<00:46,  2.61it/s]Running loglikelihood requests:  70%|██████▉   | 279/400 [01:48<00:46,  2.60it/s]Running loglikelihood requests:  70%|███████   | 280/400 [01:49<00:46,  2.59it/s]Running loglikelihood requests:  70%|███████   | 281/400 [01:49<00:45,  2.62it/s]Running loglikelihood requests:  70%|███████   | 282/400 [01:50<00:46,  2.56it/s]Running loglikelihood requests:  71%|███████   | 283/400 [01:50<00:45,  2.55it/s]Running loglikelihood requests:  71%|███████   | 284/400 [01:50<00:45,  2.57it/s]Running loglikelihood requests:  71%|███████▏  | 285/400 [01:51<00:44,  2.60it/s]Running loglikelihood requests:  72%|███████▏  | 286/400 [01:51<00:43,  2.62it/s]Running loglikelihood requests:  72%|███████▏  | 287/400 [01:51<00:43,  2.62it/s]Running loglikelihood requests:  72%|███████▏  | 288/400 [01:52<00:42,  2.64it/s]Running loglikelihood requests:  72%|███████▏  | 289/400 [01:52<00:41,  2.65it/s]Running loglikelihood requests:  72%|███████▎  | 290/400 [01:53<00:41,  2.68it/s]Running loglikelihood requests:  73%|███████▎  | 291/400 [01:53<00:40,  2.68it/s]Running loglikelihood requests:  73%|███████▎  | 292/400 [01:53<00:40,  2.67it/s]Running loglikelihood requests:  73%|███████▎  | 293/400 [01:54<00:40,  2.66it/s]Running loglikelihood requests:  74%|███████▎  | 294/400 [01:54<00:39,  2.66it/s]Running loglikelihood requests:  74%|███████▍  | 297/400 [01:54<00:23,  4.29it/s]Running loglikelihood requests:  74%|███████▍  | 298/400 [01:55<00:27,  3.77it/s]Running loglikelihood requests:  75%|███████▍  | 299/400 [01:55<00:29,  3.43it/s]Running loglikelihood requests:  75%|███████▌  | 300/400 [01:56<00:31,  3.20it/s]Running loglikelihood requests:  75%|███████▌  | 301/400 [01:56<00:32,  3.03it/s]Running loglikelihood requests:  76%|███████▌  | 302/400 [01:56<00:33,  2.91it/s]Running loglikelihood requests:  76%|███████▌  | 303/400 [01:57<00:34,  2.80it/s]Running loglikelihood requests:  77%|███████▋  | 307/400 [01:57<00:18,  5.11it/s]Running loglikelihood requests:  77%|███████▋  | 308/400 [01:58<00:21,  4.38it/s]Running loglikelihood requests:  77%|███████▋  | 309/400 [01:58<00:23,  3.87it/s]Running loglikelihood requests:  78%|███████▊  | 310/400 [01:58<00:25,  3.51it/s]Running loglikelihood requests:  78%|███████▊  | 311/400 [01:59<00:27,  3.26it/s]Running loglikelihood requests:  78%|███████▊  | 312/400 [01:59<00:28,  3.06it/s]Running loglikelihood requests:  78%|███████▊  | 313/400 [01:59<00:29,  2.92it/s]Running loglikelihood requests:  78%|███████▊  | 314/400 [02:00<00:30,  2.84it/s]Running loglikelihood requests:  79%|███████▉  | 315/400 [02:00<00:30,  2.79it/s]Running loglikelihood requests:  79%|███████▉  | 316/400 [02:01<00:30,  2.78it/s]Running loglikelihood requests:  79%|███████▉  | 317/400 [02:01<00:30,  2.76it/s]Running loglikelihood requests:  80%|███████▉  | 318/400 [02:01<00:30,  2.73it/s]Running loglikelihood requests:  80%|████████  | 321/400 [02:02<00:18,  4.35it/s]Running loglikelihood requests:  80%|████████  | 322/400 [02:02<00:20,  3.86it/s]Running loglikelihood requests:  81%|████████  | 323/400 [02:02<00:21,  3.52it/s]Running loglikelihood requests:  81%|████████  | 324/400 [02:03<00:23,  3.30it/s]Running loglikelihood requests:  81%|████████▏ | 325/400 [02:03<00:24,  3.12it/s]Running loglikelihood requests:  82%|████████▏ | 326/400 [02:03<00:24,  2.99it/s]Running loglikelihood requests:  82%|████████▏ | 327/400 [02:04<00:25,  2.91it/s]Running loglikelihood requests:  82%|████████▏ | 328/400 [02:04<00:25,  2.86it/s]Running loglikelihood requests:  82%|████████▏ | 329/400 [02:05<00:25,  2.79it/s]Running loglikelihood requests:  82%|████████▎ | 330/400 [02:05<00:25,  2.74it/s]Running loglikelihood requests:  83%|████████▎ | 331/400 [02:05<00:25,  2.76it/s]Running loglikelihood requests:  83%|████████▎ | 332/400 [02:06<00:24,  2.74it/s]Running loglikelihood requests:  83%|████████▎ | 333/400 [02:06<00:24,  2.79it/s]Running loglikelihood requests:  84%|████████▎ | 334/400 [02:06<00:24,  2.74it/s]Running loglikelihood requests:  84%|████████▍ | 336/400 [02:07<00:17,  3.57it/s]Running loglikelihood requests:  84%|████████▍ | 337/400 [02:07<00:19,  3.31it/s]Running loglikelihood requests:  84%|████████▍ | 338/400 [02:08<00:19,  3.14it/s]Running loglikelihood requests:  85%|████████▍ | 339/400 [02:08<00:20,  3.00it/s]Running loglikelihood requests:  85%|████████▌ | 340/400 [02:08<00:20,  2.90it/s]Running loglikelihood requests:  85%|████████▌ | 341/400 [02:09<00:20,  2.83it/s]Running loglikelihood requests:  86%|████████▌ | 342/400 [02:09<00:20,  2.78it/s]Running loglikelihood requests:  86%|████████▌ | 343/400 [02:09<00:20,  2.73it/s]Running loglikelihood requests:  86%|████████▌ | 344/400 [02:10<00:20,  2.73it/s]Running loglikelihood requests:  86%|████████▋ | 345/400 [02:10<00:22,  2.43it/s]Running loglikelihood requests:  86%|████████▋ | 346/400 [02:11<00:20,  2.58it/s]Running loglikelihood requests:  87%|████████▋ | 347/400 [02:11<00:20,  2.62it/s]Running loglikelihood requests:  87%|████████▋ | 348/400 [02:11<00:19,  2.64it/s]Running loglikelihood requests:  87%|████████▋ | 349/400 [02:12<00:19,  2.68it/s]Running loglikelihood requests:  88%|████████▊ | 350/400 [02:12<00:18,  2.68it/s]Running loglikelihood requests:  88%|████████▊ | 351/400 [02:12<00:18,  2.70it/s]Running loglikelihood requests:  88%|████████▊ | 352/400 [02:13<00:17,  2.77it/s]Running loglikelihood requests:  88%|████████▊ | 353/400 [02:13<00:16,  2.82it/s]Running loglikelihood requests:  88%|████████▊ | 354/400 [02:13<00:16,  2.80it/s]Running loglikelihood requests:  89%|████████▉ | 355/400 [02:14<00:16,  2.78it/s]Running loglikelihood requests:  89%|████████▉ | 356/400 [02:14<00:15,  2.82it/s]Running loglikelihood requests:  89%|████████▉ | 357/400 [02:15<00:15,  2.80it/s]Running loglikelihood requests:  90%|████████▉ | 358/400 [02:15<00:15,  2.72it/s]Running loglikelihood requests:  90%|████████▉ | 359/400 [02:15<00:15,  2.72it/s]Running loglikelihood requests:  90%|█████████ | 360/400 [02:16<00:14,  2.75it/s]Running loglikelihood requests:  91%|█████████ | 363/400 [02:16<00:08,  4.38it/s]Running loglikelihood requests:  91%|█████████ | 364/400 [02:16<00:09,  3.88it/s]Running loglikelihood requests:  91%|█████████▏| 365/400 [02:17<00:09,  3.56it/s]Running loglikelihood requests:  92%|█████████▏| 366/400 [02:17<00:10,  3.35it/s]Running loglikelihood requests:  92%|█████████▏| 367/400 [02:17<00:10,  3.21it/s]Running loglikelihood requests:  92%|█████████▏| 368/400 [02:18<00:10,  3.02it/s]Running loglikelihood requests:  92%|█████████▏| 369/400 [02:18<00:10,  2.94it/s]Running loglikelihood requests:  92%|█████████▎| 370/400 [02:19<00:10,  2.91it/s]Running loglikelihood requests:  93%|█████████▎| 371/400 [02:19<00:09,  2.91it/s]Running loglikelihood requests:  93%|█████████▎| 372/400 [02:19<00:09,  2.94it/s]Running loglikelihood requests:  93%|█████████▎| 373/400 [02:20<00:09,  2.95it/s]Running loglikelihood requests:  94%|█████████▎| 374/400 [02:20<00:08,  2.91it/s]Running loglikelihood requests:  94%|█████████▍| 377/400 [02:20<00:04,  4.72it/s]Running loglikelihood requests:  94%|█████████▍| 378/400 [02:21<00:05,  4.20it/s]Running loglikelihood requests:  95%|█████████▍| 379/400 [02:21<00:05,  3.80it/s]Running loglikelihood requests:  95%|█████████▌| 380/400 [02:21<00:05,  3.58it/s]Running loglikelihood requests:  95%|█████████▌| 381/400 [02:22<00:05,  3.43it/s]Running loglikelihood requests:  96%|█████████▌| 382/400 [02:22<00:05,  3.31it/s]Running loglikelihood requests:  96%|█████████▌| 383/400 [02:22<00:05,  3.21it/s]Running loglikelihood requests:  96%|█████████▌| 384/400 [02:23<00:05,  3.13it/s]Running loglikelihood requests:  97%|█████████▋| 387/400 [02:23<00:02,  4.89it/s]Running loglikelihood requests:  97%|█████████▋| 388/400 [02:23<00:02,  4.36it/s]Running loglikelihood requests:  98%|█████████▊| 390/400 [02:24<00:02,  4.81it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [02:24<00:02,  4.34it/s]Running loglikelihood requests:  98%|█████████▊| 392/400 [02:24<00:01,  4.00it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [02:25<00:01,  3.84it/s]Running loglikelihood requests:  98%|█████████▊| 394/400 [02:25<00:01,  3.61it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [02:25<00:01,  3.49it/s]Running loglikelihood requests:  99%|█████████▉| 396/400 [02:25<00:01,  3.45it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [02:26<00:00,  4.42it/s]Running loglikelihood requests: 100%|█████████▉| 399/400 [02:26<00:00,  4.34it/s]Running loglikelihood requests: 100%|██████████| 400/400 [02:26<00:00,  2.73it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:7'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:7'}
full model:
{'openbookqa': {'alias': 'openbookqa', 'acc,none': 0.25, 'acc_stderr,none': 0.04351941398892446, 'acc_norm,none': 0.39, 'acc_norm_stderr,none': 0.04902071300001973}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7187319796021299
0.7542307644255121
0.9218102409329888
0.9507846064925026
0.9244473932111009
0.9697851265705995
0.9088369054586506
0.9555908644563589
0.6065244449438403
0.8380982140772407
0.9227293008726757
0.940339967265193
0.8763319017557598
0.8956221731459367
0.8537768995934096
0.9486125995141542
0.9673324054839537
0.9137514724447725
0.9379677722073356
0.9822722776537973
0.9198472199854921
0.886666948473494
0.9000260607563779
0.9759633945879138
0.9869744580755335
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 3, 1, 5, 4, 7, 0]
tensor([6, 2, 3, 1, 5, 4, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 4, 5, 7, 0]
tensor([6, 2, 3, 1, 4, 5, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 4, 5, 7, 0]
tensor([6, 2, 3, 1, 4, 5, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 5, 2, 4, 3, 7, 0]
tensor([6, 1, 5, 2, 4, 3, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 1, 3, 2, 5, 4, 6, 0]
tensor([7, 1, 3, 2, 5, 4, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 5, 4, 7, 0]
tensor([6, 2, 3, 1, 5, 4, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 1.0, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Normal merging for layer 4
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 6 to 29
done!
Normal merging for layer 30
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
Normal merging for layer 31
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
all done!
Model size: 12.3238 GB
136
cuda:7
winogrande
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.30s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/winogrande HTTP/1.1" 307 67
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/allenai/winogrande HTTP/1.1" 200 1036
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/winogrande/winogrande.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): datasets-server.hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://datasets-server.hf-mirror.com:443 "GET /parquet?dataset=winogrande HTTP/1.1" 302 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET / HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/winogrande/winogrande.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/winogrande/resolve/main/winogrande.py HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/winogrande/resolve/main/winogrande.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/winogrande/resolve/main/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/winogrande/resolve/main/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/winogrande/resolve/main/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/allenai/winogrande/resolve/main/README.md HTTP/1.1" 200 0
DEBUG:filelock:Attempting to acquire lock 140514966352960 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:filelock:Lock 140514966352960 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514966352960 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:filelock:Lock 140514966352960 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_winogrande_winogrande_xl_1.1.0_a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2.lock
DEBUG:filelock:Attempting to acquire lock 140516065703840 on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:filelock:Lock 140516065703840 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516065703840 on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:filelock:Lock 140516065703840 released on /public/home/zouyifei001/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
DEBUG:lm_eval.api.task:doc_to_text returned an int. Assuming multiple inputs.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of winogrande from None to 0
INFO:lm_eval.api.task:Building contexts for winogrande on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 135825.91it/s]
DEBUG:lm_eval.evaluator:Task: winogrande; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:00<03:16,  1.01it/s]Running loglikelihood requests:   1%|          | 2/200 [00:01<02:24,  1.37it/s]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:04,  1.58it/s]Running loglikelihood requests:   2%|▏         | 4/200 [00:02<01:55,  1.70it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:03<01:49,  1.79it/s]Running loglikelihood requests:   3%|▎         | 6/200 [00:03<01:45,  1.84it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:04<01:42,  1.88it/s]Running loglikelihood requests:   4%|▍         | 8/200 [00:04<01:39,  1.92it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:05<01:37,  1.95it/s]Running loglikelihood requests:   5%|▌         | 10/200 [00:05<01:36,  1.97it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:06<01:35,  1.99it/s]Running loglikelihood requests:   6%|▌         | 12/200 [00:06<01:34,  2.00it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:07<01:33,  2.01it/s]Running loglikelihood requests:   7%|▋         | 14/200 [00:07<01:32,  2.02it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:08<01:31,  2.02it/s]Running loglikelihood requests:   8%|▊         | 16/200 [00:08<01:30,  2.02it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:09<01:30,  2.03it/s]Running loglikelihood requests:   9%|▉         | 18/200 [00:09<01:29,  2.03it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:10<01:28,  2.04it/s]Running loglikelihood requests:  10%|█         | 20/200 [00:10<01:28,  2.04it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:10<01:27,  2.04it/s]Running loglikelihood requests:  11%|█         | 22/200 [00:11<01:27,  2.04it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:11<01:26,  2.04it/s]Running loglikelihood requests:  12%|█▏        | 24/200 [00:12<01:25,  2.05it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:12<01:25,  2.05it/s]Running loglikelihood requests:  13%|█▎        | 26/200 [00:13<01:24,  2.05it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:13<01:24,  2.05it/s]Running loglikelihood requests:  14%|█▍        | 28/200 [00:14<01:23,  2.06it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:14<01:22,  2.06it/s]Running loglikelihood requests:  15%|█▌        | 30/200 [00:15<01:22,  2.07it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:15<01:21,  2.07it/s]Running loglikelihood requests:  16%|█▌        | 32/200 [00:16<01:20,  2.08it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:16<01:20,  2.08it/s]Running loglikelihood requests:  17%|█▋        | 34/200 [00:17<01:19,  2.08it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:17<01:19,  2.08it/s]Running loglikelihood requests:  18%|█▊        | 36/200 [00:18<01:18,  2.08it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:18<01:18,  2.09it/s]Running loglikelihood requests:  19%|█▉        | 38/200 [00:19<01:17,  2.09it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:19<01:16,  2.09it/s]Running loglikelihood requests:  20%|██        | 40/200 [00:20<01:16,  2.10it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:20<01:15,  2.10it/s]Running loglikelihood requests:  21%|██        | 42/200 [00:21<01:15,  2.10it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:21<01:14,  2.10it/s]Running loglikelihood requests:  22%|██▏       | 44/200 [00:22<01:14,  2.10it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:22<01:13,  2.10it/s]Running loglikelihood requests:  23%|██▎       | 46/200 [00:22<01:13,  2.10it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:23<01:12,  2.10it/s]Running loglikelihood requests:  24%|██▍       | 48/200 [00:23<01:12,  2.10it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:24<01:11,  2.10it/s]Running loglikelihood requests:  25%|██▌       | 50/200 [00:24<01:11,  2.10it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:25<01:10,  2.10it/s]Running loglikelihood requests:  26%|██▌       | 52/200 [00:25<01:10,  2.10it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:26<01:09,  2.10it/s]Running loglikelihood requests:  27%|██▋       | 54/200 [00:26<01:09,  2.10it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:27<01:08,  2.11it/s]Running loglikelihood requests:  28%|██▊       | 56/200 [00:27<01:08,  2.11it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:28<01:07,  2.11it/s]Running loglikelihood requests:  29%|██▉       | 58/200 [00:28<01:07,  2.12it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:29<01:06,  2.12it/s]Running loglikelihood requests:  30%|███       | 60/200 [00:29<01:06,  2.12it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:30<01:05,  2.12it/s]Running loglikelihood requests:  31%|███       | 62/200 [00:30<01:04,  2.12it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:31<01:04,  2.13it/s]Running loglikelihood requests:  32%|███▏      | 64/200 [00:31<01:03,  2.13it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:31<01:03,  2.13it/s]Running loglikelihood requests:  33%|███▎      | 66/200 [00:32<01:02,  2.13it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:32<01:02,  2.13it/s]Running loglikelihood requests:  34%|███▍      | 68/200 [00:33<01:02,  2.13it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:33<01:01,  2.12it/s]Running loglikelihood requests:  35%|███▌      | 70/200 [00:34<01:01,  2.12it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:34<01:00,  2.12it/s]Running loglikelihood requests:  36%|███▌      | 72/200 [00:35<01:00,  2.12it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:35<00:59,  2.12it/s]Running loglikelihood requests:  37%|███▋      | 74/200 [00:36<00:59,  2.13it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:36<00:58,  2.14it/s]Running loglikelihood requests:  38%|███▊      | 76/200 [00:37<00:58,  2.14it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:37<00:57,  2.14it/s]Running loglikelihood requests:  39%|███▉      | 78/200 [00:38<00:56,  2.14it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:38<00:56,  2.14it/s]Running loglikelihood requests:  40%|████      | 80/200 [00:39<00:55,  2.14it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:39<00:55,  2.14it/s]Running loglikelihood requests:  41%|████      | 82/200 [00:39<00:54,  2.15it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:40<00:54,  2.15it/s]Running loglikelihood requests:  42%|████▏     | 84/200 [00:40<00:53,  2.16it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:41<00:53,  2.16it/s]Running loglikelihood requests:  43%|████▎     | 86/200 [00:41<00:52,  2.16it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:42<00:52,  2.16it/s]Running loglikelihood requests:  44%|████▍     | 88/200 [00:42<00:51,  2.17it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:43<00:51,  2.17it/s]Running loglikelihood requests:  45%|████▌     | 90/200 [00:43<00:50,  2.17it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:44<00:50,  2.16it/s]Running loglikelihood requests:  46%|████▌     | 92/200 [00:44<00:49,  2.16it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:45<00:49,  2.16it/s]Running loglikelihood requests:  47%|████▋     | 94/200 [00:45<00:49,  2.16it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:45<00:48,  2.16it/s]Running loglikelihood requests:  48%|████▊     | 96/200 [00:46<00:48,  2.16it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:46<00:47,  2.17it/s]Running loglikelihood requests:  49%|████▉     | 98/200 [00:47<00:47,  2.17it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:47<00:46,  2.17it/s]Running loglikelihood requests:  50%|█████     | 100/200 [00:48<00:46,  2.17it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:48<00:45,  2.18it/s]Running loglikelihood requests:  51%|█████     | 102/200 [00:49<00:44,  2.18it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:49<00:44,  2.18it/s]Running loglikelihood requests:  52%|█████▏    | 104/200 [00:50<00:44,  2.18it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:50<00:43,  2.18it/s]Running loglikelihood requests:  53%|█████▎    | 106/200 [00:50<00:43,  2.19it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:51<00:42,  2.19it/s]Running loglikelihood requests:  54%|█████▍    | 108/200 [00:51<00:41,  2.19it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:52<00:41,  2.19it/s]Running loglikelihood requests:  55%|█████▌    | 110/200 [00:52<00:41,  2.19it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:53<00:40,  2.18it/s]Running loglikelihood requests:  56%|█████▌    | 112/200 [00:53<00:40,  2.18it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:54<00:39,  2.18it/s]Running loglikelihood requests:  57%|█████▋    | 114/200 [00:54<00:39,  2.18it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:55<00:38,  2.19it/s]Running loglikelihood requests:  58%|█████▊    | 116/200 [00:55<00:38,  2.19it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:56<00:37,  2.19it/s]Running loglikelihood requests:  59%|█████▉    | 118/200 [00:56<00:37,  2.19it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:56<00:36,  2.20it/s]Running loglikelihood requests:  60%|██████    | 120/200 [00:57<00:36,  2.19it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:57<00:36,  2.19it/s]Running loglikelihood requests:  61%|██████    | 122/200 [00:58<00:35,  2.19it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:58<00:35,  2.19it/s]Running loglikelihood requests:  62%|██████▏   | 124/200 [00:59<00:34,  2.19it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:59<00:34,  2.20it/s]Running loglikelihood requests:  63%|██████▎   | 126/200 [01:00<00:33,  2.20it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [01:00<00:33,  2.20it/s]Running loglikelihood requests:  64%|██████▍   | 128/200 [01:01<00:32,  2.20it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [01:01<00:32,  2.20it/s]Running loglikelihood requests:  65%|██████▌   | 130/200 [01:01<00:31,  2.20it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [01:02<00:31,  2.21it/s]Running loglikelihood requests:  66%|██████▌   | 132/200 [01:02<00:30,  2.21it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [01:03<00:30,  2.22it/s]Running loglikelihood requests:  67%|██████▋   | 134/200 [01:03<00:29,  2.22it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [01:04<00:29,  2.22it/s]Running loglikelihood requests:  68%|██████▊   | 136/200 [01:04<00:28,  2.22it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [01:05<00:28,  2.22it/s]Running loglikelihood requests:  69%|██████▉   | 138/200 [01:05<00:27,  2.22it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:05<00:27,  2.22it/s]Running loglikelihood requests:  70%|███████   | 140/200 [01:06<00:26,  2.23it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:06<00:26,  2.22it/s]Running loglikelihood requests:  71%|███████   | 142/200 [01:07<00:26,  2.23it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:07<00:25,  2.22it/s]Running loglikelihood requests:  72%|███████▏  | 144/200 [01:08<00:25,  2.23it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:08<00:24,  2.23it/s]Running loglikelihood requests:  73%|███████▎  | 146/200 [01:09<00:24,  2.23it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:09<00:23,  2.22it/s]Running loglikelihood requests:  74%|███████▍  | 148/200 [01:10<00:23,  2.22it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:10<00:22,  2.23it/s]Running loglikelihood requests:  75%|███████▌  | 150/200 [01:10<00:22,  2.22it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:11<00:22,  2.21it/s]Running loglikelihood requests:  76%|███████▌  | 152/200 [01:11<00:21,  2.22it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:12<00:21,  2.22it/s]Running loglikelihood requests:  77%|███████▋  | 154/200 [01:12<00:20,  2.22it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:13<00:20,  2.23it/s]Running loglikelihood requests:  78%|███████▊  | 156/200 [01:13<00:19,  2.23it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:14<00:19,  2.23it/s]Running loglikelihood requests:  79%|███████▉  | 158/200 [01:14<00:18,  2.24it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:14<00:18,  2.25it/s]Running loglikelihood requests:  80%|████████  | 160/200 [01:15<00:17,  2.25it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:15<00:17,  2.25it/s]Running loglikelihood requests:  81%|████████  | 162/200 [01:16<00:16,  2.25it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:16<00:16,  2.25it/s]Running loglikelihood requests:  82%|████████▏ | 164/200 [01:17<00:15,  2.26it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:17<00:15,  2.26it/s]Running loglikelihood requests:  83%|████████▎ | 166/200 [01:18<00:14,  2.28it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:18<00:14,  2.28it/s]Running loglikelihood requests:  84%|████████▍ | 168/200 [01:18<00:14,  2.28it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:19<00:13,  2.28it/s]Running loglikelihood requests:  85%|████████▌ | 170/200 [01:19<00:13,  2.28it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:20<00:12,  2.29it/s]Running loglikelihood requests:  86%|████████▌ | 172/200 [01:20<00:12,  2.29it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:21<00:11,  2.29it/s]Running loglikelihood requests:  87%|████████▋ | 174/200 [01:21<00:11,  2.30it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:21<00:10,  2.30it/s]Running loglikelihood requests:  88%|████████▊ | 176/200 [01:22<00:10,  2.30it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:22<00:09,  2.30it/s]Running loglikelihood requests:  89%|████████▉ | 178/200 [01:23<00:09,  2.31it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:23<00:09,  2.32it/s]Running loglikelihood requests:  90%|█████████ | 180/200 [01:24<00:08,  2.33it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:24<00:08,  2.34it/s]Running loglikelihood requests:  91%|█████████ | 182/200 [01:24<00:07,  2.34it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:25<00:07,  2.35it/s]Running loglikelihood requests:  92%|█████████▏| 184/200 [01:25<00:06,  2.30it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:26<00:06,  2.29it/s]Running loglikelihood requests:  93%|█████████▎| 186/200 [01:26<00:06,  2.29it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:27<00:05,  2.29it/s]Running loglikelihood requests:  94%|█████████▍| 188/200 [01:27<00:05,  2.29it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:28<00:04,  2.30it/s]Running loglikelihood requests:  95%|█████████▌| 190/200 [01:28<00:04,  2.32it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:28<00:03,  2.33it/s]Running loglikelihood requests:  96%|█████████▌| 192/200 [01:29<00:03,  2.34it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:29<00:02,  2.34it/s]Running loglikelihood requests:  97%|█████████▋| 194/200 [01:30<00:02,  2.35it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:30<00:02,  2.35it/s]Running loglikelihood requests:  98%|█████████▊| 196/200 [01:30<00:01,  2.35it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:31<00:01,  2.32it/s]Running loglikelihood requests:  99%|█████████▉| 198/200 [01:31<00:00,  2.34it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:32<00:00,  2.36it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:32<00:00,  2.37it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:32<00:00,  2.16it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:0'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}
full model:
{'winogrande': {'alias': 'winogrande', 'acc,none': 0.69, 'acc_stderr,none': 0.046482319871173176}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9565474555896152
0.9256436871263655
0.919777800952949
0.8502516827829548
0.9788217808920475
0.7842184683361061
0.5847175538207384
0.7741211354638314
0.8734705119073022
0.9731317123565076
0.9046875380381674
0.8858535931048256
0.9314298098713261
0.8442403312485581
0.7166029053748131
0.7061450250233585
0.7563059483803782
0.6877563559653158
0.8738989590810852
0.784285727893607
0.8409178900131131
0.8425380927376759
0.6782655591765769
0.7228968829640015
0.8418686487797186
0.9141168065903919
0.8562481461255738
0.6139553840231665
0.934088538459943
Total groups 66 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 4, 3, 5, 1, 7, 0]
tensor([6, 2, 4, 3, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 2, 3, 1, 5, 4, 7, 0]
tensor([6, 2, 3, 1, 5, 4, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 4, 1, 0, 3, 0, 1, 2]
tensor([5, 4, 1, 0, 3, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 3, 1, 4, 5, 1, 0]
tensor([0, 2, 3, 1, 4, 5, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[5, 1, 2, 3, 4, 0, 1, 0]
tensor([5, 1, 2, 3, 4, 0, 1, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 2, 1, 1, 2, 3, 3, 0]
tensor([0, 2, 1, 1, 2, 3, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 1.0, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1.0, 1.0, 1]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 2 to 7
done!
Normal merging for layer 8
tensor([3, 5])
tensor(3)
tensor([2, 6])
tensor(2)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 9 to 13
done!
Normal merging for layer 14
tensor([0, 7])
tensor(0)
tensor([3, 6])
tensor(3)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
done!
Normal merging for layer 15
tensor([5, 7])
tensor(5)
tensor([1, 6])
tensor(1)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 16
tensor([0, 7])
tensor(0)
tensor([2, 3])
tensor(2)
tensor([1, 4])
tensor(1)
tensor([5, 6])
tensor(5)
done!
Cross-layer merge completed for layers 17 to 23
done!
Normal merging for layer 24
tensor([0, 7])
tensor(0)
tensor([1, 2, 3, 4, 5, 6])
tensor(1)
done!
Normal merging for layer 25
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 26 to 31
done!
all done!
Model size: 11.8828 GB
160
cuda:0
wsc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:40<00:40, 40.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 24.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.94s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wsc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wsc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:filelock:Attempting to acquire lock 140545359708384 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140545359708384 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140545359708384 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140545359708384 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140542673281984 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140542673281984 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140542673281984 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140542673281984 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wsc from None to 0
INFO:lm_eval.api.task:Building contexts for wsc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 84819.09it/s]
DEBUG:lm_eval.evaluator:Task: wsc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<04:18,  1.30s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:05,  1.56it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:02<01:05,  2.93it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:03<01:08,  2.78it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:04<01:10,  2.69it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:05<01:11,  2.63it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:06<01:11,  2.59it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:06<01:11,  2.57it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:07<01:10,  2.56it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:08<01:09,  2.57it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:09<01:08,  2.57it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:10<01:08,  2.57it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:10<01:07,  2.57it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:11<01:06,  2.57it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:12<01:05,  2.57it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:13<01:04,  2.58it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:13<01:03,  2.58it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:14<01:02,  2.59it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:15<01:02,  2.59it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:16<01:01,  2.59it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:16<01:00,  2.59it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:17<00:59,  2.60it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:18<00:58,  2.61it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:19<00:57,  2.62it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:19<00:56,  2.63it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:20<00:55,  2.64it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:21<00:54,  2.64it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:22<00:53,  2.66it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:22<00:52,  2.67it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:23<00:51,  2.68it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:24<00:50,  2.69it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:25<00:49,  2.70it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:25<00:49,  2.71it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:26<00:48,  2.72it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:27<00:47,  2.74it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:28<00:46,  2.75it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:28<00:45,  2.75it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:29<00:44,  2.77it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:30<00:43,  2.77it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:30<00:42,  2.78it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:31<00:41,  2.81it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:32<00:40,  2.84it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:33<00:39,  2.86it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:33<00:38,  2.87it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:34<00:37,  2.88it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:35<00:36,  2.90it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:35<00:36,  2.91it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:36<00:35,  2.94it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:37<00:34,  2.97it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:37<00:33,  2.99it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:38<00:32,  3.01it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:39<00:31,  3.02it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:39<00:30,  3.03it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:40<00:29,  3.04it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:41<00:29,  3.05it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:41<00:28,  3.08it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:42<00:20,  4.05it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:42<00:16,  4.75it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:43<00:17,  4.30it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:44<00:18,  3.98it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:44<00:19,  3.77it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:45<00:19,  3.58it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:45<00:19,  3.51it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:46<00:19,  3.47it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:47<00:18,  3.44it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:47<00:18,  3.42it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:48<00:17,  3.41it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:48<00:17,  3.40it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:49<00:16,  3.39it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:50<00:16,  3.41it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:50<00:15,  3.42it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:51<00:14,  3.43it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:51<00:14,  3.44it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:52<00:13,  3.44it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:53<00:13,  3.45it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:53<00:12,  3.46it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:54<00:11,  3.47it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:54<00:11,  3.48it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [00:55<00:10,  3.49it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [00:55<00:10,  3.49it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [00:56<00:09,  3.49it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [00:57<00:08,  3.49it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [00:57<00:08,  3.39it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [00:58<00:07,  3.43it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [00:58<00:07,  3.45it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [00:59<00:06,  3.50it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [00:59<00:05,  3.54it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:00<00:05,  3.57it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:00<00:04,  3.59it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:01<00:04,  3.61it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:02<00:03,  3.62it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:02<00:03,  3.65it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:03<00:02,  3.67it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:03<00:01,  3.68it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:04<00:01,  3.70it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:04<00:00,  3.72it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:05<00:00,  3.74it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:05<00:00,  3.06it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:1'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:1'}
full model:
{'wsc': {'alias': 'wsc', 'acc,none': 0.39, 'acc_stderr,none': 0.04902071300001973}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8279363410631274
0.19299873358047231
0.24697427628971416
0.9498246079459217
0.9208790959024347
0.5433529265767616
0.7541489406957872
0.8642404861438043
0.9451768410410958
0.6364904470004392
0.8605038470072015
0.8806563523485148
0.7320941418431636
0.7167995199876819
0.9558598958577776
0.7974395317006763
0.8001507378395173
0.8689133172877098
0.6142386786363212
0.6039861720637983
0.8068446902119512
0.6154342295548068
0.8415314242927469
0.42042589459680124
0.7443787948515065
0.7081725962998161
0.7223774755032815
0.9338202429729527
0.7700364698774241
0.8279363410631274
0.19299873358047231
0.24697427628971416
0.9498246079459217
0.9208790959024347
0.5433529265767616
0.7541489406957872
0.8642404861438043
0.9451768410410958
0.6364904470004392
0.8605038470072015
0.8806563523485148
0.7320941418431636
0.7167995199876819
0.9558598958577776
0.7974395317006763
0.8001507378395173
0.8689133172877098
0.6142386786363212
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 7, 2, 4, 5, 1, 3, 0]
tensor([6, 7, 2, 4, 5, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 7, 3, 6, 2, 5, 4, 0]
tensor([1, 7, 3, 6, 2, 5, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 5, 6, 3, 7, 4, 2, 0]
tensor([1, 5, 6, 3, 7, 4, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 5, 3, 6, 7, 4, 2, 0]
tensor([1, 5, 3, 6, 7, 4, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 7, 5, 6, 1, 2, 3, 0]
tensor([4, 7, 5, 6, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 7, 1, 5, 6, 3, 2, 0]
tensor([4, 7, 1, 5, 6, 3, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1, 1.0, 0, 1.0]
tensor([0, 1, 1, 1, 1, 1, 0, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
done!
Normal merging for layer 6
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
done!
Normal merging for layer 7
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
done!
Cross-layer merge completed for layers 8 to 28
done!
Normal merging for layer 29
tensor([0, 6])
tensor(0)
tensor([1, 2, 3, 4, 5, 7])
tensor(1)
done!
Cross-layer merge completed for layers 30 to 31
done!
all done!
Model size: 12.3238 GB
247
cuda:1
wsc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.10s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wsc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wsc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 241
DEBUG:filelock:Attempting to acquire lock 140514430075152 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140514430075152 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514430075152 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140514430075152 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wsc.fixed_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140514972809216 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140514972809216 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514972809216 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140514972809216 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wsc.fixed/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wsc from None to 0
INFO:lm_eval.api.task:Building contexts for wsc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 84054.19it/s]
DEBUG:lm_eval.evaluator:Task: wsc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<04:21,  1.32s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:02<02:05,  1.57it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:02<01:05,  2.95it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:03<01:08,  2.79it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:04<01:10,  2.69it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:05<01:10,  2.64it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:06<01:11,  2.60it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:06<01:10,  2.58it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:07<01:10,  2.57it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:08<01:09,  2.57it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:09<01:08,  2.57it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:09<01:08,  2.57it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:10<01:07,  2.57it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:11<01:06,  2.57it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:12<01:05,  2.57it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:13<01:05,  2.57it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:13<01:04,  2.57it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:14<01:03,  2.58it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:15<01:02,  2.58it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:16<01:01,  2.58it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:16<01:00,  2.58it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:17<00:59,  2.59it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:18<00:59,  2.59it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:19<00:57,  2.61it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:20<00:56,  2.62it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:20<00:55,  2.63it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:21<00:54,  2.64it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:22<00:53,  2.66it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:23<00:52,  2.67it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:23<00:52,  2.65it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:24<00:51,  2.68it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:25<00:49,  2.71it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:25<00:48,  2.73it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:26<00:47,  2.74it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:27<00:46,  2.75it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:28<00:45,  2.77it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:28<00:45,  2.78it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:29<00:44,  2.79it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:30<00:43,  2.80it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:30<00:42,  2.82it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:31<00:41,  2.84it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:32<00:40,  2.86it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:33<00:39,  2.87it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:33<00:38,  2.87it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:34<00:37,  2.87it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:35<00:37,  2.88it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:35<00:36,  2.89it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:36<00:35,  2.94it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:37<00:33,  2.98it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:37<00:32,  3.01it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:38<00:31,  3.03it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:39<00:31,  3.05it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:39<00:30,  3.06it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:40<00:29,  3.07it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:40<00:28,  3.08it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:41<00:27,  3.12it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:42<00:20,  4.10it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:42<00:16,  4.82it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:43<00:17,  4.35it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:44<00:18,  4.03it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:44<00:19,  3.81it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:45<00:19,  3.67it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:45<00:19,  3.58it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:46<00:19,  3.52it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:47<00:18,  3.48it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:47<00:18,  3.45it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:48<00:17,  3.44it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:48<00:17,  3.43it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:49<00:16,  3.43it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:49<00:15,  3.44it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:50<00:15,  3.45it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:51<00:14,  3.46it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:51<00:14,  3.47it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:52<00:13,  3.48it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:52<00:12,  3.49it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:53<00:12,  3.50it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:53<00:11,  3.51it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:54<00:11,  3.52it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [00:55<00:10,  3.53it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [00:55<00:09,  3.53it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [00:56<00:09,  3.53it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [00:56<00:08,  3.53it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [00:57<00:08,  3.54it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [00:57<00:07,  3.55it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [00:58<00:07,  3.55it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [00:59<00:06,  3.58it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [00:59<00:05,  3.60it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:00<00:05,  3.63it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:00<00:04,  3.64it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:01<00:04,  3.65it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:01<00:03,  3.66it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:02<00:02,  3.69it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:02<00:02,  3.71it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:03<00:01,  3.72it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:03<00:01,  3.74it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:04<00:00,  3.76it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:04<00:00,  3.78it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:04<00:00,  3.08it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:2'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:2'}
full model:
{'wsc': {'alias': 'wsc', 'acc,none': 0.39, 'acc_stderr,none': 0.04902071300001973}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8279363410631274
0.19299873358047231
0.24697427628971416
0.9498246079459217
0.9208790959024347
0.5433529265767616
0.7541489406957872
0.8642404861438043
0.9451768410410958
0.6364904470004392
0.8605038470072015
0.8806563523485148
0.7320941418431636
0.7167995199876819
0.9558598958577776
0.7974395317006763
0.8001507378395173
0.8689133172877098
0.6142386786363212
0.6039861720637983
0.8068446902119512
0.6154342295548068
0.8415314242927469
0.42042589459680124
0.7443787948515065
0.7081725962998161
0.7223774755032815
0.9338202429729527
0.7700364698774241
0.8279363410631274
0.19299873358047231
0.24697427628971416
0.9498246079459217
0.9208790959024347
0.5433529265767616
0.7541489406957872
0.8642404861438043
0.9451768410410958
0.6364904470004392
0.8605038470072015
0.8806563523485148
0.7320941418431636
0.7167995199876819
0.9558598958577776
0.7974395317006763
0.8001507378395173
0.8689133172877098
0.6142386786363212
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 7, 2, 4, 5, 1, 3, 0]
tensor([6, 7, 2, 4, 5, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 7, 3, 6, 2, 5, 4, 0]
tensor([1, 7, 3, 6, 2, 5, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 5, 6, 3, 7, 4, 2, 0]
tensor([1, 5, 6, 3, 7, 4, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 5, 3, 6, 7, 4, 2, 0]
tensor([1, 5, 3, 6, 7, 4, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 7, 5, 6, 1, 2, 3, 0]
tensor([4, 7, 5, 6, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 7, 1, 5, 6, 3, 2, 0]
tensor([4, 7, 1, 5, 6, 3, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1, 1.0, 0, 1.0]
tensor([0, 1, 1, 1, 1, 1, 0, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
done!
Normal merging for layer 6
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
done!
Normal merging for layer 7
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
done!
Cross-layer merge completed for layers 8 to 28
done!
Normal merging for layer 29
tensor([0, 6])
tensor(0)
tensor([1, 2, 3, 4, 5, 7])
tensor(1)
done!
Cross-layer merge completed for layers 30 to 31
done!
all done!
Model size: 12.3238 GB
83
cuda:2
multirc
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.13s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: multirc] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: multirc] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 239
DEBUG:filelock:Attempting to acquire lock 140515997024624 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140515997024624 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140515997024624 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140515997024624 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_multirc_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140514988904560 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140514988904560 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514988904560 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140514988904560 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/multirc/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of multirc from None to 0
INFO:lm_eval.api.task:Building contexts for multirc on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1267.20it/s]
DEBUG:lm_eval.evaluator:Task: multirc; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:03<10:29,  3.16s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:05<05:42,  1.74s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:08<04:47,  1.48s/it]Running loglikelihood requests:   4%|▎         | 7/200 [00:10<04:24,  1.37s/it]Running loglikelihood requests:   4%|▍         | 9/200 [00:13<04:11,  1.32s/it]Running loglikelihood requests:   6%|▌         | 11/200 [00:15<04:02,  1.28s/it]Running loglikelihood requests:   6%|▋         | 13/200 [00:17<03:55,  1.26s/it]Running loglikelihood requests:   8%|▊         | 15/200 [00:20<03:50,  1.25s/it]Running loglikelihood requests:   8%|▊         | 17/200 [00:22<03:46,  1.24s/it]Running loglikelihood requests:  10%|▉         | 19/200 [00:25<03:42,  1.23s/it]Running loglikelihood requests:  10%|█         | 21/200 [00:27<03:39,  1.22s/it]Running loglikelihood requests:  12%|█▏        | 23/200 [00:30<03:44,  1.27s/it]Running loglikelihood requests:  12%|█▎        | 25/200 [00:32<03:45,  1.29s/it]Running loglikelihood requests:  14%|█▎        | 27/200 [00:35<03:40,  1.27s/it]Running loglikelihood requests:  14%|█▍        | 29/200 [00:37<03:34,  1.26s/it]Running loglikelihood requests:  16%|█▌        | 31/200 [00:40<03:30,  1.24s/it]Running loglikelihood requests:  16%|█▋        | 33/200 [00:42<03:29,  1.25s/it]Running loglikelihood requests:  18%|█▊        | 35/200 [00:45<03:26,  1.25s/it]Running loglikelihood requests:  18%|█▊        | 37/200 [00:47<03:22,  1.24s/it]Running loglikelihood requests:  20%|█▉        | 39/200 [00:50<03:20,  1.24s/it]Running loglikelihood requests:  20%|██        | 41/200 [00:52<03:22,  1.27s/it]Running loglikelihood requests:  22%|██▏       | 43/200 [00:55<03:17,  1.26s/it]Running loglikelihood requests:  22%|██▎       | 45/200 [00:57<03:12,  1.24s/it]Running loglikelihood requests:  24%|██▎       | 47/200 [01:00<03:09,  1.24s/it]Running loglikelihood requests:  24%|██▍       | 49/200 [01:02<03:05,  1.23s/it]Running loglikelihood requests:  26%|██▌       | 51/200 [01:05<03:02,  1.22s/it]Running loglikelihood requests:  26%|██▋       | 53/200 [01:07<02:59,  1.22s/it]Running loglikelihood requests:  28%|██▊       | 55/200 [01:09<02:56,  1.22s/it]Running loglikelihood requests:  28%|██▊       | 57/200 [01:12<02:54,  1.22s/it]Running loglikelihood requests:  30%|██▉       | 59/200 [01:14<02:51,  1.22s/it]Running loglikelihood requests:  30%|███       | 61/200 [01:17<02:49,  1.22s/it]Running loglikelihood requests:  32%|███▏      | 63/200 [01:19<02:48,  1.23s/it]Running loglikelihood requests:  32%|███▎      | 65/200 [01:22<02:45,  1.22s/it]Running loglikelihood requests:  34%|███▎      | 67/200 [01:24<02:42,  1.22s/it]Running loglikelihood requests:  34%|███▍      | 69/200 [01:27<02:39,  1.22s/it]Running loglikelihood requests:  36%|███▌      | 71/200 [01:29<02:36,  1.22s/it]Running loglikelihood requests:  36%|███▋      | 73/200 [01:31<02:34,  1.21s/it]Running loglikelihood requests:  38%|███▊      | 75/200 [01:34<02:31,  1.21s/it]Running loglikelihood requests:  38%|███▊      | 77/200 [01:36<02:29,  1.21s/it]Running loglikelihood requests:  40%|███▉      | 79/200 [01:39<02:27,  1.22s/it]Running loglikelihood requests:  40%|████      | 81/200 [01:41<02:24,  1.21s/it]Running loglikelihood requests:  42%|████▏     | 83/200 [01:44<02:20,  1.20s/it]Running loglikelihood requests:  42%|████▎     | 85/200 [01:46<02:18,  1.20s/it]Running loglikelihood requests:  44%|████▎     | 87/200 [01:48<02:15,  1.20s/it]Running loglikelihood requests:  44%|████▍     | 89/200 [01:51<02:12,  1.20s/it]Running loglikelihood requests:  46%|████▌     | 91/200 [01:53<02:10,  1.20s/it]Running loglikelihood requests:  46%|████▋     | 93/200 [01:55<02:07,  1.19s/it]Running loglikelihood requests:  48%|████▊     | 95/200 [01:58<02:05,  1.19s/it]Running loglikelihood requests:  48%|████▊     | 97/200 [02:00<02:02,  1.19s/it]Running loglikelihood requests:  50%|████▉     | 99/200 [02:03<02:00,  1.19s/it]Running loglikelihood requests:  50%|█████     | 101/200 [02:05<01:58,  1.20s/it]Running loglikelihood requests:  52%|█████▏    | 103/200 [02:07<01:56,  1.20s/it]Running loglikelihood requests:  52%|█████▎    | 105/200 [02:10<01:53,  1.19s/it]Running loglikelihood requests:  54%|█████▎    | 107/200 [02:12<01:50,  1.19s/it]Running loglikelihood requests:  55%|█████▍    | 109/200 [02:14<01:47,  1.18s/it]Running loglikelihood requests:  56%|█████▌    | 111/200 [02:17<01:45,  1.18s/it]Running loglikelihood requests:  56%|█████▋    | 113/200 [02:19<01:42,  1.18s/it]Running loglikelihood requests:  57%|█████▊    | 115/200 [02:21<01:39,  1.18s/it]Running loglikelihood requests:  58%|█████▊    | 117/200 [02:24<01:37,  1.17s/it]Running loglikelihood requests:  60%|█████▉    | 119/200 [02:26<01:34,  1.17s/it]Running loglikelihood requests:  60%|██████    | 121/200 [02:28<01:32,  1.17s/it]Running loglikelihood requests:  62%|██████▏   | 123/200 [02:31<01:30,  1.17s/it]Running loglikelihood requests:  62%|██████▎   | 125/200 [02:33<01:27,  1.17s/it]Running loglikelihood requests:  64%|██████▎   | 127/200 [02:35<01:25,  1.17s/it]Running loglikelihood requests:  64%|██████▍   | 129/200 [02:38<01:22,  1.17s/it]Running loglikelihood requests:  66%|██████▌   | 131/200 [02:40<01:20,  1.16s/it]Running loglikelihood requests:  66%|██████▋   | 133/200 [02:42<01:17,  1.16s/it]Running loglikelihood requests:  68%|██████▊   | 135/200 [02:44<01:07,  1.05s/it]Running loglikelihood requests:  68%|██████▊   | 137/200 [02:45<01:00,  1.04it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [02:47<00:54,  1.12it/s]Running loglikelihood requests:  70%|███████   | 141/200 [02:48<00:50,  1.18it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [02:50<00:46,  1.22it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [02:51<00:43,  1.26it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [02:53<00:40,  1.29it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [02:54<00:38,  1.32it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [02:56<00:36,  1.34it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [02:57<00:34,  1.36it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [02:59<00:32,  1.37it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [03:00<00:31,  1.38it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [03:01<00:29,  1.39it/s]Running loglikelihood requests:  80%|████████  | 161/200 [03:03<00:27,  1.41it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [03:04<00:25,  1.43it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [03:05<00:23,  1.47it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [03:07<00:22,  1.49it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [03:08<00:20,  1.51it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [03:09<00:19,  1.52it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [03:11<00:17,  1.53it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [03:12<00:16,  1.54it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [03:13<00:14,  1.55it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [03:14<00:13,  1.56it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [03:16<00:12,  1.56it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [03:17<00:10,  1.57it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [03:18<00:09,  1.57it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [03:19<00:08,  1.58it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [03:21<00:06,  1.59it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [03:22<00:05,  1.59it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [03:23<00:04,  1.59it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [03:25<00:03,  1.59it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [03:26<00:01,  1.60it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [03:27<00:00,  1.60it/s]Running loglikelihood requests: 100%|██████████| 200/200 [03:27<00:00,  1.04s/it]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:3'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:3'}
full model:
{'multirc': {'alias': 'multirc', 'acc,none': 0.54, 'acc_stderr,none': 0.05009082659620331}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
0.28954340172694804
0.7956189982678256
0.789982900191189
0.3644425339397572
0.37373995160413354
0.19965826892219962
0.3752330861186013
0.6279352732581062
0.3474029418882723
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
0.28954340172694804
0.7956189982678256
0.789982900191189
0.3644425339397572
0.37373995160413354
0.19965826892219962
0.3752330861186013
0.6279352732581062
0.3474029418882723
0.6089030413152259
0.13723554564825818
0.32639298774801107
0.39554004962513223
0.17838079452737773
0.18172395498612623
0.4458149326815675
0.2599923469162762
0.44953845901781375
0.264228421159195
0.28453253935496087
0.5178234519727917
0.18549509579986273
0.3164231435223059
0.47622435667997154
0.5789910219470605
0.38414027299756903
0.6648306636715198
0.2288657593104162
0.30014630918632723
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 0, 6, 2, 4, 1]
tensor([5, 3, 7, 0, 6, 2, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 7, 4, 2, 1, 3, 6, 0]
tensor([5, 7, 4, 2, 1, 3, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 1, 3, 6, 0, 5, 7, 4]
tensor([2, 1, 3, 6, 0, 5, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 4, 1, 6, 0, 2, 3, 5]
tensor([7, 4, 1, 6, 0, 2, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 6, 3, 2, 5, 1, 7, 0]
tensor([4, 6, 3, 2, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 0, 2, 1, 4, 7, 3, 6]
tensor([5, 0, 2, 1, 4, 7, 3, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1, 1.0, 1.0, 1.0]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
done!
Normal merging for layer 2
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
done!
Cross-layer merge completed for layers 3 to 4
done!
Normal merging for layer 5
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
done!
Normal merging for layer 6
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
done!
Normal merging for layer 7
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
done!
Cross-layer merge completed for layers 8 to 25
done!
Normal merging for layer 26
tensor([0, 3])
tensor(0)
tensor([1, 2, 4, 5, 6, 7])
tensor(1)
done!
Cross-layer merge completed for layers 27 to 31
done!
all done!
Model size: 12.3238 GB
144
cuda:3
boolq
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:42<00:42, 42.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.28s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 237
DEBUG:filelock:Attempting to acquire lock 140516043398592 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140516043398592 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516043398592 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140516043398592 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_boolq_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140515989180144 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140515989180144 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140515989180144 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140515989180144 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/boolq/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of boolq from None to 0
INFO:lm_eval.api.task:Building contexts for boolq on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 2653.45it/s]
DEBUG:lm_eval.evaluator:Task: boolq; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:04<14:19,  4.32s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:06<05:50,  1.78s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:07<04:15,  1.31s/it]Running loglikelihood requests:   4%|▎         | 7/200 [00:09<03:35,  1.12s/it]Running loglikelihood requests:   4%|▍         | 9/200 [00:11<03:11,  1.00s/it]Running loglikelihood requests:   6%|▌         | 11/200 [00:12<02:54,  1.08it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:14<02:42,  1.15it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:15<02:34,  1.20it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:17<02:26,  1.25it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:18<02:20,  1.29it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:20<02:15,  1.32it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:21<02:11,  1.35it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:22<02:06,  1.38it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:24<02:03,  1.40it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:25<02:01,  1.41it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:27<01:58,  1.42it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:28<01:56,  1.43it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:29<01:54,  1.45it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:31<01:51,  1.46it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:32<01:49,  1.47it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:33<01:47,  1.48it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:35<01:45,  1.49it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:36<01:43,  1.50it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:37<01:40,  1.52it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:38<01:37,  1.54it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:40<01:35,  1.56it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:41<01:33,  1.57it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:42<01:31,  1.58it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:43<01:29,  1.60it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:45<01:27,  1.61it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:46<01:25,  1.63it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:47<01:23,  1.64it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:48<01:21,  1.65it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:49<01:19,  1.66it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:51<01:17,  1.68it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:52<01:15,  1.71it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:53<01:13,  1.73it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:54<01:11,  1.74it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:55<01:10,  1.76it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:56<01:08,  1.77it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:57<01:06,  1.78it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:58<01:05,  1.79it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:59<01:03,  1.80it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [01:01<01:02,  1.82it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [01:02<01:00,  1.83it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [01:03<00:59,  1.83it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [01:04<00:58,  1.84it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [01:05<00:56,  1.85it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [01:06<00:55,  1.85it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [01:07<00:55,  1.82it/s]Running loglikelihood requests:  50%|█████     | 101/200 [01:08<00:54,  1.83it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [01:09<00:53,  1.80it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [01:10<00:53,  1.78it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [01:12<00:52,  1.78it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [01:13<00:51,  1.77it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [01:14<00:48,  1.82it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [01:15<00:46,  1.86it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [01:16<00:45,  1.89it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [01:17<00:43,  1.91it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [01:18<00:41,  1.94it/s]Running loglikelihood requests:  60%|██████    | 121/200 [01:19<00:40,  1.97it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [01:20<00:38,  2.00it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [01:21<00:36,  2.03it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [01:22<00:35,  2.05it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [01:23<00:33,  2.09it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [01:23<00:32,  2.12it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [01:24<00:31,  2.15it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [01:25<00:29,  2.17it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [01:26<00:28,  2.19it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:27<00:27,  2.20it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:28<00:26,  2.22it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:29<00:25,  2.25it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:30<00:24,  2.28it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:31<00:23,  2.30it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:31<00:21,  2.32it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:32<00:20,  2.34it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:33<00:20,  2.35it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:34<00:19,  2.37it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:35<00:17,  2.39it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:36<00:16,  2.42it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:36<00:15,  2.44it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:37<00:15,  2.46it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:38<00:14,  2.48it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:39<00:13,  2.51it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:39<00:12,  2.53it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:40<00:11,  2.56it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:41<00:10,  2.60it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:42<00:09,  2.63it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:42<00:08,  2.67it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:43<00:07,  2.69it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:44<00:07,  2.71it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:45<00:06,  2.73it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:45<00:05,  2.77it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:46<00:04,  2.82it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:47<00:03,  2.85it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:47<00:03,  2.88it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:48<00:02,  2.90it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:49<00:01,  2.95it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:49<00:00,  3.08it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:50<00:00,  3.23it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:50<00:00,  1.81it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:4'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:4'}
full model:
{'boolq': {'alias': 'boolq', 'acc,none': 0.67, 'acc_stderr,none': 0.04725815626252609}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9782993007407264
0.4824830207482711
0.6583529368570694
0.8822475410601947
0.3059812833421515
0.7645942683746021
0.5411564675926485
0.6399758236302138
0.752913626333875
0.9170504110771489
0.8755679311709376
0.9130694495301263
0.6399088029710222
0.5907791688883935
0.8704540476128766
0.484807489124531
0.7579322019225017
0.8465026175931075
0.8104840653949666
0.671147278193032
0.7709951349967222
0.532915988335396
0.6066099270395096
0.5511989097245372
0.4671998655475952
0.6078287002452507
0.3992240879306912
0.5299030614769079
0.5709371890677749
0.9782993007407264
0.4824830207482711
0.6583529368570694
0.8822475410601947
0.3059812833421515
0.7645942683746021
0.5411564675926485
0.6399758236302138
0.752913626333875
0.9170504110771489
0.8755679311709376
0.9130694495301263
0.6399088029710222
0.5907791688883935
0.8704540476128766
0.484807489124531
0.7579322019225017
0.8465026175931075
0.8104840653949666
0.671147278193032
0.7709951349967222
0.532915988335396
0.6066099270395096
0.5511989097245372
0.4671998655475952
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 7, 0, 5, 3, 4, 1]
tensor([6, 2, 7, 0, 5, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 2, 3, 0, 6, 1, 4, 5]
tensor([7, 2, 3, 0, 6, 1, 4, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 1, 7, 0, 6, 4, 2, 3]
tensor([5, 1, 7, 0, 6, 4, 2, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 3, 5, 1, 7, 2, 6, 0]
tensor([4, 3, 5, 1, 7, 2, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 6, 7, 3, 4, 5, 0, 2]
tensor([1, 6, 7, 3, 4, 5, 0, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 4, 1, 7, 2, 5, 0]
tensor([6, 3, 4, 1, 7, 2, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
done!
Normal merging for layer 4
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
done!
Normal merging for layer 5
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 6 to 31
done!
all done!
Model size: 12.0718 GB
12
cuda:4
wic
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.08s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wic] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wic] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/super_glue/super_glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/super_glue HTTP/1.1" 307 63
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/aps/super_glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 234
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/aps/super_glue/resolve/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 307 115
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/aps/super_glue/paths-info/3de24cf8022e94f4ee4b9d55a6f539891524d646 HTTP/1.1" 200 235
DEBUG:filelock:Attempting to acquire lock 140514961683952 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140514961683952 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514961683952 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Lock 140514961683952 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_super_glue_wic_0.0.0_3de24cf8022e94f4ee4b9d55a6f539891524d646.lock
DEBUG:filelock:Attempting to acquire lock 140515983848368 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140515983848368 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646/dataset_info.json
DEBUG:filelock:Attempting to release lock 140515983848368 on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:filelock:Lock 140515983848368 released on /public/home/zouyifei001/.cache/huggingface/datasets/super_glue/wic/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wic from None to 0
INFO:lm_eval.api.task:Building contexts for wic on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1538.01it/s]
DEBUG:lm_eval.evaluator:Task: wic; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<03:49,  1.15s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:01<01:50,  1.79it/s]Running loglikelihood requests:   2%|▎         | 5/200 [00:02<01:27,  2.22it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:03<01:18,  2.47it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:03<01:12,  2.62it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:04<01:09,  2.73it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:05<01:06,  2.81it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:05<01:04,  2.87it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:06<01:02,  2.92it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:07<01:01,  2.95it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:07<01:00,  2.98it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:08<00:58,  3.00it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:09<00:58,  3.02it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:09<00:57,  3.03it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:10<00:56,  3.04it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:11<00:55,  3.05it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:11<00:54,  3.07it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:12<00:53,  3.08it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:13<00:52,  3.09it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:13<00:51,  3.11it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:14<00:51,  3.12it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:15<00:50,  3.10it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:15<00:49,  3.12it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:16<00:48,  3.14it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:16<00:47,  3.15it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:17<00:47,  3.16it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:18<00:46,  3.17it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:18<00:45,  3.18it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:19<00:44,  3.19it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:20<00:44,  3.20it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:20<00:43,  3.20it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:21<00:42,  3.20it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:21<00:42,  3.21it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:22<00:41,  3.21it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:23<00:40,  3.22it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:23<00:40,  3.22it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:24<00:39,  3.23it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:24<00:38,  3.24it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:25<00:37,  3.24it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:26<00:37,  3.25it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:26<00:36,  3.26it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:27<00:35,  3.26it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:28<00:35,  3.27it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:28<00:34,  3.27it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:29<00:33,  3.27it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:29<00:33,  3.27it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:30<00:32,  3.28it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:31<00:32,  3.28it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [00:31<00:31,  3.28it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [00:32<00:30,  3.28it/s]Running loglikelihood requests:  50%|█████     | 101/200 [00:32<00:30,  3.28it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [00:33<00:29,  3.28it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [00:34<00:28,  3.29it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [00:34<00:28,  3.30it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [00:35<00:27,  3.30it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [00:35<00:26,  3.30it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [00:36<00:26,  3.31it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [00:37<00:25,  3.31it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [00:37<00:25,  3.31it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [00:38<00:24,  3.31it/s]Running loglikelihood requests:  60%|██████    | 121/200 [00:38<00:23,  3.31it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [00:39<00:23,  3.31it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [00:40<00:22,  3.31it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [00:40<00:22,  3.31it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:41<00:21,  3.31it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [00:41<00:20,  3.32it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [00:42<00:20,  3.32it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [00:43<00:19,  3.31it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [00:43<00:18,  3.32it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [00:44<00:18,  3.33it/s]Running loglikelihood requests:  70%|███████   | 141/200 [00:44<00:17,  3.34it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [00:45<00:17,  3.34it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [00:46<00:16,  3.35it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [00:46<00:15,  3.35it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [00:47<00:15,  3.35it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [00:47<00:14,  3.35it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [00:48<00:13,  3.36it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [00:49<00:13,  3.35it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [00:49<00:12,  3.37it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [00:50<00:12,  3.38it/s]Running loglikelihood requests:  80%|████████  | 161/200 [00:50<00:11,  3.38it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [00:51<00:10,  3.38it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [00:52<00:10,  3.38it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [00:52<00:09,  3.39it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [00:53<00:09,  3.40it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [00:53<00:08,  3.42it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [00:54<00:07,  3.43it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [00:55<00:07,  3.43it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [00:55<00:06,  3.44it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [00:56<00:06,  3.45it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [00:56<00:05,  3.46it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [00:57<00:04,  3.46it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [00:57<00:04,  3.47it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [00:58<00:03,  3.48it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [00:59<00:03,  3.49it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [00:59<00:02,  3.50it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:00<00:01,  3.51it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:00<00:01,  3.52it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:01<00:00,  3.53it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:01<00:00,  3.54it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:01<00:00,  3.23it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:5'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:5'}
full model:
{'wic': {'alias': 'wic', 'acc,none': 0.47, 'acc_stderr,none': 0.05016135580465919}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7015569150227223
0.5302361010977743
0.6131123609930033
0.8131827739550247
0.5456264918897312
0.5653128506125247
0.9024119585362896
0.8122852497904204
0.9072724946141106
0.866764102055741
0.8260299199157425
0.7472915500213457
0.8874866998217976
0.7441602305581367
0.22948143665096393
0.6763976434023368
0.5909756859477309
0.6775915070630182
0.8311737665735953
0.5882947608660276
0.7888779075700829
0.9530393862783458
0.7563942945196994
0.7021129984434293
0.9133573687405422
0.8864659884483975
0.43949477197814607
0.49530015739760547
0.9835705252160515
0.7015569150227223
0.5302361010977743
0.6131123609930033
0.8131827739550247
0.5456264918897312
0.5653128506125247
0.9024119585362896
0.8122852497904204
0.9072724946141106
0.866764102055741
0.8260299199157425
0.7472915500213457
0.8874866998217976
0.7441602305581367
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[6, 5, 2, 3, 7, 0, 4, 1]
tensor([6, 5, 2, 3, 7, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 6, 1, 5, 4, 2, 3, 0]
tensor([7, 6, 1, 5, 4, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 6, 2, 4, 5, 0, 3, 1]
tensor([7, 6, 2, 4, 5, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 0, 5, 1, 1, 3, 2]
tensor([4, 0, 0, 5, 1, 1, 3, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 0, 2, 5, 3, 4, 1]
tensor([0, 1, 0, 2, 5, 3, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 0, 1, 4, 1, 2, 3, 0]
tensor([5, 0, 1, 4, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 1, 1, 3, 4, 2, 5, 0]
tensor([0, 1, 1, 3, 4, 2, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 1, 0, 2, 3, 1, 2, 3]
tensor([0, 1, 0, 2, 3, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 2 to 3
done!
Normal merging for layer 4
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 7
done!
Normal merging for layer 8
tensor([1, 2])
tensor(1)
tensor([4, 5])
tensor(4)
tensor([7])
tensor(7)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
done!
Cross-layer merge completed for layers 9 to 10
done!
Normal merging for layer 11
tensor([0, 2])
tensor(0)
tensor([1, 7])
tensor(1)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 12 to 13
done!
Normal merging for layer 14
tensor([1, 7])
tensor(1)
tensor([2, 4])
tensor(2)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
done!
Normal merging for layer 15
tensor([0, 7])
tensor(0)
tensor([1, 2])
tensor(1)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
done!
Normal merging for layer 16
tensor([0, 2])
tensor(0)
tensor([1, 5])
tensor(1)
tensor([3, 6])
tensor(3)
tensor([4, 7])
tensor(4)
done!
Cross-layer merge completed for layers 17 to 31
done!
all done!
Model size: 12.5757 GB
156
cuda:5
fda
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.33s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:None: No `generation_kwargs` specified in task config, defaulting to {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda HTTP/1.1" 200 1036
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/hazyresearch/based-fda/hazyresearch/based-fda.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda HTTP/1.1" 200 1043
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/hazyresearch/based-fda/resolve/42569d301e12fbcf8d5a69e04e892aa013e20314/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda/revision/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 1043
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda/tree/42569d301e12fbcf8d5a69e04e892aa013e20314?recursive=False&expand=False HTTP/1.1" 200 291
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/hazyresearch/based-fda/paths-info/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda/tree/42569d301e12fbcf8d5a69e04e892aa013e20314/data?recursive=False&expand=False HTTP/1.1" 200 484
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/hazyresearch/based-fda/paths-info/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/hazyresearch/based-fda/revision/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 1043
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/hazyresearch/based-fda/resolve/42569d301e12fbcf8d5a69e04e892aa013e20314/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/hazyresearch/based-fda/paths-info/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 218
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/hazyresearch/based-fda/paths-info/42569d301e12fbcf8d5a69e04e892aa013e20314 HTTP/1.1" 200 218
DEBUG:filelock:Attempting to acquire lock 140516063097776 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_hazyresearch___based-fda_default_0.0.0_42569d301e12fbcf8d5a69e04e892aa013e20314.lock
DEBUG:filelock:Lock 140516063097776 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_hazyresearch___based-fda_default_0.0.0_42569d301e12fbcf8d5a69e04e892aa013e20314.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516063097776 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_hazyresearch___based-fda_default_0.0.0_42569d301e12fbcf8d5a69e04e892aa013e20314.lock
DEBUG:filelock:Lock 140516063097776 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_hazyresearch___based-fda_default_0.0.0_42569d301e12fbcf8d5a69e04e892aa013e20314.lock
DEBUG:filelock:Attempting to acquire lock 140515998545344 on /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314_builder.lock
DEBUG:filelock:Lock 140515998545344 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314/dataset_info.json
DEBUG:filelock:Attempting to release lock 140515998545344 on /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314_builder.lock
DEBUG:filelock:Lock 140515998545344 released on /public/home/zouyifei001/.cache/huggingface/datasets/hazyresearch___based-fda/default/0.0.0/42569d301e12fbcf8d5a69e04e892aa013e20314_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
INFO:lm_eval.evaluator:fda: Using gen_kwargs: {'until': ['\n\n'], 'do_sample': False, 'temperature': 0}
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of fda from None to 0
INFO:lm_eval.api.task:Building contexts for fda on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 276304.61it/s]
DEBUG:lm_eval.evaluator:Task: fda; number of requests on this rank: 100
INFO:lm_eval.evaluator:Running generate_until requests
Running generate_until requests:   0%|          | 0/100 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/100 [00:18<30:07, 18.26s/it]Running generate_until requests:   2%|▏         | 2/100 [00:30<24:24, 14.94s/it]Running generate_until requests:   3%|▎         | 3/100 [00:43<22:15, 13.77s/it]Running generate_until requests:   4%|▍         | 4/100 [01:00<24:03, 15.04s/it]Running generate_until requests:   5%|▌         | 5/100 [01:14<23:33, 14.88s/it]Running generate_until requests:   6%|▌         | 6/100 [01:28<22:39, 14.46s/it]Running generate_until requests:   7%|▋         | 7/100 [01:45<23:53, 15.42s/it]Running generate_until requests:   8%|▊         | 8/100 [02:02<24:21, 15.88s/it]Running generate_until requests:   9%|▉         | 9/100 [02:14<22:17, 14.69s/it]Running generate_until requests:  10%|█         | 10/100 [02:25<20:21, 13.58s/it]Running generate_until requests:  11%|█         | 11/100 [02:36<18:59, 12.81s/it]Running generate_until requests:  12%|█▏        | 12/100 [02:54<20:40, 14.10s/it]Running generate_until requests:  13%|█▎        | 13/100 [03:05<19:20, 13.34s/it]Running generate_until requests:  14%|█▍        | 14/100 [03:22<20:40, 14.42s/it]Running generate_until requests:  15%|█▌        | 15/100 [03:33<19:10, 13.53s/it]Running generate_until requests:  16%|█▌        | 16/100 [03:46<18:18, 13.08s/it]Running generate_until requests:  17%|█▋        | 17/100 [03:57<17:36, 12.73s/it]Running generate_until requests:  18%|█▊        | 18/100 [04:14<19:08, 14.00s/it]Running generate_until requests:  19%|█▉        | 19/100 [04:26<17:50, 13.21s/it]Running generate_until requests:  20%|██        | 20/100 [04:43<19:02, 14.29s/it]Running generate_until requests:  21%|██        | 21/100 [05:00<19:53, 15.10s/it]Running generate_until requests:  22%|██▏       | 22/100 [05:13<19:06, 14.69s/it]Running generate_until requests:  23%|██▎       | 23/100 [05:30<19:42, 15.35s/it]Running generate_until requests:  24%|██▍       | 24/100 [05:42<18:09, 14.33s/it]Running generate_until requests:  25%|██▌       | 25/100 [05:59<18:52, 15.11s/it]Running generate_until requests:  26%|██▌       | 26/100 [06:16<19:13, 15.58s/it]Running generate_until requests:  27%|██▋       | 27/100 [06:33<19:24, 15.95s/it]Running generate_until requests:  28%|██▊       | 28/100 [06:45<17:50, 14.87s/it]Running generate_until requests:  29%|██▉       | 29/100 [07:02<18:17, 15.46s/it]Running generate_until requests:  30%|███       | 30/100 [07:15<17:12, 14.75s/it]Running generate_until requests:  31%|███       | 31/100 [07:32<17:40, 15.38s/it]Running generate_until requests:  32%|███▏      | 32/100 [07:45<16:40, 14.72s/it]Running generate_until requests:  33%|███▎      | 33/100 [08:02<17:07, 15.34s/it]Running generate_until requests:  34%|███▍      | 34/100 [08:19<17:22, 15.80s/it]Running generate_until requests:  35%|███▌      | 35/100 [08:31<16:09, 14.92s/it]Running generate_until requests:  36%|███▌      | 36/100 [08:43<14:56, 14.01s/it]Running generate_until requests:  37%|███▋      | 37/100 [08:55<13:53, 13.22s/it]Running generate_until requests:  38%|███▊      | 38/100 [09:07<13:32, 13.11s/it]Running generate_until requests:  39%|███▉      | 39/100 [09:24<14:26, 14.20s/it]Running generate_until requests:  40%|████      | 40/100 [09:37<13:39, 13.65s/it]Running generate_until requests:  41%|████      | 41/100 [09:53<14:20, 14.58s/it]Running generate_until requests:  42%|████▏     | 42/100 [10:05<13:11, 13.65s/it]Running generate_until requests:  43%|████▎     | 43/100 [10:16<12:22, 13.03s/it]Running generate_until requests:  44%|████▍     | 44/100 [10:28<11:47, 12.64s/it]Running generate_until requests:  45%|████▌     | 45/100 [10:39<11:04, 12.09s/it]Running generate_until requests:  46%|████▌     | 46/100 [10:56<12:06, 13.46s/it]Running generate_until requests:  47%|████▋     | 47/100 [11:12<12:44, 14.43s/it]Running generate_until requests:  48%|████▊     | 48/100 [11:24<11:51, 13.69s/it]Running generate_until requests:  49%|████▉     | 49/100 [11:35<10:57, 12.89s/it]Running generate_until requests:  50%|█████     | 50/100 [11:48<10:42, 12.85s/it]Running generate_until requests:  51%|█████     | 51/100 [12:00<10:13, 12.52s/it]Running generate_until requests:  52%|█████▏    | 52/100 [12:16<11:00, 13.77s/it]Running generate_until requests:  53%|█████▎    | 53/100 [12:28<10:20, 13.21s/it]Running generate_until requests:  54%|█████▍    | 54/100 [12:40<09:46, 12.75s/it]Running generate_until requests:  55%|█████▌    | 55/100 [12:57<10:25, 13.90s/it]Running generate_until requests:  56%|█████▌    | 56/100 [13:08<09:35, 13.09s/it]Running generate_until requests:  57%|█████▋    | 57/100 [13:23<09:49, 13.71s/it]Running generate_until requests:  58%|█████▊    | 58/100 [13:37<09:39, 13.81s/it]Running generate_until requests:  59%|█████▉    | 59/100 [13:54<10:03, 14.73s/it]Running generate_until requests:  60%|██████    | 60/100 [14:11<10:11, 15.29s/it]Running generate_until requests:  61%|██████    | 61/100 [14:26<10:02, 15.46s/it]Running generate_until requests:  62%|██████▏   | 62/100 [14:38<09:08, 14.43s/it]Running generate_until requests:  63%|██████▎   | 63/100 [14:50<08:17, 13.43s/it]Running generate_until requests:  64%|██████▍   | 64/100 [15:01<07:39, 12.77s/it]Running generate_until requests:  65%|██████▌   | 65/100 [15:17<08:08, 13.95s/it]Running generate_until requests:  66%|██████▌   | 66/100 [15:34<08:21, 14.75s/it]Running generate_until requests:  67%|██████▋   | 67/100 [15:51<08:26, 15.34s/it]Running generate_until requests:  68%|██████▊   | 68/100 [16:07<08:23, 15.73s/it]Running generate_until requests:  69%|██████▉   | 69/100 [16:24<08:17, 16.04s/it]Running generate_until requests:  70%|███████   | 70/100 [16:38<07:39, 15.31s/it]Running generate_until requests:  71%|███████   | 71/100 [16:54<07:35, 15.69s/it]Running generate_until requests:  72%|███████▏  | 72/100 [17:11<07:27, 15.97s/it]Running generate_until requests:  73%|███████▎  | 73/100 [17:22<06:34, 14.63s/it]Running generate_until requests:  74%|███████▍  | 74/100 [17:39<06:36, 15.23s/it]Running generate_until requests:  75%|███████▌  | 75/100 [17:51<05:55, 14.20s/it]Running generate_until requests:  76%|███████▌  | 76/100 [18:02<05:18, 13.26s/it]Running generate_until requests:  77%|███████▋  | 77/100 [18:13<04:48, 12.56s/it]Running generate_until requests:  78%|███████▊  | 78/100 [18:24<04:25, 12.07s/it]Running generate_until requests:  79%|███████▉  | 79/100 [18:40<04:40, 13.34s/it]Running generate_until requests:  80%|████████  | 80/100 [18:53<04:22, 13.14s/it]Running generate_until requests:  81%|████████  | 81/100 [19:09<04:27, 14.10s/it]Running generate_until requests:  82%|████████▏ | 82/100 [19:26<04:26, 14.79s/it]Running generate_until requests:  83%|████████▎ | 83/100 [19:39<04:02, 14.28s/it]Running generate_until requests:  84%|████████▍ | 84/100 [19:50<03:36, 13.56s/it]Running generate_until requests:  85%|████████▌ | 85/100 [20:02<03:12, 12.80s/it]Running generate_until requests:  86%|████████▌ | 86/100 [20:17<03:08, 13.45s/it]Running generate_until requests:  87%|████████▋ | 87/100 [20:28<02:47, 12.85s/it]Running generate_until requests:  88%|████████▊ | 88/100 [20:39<02:28, 12.41s/it]Running generate_until requests:  89%|████████▉ | 89/100 [20:53<02:21, 12.89s/it]Running generate_until requests:  90%|█████████ | 90/100 [21:03<02:00, 12.05s/it]Running generate_until requests:  91%|█████████ | 91/100 [21:10<01:32, 10.32s/it]Running generate_until requests:  92%|█████████▏| 92/100 [21:17<01:15,  9.42s/it]Running generate_until requests:  93%|█████████▎| 93/100 [21:27<01:07,  9.64s/it]Running generate_until requests:  94%|█████████▍| 94/100 [21:35<00:54,  9.08s/it]Running generate_until requests:  95%|█████████▌| 95/100 [21:41<00:40,  8.17s/it]Running generate_until requests:  96%|█████████▌| 96/100 [21:51<00:34,  8.73s/it]Running generate_until requests:  97%|█████████▋| 97/100 [21:55<00:22,  7.38s/it]Running generate_until requests:  98%|█████████▊| 98/100 [22:03<00:15,  7.59s/it]Running generate_until requests:  99%|█████████▉| 99/100 [22:11<00:07,  7.67s/it]Running generate_until requests: 100%|██████████| 100/100 [22:15<00:00,  6.60s/it]Running generate_until requests: 100%|██████████| 100/100 [22:15<00:00, 13.36s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:6'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:6'}
full model:
{'fda': {'alias': 'fda', 'contains,none': np.float64(0.87), 'contains_stderr,none': 'N/A'}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.21659225770002863
0.3676799923890141
0.7718276709674655
0.684178265870135
0.5285482451872934
0.273816514287302
0.7566340923804737
0.4195302615712732
0.7048918175177445
0.8024335694665047
0.2596571673087474
0.2906758673048459
0.6559956854150746
0.2288937672529506
0.2941846927212091
0.14545644431597915
0.27237013713290037
0.1954083722558289
0.191889756656264
0.3231769309584375
0.763391177198558
0.7591908569872912
0.47699609290977873
0.24147181909203896
0.6276241833035655
0.5039903698909594
0.506775271104323
0.17806387173399968
0.3609346134453604
0.21659225770002863
0.3676799923890141
0.7718276709674655
0.684178265870135
0.5285482451872934
0.273816514287302
0.7566340923804737
0.4195302615712732
0.7048918175177445
0.8024335694665047
0.2596571673087474
0.2906758673048459
0.6559956854150746
0.2288937672529506
0.2941846927212091
0.14545644431597915
0.27237013713290037
0.1954083722558289
0.191889756656264
0.3231769309584375
0.763391177198558
0.7591908569872912
0.47699609290977873
0.24147181909203896
0.6276241833035655
0.5039903698909594
0.506775271104323
0.17806387173399968
0.3609346134453604
0.21659225770002863
0.3676799923890141
0.7718276709674655
0.684178265870135
0.5285482451872934
0.273816514287302
0.7566340923804737
0.4195302615712732
0.7048918175177445
0.8024335694665047
0.2596571673087474
0.2906758673048459
0.6559956854150746
0.2288937672529506
0.2941846927212091
0.14545644431597915
0.27237013713290037
Total groups 70 exceeded the threshold, stopping comparison.
The group tensor is
[6, 2, 7, 0, 5, 1, 4, 3]
tensor([6, 2, 7, 0, 5, 1, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 0, 1, 5, 2, 7, 6, 3]
tensor([4, 0, 1, 5, 2, 7, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 0, 4, 2, 6, 3, 7, 5]
tensor([1, 0, 4, 2, 6, 3, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 6, 3, 5, 1, 4, 2]
tensor([7, 0, 6, 3, 5, 1, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 0, 2, 5, 1, 1, 4]
tensor([0, 3, 0, 2, 5, 1, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 1, 4, 1, 5, 2, 0]
tensor([0, 3, 1, 4, 1, 5, 2, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([1])
tensor(1)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([7])
tensor(7)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([5])
tensor(5)
done!
Normal merging for layer 2
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([2])
tensor(2)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
done!
Normal merging for layer 3
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([6])
tensor(6)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 4 to 11
done!
Normal merging for layer 12
tensor([0, 2])
tensor(0)
tensor([5, 6])
tensor(5)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
done!
Normal merging for layer 13
tensor([0, 7])
tensor(0)
tensor([2, 4])
tensor(2)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
done!
Cross-layer merge completed for layers 14 to 31
done!
all done!
Model size: 11.8828 GB
84
cuda:6
logiqa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.23s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/EleutherAI/logiqa HTTP/1.1" 200 743
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/EleutherAI/logiqa/EleutherAI/logiqa.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): datasets-server.hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://datasets-server.hf-mirror.com:443 "GET /parquet?dataset=EleutherAI/logiqa HTTP/1.1" 302 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET / HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/EleutherAI/logiqa/EleutherAI/logiqa.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/logiqa/resolve/main/logiqa.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/logiqa/resolve/main/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/EleutherAI/logiqa/resolve/main/README.md HTTP/1.1" 200 0
DEBUG:filelock:Attempting to acquire lock 140515978320336 on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Lock 140515978320336 acquired on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Attempting to release lock 140515978320336 on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Lock 140515978320336 released on /public/home/zouyifei001/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--logiqa.lock
DEBUG:filelock:Attempting to acquire lock 140515988316352 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:filelock:Lock 140515988316352 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81/dataset_info.json
DEBUG:filelock:Attempting to release lock 140515988316352 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:filelock:Lock 140515988316352 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_EleutherAI___logiqa_logiqa_0.0.1_5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81.lock
DEBUG:filelock:Attempting to acquire lock 140515988316352 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:filelock:Lock 140515988316352 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81/dataset_info.json
DEBUG:filelock:Attempting to release lock 140515988316352 on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:filelock:Lock 140515988316352 released on /public/home/zouyifei001/.cache/huggingface/datasets/EleutherAI___logiqa/logiqa/0.0.1/5f02e925d138de34af1cba698b682982c58753f9d7e741715d6b5171f60ede81_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of logiqa from None to 0
INFO:lm_eval.api.task:Building contexts for logiqa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 3125.55it/s]
DEBUG:lm_eval.evaluator:Task: logiqa; number of requests on this rank: 400
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/400 [00:02<19:35,  2.95s/it]Running loglikelihood requests:   0%|          | 2/400 [00:05<17:11,  2.59s/it]Running loglikelihood requests:   1%|          | 3/400 [00:07<16:15,  2.46s/it]Running loglikelihood requests:   1%|          | 4/400 [00:09<15:51,  2.40s/it]Running loglikelihood requests:   1%|▏         | 5/400 [00:12<15:34,  2.37s/it]Running loglikelihood requests:   2%|▏         | 6/400 [00:14<15:12,  2.31s/it]Running loglikelihood requests:   2%|▏         | 7/400 [00:16<14:56,  2.28s/it]Running loglikelihood requests:   2%|▏         | 8/400 [00:18<14:43,  2.25s/it]Running loglikelihood requests:   2%|▏         | 9/400 [00:21<14:33,  2.23s/it]Running loglikelihood requests:   2%|▎         | 10/400 [00:23<14:22,  2.21s/it]Running loglikelihood requests:   3%|▎         | 11/400 [00:25<14:10,  2.19s/it]Running loglikelihood requests:   3%|▎         | 12/400 [00:27<13:55,  2.15s/it]Running loglikelihood requests:   3%|▎         | 13/400 [00:29<13:44,  2.13s/it]Running loglikelihood requests:   4%|▎         | 14/400 [00:31<13:37,  2.12s/it]Running loglikelihood requests:   4%|▍         | 15/400 [00:33<13:33,  2.11s/it]Running loglikelihood requests:   4%|▍         | 16/400 [00:35<13:29,  2.11s/it]Running loglikelihood requests:   4%|▍         | 17/400 [00:37<13:21,  2.09s/it]Running loglikelihood requests:   4%|▍         | 18/400 [00:39<13:15,  2.08s/it]Running loglikelihood requests:   5%|▍         | 19/400 [00:41<13:07,  2.07s/it]Running loglikelihood requests:   5%|▌         | 20/400 [00:44<13:10,  2.08s/it]Running loglikelihood requests:   5%|▌         | 21/400 [00:46<13:08,  2.08s/it]Running loglikelihood requests:   6%|▌         | 22/400 [00:48<13:06,  2.08s/it]Running loglikelihood requests:   6%|▌         | 23/400 [00:50<13:02,  2.08s/it]Running loglikelihood requests:   6%|▌         | 24/400 [00:52<12:53,  2.06s/it]Running loglikelihood requests:   6%|▋         | 25/400 [00:54<12:45,  2.04s/it]Running loglikelihood requests:   6%|▋         | 26/400 [00:56<12:37,  2.02s/it]Running loglikelihood requests:   7%|▋         | 27/400 [00:58<12:29,  2.01s/it]Running loglikelihood requests:   7%|▋         | 28/400 [01:00<12:24,  2.00s/it]Running loglikelihood requests:   7%|▋         | 29/400 [01:02<12:16,  1.99s/it]Running loglikelihood requests:   8%|▊         | 30/400 [01:04<12:11,  1.98s/it]Running loglikelihood requests:   8%|▊         | 31/400 [01:06<12:06,  1.97s/it]Running loglikelihood requests:   8%|▊         | 32/400 [01:08<12:02,  1.96s/it]Running loglikelihood requests:   8%|▊         | 33/400 [01:09<11:58,  1.96s/it]Running loglikelihood requests:   8%|▊         | 34/400 [01:11<11:54,  1.95s/it]Running loglikelihood requests:   9%|▉         | 35/400 [01:13<11:50,  1.95s/it]Running loglikelihood requests:   9%|▉         | 36/400 [01:15<11:47,  1.94s/it]Running loglikelihood requests:   9%|▉         | 37/400 [01:17<11:42,  1.94s/it]Running loglikelihood requests:  10%|▉         | 38/400 [01:19<11:37,  1.93s/it]Running loglikelihood requests:  10%|▉         | 39/400 [01:21<11:33,  1.92s/it]Running loglikelihood requests:  10%|█         | 40/400 [01:23<11:28,  1.91s/it]Running loglikelihood requests:  10%|█         | 41/400 [01:25<11:25,  1.91s/it]Running loglikelihood requests:  10%|█         | 42/400 [01:27<11:21,  1.90s/it]Running loglikelihood requests:  11%|█         | 43/400 [01:29<11:17,  1.90s/it]Running loglikelihood requests:  11%|█         | 44/400 [01:30<11:13,  1.89s/it]Running loglikelihood requests:  11%|█▏        | 45/400 [01:32<11:10,  1.89s/it]Running loglikelihood requests:  12%|█▏        | 46/400 [01:34<11:06,  1.88s/it]Running loglikelihood requests:  12%|█▏        | 47/400 [01:36<11:02,  1.88s/it]Running loglikelihood requests:  12%|█▏        | 48/400 [01:38<10:58,  1.87s/it]Running loglikelihood requests:  12%|█▏        | 49/400 [01:40<10:56,  1.87s/it]Running loglikelihood requests:  12%|█▎        | 50/400 [01:42<10:52,  1.86s/it]Running loglikelihood requests:  13%|█▎        | 51/400 [01:43<10:50,  1.86s/it]Running loglikelihood requests:  13%|█▎        | 52/400 [01:45<10:46,  1.86s/it]Running loglikelihood requests:  13%|█▎        | 53/400 [01:47<10:44,  1.86s/it]Running loglikelihood requests:  14%|█▎        | 54/400 [01:49<10:41,  1.85s/it]Running loglikelihood requests:  14%|█▍        | 55/400 [01:51<10:37,  1.85s/it]Running loglikelihood requests:  14%|█▍        | 56/400 [01:53<10:33,  1.84s/it]Running loglikelihood requests:  14%|█▍        | 57/400 [01:55<10:28,  1.83s/it]Running loglikelihood requests:  14%|█▍        | 58/400 [01:56<10:23,  1.82s/it]Running loglikelihood requests:  15%|█▍        | 59/400 [01:58<10:21,  1.82s/it]Running loglikelihood requests:  15%|█▌        | 60/400 [02:00<10:18,  1.82s/it]Running loglikelihood requests:  15%|█▌        | 61/400 [02:02<10:13,  1.81s/it]Running loglikelihood requests:  16%|█▌        | 62/400 [02:04<10:08,  1.80s/it]Running loglikelihood requests:  16%|█▌        | 63/400 [02:05<10:02,  1.79s/it]Running loglikelihood requests:  16%|█▌        | 64/400 [02:07<09:56,  1.77s/it]Running loglikelihood requests:  16%|█▋        | 65/400 [02:09<09:50,  1.76s/it]Running loglikelihood requests:  16%|█▋        | 66/400 [02:10<09:45,  1.75s/it]Running loglikelihood requests:  17%|█▋        | 67/400 [02:12<09:40,  1.74s/it]Running loglikelihood requests:  17%|█▋        | 68/400 [02:14<09:34,  1.73s/it]Running loglikelihood requests:  17%|█▋        | 69/400 [02:16<09:29,  1.72s/it]Running loglikelihood requests:  18%|█▊        | 70/400 [02:17<09:28,  1.72s/it]Running loglikelihood requests:  18%|█▊        | 71/400 [02:19<09:31,  1.74s/it]Running loglikelihood requests:  18%|█▊        | 72/400 [02:21<09:27,  1.73s/it]Running loglikelihood requests:  18%|█▊        | 73/400 [02:22<09:20,  1.71s/it]Running loglikelihood requests:  18%|█▊        | 74/400 [02:24<09:12,  1.69s/it]Running loglikelihood requests:  19%|█▉        | 75/400 [02:26<09:05,  1.68s/it]Running loglikelihood requests:  19%|█▉        | 76/400 [02:27<08:59,  1.66s/it]Running loglikelihood requests:  19%|█▉        | 77/400 [02:29<08:52,  1.65s/it]Running loglikelihood requests:  20%|█▉        | 78/400 [02:31<08:46,  1.64s/it]Running loglikelihood requests:  20%|█▉        | 79/400 [02:32<08:41,  1.62s/it]Running loglikelihood requests:  20%|██        | 80/400 [02:34<08:36,  1.62s/it]Running loglikelihood requests:  20%|██        | 81/400 [02:35<08:32,  1.61s/it]Running loglikelihood requests:  20%|██        | 82/400 [02:37<08:28,  1.60s/it]Running loglikelihood requests:  21%|██        | 83/400 [02:39<08:24,  1.59s/it]Running loglikelihood requests:  21%|██        | 84/400 [02:40<08:21,  1.59s/it]Running loglikelihood requests:  21%|██▏       | 85/400 [02:42<08:17,  1.58s/it]Running loglikelihood requests:  22%|██▏       | 86/400 [02:43<08:14,  1.58s/it]Running loglikelihood requests:  22%|██▏       | 87/400 [02:45<08:11,  1.57s/it]Running loglikelihood requests:  22%|██▏       | 88/400 [02:46<08:10,  1.57s/it]Running loglikelihood requests:  22%|██▏       | 89/400 [02:48<08:09,  1.57s/it]Running loglikelihood requests:  22%|██▎       | 90/400 [02:50<08:07,  1.57s/it]Running loglikelihood requests:  23%|██▎       | 91/400 [02:51<08:06,  1.57s/it]Running loglikelihood requests:  23%|██▎       | 92/400 [02:53<08:03,  1.57s/it]Running loglikelihood requests:  23%|██▎       | 93/400 [02:54<08:01,  1.57s/it]Running loglikelihood requests:  24%|██▎       | 94/400 [02:56<07:59,  1.57s/it]Running loglikelihood requests:  24%|██▍       | 95/400 [02:57<07:56,  1.56s/it]Running loglikelihood requests:  24%|██▍       | 96/400 [02:59<07:54,  1.56s/it]Running loglikelihood requests:  24%|██▍       | 97/400 [03:00<07:51,  1.56s/it]Running loglikelihood requests:  24%|██▍       | 98/400 [03:02<07:49,  1.55s/it]Running loglikelihood requests:  25%|██▍       | 99/400 [03:04<07:47,  1.55s/it]Running loglikelihood requests:  25%|██▌       | 100/400 [03:05<07:46,  1.55s/it]Running loglikelihood requests:  25%|██▌       | 101/400 [03:07<07:43,  1.55s/it]Running loglikelihood requests:  26%|██▌       | 102/400 [03:08<07:41,  1.55s/it]Running loglikelihood requests:  26%|██▌       | 103/400 [03:10<07:44,  1.56s/it]Running loglikelihood requests:  26%|██▌       | 104/400 [03:11<07:50,  1.59s/it]Running loglikelihood requests:  26%|██▋       | 105/400 [03:13<07:50,  1.60s/it]Running loglikelihood requests:  26%|██▋       | 106/400 [03:15<07:45,  1.58s/it]Running loglikelihood requests:  27%|██▋       | 107/400 [03:16<07:40,  1.57s/it]Running loglikelihood requests:  27%|██▋       | 108/400 [03:18<07:36,  1.56s/it]Running loglikelihood requests:  27%|██▋       | 109/400 [03:19<07:32,  1.56s/it]Running loglikelihood requests:  28%|██▊       | 110/400 [03:21<07:29,  1.55s/it]Running loglikelihood requests:  28%|██▊       | 111/400 [03:22<07:27,  1.55s/it]Running loglikelihood requests:  28%|██▊       | 112/400 [03:24<07:26,  1.55s/it]Running loglikelihood requests:  28%|██▊       | 113/400 [03:25<07:24,  1.55s/it]Running loglikelihood requests:  28%|██▊       | 114/400 [03:27<07:22,  1.55s/it]Running loglikelihood requests:  29%|██▉       | 115/400 [03:29<07:20,  1.54s/it]Running loglikelihood requests:  29%|██▉       | 116/400 [03:30<07:16,  1.54s/it]Running loglikelihood requests:  29%|██▉       | 117/400 [03:32<07:12,  1.53s/it]Running loglikelihood requests:  30%|██▉       | 118/400 [03:33<07:09,  1.52s/it]Running loglikelihood requests:  30%|██▉       | 119/400 [03:35<07:06,  1.52s/it]Running loglikelihood requests:  30%|███       | 120/400 [03:36<07:03,  1.51s/it]Running loglikelihood requests:  30%|███       | 121/400 [03:38<07:01,  1.51s/it]Running loglikelihood requests:  30%|███       | 122/400 [03:39<06:58,  1.50s/it]Running loglikelihood requests:  31%|███       | 123/400 [03:41<06:55,  1.50s/it]Running loglikelihood requests:  31%|███       | 124/400 [03:42<06:53,  1.50s/it]Running loglikelihood requests:  31%|███▏      | 125/400 [03:44<06:51,  1.49s/it]Running loglikelihood requests:  32%|███▏      | 126/400 [03:45<06:49,  1.49s/it]Running loglikelihood requests:  32%|███▏      | 127/400 [03:46<06:46,  1.49s/it]Running loglikelihood requests:  32%|███▏      | 128/400 [03:48<06:44,  1.49s/it]Running loglikelihood requests:  32%|███▏      | 129/400 [03:49<06:42,  1.48s/it]Running loglikelihood requests:  32%|███▎      | 130/400 [03:51<06:40,  1.48s/it]Running loglikelihood requests:  33%|███▎      | 131/400 [03:52<06:38,  1.48s/it]Running loglikelihood requests:  33%|███▎      | 132/400 [03:54<06:36,  1.48s/it]Running loglikelihood requests:  33%|███▎      | 133/400 [03:55<06:34,  1.48s/it]Running loglikelihood requests:  34%|███▎      | 134/400 [03:57<06:32,  1.48s/it]Running loglikelihood requests:  34%|███▍      | 135/400 [03:58<06:31,  1.48s/it]Running loglikelihood requests:  34%|███▍      | 136/400 [04:00<06:30,  1.48s/it]Running loglikelihood requests:  34%|███▍      | 137/400 [04:01<06:28,  1.48s/it]Running loglikelihood requests:  34%|███▍      | 138/400 [04:03<06:26,  1.48s/it]Running loglikelihood requests:  35%|███▍      | 139/400 [04:04<06:24,  1.47s/it]Running loglikelihood requests:  35%|███▌      | 140/400 [04:06<06:22,  1.47s/it]Running loglikelihood requests:  36%|███▌      | 142/400 [04:07<04:52,  1.13s/it]Running loglikelihood requests:  36%|███▌      | 143/400 [04:09<05:13,  1.22s/it]Running loglikelihood requests:  36%|███▌      | 144/400 [04:10<05:29,  1.29s/it]Running loglikelihood requests:  36%|███▋      | 145/400 [04:12<05:40,  1.34s/it]Running loglikelihood requests:  36%|███▋      | 146/400 [04:13<05:48,  1.37s/it]Running loglikelihood requests:  37%|███▋      | 147/400 [04:15<05:54,  1.40s/it]Running loglikelihood requests:  37%|███▋      | 148/400 [04:16<05:57,  1.42s/it]Running loglikelihood requests:  37%|███▋      | 149/400 [04:17<05:59,  1.43s/it]Running loglikelihood requests:  38%|███▊      | 150/400 [04:19<06:00,  1.44s/it]Running loglikelihood requests:  38%|███▊      | 151/400 [04:20<06:00,  1.45s/it]Running loglikelihood requests:  38%|███▊      | 152/400 [04:22<05:59,  1.45s/it]Running loglikelihood requests:  38%|███▊      | 153/400 [04:23<05:58,  1.45s/it]Running loglikelihood requests:  38%|███▊      | 154/400 [04:25<05:58,  1.46s/it]Running loglikelihood requests:  39%|███▉      | 155/400 [04:26<05:56,  1.46s/it]Running loglikelihood requests:  39%|███▉      | 156/400 [04:28<05:55,  1.46s/it]Running loglikelihood requests:  39%|███▉      | 157/400 [04:29<05:53,  1.46s/it]Running loglikelihood requests:  40%|███▉      | 158/400 [04:31<05:51,  1.45s/it]Running loglikelihood requests:  40%|███▉      | 159/400 [04:32<05:50,  1.45s/it]Running loglikelihood requests:  40%|████      | 160/400 [04:33<05:48,  1.45s/it]Running loglikelihood requests:  40%|████      | 161/400 [04:35<05:46,  1.45s/it]Running loglikelihood requests:  40%|████      | 162/400 [04:36<05:44,  1.45s/it]Running loglikelihood requests:  41%|████      | 163/400 [04:38<05:43,  1.45s/it]Running loglikelihood requests:  41%|████      | 164/400 [04:39<05:41,  1.45s/it]Running loglikelihood requests:  41%|████▏     | 165/400 [04:41<05:39,  1.45s/it]Running loglikelihood requests:  42%|████▏     | 166/400 [04:42<05:37,  1.44s/it]Running loglikelihood requests:  42%|████▏     | 167/400 [04:44<05:36,  1.44s/it]Running loglikelihood requests:  42%|████▏     | 168/400 [04:45<05:34,  1.44s/it]Running loglikelihood requests:  42%|████▏     | 169/400 [04:46<05:33,  1.44s/it]Running loglikelihood requests:  42%|████▎     | 170/400 [04:48<05:33,  1.45s/it]Running loglikelihood requests:  43%|████▎     | 171/400 [04:49<05:32,  1.45s/it]Running loglikelihood requests:  43%|████▎     | 172/400 [04:51<05:31,  1.45s/it]Running loglikelihood requests:  43%|████▎     | 173/400 [04:52<05:29,  1.45s/it]Running loglikelihood requests:  44%|████▎     | 174/400 [04:54<05:26,  1.44s/it]Running loglikelihood requests:  44%|████▍     | 175/400 [04:55<05:23,  1.44s/it]Running loglikelihood requests:  44%|████▍     | 176/400 [04:57<05:20,  1.43s/it]Running loglikelihood requests:  44%|████▍     | 177/400 [04:58<05:18,  1.43s/it]Running loglikelihood requests:  44%|████▍     | 178/400 [04:59<05:15,  1.42s/it]Running loglikelihood requests:  45%|████▍     | 179/400 [05:01<05:14,  1.42s/it]Running loglikelihood requests:  45%|████▌     | 180/400 [05:02<05:11,  1.41s/it]Running loglikelihood requests:  45%|████▌     | 181/400 [05:04<05:09,  1.41s/it]Running loglikelihood requests:  46%|████▌     | 182/400 [05:05<05:07,  1.41s/it]Running loglikelihood requests:  46%|████▌     | 183/400 [05:06<05:04,  1.40s/it]Running loglikelihood requests:  46%|████▌     | 184/400 [05:08<05:02,  1.40s/it]Running loglikelihood requests:  46%|████▋     | 185/400 [05:09<05:00,  1.40s/it]Running loglikelihood requests:  46%|████▋     | 186/400 [05:11<04:59,  1.40s/it]Running loglikelihood requests:  47%|████▋     | 187/400 [05:12<04:57,  1.40s/it]Running loglikelihood requests:  47%|████▋     | 188/400 [05:13<04:55,  1.39s/it]Running loglikelihood requests:  47%|████▋     | 189/400 [05:15<04:53,  1.39s/it]Running loglikelihood requests:  48%|████▊     | 190/400 [05:16<04:51,  1.39s/it]Running loglikelihood requests:  48%|████▊     | 191/400 [05:18<04:50,  1.39s/it]Running loglikelihood requests:  48%|████▊     | 192/400 [05:19<04:48,  1.39s/it]Running loglikelihood requests:  48%|████▊     | 193/400 [05:20<04:46,  1.38s/it]Running loglikelihood requests:  48%|████▊     | 194/400 [05:22<04:44,  1.38s/it]Running loglikelihood requests:  49%|████▉     | 195/400 [05:23<04:42,  1.38s/it]Running loglikelihood requests:  49%|████▉     | 196/400 [05:24<04:40,  1.38s/it]Running loglikelihood requests:  49%|████▉     | 197/400 [05:26<04:38,  1.37s/it]Running loglikelihood requests:  50%|████▉     | 198/400 [05:27<04:37,  1.37s/it]Running loglikelihood requests:  50%|████▉     | 199/400 [05:28<04:35,  1.37s/it]Running loglikelihood requests:  50%|█████     | 200/400 [05:30<04:33,  1.37s/it]Running loglikelihood requests:  50%|█████     | 201/400 [05:31<04:31,  1.37s/it]Running loglikelihood requests:  50%|█████     | 202/400 [05:33<04:30,  1.37s/it]Running loglikelihood requests:  51%|█████     | 203/400 [05:34<04:28,  1.36s/it]Running loglikelihood requests:  51%|█████     | 204/400 [05:35<04:27,  1.36s/it]Running loglikelihood requests:  51%|█████▏    | 205/400 [05:37<04:25,  1.36s/it]Running loglikelihood requests:  52%|█████▏    | 206/400 [05:38<04:24,  1.36s/it]Running loglikelihood requests:  52%|█████▏    | 207/400 [05:39<04:22,  1.36s/it]Running loglikelihood requests:  52%|█████▏    | 208/400 [05:41<04:21,  1.36s/it]Running loglikelihood requests:  52%|█████▏    | 209/400 [05:42<04:19,  1.36s/it]Running loglikelihood requests:  52%|█████▎    | 210/400 [05:43<04:18,  1.36s/it]Running loglikelihood requests:  53%|█████▎    | 211/400 [05:45<04:16,  1.36s/it]Running loglikelihood requests:  53%|█████▎    | 212/400 [05:46<04:15,  1.36s/it]Running loglikelihood requests:  53%|█████▎    | 213/400 [05:48<04:15,  1.36s/it]Running loglikelihood requests:  54%|█████▎    | 214/400 [05:49<04:14,  1.37s/it]Running loglikelihood requests:  54%|█████▍    | 215/400 [05:50<04:12,  1.36s/it]Running loglikelihood requests:  54%|█████▍    | 216/400 [05:52<04:11,  1.37s/it]Running loglikelihood requests:  54%|█████▍    | 217/400 [05:53<04:10,  1.37s/it]Running loglikelihood requests:  55%|█████▍    | 218/400 [05:54<04:08,  1.37s/it]Running loglikelihood requests:  55%|█████▍    | 219/400 [05:56<04:06,  1.36s/it]Running loglikelihood requests:  55%|█████▌    | 220/400 [05:57<04:05,  1.36s/it]Running loglikelihood requests:  55%|█████▌    | 221/400 [05:58<04:03,  1.36s/it]Running loglikelihood requests:  56%|█████▌    | 222/400 [06:00<04:01,  1.36s/it]Running loglikelihood requests:  56%|█████▌    | 223/400 [06:01<04:00,  1.36s/it]Running loglikelihood requests:  56%|█████▌    | 224/400 [06:03<03:59,  1.36s/it]Running loglikelihood requests:  56%|█████▋    | 225/400 [06:04<03:57,  1.36s/it]Running loglikelihood requests:  56%|█████▋    | 226/400 [06:05<03:55,  1.35s/it]Running loglikelihood requests:  57%|█████▋    | 227/400 [06:07<03:54,  1.35s/it]Running loglikelihood requests:  57%|█████▋    | 228/400 [06:08<03:52,  1.35s/it]Running loglikelihood requests:  57%|█████▋    | 229/400 [06:09<03:51,  1.35s/it]Running loglikelihood requests:  57%|█████▊    | 230/400 [06:11<03:49,  1.35s/it]Running loglikelihood requests:  58%|█████▊    | 231/400 [06:12<03:48,  1.35s/it]Running loglikelihood requests:  58%|█████▊    | 232/400 [06:13<03:48,  1.36s/it]Running loglikelihood requests:  58%|█████▊    | 233/400 [06:15<03:47,  1.36s/it]Running loglikelihood requests:  58%|█████▊    | 234/400 [06:16<03:46,  1.36s/it]Running loglikelihood requests:  59%|█████▉    | 235/400 [06:17<03:44,  1.36s/it]Running loglikelihood requests:  59%|█████▉    | 236/400 [06:19<03:42,  1.36s/it]Running loglikelihood requests:  59%|█████▉    | 237/400 [06:20<03:39,  1.35s/it]Running loglikelihood requests:  60%|█████▉    | 238/400 [06:21<03:37,  1.34s/it]Running loglikelihood requests:  60%|█████▉    | 239/400 [06:23<03:35,  1.34s/it]Running loglikelihood requests:  60%|██████    | 240/400 [06:24<03:33,  1.34s/it]Running loglikelihood requests:  60%|██████    | 241/400 [06:25<03:32,  1.34s/it]Running loglikelihood requests:  60%|██████    | 242/400 [06:27<03:30,  1.33s/it]Running loglikelihood requests:  61%|██████    | 243/400 [06:28<03:29,  1.33s/it]Running loglikelihood requests:  61%|██████    | 244/400 [06:29<03:27,  1.33s/it]Running loglikelihood requests:  61%|██████▏   | 245/400 [06:31<03:26,  1.33s/it]Running loglikelihood requests:  62%|██████▏   | 246/400 [06:32<03:24,  1.33s/it]Running loglikelihood requests:  62%|██████▏   | 247/400 [06:33<03:23,  1.33s/it]Running loglikelihood requests:  62%|██████▏   | 248/400 [06:35<03:21,  1.33s/it]Running loglikelihood requests:  62%|██████▏   | 249/400 [06:36<03:20,  1.33s/it]Running loglikelihood requests:  62%|██████▎   | 250/400 [06:37<03:18,  1.32s/it]Running loglikelihood requests:  63%|██████▎   | 251/400 [06:39<03:16,  1.32s/it]Running loglikelihood requests:  63%|██████▎   | 252/400 [06:40<03:15,  1.32s/it]Running loglikelihood requests:  63%|██████▎   | 253/400 [06:41<03:13,  1.32s/it]Running loglikelihood requests:  64%|██████▎   | 254/400 [06:43<03:12,  1.32s/it]Running loglikelihood requests:  64%|██████▍   | 255/400 [06:44<03:10,  1.32s/it]Running loglikelihood requests:  64%|██████▍   | 256/400 [06:45<03:09,  1.31s/it]Running loglikelihood requests:  64%|██████▍   | 257/400 [06:47<03:07,  1.31s/it]Running loglikelihood requests:  64%|██████▍   | 258/400 [06:48<03:06,  1.31s/it]Running loglikelihood requests:  65%|██████▍   | 259/400 [06:49<03:05,  1.31s/it]Running loglikelihood requests:  65%|██████▌   | 260/400 [06:51<03:03,  1.31s/it]Running loglikelihood requests:  65%|██████▌   | 261/400 [06:52<03:02,  1.31s/it]Running loglikelihood requests:  66%|██████▌   | 262/400 [06:53<03:00,  1.31s/it]Running loglikelihood requests:  66%|██████▌   | 263/400 [06:54<02:58,  1.30s/it]Running loglikelihood requests:  66%|██████▌   | 264/400 [06:56<02:56,  1.30s/it]Running loglikelihood requests:  66%|██████▋   | 265/400 [06:57<02:55,  1.30s/it]Running loglikelihood requests:  66%|██████▋   | 266/400 [06:58<02:53,  1.30s/it]Running loglikelihood requests:  67%|██████▋   | 267/400 [07:00<02:52,  1.30s/it]Running loglikelihood requests:  67%|██████▋   | 268/400 [07:01<02:51,  1.30s/it]Running loglikelihood requests:  67%|██████▋   | 269/400 [07:02<02:49,  1.30s/it]Running loglikelihood requests:  68%|██████▊   | 270/400 [07:03<02:48,  1.30s/it]Running loglikelihood requests:  68%|██████▊   | 271/400 [07:05<02:47,  1.30s/it]Running loglikelihood requests:  68%|██████▊   | 272/400 [07:06<02:45,  1.30s/it]Running loglikelihood requests:  68%|██████▊   | 273/400 [07:07<02:44,  1.29s/it]Running loglikelihood requests:  68%|██████▊   | 274/400 [07:09<02:42,  1.29s/it]Running loglikelihood requests:  69%|██████▉   | 275/400 [07:10<02:41,  1.29s/it]Running loglikelihood requests:  69%|██████▉   | 276/400 [07:11<02:39,  1.29s/it]Running loglikelihood requests:  69%|██████▉   | 277/400 [07:12<02:37,  1.28s/it]Running loglikelihood requests:  70%|██████▉   | 278/400 [07:14<02:36,  1.28s/it]Running loglikelihood requests:  70%|███████   | 280/400 [07:15<01:58,  1.02it/s]Running loglikelihood requests:  70%|███████   | 281/400 [07:16<02:05,  1.06s/it]Running loglikelihood requests:  70%|███████   | 282/400 [07:18<02:11,  1.11s/it]Running loglikelihood requests:  71%|███████   | 283/400 [07:19<02:14,  1.15s/it]Running loglikelihood requests:  71%|███████   | 284/400 [07:20<02:17,  1.18s/it]Running loglikelihood requests:  71%|███████▏  | 285/400 [07:21<02:18,  1.20s/it]Running loglikelihood requests:  72%|███████▏  | 286/400 [07:23<02:18,  1.22s/it]Running loglikelihood requests:  72%|███████▏  | 287/400 [07:24<02:18,  1.23s/it]Running loglikelihood requests:  72%|███████▏  | 288/400 [07:25<02:18,  1.24s/it]Running loglikelihood requests:  72%|███████▏  | 289/400 [07:26<02:17,  1.24s/it]Running loglikelihood requests:  72%|███████▎  | 290/400 [07:28<02:16,  1.24s/it]Running loglikelihood requests:  73%|███████▎  | 291/400 [07:29<02:14,  1.24s/it]Running loglikelihood requests:  73%|███████▎  | 292/400 [07:30<02:13,  1.23s/it]Running loglikelihood requests:  73%|███████▎  | 293/400 [07:31<02:12,  1.23s/it]Running loglikelihood requests:  74%|███████▎  | 294/400 [07:33<02:10,  1.23s/it]Running loglikelihood requests:  74%|███████▍  | 295/400 [07:34<02:09,  1.23s/it]Running loglikelihood requests:  74%|███████▍  | 296/400 [07:35<02:07,  1.23s/it]Running loglikelihood requests:  74%|███████▍  | 297/400 [07:36<02:06,  1.22s/it]Running loglikelihood requests:  74%|███████▍  | 298/400 [07:37<02:04,  1.22s/it]Running loglikelihood requests:  75%|███████▍  | 299/400 [07:39<02:03,  1.23s/it]Running loglikelihood requests:  75%|███████▌  | 300/400 [07:40<02:02,  1.23s/it]Running loglikelihood requests:  75%|███████▌  | 301/400 [07:41<02:01,  1.23s/it]Running loglikelihood requests:  76%|███████▌  | 302/400 [07:42<02:00,  1.23s/it]Running loglikelihood requests:  76%|███████▌  | 303/400 [07:44<01:59,  1.23s/it]Running loglikelihood requests:  76%|███████▌  | 304/400 [07:45<01:57,  1.22s/it]Running loglikelihood requests:  76%|███████▋  | 305/400 [07:46<01:55,  1.22s/it]Running loglikelihood requests:  76%|███████▋  | 306/400 [07:47<01:53,  1.21s/it]Running loglikelihood requests:  77%|███████▋  | 307/400 [07:48<01:52,  1.21s/it]Running loglikelihood requests:  77%|███████▋  | 308/400 [07:50<01:50,  1.20s/it]Running loglikelihood requests:  77%|███████▋  | 309/400 [07:51<01:49,  1.20s/it]Running loglikelihood requests:  78%|███████▊  | 310/400 [07:52<01:47,  1.20s/it]Running loglikelihood requests:  78%|███████▊  | 311/400 [07:53<01:46,  1.20s/it]Running loglikelihood requests:  78%|███████▊  | 312/400 [07:54<01:45,  1.20s/it]Running loglikelihood requests:  78%|███████▊  | 313/400 [07:56<01:43,  1.19s/it]Running loglikelihood requests:  78%|███████▊  | 314/400 [07:57<01:42,  1.19s/it]Running loglikelihood requests:  79%|███████▉  | 315/400 [07:58<01:41,  1.19s/it]Running loglikelihood requests:  79%|███████▉  | 316/400 [07:59<01:40,  1.19s/it]Running loglikelihood requests:  79%|███████▉  | 317/400 [08:00<01:38,  1.19s/it]Running loglikelihood requests:  80%|███████▉  | 318/400 [08:01<01:37,  1.19s/it]Running loglikelihood requests:  80%|███████▉  | 319/400 [08:03<01:36,  1.19s/it]Running loglikelihood requests:  80%|████████  | 320/400 [08:04<01:34,  1.18s/it]Running loglikelihood requests:  80%|████████  | 321/400 [08:05<01:33,  1.18s/it]Running loglikelihood requests:  80%|████████  | 322/400 [08:06<01:32,  1.18s/it]Running loglikelihood requests:  81%|████████  | 323/400 [08:07<01:30,  1.18s/it]Running loglikelihood requests:  81%|████████  | 324/400 [08:09<01:29,  1.18s/it]Running loglikelihood requests:  81%|████████▏ | 325/400 [08:10<01:28,  1.17s/it]Running loglikelihood requests:  82%|████████▏ | 326/400 [08:11<01:26,  1.17s/it]Running loglikelihood requests:  82%|████████▏ | 327/400 [08:12<01:25,  1.17s/it]Running loglikelihood requests:  82%|████████▏ | 328/400 [08:13<01:24,  1.17s/it]Running loglikelihood requests:  82%|████████▏ | 329/400 [08:14<01:22,  1.17s/it]Running loglikelihood requests:  82%|████████▎ | 330/400 [08:16<01:21,  1.17s/it]Running loglikelihood requests:  83%|████████▎ | 331/400 [08:17<01:20,  1.17s/it]Running loglikelihood requests:  83%|████████▎ | 332/400 [08:18<01:19,  1.17s/it]Running loglikelihood requests:  83%|████████▎ | 333/400 [08:19<01:17,  1.16s/it]Running loglikelihood requests:  84%|████████▎ | 334/400 [08:20<01:16,  1.16s/it]Running loglikelihood requests:  84%|████████▍ | 335/400 [08:21<01:15,  1.16s/it]Running loglikelihood requests:  84%|████████▍ | 336/400 [08:22<01:13,  1.16s/it]Running loglikelihood requests:  84%|████████▍ | 337/400 [08:24<01:12,  1.15s/it]Running loglikelihood requests:  84%|████████▍ | 338/400 [08:25<01:11,  1.15s/it]Running loglikelihood requests:  85%|████████▍ | 339/400 [08:26<01:10,  1.15s/it]Running loglikelihood requests:  85%|████████▌ | 340/400 [08:27<01:09,  1.15s/it]Running loglikelihood requests:  85%|████████▌ | 341/400 [08:28<01:07,  1.15s/it]Running loglikelihood requests:  86%|████████▌ | 342/400 [08:29<01:06,  1.15s/it]Running loglikelihood requests:  86%|████████▌ | 343/400 [08:30<01:05,  1.14s/it]Running loglikelihood requests:  86%|████████▌ | 344/400 [08:32<01:03,  1.14s/it]Running loglikelihood requests:  86%|████████▋ | 345/400 [08:33<01:02,  1.14s/it]Running loglikelihood requests:  86%|████████▋ | 346/400 [08:34<01:01,  1.14s/it]Running loglikelihood requests:  87%|████████▋ | 347/400 [08:35<01:00,  1.14s/it]Running loglikelihood requests:  87%|████████▋ | 348/400 [08:36<00:59,  1.14s/it]Running loglikelihood requests:  87%|████████▋ | 349/400 [08:37<00:57,  1.13s/it]Running loglikelihood requests:  88%|████████▊ | 350/400 [08:38<00:56,  1.13s/it]Running loglikelihood requests:  88%|████████▊ | 351/400 [08:40<00:55,  1.13s/it]Running loglikelihood requests:  88%|████████▊ | 352/400 [08:41<00:54,  1.13s/it]Running loglikelihood requests:  88%|████████▊ | 353/400 [08:42<00:52,  1.12s/it]Running loglikelihood requests:  88%|████████▊ | 354/400 [08:43<00:51,  1.12s/it]Running loglikelihood requests:  89%|████████▉ | 355/400 [08:44<00:50,  1.12s/it]Running loglikelihood requests:  89%|████████▉ | 356/400 [08:45<00:49,  1.12s/it]Running loglikelihood requests:  89%|████████▉ | 357/400 [08:46<00:47,  1.12s/it]Running loglikelihood requests:  90%|████████▉ | 358/400 [08:47<00:46,  1.11s/it]Running loglikelihood requests:  90%|████████▉ | 359/400 [08:48<00:45,  1.11s/it]Running loglikelihood requests:  90%|█████████ | 360/400 [08:50<00:44,  1.11s/it]Running loglikelihood requests:  90%|█████████ | 361/400 [08:51<00:43,  1.11s/it]Running loglikelihood requests:  90%|█████████ | 362/400 [08:52<00:41,  1.10s/it]Running loglikelihood requests:  91%|█████████ | 363/400 [08:53<00:40,  1.10s/it]Running loglikelihood requests:  91%|█████████ | 364/400 [08:54<00:39,  1.10s/it]Running loglikelihood requests:  91%|█████████▏| 365/400 [08:55<00:38,  1.10s/it]Running loglikelihood requests:  92%|█████████▏| 366/400 [08:56<00:37,  1.10s/it]Running loglikelihood requests:  92%|█████████▏| 367/400 [08:57<00:36,  1.10s/it]Running loglikelihood requests:  92%|█████████▏| 368/400 [08:58<00:35,  1.10s/it]Running loglikelihood requests:  92%|█████████▏| 369/400 [08:59<00:33,  1.09s/it]Running loglikelihood requests:  92%|█████████▎| 370/400 [09:00<00:32,  1.09s/it]Running loglikelihood requests:  93%|█████████▎| 371/400 [09:02<00:31,  1.09s/it]Running loglikelihood requests:  93%|█████████▎| 372/400 [09:03<00:30,  1.09s/it]Running loglikelihood requests:  93%|█████████▎| 373/400 [09:04<00:29,  1.09s/it]Running loglikelihood requests:  94%|█████████▎| 374/400 [09:05<00:28,  1.08s/it]Running loglikelihood requests:  94%|█████████▍| 375/400 [09:06<00:27,  1.08s/it]Running loglikelihood requests:  94%|█████████▍| 376/400 [09:07<00:25,  1.08s/it]Running loglikelihood requests:  94%|█████████▍| 377/400 [09:08<00:24,  1.07s/it]Running loglikelihood requests:  94%|█████████▍| 378/400 [09:09<00:23,  1.07s/it]Running loglikelihood requests:  95%|█████████▍| 379/400 [09:10<00:22,  1.06s/it]Running loglikelihood requests:  95%|█████████▌| 380/400 [09:11<00:20,  1.05s/it]Running loglikelihood requests:  95%|█████████▌| 381/400 [09:12<00:19,  1.04s/it]Running loglikelihood requests:  96%|█████████▌| 382/400 [09:13<00:18,  1.04s/it]Running loglikelihood requests:  96%|█████████▌| 383/400 [09:14<00:17,  1.03s/it]Running loglikelihood requests:  96%|█████████▌| 384/400 [09:15<00:16,  1.03s/it]Running loglikelihood requests:  96%|█████████▋| 385/400 [09:16<00:15,  1.02s/it]Running loglikelihood requests:  96%|█████████▋| 386/400 [09:17<00:14,  1.02s/it]Running loglikelihood requests:  97%|█████████▋| 387/400 [09:18<00:13,  1.02s/it]Running loglikelihood requests:  97%|█████████▋| 389/400 [09:19<00:08,  1.29it/s]Running loglikelihood requests:  98%|█████████▊| 390/400 [09:20<00:08,  1.22it/s]Running loglikelihood requests:  98%|█████████▊| 391/400 [09:21<00:07,  1.18it/s]Running loglikelihood requests:  98%|█████████▊| 393/400 [09:22<00:04,  1.49it/s]Running loglikelihood requests:  98%|█████████▊| 394/400 [09:23<00:04,  1.40it/s]Running loglikelihood requests:  99%|█████████▉| 395/400 [09:24<00:03,  1.33it/s]Running loglikelihood requests:  99%|█████████▉| 396/400 [09:25<00:03,  1.29it/s]Running loglikelihood requests:  99%|█████████▉| 397/400 [09:25<00:02,  1.25it/s]Running loglikelihood requests: 100%|█████████▉| 398/400 [09:26<00:01,  1.23it/s]Running loglikelihood requests: 100%|█████████▉| 399/400 [09:27<00:00,  1.22it/s]Running loglikelihood requests: 100%|██████████| 400/400 [09:28<00:00,  1.21it/s]Running loglikelihood requests: 100%|██████████| 400/400 [09:28<00:00,  1.42s/it]
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
DEBUG:lm_eval.tasks:File _evalita-mp_ner_adg.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_fic.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
DEBUG:lm_eval.tasks:File _evalita-mp_ner_wn.yaml in /public/home/zouyifei001/project/mo-e_-merge_and_-update_-mec/lm_eval/tasks/evalita_llm could not be loaded
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:lm_eval.models.huggingface:Using device 'cuda:7'
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:7'}
full model:
{'logiqa': {'alias': 'logiqa', 'acc,none': 0.29, 'acc_stderr,none': 0.045604802157206865, 'acc_norm,none': 0.33, 'acc_norm_stderr,none': 0.04725815626252609}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9179964803140478
0.7817057225882229
0.8413553272072316
0.9274797474668193
0.8768807463293081
0.9494139907523571
0.8960692461846443
0.9131107283061946
0.6329173647892901
0.8375042173336539
0.8817471801904351
0.8172295355829869
0.7824572665005357
0.9227400642857845
0.9246594853497696
0.8075911590072223
0.6900210787422486
0.599615993999193
0.9308030044211123
0.9504015361511146
0.8866807231108503
0.540104242930401
0.6701728801805507
0.9744992661822648
0.8193037468812308
0.840784693447352
0.9052511591891966
Total groups 70 exceeded the threshold, stopping comparison.
The group tensor is
[3, 6, 7, 1, 5, 2, 4, 0]
tensor([3, 6, 7, 1, 5, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 6, 3, 7, 1, 5, 0]
tensor([4, 2, 6, 3, 7, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 6, 1, 7, 2, 4, 0]
tensor([5, 3, 6, 1, 7, 2, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 1, 7, 2, 3, 0]
tensor([5, 4, 6, 1, 7, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 3, 0, 2, 1, 0, 4, 1]
tensor([5, 3, 0, 2, 1, 0, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 5, 0, 1, 2, 3, 1]
tensor([4, 0, 5, 0, 1, 2, 3, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([3])
tensor(3)
tensor([0])
tensor(0)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 2
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([1])
tensor(1)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
done!
Cross-layer merge completed for layers 4 to 8
done!
Normal merging for layer 9
tensor([2, 5])
tensor(2)
tensor([4, 7])
tensor(4)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 10 to 13
done!
Normal merging for layer 14
tensor([1, 3])
tensor(1)
tensor([4, 7])
tensor(4)
tensor([5])
tensor(5)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
tensor([2])
tensor(2)
done!
Cross-layer merge completed for layers 15 to 31
done!
all done!
Model size: 11.9458 GB
6
cuda:7
piqa
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:41<00:41, 41.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 24.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.20s/it]
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/baber/piqa HTTP/1.1" 200 388
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/baber/piqa/baber/piqa.py HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/baber/piqa HTTP/1.1" 200 388
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/baber/piqa/resolve/142f6d7367fd9877f0fb3b5734ea6a545f54cdd1/README.md HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/baber/piqa/revision/142f6d7367fd9877f0fb3b5734ea6a545f54cdd1 HTTP/1.1" 200 388
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/baber/piqa/tree/142f6d7367fd9877f0fb3b5734ea6a545f54cdd1?recursive=False&expand=False HTTP/1.1" 200 513
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/baber/piqa/tree/142f6d7367fd9877f0fb3b5734ea6a545f54cdd1/data?recursive=False&expand=False HTTP/1.1" 404 79
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/baber/piqa/tree/142f6d7367fd9877f0fb3b5734ea6a545f54cdd1/data?recursive=False&expand=False HTTP/1.1" 404 79
DEBUG:urllib3.connectionpool:Resetting dropped connection: hf-mirror.com
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/baber/piqa/revision/142f6d7367fd9877f0fb3b5734ea6a545f54cdd1 HTTP/1.1" 200 388
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/baber/piqa/resolve/142f6d7367fd9877f0fb3b5734ea6a545f54cdd1/dataset_infos.json HTTP/1.1" 404 0
DEBUG:filelock:Attempting to acquire lock 140516025433840 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_baber___piqa_default_0.0.0_142f6d7367fd9877f0fb3b5734ea6a545f54cdd1.lock
DEBUG:filelock:Lock 140516025433840 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_baber___piqa_default_0.0.0_142f6d7367fd9877f0fb3b5734ea6a545f54cdd1.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/baber___piqa/default/0.0.0/142f6d7367fd9877f0fb3b5734ea6a545f54cdd1/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516025433840 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_baber___piqa_default_0.0.0_142f6d7367fd9877f0fb3b5734ea6a545f54cdd1.lock
DEBUG:filelock:Lock 140516025433840 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_baber___piqa_default_0.0.0_142f6d7367fd9877f0fb3b5734ea6a545f54cdd1.lock
DEBUG:filelock:Attempting to acquire lock 140516032693312 on /public/home/zouyifei001/.cache/huggingface/datasets/baber___piqa/default/0.0.0/142f6d7367fd9877f0fb3b5734ea6a545f54cdd1_builder.lock
DEBUG:filelock:Lock 140516032693312 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/baber___piqa/default/0.0.0/142f6d7367fd9877f0fb3b5734ea6a545f54cdd1_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/baber___piqa/default/0.0.0/142f6d7367fd9877f0fb3b5734ea6a545f54cdd1/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516032693312 on /public/home/zouyifei001/.cache/huggingface/datasets/baber___piqa/default/0.0.0/142f6d7367fd9877f0fb3b5734ea6a545f54cdd1_builder.lock
DEBUG:filelock:Lock 140516032693312 released on /public/home/zouyifei001/.cache/huggingface/datasets/baber___piqa/default/0.0.0/142f6d7367fd9877f0fb3b5734ea6a545f54cdd1_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of piqa from None to 0
INFO:lm_eval.api.task:Building contexts for piqa on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [00:00<00:00, 1421.85it/s]
DEBUG:lm_eval.evaluator:Task: piqa; number of requests on this rank: 200
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:01<05:42,  1.72s/it]Running loglikelihood requests:   1%|          | 2/200 [00:02<04:33,  1.38s/it]Running loglikelihood requests:   2%|▏         | 3/200 [00:03<03:46,  1.15s/it]Running loglikelihood requests:   2%|▏         | 4/200 [00:04<03:22,  1.03s/it]Running loglikelihood requests:   2%|▎         | 5/200 [00:05<03:06,  1.04it/s]Running loglikelihood requests:   3%|▎         | 6/200 [00:06<02:57,  1.10it/s]Running loglikelihood requests:   4%|▎         | 7/200 [00:07<02:48,  1.14it/s]Running loglikelihood requests:   4%|▍         | 8/200 [00:07<02:41,  1.19it/s]Running loglikelihood requests:   4%|▍         | 9/200 [00:08<02:35,  1.23it/s]Running loglikelihood requests:   5%|▌         | 10/200 [00:09<02:29,  1.27it/s]Running loglikelihood requests:   6%|▌         | 11/200 [00:10<02:24,  1.31it/s]Running loglikelihood requests:   6%|▌         | 12/200 [00:10<02:21,  1.33it/s]Running loglikelihood requests:   6%|▋         | 13/200 [00:11<02:18,  1.35it/s]Running loglikelihood requests:   7%|▋         | 14/200 [00:12<02:15,  1.37it/s]Running loglikelihood requests:   8%|▊         | 15/200 [00:12<02:13,  1.39it/s]Running loglikelihood requests:   8%|▊         | 16/200 [00:13<02:10,  1.41it/s]Running loglikelihood requests:   8%|▊         | 17/200 [00:14<02:07,  1.43it/s]Running loglikelihood requests:   9%|▉         | 18/200 [00:14<02:04,  1.46it/s]Running loglikelihood requests:  10%|▉         | 19/200 [00:15<02:02,  1.47it/s]Running loglikelihood requests:  10%|█         | 20/200 [00:16<02:00,  1.49it/s]Running loglikelihood requests:  10%|█         | 21/200 [00:16<01:58,  1.52it/s]Running loglikelihood requests:  11%|█         | 22/200 [00:17<01:56,  1.53it/s]Running loglikelihood requests:  12%|█▏        | 23/200 [00:18<01:54,  1.54it/s]Running loglikelihood requests:  12%|█▏        | 24/200 [00:18<01:53,  1.55it/s]Running loglikelihood requests:  12%|█▎        | 25/200 [00:19<01:52,  1.56it/s]Running loglikelihood requests:  13%|█▎        | 26/200 [00:19<01:50,  1.57it/s]Running loglikelihood requests:  14%|█▎        | 27/200 [00:20<01:49,  1.57it/s]Running loglikelihood requests:  14%|█▍        | 28/200 [00:21<01:49,  1.58it/s]Running loglikelihood requests:  14%|█▍        | 29/200 [00:21<01:47,  1.58it/s]Running loglikelihood requests:  15%|█▌        | 30/200 [00:22<01:46,  1.59it/s]Running loglikelihood requests:  16%|█▌        | 31/200 [00:23<01:45,  1.60it/s]Running loglikelihood requests:  16%|█▌        | 32/200 [00:23<01:44,  1.61it/s]Running loglikelihood requests:  16%|█▋        | 33/200 [00:24<01:43,  1.62it/s]Running loglikelihood requests:  17%|█▋        | 34/200 [00:24<01:42,  1.62it/s]Running loglikelihood requests:  18%|█▊        | 35/200 [00:25<01:41,  1.62it/s]Running loglikelihood requests:  18%|█▊        | 36/200 [00:26<01:40,  1.63it/s]Running loglikelihood requests:  18%|█▊        | 37/200 [00:26<01:40,  1.63it/s]Running loglikelihood requests:  19%|█▉        | 38/200 [00:27<01:39,  1.63it/s]Running loglikelihood requests:  20%|█▉        | 39/200 [00:27<01:38,  1.64it/s]Running loglikelihood requests:  20%|██        | 40/200 [00:28<01:37,  1.65it/s]Running loglikelihood requests:  20%|██        | 41/200 [00:29<01:35,  1.66it/s]Running loglikelihood requests:  21%|██        | 42/200 [00:29<01:35,  1.66it/s]Running loglikelihood requests:  22%|██▏       | 43/200 [00:30<01:34,  1.67it/s]Running loglikelihood requests:  22%|██▏       | 44/200 [00:30<01:33,  1.67it/s]Running loglikelihood requests:  22%|██▎       | 45/200 [00:31<01:32,  1.67it/s]Running loglikelihood requests:  23%|██▎       | 46/200 [00:32<01:31,  1.68it/s]Running loglikelihood requests:  24%|██▎       | 47/200 [00:32<01:30,  1.69it/s]Running loglikelihood requests:  24%|██▍       | 48/200 [00:33<01:29,  1.70it/s]Running loglikelihood requests:  24%|██▍       | 49/200 [00:33<01:28,  1.71it/s]Running loglikelihood requests:  25%|██▌       | 50/200 [00:34<01:27,  1.71it/s]Running loglikelihood requests:  26%|██▌       | 51/200 [00:35<01:26,  1.71it/s]Running loglikelihood requests:  26%|██▌       | 52/200 [00:35<01:26,  1.72it/s]Running loglikelihood requests:  26%|██▋       | 53/200 [00:36<01:25,  1.72it/s]Running loglikelihood requests:  27%|██▋       | 54/200 [00:36<01:24,  1.73it/s]Running loglikelihood requests:  28%|██▊       | 55/200 [00:37<01:23,  1.73it/s]Running loglikelihood requests:  28%|██▊       | 56/200 [00:37<01:23,  1.73it/s]Running loglikelihood requests:  28%|██▊       | 57/200 [00:38<01:22,  1.73it/s]Running loglikelihood requests:  29%|██▉       | 58/200 [00:39<01:21,  1.73it/s]Running loglikelihood requests:  30%|██▉       | 59/200 [00:39<01:21,  1.74it/s]Running loglikelihood requests:  30%|███       | 60/200 [00:40<01:20,  1.74it/s]Running loglikelihood requests:  30%|███       | 61/200 [00:40<01:19,  1.75it/s]Running loglikelihood requests:  31%|███       | 62/200 [00:41<01:18,  1.75it/s]Running loglikelihood requests:  32%|███▏      | 63/200 [00:41<01:18,  1.75it/s]Running loglikelihood requests:  32%|███▏      | 64/200 [00:42<01:17,  1.75it/s]Running loglikelihood requests:  32%|███▎      | 65/200 [00:43<01:16,  1.76it/s]Running loglikelihood requests:  33%|███▎      | 66/200 [00:43<01:16,  1.75it/s]Running loglikelihood requests:  34%|███▎      | 67/200 [00:44<01:15,  1.76it/s]Running loglikelihood requests:  34%|███▍      | 68/200 [00:44<01:15,  1.76it/s]Running loglikelihood requests:  34%|███▍      | 69/200 [00:45<01:14,  1.76it/s]Running loglikelihood requests:  35%|███▌      | 70/200 [00:45<01:13,  1.76it/s]Running loglikelihood requests:  36%|███▌      | 71/200 [00:46<01:13,  1.76it/s]Running loglikelihood requests:  36%|███▌      | 72/200 [00:47<01:12,  1.77it/s]Running loglikelihood requests:  36%|███▋      | 73/200 [00:47<01:11,  1.77it/s]Running loglikelihood requests:  37%|███▋      | 74/200 [00:48<01:11,  1.77it/s]Running loglikelihood requests:  38%|███▊      | 75/200 [00:48<01:10,  1.78it/s]Running loglikelihood requests:  38%|███▊      | 76/200 [00:49<01:09,  1.79it/s]Running loglikelihood requests:  38%|███▊      | 77/200 [00:49<01:08,  1.80it/s]Running loglikelihood requests:  39%|███▉      | 78/200 [00:50<01:07,  1.81it/s]Running loglikelihood requests:  40%|███▉      | 79/200 [00:50<01:06,  1.82it/s]Running loglikelihood requests:  40%|████      | 80/200 [00:51<01:05,  1.82it/s]Running loglikelihood requests:  40%|████      | 81/200 [00:52<01:05,  1.83it/s]Running loglikelihood requests:  41%|████      | 82/200 [00:52<01:04,  1.83it/s]Running loglikelihood requests:  42%|████▏     | 83/200 [00:53<01:03,  1.83it/s]Running loglikelihood requests:  42%|████▏     | 84/200 [00:53<01:03,  1.82it/s]Running loglikelihood requests:  42%|████▎     | 85/200 [00:54<01:02,  1.83it/s]Running loglikelihood requests:  43%|████▎     | 86/200 [00:54<01:01,  1.85it/s]Running loglikelihood requests:  44%|████▎     | 87/200 [00:55<01:00,  1.86it/s]Running loglikelihood requests:  44%|████▍     | 88/200 [00:55<01:00,  1.87it/s]Running loglikelihood requests:  44%|████▍     | 89/200 [00:56<00:59,  1.87it/s]Running loglikelihood requests:  45%|████▌     | 90/200 [00:56<00:58,  1.87it/s]Running loglikelihood requests:  46%|████▌     | 91/200 [00:57<00:58,  1.87it/s]Running loglikelihood requests:  46%|████▌     | 92/200 [00:57<00:57,  1.87it/s]Running loglikelihood requests:  46%|████▋     | 93/200 [00:58<00:56,  1.88it/s]Running loglikelihood requests:  47%|████▋     | 94/200 [00:58<00:56,  1.89it/s]Running loglikelihood requests:  48%|████▊     | 95/200 [00:59<00:55,  1.90it/s]Running loglikelihood requests:  48%|████▊     | 96/200 [01:00<00:54,  1.91it/s]Running loglikelihood requests:  48%|████▊     | 97/200 [01:00<00:53,  1.92it/s]Running loglikelihood requests:  49%|████▉     | 98/200 [01:01<00:53,  1.92it/s]Running loglikelihood requests:  50%|████▉     | 99/200 [01:01<00:52,  1.93it/s]Running loglikelihood requests:  50%|█████     | 100/200 [01:02<00:51,  1.94it/s]Running loglikelihood requests:  50%|█████     | 101/200 [01:02<00:51,  1.94it/s]Running loglikelihood requests:  51%|█████     | 102/200 [01:03<00:50,  1.95it/s]Running loglikelihood requests:  52%|█████▏    | 103/200 [01:03<00:49,  1.95it/s]Running loglikelihood requests:  52%|█████▏    | 104/200 [01:04<00:49,  1.96it/s]Running loglikelihood requests:  52%|█████▎    | 105/200 [01:04<00:48,  1.96it/s]Running loglikelihood requests:  53%|█████▎    | 106/200 [01:05<00:47,  1.97it/s]Running loglikelihood requests:  54%|█████▎    | 107/200 [01:05<00:47,  1.97it/s]Running loglikelihood requests:  54%|█████▍    | 108/200 [01:06<00:46,  1.98it/s]Running loglikelihood requests:  55%|█████▍    | 109/200 [01:06<00:45,  1.98it/s]Running loglikelihood requests:  55%|█████▌    | 110/200 [01:07<00:45,  1.99it/s]Running loglikelihood requests:  56%|█████▌    | 111/200 [01:07<00:44,  2.00it/s]Running loglikelihood requests:  56%|█████▌    | 112/200 [01:08<00:43,  2.01it/s]Running loglikelihood requests:  56%|█████▋    | 113/200 [01:08<00:43,  2.02it/s]Running loglikelihood requests:  57%|█████▋    | 114/200 [01:09<00:42,  2.03it/s]Running loglikelihood requests:  57%|█████▊    | 115/200 [01:09<00:41,  2.03it/s]Running loglikelihood requests:  58%|█████▊    | 116/200 [01:10<00:41,  2.04it/s]Running loglikelihood requests:  58%|█████▊    | 117/200 [01:10<00:40,  2.04it/s]Running loglikelihood requests:  59%|█████▉    | 118/200 [01:11<00:40,  2.05it/s]Running loglikelihood requests:  60%|█████▉    | 119/200 [01:11<00:39,  2.05it/s]Running loglikelihood requests:  60%|██████    | 120/200 [01:12<00:39,  2.05it/s]Running loglikelihood requests:  60%|██████    | 121/200 [01:12<00:38,  2.06it/s]Running loglikelihood requests:  61%|██████    | 122/200 [01:12<00:37,  2.06it/s]Running loglikelihood requests:  62%|██████▏   | 123/200 [01:13<00:37,  2.07it/s]Running loglikelihood requests:  62%|██████▏   | 124/200 [01:13<00:36,  2.07it/s]Running loglikelihood requests:  62%|██████▎   | 125/200 [01:14<00:37,  2.02it/s]Running loglikelihood requests:  63%|██████▎   | 126/200 [01:14<00:36,  2.00it/s]Running loglikelihood requests:  64%|██████▎   | 127/200 [01:15<00:36,  2.02it/s]Running loglikelihood requests:  64%|██████▍   | 128/200 [01:15<00:35,  2.03it/s]Running loglikelihood requests:  64%|██████▍   | 129/200 [01:16<00:34,  2.05it/s]Running loglikelihood requests:  65%|██████▌   | 130/200 [01:16<00:33,  2.06it/s]Running loglikelihood requests:  66%|██████▌   | 131/200 [01:17<00:33,  2.07it/s]Running loglikelihood requests:  66%|██████▌   | 132/200 [01:17<00:32,  2.08it/s]Running loglikelihood requests:  66%|██████▋   | 133/200 [01:18<00:31,  2.09it/s]Running loglikelihood requests:  67%|██████▋   | 134/200 [01:18<00:31,  2.10it/s]Running loglikelihood requests:  68%|██████▊   | 135/200 [01:19<00:30,  2.10it/s]Running loglikelihood requests:  68%|██████▊   | 136/200 [01:19<00:30,  2.10it/s]Running loglikelihood requests:  68%|██████▊   | 137/200 [01:20<00:29,  2.11it/s]Running loglikelihood requests:  69%|██████▉   | 138/200 [01:20<00:29,  2.12it/s]Running loglikelihood requests:  70%|██████▉   | 139/200 [01:21<00:28,  2.12it/s]Running loglikelihood requests:  70%|███████   | 140/200 [01:21<00:28,  2.12it/s]Running loglikelihood requests:  70%|███████   | 141/200 [01:22<00:27,  2.12it/s]Running loglikelihood requests:  71%|███████   | 142/200 [01:22<00:27,  2.12it/s]Running loglikelihood requests:  72%|███████▏  | 143/200 [01:23<00:26,  2.12it/s]Running loglikelihood requests:  72%|███████▏  | 144/200 [01:23<00:26,  2.12it/s]Running loglikelihood requests:  72%|███████▎  | 145/200 [01:23<00:25,  2.12it/s]Running loglikelihood requests:  73%|███████▎  | 146/200 [01:24<00:25,  2.13it/s]Running loglikelihood requests:  74%|███████▎  | 147/200 [01:24<00:24,  2.14it/s]Running loglikelihood requests:  74%|███████▍  | 148/200 [01:25<00:24,  2.14it/s]Running loglikelihood requests:  74%|███████▍  | 149/200 [01:25<00:23,  2.15it/s]Running loglikelihood requests:  75%|███████▌  | 150/200 [01:26<00:23,  2.15it/s]Running loglikelihood requests:  76%|███████▌  | 151/200 [01:26<00:22,  2.16it/s]Running loglikelihood requests:  76%|███████▌  | 152/200 [01:27<00:22,  2.16it/s]Running loglikelihood requests:  76%|███████▋  | 153/200 [01:27<00:21,  2.17it/s]Running loglikelihood requests:  77%|███████▋  | 154/200 [01:28<00:21,  2.17it/s]Running loglikelihood requests:  78%|███████▊  | 155/200 [01:28<00:20,  2.18it/s]Running loglikelihood requests:  78%|███████▊  | 156/200 [01:29<00:20,  2.18it/s]Running loglikelihood requests:  78%|███████▊  | 157/200 [01:29<00:19,  2.19it/s]Running loglikelihood requests:  79%|███████▉  | 158/200 [01:29<00:19,  2.18it/s]Running loglikelihood requests:  80%|███████▉  | 159/200 [01:30<00:18,  2.19it/s]Running loglikelihood requests:  80%|████████  | 160/200 [01:30<00:18,  2.19it/s]Running loglikelihood requests:  80%|████████  | 161/200 [01:31<00:17,  2.19it/s]Running loglikelihood requests:  81%|████████  | 162/200 [01:31<00:17,  2.19it/s]Running loglikelihood requests:  82%|████████▏ | 163/200 [01:32<00:16,  2.19it/s]Running loglikelihood requests:  82%|████████▏ | 164/200 [01:32<00:16,  2.19it/s]Running loglikelihood requests:  82%|████████▎ | 165/200 [01:33<00:15,  2.19it/s]Running loglikelihood requests:  83%|████████▎ | 166/200 [01:33<00:15,  2.19it/s]Running loglikelihood requests:  84%|████████▎ | 167/200 [01:34<00:15,  2.19it/s]Running loglikelihood requests:  84%|████████▍ | 168/200 [01:34<00:14,  2.20it/s]Running loglikelihood requests:  84%|████████▍ | 169/200 [01:34<00:14,  2.20it/s]Running loglikelihood requests:  85%|████████▌ | 170/200 [01:35<00:13,  2.20it/s]Running loglikelihood requests:  86%|████████▌ | 171/200 [01:35<00:13,  2.21it/s]Running loglikelihood requests:  86%|████████▌ | 172/200 [01:36<00:12,  2.21it/s]Running loglikelihood requests:  86%|████████▋ | 173/200 [01:36<00:12,  2.21it/s]Running loglikelihood requests:  87%|████████▋ | 174/200 [01:37<00:11,  2.21it/s]Running loglikelihood requests:  88%|████████▊ | 175/200 [01:37<00:11,  2.21it/s]Running loglikelihood requests:  88%|████████▊ | 176/200 [01:38<00:10,  2.22it/s]Running loglikelihood requests:  88%|████████▊ | 177/200 [01:38<00:10,  2.23it/s]Running loglikelihood requests:  89%|████████▉ | 178/200 [01:39<00:09,  2.23it/s]Running loglikelihood requests:  90%|████████▉ | 179/200 [01:39<00:09,  2.24it/s]Running loglikelihood requests:  90%|█████████ | 180/200 [01:39<00:08,  2.24it/s]Running loglikelihood requests:  90%|█████████ | 181/200 [01:40<00:08,  2.25it/s]Running loglikelihood requests:  91%|█████████ | 182/200 [01:40<00:07,  2.27it/s]Running loglikelihood requests:  92%|█████████▏| 183/200 [01:41<00:07,  2.28it/s]Running loglikelihood requests:  92%|█████████▏| 184/200 [01:41<00:07,  2.28it/s]Running loglikelihood requests:  92%|█████████▎| 185/200 [01:42<00:06,  2.28it/s]Running loglikelihood requests:  93%|█████████▎| 186/200 [01:42<00:06,  2.27it/s]Running loglikelihood requests:  94%|█████████▎| 187/200 [01:42<00:05,  2.28it/s]Running loglikelihood requests:  94%|█████████▍| 188/200 [01:43<00:05,  2.28it/s]Running loglikelihood requests:  94%|█████████▍| 189/200 [01:43<00:04,  2.30it/s]Running loglikelihood requests:  95%|█████████▌| 190/200 [01:44<00:04,  2.31it/s]Running loglikelihood requests:  96%|█████████▌| 191/200 [01:44<00:03,  2.32it/s]Running loglikelihood requests:  96%|█████████▌| 192/200 [01:45<00:03,  2.32it/s]Running loglikelihood requests:  96%|█████████▋| 193/200 [01:45<00:02,  2.34it/s]Running loglikelihood requests:  97%|█████████▋| 194/200 [01:45<00:02,  2.38it/s]Running loglikelihood requests:  98%|█████████▊| 195/200 [01:46<00:02,  2.43it/s]Running loglikelihood requests:  98%|█████████▊| 196/200 [01:46<00:01,  2.47it/s]Running loglikelihood requests:  98%|█████████▊| 197/200 [01:47<00:01,  2.53it/s]Running loglikelihood requests:  99%|█████████▉| 198/200 [01:47<00:00,  2.56it/s]Running loglikelihood requests: 100%|█████████▉| 199/200 [01:47<00:00,  2.58it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:48<00:00,  2.59it/s]Running loglikelihood requests: 100%|██████████| 200/200 [01:48<00:00,  1.85it/s]
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/models/llama-moe/LLaMA-MoE-v1-3_5B-2_8/revision/main HTTP/1.1" 200 927
WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
INFO:lm_eval.models.huggingface:Model type cannot be determined. Using default model type 'causal'
WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514967117200 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514967117200 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514967117200 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514967117200 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514977775296 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514977775296 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514977775296 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514977775296 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
full model:
{'piqa': {'alias': 'piqa', 'acc,none': 0.72, 'acc_stderr,none': 0.045126085985421296, 'acc_norm,none': 0.77, 'acc_norm_stderr,none': 0.042295258468165065}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9064246834015742
0.6971166389992818
0.984178396878105
0.6423720440493765
0.5982521563321566
0.8713841941840345
0.9082138941524238
0.5158918062056659
0.5749897473491503
0.6077434510351011
0.43990863946687114
0.513037250698066
0.6008178290360138
0.6217761889117341
0.7015566479448269
0.9265609444578461
0.9509391107686297
0.916894288805361
0.6283461914591203
0.8581337818466952
0.9688813419052539
0.8840066034830325
0.9210978928752702
0.6224518401606872
0.9119076639613269
0.9079101501328887
0.6461990702766951
0.594705914504569
0.6124297016957765
0.9064246834015742
0.6971166389992818
0.984178396878105
0.6423720440493765
0.5982521563321566
0.8713841941840345
0.9082138941524238
0.5158918062056659
0.5749897473491503
0.6077434510351011
0.43990863946687114
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[6, 3, 2, 4, 1, 5, 7, 0]
tensor([6, 3, 2, 4, 1, 5, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 0, 6, 3, 4, 2, 5, 1]
tensor([7, 0, 6, 3, 4, 2, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 3, 5, 2, 4, 0, 6, 1]
tensor([7, 3, 5, 2, 4, 0, 6, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 1, 4, 3, 5, 2, 6, 0]
tensor([7, 1, 4, 3, 5, 2, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 1, 5, 4, 3, 0, 6, 2]
tensor([7, 1, 5, 4, 3, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 2, 2, 1, 3, 0]
tensor([0, 3, 1, 2, 2, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 3, 1, 2, 2, 0, 3, 1]
tensor([0, 3, 1, 2, 2, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 0, 1, 2, 3, 1, 3, 2]
tensor([0, 0, 1, 2, 3, 1, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Normal merging for layer 1
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([2])
tensor(2)
tensor([0])
tensor(0)
done!
Normal merging for layer 2
tensor([5])
tensor(5)
tensor([7])
tensor(7)
tensor([3])
tensor(3)
tensor([1])
tensor(1)
tensor([4])
tensor(4)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Normal merging for layer 3
tensor([7])
tensor(7)
tensor([1])
tensor(1)
tensor([5])
tensor(5)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([4])
tensor(4)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Normal merging for layer 4
tensor([5])
tensor(5)
tensor([1])
tensor(1)
tensor([7])
tensor(7)
tensor([4])
tensor(4)
tensor([3])
tensor(3)
tensor([2])
tensor(2)
tensor([6])
tensor(6)
tensor([0])
tensor(0)
done!
Cross-layer merge completed for layers 5 to 16
done!
Normal merging for layer 17
tensor([0, 7])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3, 4])
tensor(3)
tensor([1, 6])
tensor(1)
done!
Normal merging for layer 18
tensor([0, 5])
tensor(0)
tensor([2, 7])
tensor(2)
tensor([3, 4])
tensor(3)
tensor([1, 6])
tensor(1)
done!
Normal merging for layer 19
tensor([0, 1])
tensor(0)
tensor([2, 5])
tensor(2)
tensor([3, 7])
tensor(3)
tensor([4, 6])
tensor(4)
done!
Cross-layer merge completed for layers 20 to 31
done!
all done!
Model size: 12.3867 GB
[150] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2376.57it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:09,  2.62s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:16,  1.41s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:06<02:42,  1.18s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:59,  1.33s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:38,  1.19s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:23,  1.10s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:13,  1.03s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:04,  1.02it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<02:01,  1.03it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:53,  1.08it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:47,  1.13it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:40,  1.18it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:35,  1.22it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:31,  1.26it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:33,  1.21it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:29,  1.24it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:26,  1.26it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:23,  1.28it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:21,  1.29it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:19,  1.30it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:16,  1.31it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:18,  1.26it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:14,  1.30it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:10,  1.34it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:07,  1.37it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:05,  1.39it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<01:03,  1.41it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<01:00,  1.43it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:49<01:07,  1.26it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<01:02,  1.32it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:52<00:59,  1.37it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:53<00:56,  1.40it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:54<00:53,  1.43it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:56<00:51,  1.44it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:57<00:49,  1.48it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:58<00:49,  1.43it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:00<00:46,  1.47it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:01<00:45,  1.48it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:02<00:43,  1.51it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:04<00:41,  1.52it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:05<00:40,  1.52it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:06<00:38,  1.54it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:08<00:43,  1.30it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:10<00:39,  1.38it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:11<00:36,  1.45it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:12<00:34,  1.49it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:13<00:32,  1.53it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:14<00:30,  1.56it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:16<00:29,  1.54it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:17<00:27,  1.57it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:19<00:27,  1.49it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:20<00:25,  1.52it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:21<00:23,  1.55it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:22<00:22,  1.58it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:23<00:20,  1.59it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:25<00:19,  1.61it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:26<00:17,  1.63it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:27<00:16,  1.64it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:29<00:16,  1.55it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:30<00:14,  1.59it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:31<00:13,  1.61it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:32<00:11,  1.63it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:33<00:10,  1.65it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:34<00:09,  1.66it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:36<00:07,  1.67it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:37<00:06,  1.68it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:38<00:05,  1.69it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:40<00:04,  1.43it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:41<00:03,  1.53it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:42<00:01,  1.61it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:43<00:00,  1.66it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:43<00:00,  1.37it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-15): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (18-30): 13 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-15): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (18-30): 13 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/150.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140515982333216 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140515982333216 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140515982333216 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140515982333216 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140522209305920 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140522209305920 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140522209305920 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140522209305920 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[150] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8980271762152415
0.2639787465429745
0.5625332696290335
0.793649776762635
0.3255906663223904
0.0484338095439226
0.14923347971103265
0.5710137578745996
0.4024845012023718
0.40192324238029414
0.7421347168699821
0.33528288680973795
0.3799695309319889
0.38260979896102654
0.8288488026343434
0.5741428622514854
0.32292256805357755
0.2608862087917634
0.5815302784189874
0.6526878745506514
0.23164180573632143
0.7209107763819704
0.641400211099821
0.36378810546943974
0.5869502091248732
0.1447672816588167
0.22624580082754567
0.43042437260655503
0.5802248606876254
0.8980271762152415
0.2639787465429745
0.5625332696290335
0.793649776762635
0.3255906663223904
0.0484338095439226
0.14923347971103265
0.5710137578745996
0.4024845012023718
0.40192324238029414
0.7421347168699821
0.33528288680973795
0.3799695309319889
0.38260979896102654
0.8288488026343434
0.5741428622514854
0.32292256805357755
0.2608862087917634
0.5815302784189874
0.6526878745506514
0.23164180573632143
0.7209107763819704
0.641400211099821
0.36378810546943974
0.5869502091248732
0.1447672816588167
0.22624580082754567
0.43042437260655503
Total groups 73 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 4, 1, 0, 6, 2]
tensor([5, 3, 7, 4, 1, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 5, 1, 2, 4, 0, 7, 3]
tensor([6, 5, 1, 2, 4, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 4, 3, 1, 5, 0, 7, 2]
tensor([6, 4, 3, 1, 5, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 3, 6, 0, 4, 2, 5, 1]
tensor([7, 3, 6, 0, 4, 2, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 4, 1, 2, 0, 3, 5, 1]
tensor([0, 4, 1, 2, 0, 3, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 0, 1, 3, 1, 0, 2, 3]
tensor([2, 0, 1, 3, 1, 0, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 1, 3, 0, 2, 3, 2]
tensor([0, 1, 1, 3, 0, 2, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 1.0, 1.0, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/150.pt
[27] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2605.39it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:07,  3.03s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:36,  1.56s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:55,  1.28s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:37,  1.17s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:30,  1.13s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:20,  1.07s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:13,  1.03s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:06,  1.00it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<02:00,  1.04it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:53,  1.08it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:53,  1.06it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:45,  1.13it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:39,  1.17it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:35,  1.21it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:31,  1.24it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:28,  1.26it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:30,  1.20it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:26,  1.24it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:23,  1.26it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:20,  1.27it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:18,  1.29it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:15,  1.30it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:12,  1.33it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:12,  1.31it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:08,  1.35it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:06,  1.37it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<01:04,  1.39it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:48<01:01,  1.41it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:49<00:59,  1.43it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<00:57,  1.45it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:52<00:57,  1.40it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:53<00:56,  1.41it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:55<00:53,  1.43it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:56<00:51,  1.45it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:57<00:49,  1.48it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:59<00:48,  1.45it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:00<00:46,  1.47it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:02<00:55,  1.21it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:04<00:50,  1.27it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:05<00:46,  1.35it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:06<00:43,  1.41it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:07<00:40,  1.46it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:09<00:38,  1.49it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:10<00:36,  1.53it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:11<00:34,  1.55it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:13<00:34,  1.48it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:14<00:32,  1.52it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:15<00:30,  1.55it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:16<00:28,  1.57it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:18<00:27,  1.59it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:19<00:25,  1.61it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:20<00:24,  1.62it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:21<00:22,  1.63it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:23<00:25,  1.36it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:24<00:23,  1.43it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:26<00:20,  1.50it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:27<00:18,  1.55it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:28<00:17,  1.59it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:29<00:15,  1.61it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:30<00:14,  1.64it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:32<00:12,  1.65it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:33<00:11,  1.59it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:34<00:10,  1.62it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:35<00:09,  1.64it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:36<00:07,  1.66it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:38<00:06,  1.68it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:39<00:05,  1.69it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:40<00:04,  1.70it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:41<00:02,  1.72it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:42<00:01,  1.67it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:43<00:00,  1.72it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:43<00:00,  1.37it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-4): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-6): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-8): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-12): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-31): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-4): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-6): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-8): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-12): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-31): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/27.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140515998532192 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140515998532192 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140515998532192 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140515998532192 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514985366432 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514985366432 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514985366432 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514985366432 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[27] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9728816226148068
0.5587440604316873
0.8603958805228331
0.8559030347476725
0.9827298775692989
0.9752051704922662
0.9320451620794601
0.8534915859631292
0.8216376829910712
0.7152752058820862
0.865677149871932
0.6996438609344103
0.5450489764598446
0.9459412501293835
0.800938078166953
0.9388899316299849
0.6171018502143922
0.5700995146297211
0.9725502911927032
0.8593526631716398
0.8340908152739906
0.8867570952520308
0.8278223956249111
0.6726110030960961
0.822005113004547
0.952024055144357
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[2, 6, 4, 1, 3, 0, 7, 5]
tensor([2, 6, 4, 1, 3, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 6, 3, 1, 5, 0, 7, 2]
tensor([4, 6, 3, 1, 5, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 4, 2, 5, 0, 7, 3]
tensor([6, 1, 4, 2, 5, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 3, 7, 1, 6, 0, 5, 2]
tensor([4, 3, 7, 1, 6, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 6, 5, 1, 3, 0, 4, 2]
tensor([7, 6, 5, 1, 3, 0, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 0, 5, 4, 3, 0, 1, 2]
tensor([1, 0, 5, 4, 3, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 0, 2, 1, 3, 2]
tensor([0, 1, 3, 0, 2, 1, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/27.pt
[253] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2426.53it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<07:01,  2.99s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:29,  1.51s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:54,  1.27s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:34,  1.15s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:22,  1.07s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:14,  1.02s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:12,  1.03s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:04,  1.02it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:57,  1.07it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:50,  1.11it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:45,  1.15it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:45,  1.12it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:50,  1.06it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:41,  1.13it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:35,  1.18it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:30,  1.23it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:26,  1.26it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:23,  1.28it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:23,  1.25it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:20,  1.28it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:18,  1.29it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:16,  1.29it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:12,  1.33it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:09,  1.36it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:07,  1.38it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:06,  1.36it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<01:03,  1.40it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<01:01,  1.42it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:49<00:59,  1.44it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<00:56,  1.46it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:55,  1.47it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:53<00:54,  1.46it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:54<00:57,  1.33it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:56<00:54,  1.37it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:57<00:51,  1.42it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:58<00:49,  1.45it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:00<00:46,  1.48it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:01<00:44,  1.50it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:02<00:42,  1.52it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:04<00:45,  1.39it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:05<00:42,  1.44it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:06<00:39,  1.48it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:08<00:37,  1.51it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:09<00:35,  1.53it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:10<00:34,  1.54it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:12<00:32,  1.56it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:13<00:31,  1.56it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:15<00:39,  1.20it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:17<00:34,  1.30it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:18<00:31,  1.38it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:19<00:28,  1.44it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:20<00:25,  1.50it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:22<00:26,  1.38it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:23<00:24,  1.45it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:25<00:22,  1.44it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:26<00:20,  1.51it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:27<00:18,  1.55it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:28<00:17,  1.59it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:29<00:15,  1.61it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:31<00:14,  1.63it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:32<00:13,  1.59it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:33<00:11,  1.63it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:35<00:10,  1.55it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:36<00:09,  1.59it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:37<00:08,  1.52it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:38<00:06,  1.57it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:39<00:05,  1.62it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:41<00:04,  1.66it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:42<00:02,  1.70it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:43<00:01,  1.73it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:44<00:00,  1.77it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:44<00:00,  1.36it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/253.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140516042188768 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140516042188768 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516042188768 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140516042188768 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514987573168 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514987573168 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514987573168 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514987573168 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[253] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4084507042253521, 'acc_stderr,none': 0.058751136942575236}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8470147925890528
0.7476864669376683
0.7057458194715966
0.9259961826035564
0.9063486644110201
0.24963708122926456
0.19827660218091203
0.8342224461031358
0.674768686362321
0.8317740713616216
0.8052770539851904
0.87411142785436
0.9423251627051763
0.6325449231299249
0.5709019604280002
0.492272078021527
0.5470094105753499
0.8106788106419917
0.8007286389438932
0.8514363351777328
0.5678533410855942
0.6964663000476322
0.43791892189616227
0.24333097709743945
0.6773756911035087
0.7468917488225975
0.9536421875896308
0.6970547096788922
0.8446279043078407
0.8470147925890528
0.7476864669376683
0.7057458194715966
0.9259961826035564
0.9063486644110201
0.24963708122926456
0.19827660218091203
0.8342224461031358
0.674768686362321
0.8317740713616216
0.8052770539851904
0.87411142785436
0.9423251627051763
0.6325449231299249
0.5709019604280002
0.492272078021527
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[3, 4, 6, 5, 2, 0, 7, 1]
tensor([3, 4, 6, 5, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 5, 3, 0, 7, 1]
tensor([2, 4, 6, 5, 3, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 3, 4, 5, 2, 0, 7, 1]
tensor([6, 3, 4, 5, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 4, 7, 2, 5, 0, 6, 3]
tensor([1, 4, 7, 2, 5, 0, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[0, 3, 5, 2, 0, 1, 1, 4]
tensor([0, 3, 5, 2, 0, 1, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[4, 3, 0, 5, 2, 0, 1, 1]
tensor([4, 3, 0, 5, 2, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 5, 1, 4, 0, 1, 2]
tensor([0, 3, 5, 1, 4, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 0, 1.0, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/253.pt
[115] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2566.98it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:13,  2.65s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:17,  1.42s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:06<02:56,  1.29s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:08<02:35,  1.15s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:22,  1.07s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:13,  1.02s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:06,  1.02it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:05,  1.01it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:57,  1.06it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:49,  1.12it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:43,  1.17it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:22<01:38,  1.21it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:33,  1.25it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:25<01:29,  1.28it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:31,  1.23it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:28<01:28,  1.26it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:24,  1.29it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:22,  1.30it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:20,  1.31it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:34<01:17,  1.33it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:17,  1.30it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:17,  1.27it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:13,  1.33it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:09,  1.37it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:06,  1.40it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:03,  1.44it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:44<01:02,  1.42it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:50<01:49,  1.26s/it]Running loglikelihood requests:  40%|████      | 57/142 [00:51<01:31,  1.07s/it]Running loglikelihood requests:  42%|████▏     | 59/142 [00:52<01:18,  1.06it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:53<01:09,  1.17it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:55<01:02,  1.27it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:56<00:56,  1.35it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:58<00:58,  1.28it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:59<00:53,  1.36it/s]Running loglikelihood requests:  50%|█████     | 71/142 [01:00<00:50,  1.42it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:01<00:46,  1.47it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:03<00:44,  1.51it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:04<00:42,  1.54it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:05<00:40,  1.56it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:06<00:38,  1.59it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:08<00:39,  1.51it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:09<00:36,  1.54it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:10<00:34,  1.57it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:12<00:33,  1.59it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:13<00:32,  1.58it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:15<00:34,  1.43it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:16<00:31,  1.48it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:17<00:31,  1.41it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:19<00:29,  1.48it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:20<00:26,  1.54it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:21<00:24,  1.58it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:22<00:23,  1.60it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:23<00:21,  1.63it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:25<00:20,  1.65it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:26<00:18,  1.66it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:27<00:17,  1.68it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:28<00:16,  1.63it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:29<00:15,  1.65it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:30<00:13,  1.68it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:32<00:12,  1.70it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:33<00:11,  1.72it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:34<00:09,  1.74it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:35<00:08,  1.75it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:36<00:07,  1.76it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:37<00:06,  1.68it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:39<00:05,  1.71it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:40<00:04,  1.74it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:41<00:02,  1.77it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:42<00:01,  1.80it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:43<00:00,  1.83it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:43<00:00,  1.37it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-31): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-31): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/115.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140541531825040 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140541531825040 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140541531825040 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140541531825040 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514986147328 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514986147328 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514986147328 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514986147328 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[115] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4647887323943662, 'acc_stderr,none': 0.05961305784972239}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8184046739906345
0.7274935575095154
0.7761510346641957
0.4952640991880176
0.6026187510593423
0.9057397954213379
0.8847150465408834
0.8902479029100969
0.8909574741962474
0.9547688146913799
0.8922781384249192
0.8543488371673005
0.875959532399785
0.5999975074231996
0.5166995128054149
0.9251070768460925
0.9548013456523431
0.895916642208903
0.816962777390054
0.8352073170783986
0.8879963837403804
0.7652959230128763
0.6541271822267725
0.6185613360581901
0.9254826611316034
0.6384241255415785
0.5206869695172345
0.6788316772896046
0.8168301435765506
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[3, 2, 6, 5, 1, 0, 7, 4]
tensor([3, 2, 6, 5, 1, 0, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 3, 6, 5, 2, 0, 7, 4]
tensor([1, 3, 6, 5, 2, 0, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 1, 4, 2, 5, 0, 7, 3]
tensor([6, 1, 4, 2, 5, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 0, 7, 4, 3, 1, 6, 5]
tensor([2, 0, 7, 4, 3, 1, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 4, 1, 3, 1, 0, 5, 2]
tensor([0, 4, 1, 3, 1, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[3, 4, 0, 1, 1, 0, 2, 5]
tensor([3, 4, 0, 1, 1, 0, 2, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 3, 1, 2, 1, 2, 3]
tensor([0, 0, 3, 1, 2, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1.0, 1.0, 1, 1.0, 1.0]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/115.pt
[3] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2565.46it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:26,  2.74s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:19,  1.44s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:06<02:43,  1.19s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:08<02:27,  1.09s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:22,  1.07s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:13,  1.02s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:06,  1.02it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:15<01:59,  1.06it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:38,  1.27s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<02:24,  1.17s/it]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<02:16,  1.13s/it]Running loglikelihood requests:  16%|█▌        | 23/142 [00:25<02:01,  1.02s/it]Running loglikelihood requests:  18%|█▊        | 25/142 [00:27<01:51,  1.05it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:43,  1.12it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:30<01:43,  1.09it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:32<01:38,  1.13it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:33<01:32,  1.18it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:35<01:31,  1.17it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:36<01:27,  1.20it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:38<01:23,  1.24it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:40<01:23,  1.21it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:41<01:19,  1.25it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:43<01:14,  1.30it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:44<01:10,  1.34it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:45<01:07,  1.38it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:47<01:04,  1.42it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:48<01:01,  1.44it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:49<00:59,  1.46it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:51<00:59,  1.43it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:52<00:56,  1.46it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:53<00:54,  1.48it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:55<00:52,  1.50it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:56<00:50,  1.51it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:57<00:49,  1.52it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:59<00:47,  1.53it/s]Running loglikelihood requests:  50%|█████     | 71/142 [01:00<00:47,  1.50it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:01<00:45,  1.51it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:03<00:44,  1.52it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:04<00:42,  1.54it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:05<00:40,  1.55it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:06<00:39,  1.56it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:08<00:37,  1.56it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:09<00:36,  1.57it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:10<00:34,  1.59it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:12<00:35,  1.51it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:13<00:33,  1.53it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:14<00:31,  1.56it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:15<00:29,  1.57it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:17<00:28,  1.58it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:18<00:26,  1.60it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:19<00:25,  1.60it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:21<00:27,  1.44it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:22<00:24,  1.50it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:23<00:22,  1.55it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:24<00:20,  1.57it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:26<00:19,  1.60it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:27<00:17,  1.62it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:28<00:16,  1.64it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:29<00:15,  1.65it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:30<00:13,  1.67it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:32<00:12,  1.62it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:33<00:11,  1.64it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:34<00:10,  1.67it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:35<00:08,  1.69it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:36<00:07,  1.71it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:37<00:06,  1.72it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:39<00:05,  1.72it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:40<00:04,  1.74it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:41<00:03,  1.54it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:42<00:01,  1.61it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:44<00:00,  1.67it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:44<00:00,  1.36it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-20): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (21): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-20): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (21): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/3.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514986131632 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514986131632 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514986131632 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514986131632 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140516062316832 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140516062316832 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516062316832 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140516062316832 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[3] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8745617680355892
0.4870053329545456
0.7039635907698274
0.13683136283338845
0.45491781836309914
0.7394007339170455
0.7100810563328822
0.5089938007827006
0.6148932447522303
0.6097500975530942
0.2557945445996536
0.6585024343395449
0.3077968033302773
0.5696663704302732
0.4357302352587323
0.8508165638532892
0.2937402676601578
0.5546720889398816
0.456887590217546
0.8515148368106646
0.6351128178266827
0.587082322906938
0.4014945886309545
0.6816643170727646
0.7647343638504926
0.15578350583970074
0.7065213224778255
0.687952127174302
0.881700230921547
0.8745617680355892
0.4870053329545456
0.7039635907698274
0.13683136283338845
0.45491781836309914
0.7394007339170455
0.7100810563328822
0.5089938007827006
0.6148932447522303
0.6097500975530942
0.2557945445996536
0.6585024343395449
0.3077968033302773
0.5696663704302732
0.4357302352587323
0.8508165638532892
0.2937402676601578
0.5546720889398816
0.456887590217546
0.8515148368106646
0.6351128178266827
0.587082322906938
0.4014945886309545
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 6, 1, 0, 4, 2]
tensor([5, 3, 7, 6, 1, 0, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 7, 1, 5, 6, 0, 4, 2]
tensor([3, 7, 1, 5, 6, 0, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 3, 2, 4, 5, 1, 7, 0]
tensor([6, 3, 2, 4, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 7, 5, 1, 3, 6, 0]
tensor([2, 4, 7, 5, 1, 3, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[0, 2, 4, 6, 5, 1, 7, 3]
tensor([0, 2, 4, 6, 5, 1, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 5, 3, 6, 0, 7, 1]
tensor([4, 2, 5, 3, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/3.pt
[14] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2524.33it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:43,  3.29s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:40,  1.59s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:56,  1.29s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:37,  1.17s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:24,  1.08s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:43,  1.25s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:25,  1.13s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:22,  1.12s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:09,  1.03s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:57,  1.04it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:53,  1.06it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:25<01:58,  1.01it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:27<01:48,  1.08it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:39,  1.15it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:30<01:40,  1.12it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:32<01:35,  1.16it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:33<01:29,  1.21it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:34<01:24,  1.26it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:36<01:20,  1.30it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:37<01:17,  1.33it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:39<01:14,  1.35it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:40<01:12,  1.37it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:42<01:09,  1.39it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:43<01:12,  1.31it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:45<01:08,  1.36it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:46<01:04,  1.41it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:47<01:01,  1.45it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:48<00:58,  1.49it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:50<00:56,  1.51it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:51<00:54,  1.53it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:53<01:00,  1.34it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:54<00:56,  1.41it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:55<00:52,  1.46it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:57<00:50,  1.50it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:58<00:47,  1.52it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:59<00:45,  1.55it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:00<00:43,  1.57it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:02<00:42,  1.58it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:03<00:46,  1.40it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:05<00:42,  1.47it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:06<00:40,  1.51it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:07<00:38,  1.55it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:08<00:36,  1.58it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:10<00:34,  1.61it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:11<00:32,  1.63it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:12<00:31,  1.64it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:14<00:34,  1.41it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:15<00:31,  1.49it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:16<00:29,  1.54it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:17<00:27,  1.59it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:19<00:25,  1.62it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:20<00:23,  1.65it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:21<00:22,  1.67it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:22<00:20,  1.69it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:23<00:20,  1.62it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:25<00:21,  1.45it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:26<00:18,  1.53it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:27<00:16,  1.59it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:29<00:15,  1.63it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:30<00:13,  1.68it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:31<00:12,  1.70it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:32<00:11,  1.73it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:33<00:10,  1.66it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:34<00:08,  1.69it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:35<00:07,  1.72it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:37<00:06,  1.74it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:38<00:05,  1.76it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:39<00:03,  1.78it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:40<00:02,  1.80it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:41<00:01,  1.81it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:42<00:00,  1.82it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:42<00:00,  1.39it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-5): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-9): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-26): 16 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28-30): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-5): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-9): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-26): 16 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28-30): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/14.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514735316320 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514735316320 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514735316320 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514735316320 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140515989106528 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140515989106528 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140515989106528 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140515989106528 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[14] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7169249479288816
0.6183111372245332
0.46095560668163316
0.45412629074361927
0.5961385422474115
0.8890533054728373
0.9122749865545955
0.7722412965326397
0.28598717020359266
0.16135518374824295
0.6405733513488978
0.34881239744915987
0.02163114281968652
0.8586809120820075
0.5280864903169471
0.5549214872848108
0.8809884720847853
0.8184475126773784
0.7411818478320343
0.9204128494945557
0.8828541622108917
0.8520796846320295
0.9938665155729866
0.8532228506983649
0.8363754351071634
0.7452449391867536
0.8461045724576085
0.3693743162395131
0.3582309316878524
0.7169249479288816
0.6183111372245332
0.46095560668163316
0.45412629074361927
0.5961385422474115
0.8890533054728373
0.9122749865545955
0.7722412965326397
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[5, 4, 6, 3, 2, 0, 7, 1]
tensor([5, 4, 6, 3, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 4, 0, 5, 3]
tensor([6, 1, 7, 2, 4, 0, 5, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 3, 1, 0, 7, 5]
tensor([2, 4, 6, 3, 1, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 0, 6, 4, 3, 1, 5, 7]
tensor([2, 0, 6, 4, 3, 1, 5, 7], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[3, 1, 0, 1, 2, 0, 2, 3]
tensor([3, 1, 0, 1, 2, 0, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 2, 1, 0, 2, 3, 3]
tensor([0, 1, 2, 1, 0, 2, 3, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[3, 1, 0, 1, 2, 0, 3, 2]
tensor([3, 1, 0, 1, 2, 0, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 2, 2, 3, 0, 3, 1]
tensor([0, 1, 2, 2, 3, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[3, 0, 1, 2, 1, 2, 3, 0]
tensor([3, 0, 1, 2, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/14.pt
[82] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2612.98it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:13,  3.07s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:32,  1.53s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:56,  1.29s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:35,  1.15s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:21,  1.07s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:13,  1.02s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:05,  1.02it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:04,  1.02it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:56,  1.07it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:49,  1.12it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:44,  1.16it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:22<01:39,  1.20it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:35,  1.22it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:25<01:31,  1.26it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:30,  1.25it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:26,  1.29it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:23,  1.31it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:20,  1.33it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:18,  1.34it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:34<01:15,  1.36it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:13,  1.37it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:37<01:15,  1.31it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:11,  1.35it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:08,  1.38it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:05,  1.41it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:02,  1.44it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:44<01:00,  1.47it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<00:58,  1.49it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<00:58,  1.45it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:48<00:56,  1.48it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:53,  1.50it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:51,  1.52it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:50,  1.54it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:48,  1.55it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:46,  1.57it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:45,  1.57it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:46,  1.48it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:44,  1.51it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:42,  1.54it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:40,  1.57it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:38,  1.59it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:36,  1.60it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:35,  1.61it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:33,  1.62it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:35,  1.50it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:33,  1.54it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:31,  1.57it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:29,  1.59it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:27,  1.61it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:14<00:26,  1.62it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:25,  1.63it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:23,  1.65it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:23,  1.59it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:21,  1.62it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:20,  1.64it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:21<00:18,  1.67it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:22<00:17,  1.69it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:23<00:15,  1.70it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:24<00:14,  1.70it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:13,  1.72it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:27<00:12,  1.72it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:28<00:12,  1.54it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:29<00:10,  1.59it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:31<00:09,  1.64it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:32<00:07,  1.67it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:33<00:06,  1.70it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:34<00:05,  1.71it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:35<00:04,  1.73it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:36<00:02,  1.74it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:37<00:01,  1.76it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:39<00:00,  1.74it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:39<00:00,  1.43it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-22): 13 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-31): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-22): 13 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-31): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/82.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140515982341280 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140515982341280 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140515982341280 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140515982341280 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514985945248 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514985945248 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514985945248 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514985945248 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[82] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4647887323943662, 'acc_stderr,none': 0.05961305784972239}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8670569790937798
0.5488271932172035
0.673013746499191
0.7739554714966184
0.626122351153327
0.7480239804573661
0.8404062667282521
0.585845593784257
0.5486089643527782
0.295628094796439
0.8187255304150794
0.4183228390778105
0.5130578032019412
0.8769017151034573
0.9364820135353734
0.9916222006713932
0.9862354083632185
0.9582978800582982
0.8980649992917061
0.8669816466709701
0.9008509099634621
0.9321426254796342
0.8646919425811247
0.6505326186022521
0.6612551300691957
0.8835733690017037
0.7234157945938043
0.7342253877992871
0.5683390151095499
0.8670569790937798
0.5488271932172035
0.673013746499191
0.7739554714966184
0.626122351153327
0.7480239804573661
0.8404062667282521
0.585845593784257
0.5486089643527782
0.295628094796439
0.8187255304150794
0.4183228390778105
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 6, 2, 4, 0, 7, 1]
tensor([5, 3, 6, 2, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 4, 6, 7, 2, 0, 5, 1]
tensor([3, 4, 6, 7, 2, 0, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 6, 2, 1, 3, 0, 7, 5]
tensor([4, 6, 2, 1, 3, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 0, 1, 2, 7, 3]
tensor([5, 4, 6, 0, 1, 2, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 2, 6, 5, 4, 0, 7, 1]
tensor([3, 2, 6, 5, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 2, 5, 6, 3, 0, 7, 1]
tensor([4, 2, 5, 6, 3, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/82.pt
[103] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2585.46it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:08,  3.04s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:35,  1.55s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:53,  1.27s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:35,  1.15s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:24,  1.09s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:19,  1.07s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:12,  1.02s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:05,  1.01it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<02:01,  1.03it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:59,  1.03it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:52,  1.08it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:47,  1.11it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:41,  1.15it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:36,  1.19it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:32,  1.23it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:28,  1.25it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:25,  1.27it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:27,  1.23it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:23,  1.25it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:20,  1.28it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:18,  1.29it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:15,  1.32it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:12,  1.34it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:09,  1.36it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:44<01:21,  1.15it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:14,  1.23it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:47<01:08,  1.30it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:48<01:04,  1.35it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:49<01:00,  1.39it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:51<00:58,  1.43it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:52<00:55,  1.45it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:54<00:56,  1.41it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:55<00:53,  1.44it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:56<00:51,  1.47it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:58<00:49,  1.48it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:59<00:47,  1.50it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:00<00:45,  1.51it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:01<00:44,  1.52it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:03<00:42,  1.52it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:04<00:42,  1.48it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:05<00:40,  1.51it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:07<00:38,  1.53it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:08<00:37,  1.53it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:10<00:40,  1.37it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:11<00:36,  1.43it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:12<00:34,  1.47it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:14<00:33,  1.46it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:15<00:31,  1.48it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:16<00:29,  1.51it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:18<00:27,  1.54it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:19<00:26,  1.55it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:20<00:24,  1.57it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:21<00:23,  1.59it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:23<00:21,  1.60it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:24<00:21,  1.55it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:25<00:19,  1.57it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:26<00:18,  1.59it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:28<00:16,  1.59it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:29<00:15,  1.60it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:31<00:15,  1.47it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:32<00:13,  1.52it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:33<00:12,  1.56it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:35<00:12,  1.35it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:36<00:10,  1.44it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:37<00:09,  1.43it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:39<00:07,  1.51it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:40<00:05,  1.57it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:41<00:04,  1.62it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:42<00:02,  1.67it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:43<00:01,  1.70it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:45<00:00,  1.58it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:45<00:00,  1.35it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-10): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12-13): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-31): 15 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-10): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12-13): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-31): 15 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/103.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514428165040 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514428165040 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514428165040 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514428165040 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514751213744 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514751213744 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514751213744 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514751213744 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[103] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6384788962207051
0.4946572039289874
0.7062468225311351
0.5924118081316296
0.8055799477986278
0.4290834385090486
0.5207152243980063
0.40307410421369466
0.575849756312845
0.37833122092903004
0.559402975459772
0.9202435099718248
0.7328649512638977
0.4656521136449837
0.6226167455222542
0.6500475926241498
0.14933051029764177
0.852848973497393
0.7020336348742873
0.8036038458930006
0.25840240277743587
0.45348406473870356
0.7291236246725161
0.5999541971262276
0.7071125031923391
0.6523270915574834
0.5982105985982268
0.6520903105513685
0.39933491227192136
0.6384788962207051
0.4946572039289874
0.7062468225311351
0.5924118081316296
0.8055799477986278
0.4290834385090486
0.5207152243980063
0.40307410421369466
0.575849756312845
0.37833122092903004
0.559402975459772
0.9202435099718248
0.7328649512638977
0.4656521136449837
0.6226167455222542
0.6500475926241498
0.14933051029764177
0.852848973497393
0.7020336348742873
0.8036038458930006
0.25840240277743587
0.45348406473870356
0.7291236246725161
0.5999541971262276
0.7071125031923391
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[6, 3, 2, 1, 5, 0, 7, 4]
tensor([6, 3, 2, 1, 5, 0, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 5, 4, 2, 6, 0, 7, 1]
tensor([3, 5, 4, 2, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 7, 6, 3, 4, 1, 5, 0]
tensor([2, 7, 6, 3, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 2, 5, 4, 6, 0, 7, 1]
tensor([3, 2, 5, 4, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 5, 7, 0, 3, 2, 6, 1]
tensor([4, 5, 7, 0, 3, 2, 6, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 6, 3, 4, 2, 1, 7, 0]
tensor([5, 6, 3, 4, 2, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/103.pt
[100] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2598.81it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:54,  2.94s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:27,  1.49s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:06<02:49,  1.23s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:08<02:32,  1.13s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:22,  1.07s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:17,  1.05s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:08,  1.00it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:01,  1.05it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:54,  1.09it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:47,  1.14it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:42,  1.18it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:22<01:39,  1.19it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:35,  1.23it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:25<01:30,  1.27it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:27,  1.29it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:28<01:24,  1.31it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:22,  1.32it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:20,  1.32it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:22,  1.27it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:34<01:19,  1.29it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:16,  1.31it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:37<01:14,  1.33it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:11,  1.36it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:08,  1.38it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:41<01:06,  1.41it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:06,  1.38it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:44<01:03,  1.41it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<01:00,  1.43it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<00:58,  1.45it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:48<00:56,  1.47it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:55,  1.46it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:53,  1.48it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:51,  1.51it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:50,  1.47it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:48,  1.50it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:46,  1.53it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:44,  1.55it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:42,  1.56it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:41,  1.57it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:39,  1.58it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:38,  1.59it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:42,  1.38it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:39,  1.44it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:36,  1.49it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:34,  1.53it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:32,  1.56it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:31,  1.58it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:29,  1.60it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:30,  1.47it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:28,  1.50it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:32,  1.26it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:28,  1.36it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:25,  1.44it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:20<00:23,  1.50it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:22<00:23,  1.39it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:24<00:22,  1.35it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:26<00:22,  1.27it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:27<00:19,  1.38it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:28<00:17,  1.46it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:29<00:15,  1.53it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:30<00:13,  1.58it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:31<00:11,  1.63it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:33<00:10,  1.56it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:34<00:09,  1.56it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:35<00:08,  1.62it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:36<00:06,  1.66it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:37<00:05,  1.70it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:39<00:04,  1.73it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:40<00:02,  1.75it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:41<00:01,  1.77it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:42<00:00,  1.79it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:42<00:00,  1.39it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-4): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-9): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-12): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-19): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20-21): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25-31): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-4): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-9): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-12): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-19): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20-21): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25-31): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/100.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140515989106096 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140515989106096 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140515989106096 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140515989106096 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140515984598384 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140515984598384 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140515984598384 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140515984598384 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[100] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8840011732877091
0.9530187867791682
0.922962504956303
0.7839472459406365
0.8981387177104178
0.9267264969548299
0.5821108615798573
0.2837945650872556
0.6077879555368297
0.40197237439213185
0.8904940347912964
0.7509530386402397
0.901875691813653
0.9122390055224058
0.7316046462179062
0.7968435583929763
0.548007144578676
0.5393492220671232
0.9101317167988644
0.5489553505889563
0.4310648048222707
0.3687557715800523
0.8114931082909914
0.5012706843250611
0.2095243909347347
0.3518983916697559
0.9676050865236582
0.8840253404453832
0.6389046419601837
0.8840011732877091
0.9530187867791682
0.922962504956303
0.7839472459406365
0.8981387177104178
0.9267264969548299
0.5821108615798573
0.2837945650872556
0.6077879555368297
0.40197237439213185
0.8904940347912964
0.7509530386402397
0.901875691813653
0.9122390055224058
0.7316046462179062
0.7968435583929763
0.548007144578676
0.5393492220671232
0.9101317167988644
0.5489553505889563
Total groups 72 exceeded the threshold, stopping comparison.
The group tensor is
[1, 2, 7, 4, 3, 0, 6, 5]
tensor([1, 2, 7, 4, 3, 0, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 5, 4, 1, 0, 7, 2]
tensor([6, 3, 5, 4, 1, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 3, 6, 4, 2, 0, 7, 5]
tensor([1, 3, 6, 4, 2, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 3, 6, 4, 0, 1, 7, 2]
tensor([5, 3, 6, 4, 0, 1, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 5, 3, 0, 7, 1]
tensor([2, 4, 6, 5, 3, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 5, 2, 4, 0, 1, 3]
tensor([0, 1, 5, 2, 4, 0, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/100.pt
[126] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2393.41it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:30,  3.62s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:52,  1.67s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:10,  1.39s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:43,  1.21s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:27,  1.11s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:16,  1.04s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:08,  1.00it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:05,  1.01it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:57,  1.07it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:48,  1.13it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:42,  1.18it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:37,  1.22it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:32,  1.26it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:28,  1.29it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:28,  1.27it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:25,  1.30it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:22,  1.32it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:20,  1.34it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:17,  1.35it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:15,  1.36it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:13,  1.37it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:13,  1.34it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:10,  1.37it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:07,  1.40it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:04,  1.43it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:02,  1.46it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:01,  1.44it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<00:59,  1.46it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<00:59,  1.42it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:56,  1.46it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:54,  1.48it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:52,  1.50it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:50,  1.52it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:48,  1.54it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:46,  1.56it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:45,  1.56it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:45,  1.53it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:43,  1.55it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:41,  1.57it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:39,  1.58it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:41,  1.49it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:38,  1.52it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:37,  1.52it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:37,  1.45it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:36,  1.45it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:33,  1.50it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:31,  1.54it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:29,  1.57it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:28,  1.60it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:26,  1.62it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:25,  1.63it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:23,  1.64it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:23,  1.56it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:20<00:25,  1.35it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:23,  1.43it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:23<00:20,  1.51it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:24<00:18,  1.57it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:25<00:16,  1.61it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:26<00:15,  1.63it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:27<00:13,  1.67it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:29<00:12,  1.63it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:30<00:11,  1.66it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:31<00:10,  1.68it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:32<00:08,  1.69it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:33<00:07,  1.71it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:34<00:06,  1.72it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:36<00:05,  1.70it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:37<00:04,  1.73it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:39<00:03,  1.47it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:40<00:01,  1.57it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:41<00:00,  1.65it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:41<00:00,  1.40it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-28): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-28): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/126.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140513010067616 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140513010067616 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140513010067616 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140513010067616 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514972808880 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514972808880 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514972808880 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514972808880 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[126] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6024646571659534
0.8979951240825792
0.626373129339755
0.44935206242731585
0.8784896158641048
0.6837759248511185
0.6015518094762149
0.3505110226931915
0.8777011536981446
0.9326494344559366
0.9677722596476288
0.8304234396005722
0.8190920250237561
0.9903242845857883
0.8267732717549456
0.828751210253346
0.9341138981557521
0.49221062986878583
0.3069647614672379
0.9540155558764357
0.17208911684447897
0.24981632210046514
0.9635863021858757
0.7455052133761656
0.8389345141460378
0.9946030373097186
0.6620732151580052
0.6342744878528105
0.8890512085844174
Total groups 67 exceeded the threshold, stopping comparison.
The group tensor is
[2, 4, 7, 1, 3, 0, 6, 5]
tensor([2, 4, 7, 1, 3, 0, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 7, 6, 5, 0, 1, 4, 3]
tensor([2, 7, 6, 5, 0, 1, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 6, 4, 3, 0, 1, 7, 2]
tensor([5, 6, 4, 3, 0, 1, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 4, 1, 0, 1, 2, 5, 3]
tensor([0, 4, 1, 0, 1, 2, 5, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 4, 0, 0, 5, 1, 1, 3]
tensor([2, 4, 0, 0, 5, 1, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1.0, 1.0, 1, 1.0, 1.0]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1.0, 0, 1, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 1, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1.0, 1.0, 1]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
Model saved locally at saved_models/126.pt
[21] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2571.99it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:58,  2.97s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:42,  1.60s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:55,  1.28s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:35,  1.15s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:23,  1.08s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:19,  1.07s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:11,  1.02s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:03,  1.03it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:57,  1.07it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:50,  1.12it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:48,  1.12it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:49,  1.08it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:42,  1.14it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:36,  1.19it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:32,  1.22it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:28,  1.25it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:25,  1.28it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:26,  1.24it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:22,  1.27it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:19,  1.29it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:17,  1.31it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:14,  1.33it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:11,  1.36it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:08,  1.38it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:09,  1.33it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:06,  1.38it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<01:02,  1.42it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<01:00,  1.45it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:57,  1.47it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<00:55,  1.50it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:53,  1.51it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:53,  1.46it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:54<00:51,  1.49it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:55<00:49,  1.51it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:56<00:47,  1.52it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:46,  1.52it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:59<00:45,  1.53it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:43,  1.55it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:41,  1.55it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:03<00:47,  1.34it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:43,  1.41it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:06<00:40,  1.47it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:07<00:37,  1.51it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:08<00:35,  1.55it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:33,  1.57it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:11<00:31,  1.59it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:30,  1.62it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:29,  1.58it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:27,  1.61it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:26,  1.63it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:24,  1.65it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:23,  1.66it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:22,  1.67it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:20<00:20,  1.68it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:19,  1.68it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:23<00:20,  1.55it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:24<00:18,  1.60it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:25<00:16,  1.63it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:26<00:15,  1.65it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:28<00:13,  1.67it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:29<00:13,  1.55it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:30<00:12,  1.55it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:32<00:10,  1.58it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:33<00:09,  1.54it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:34<00:08,  1.60it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:35<00:06,  1.64it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:36<00:05,  1.68it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:37<00:04,  1.73it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:39<00:02,  1.76it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:40<00:01,  1.79it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:41<00:00,  1.82it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:41<00:00,  1.40it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-5): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-9): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-26): 16 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28-30): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-5): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-9): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-26): 16 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28-30): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/21.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140516063632736 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140516063632736 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516063632736 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140516063632736 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140516026140624 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140516026140624 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516026140624 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140516026140624 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[21] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7169249479288816
0.6183111372245332
0.46095560668163316
0.45412629074361927
0.5961385422474115
0.8890533054728373
0.9122749865545955
0.7722412965326397
0.28598717020359266
0.16135518374824295
0.6405733513488978
0.34881239744915987
0.02163114281968652
0.8586809120820075
0.5280864903169471
0.5549214872848108
0.8809884720847853
0.8184475126773784
0.7411818478320343
0.9204128494945557
0.8828541622108917
0.8520796846320295
0.9938665155729866
0.8532228506983649
0.8363754351071634
0.7452449391867536
0.8461045724576085
0.3693743162395131
0.3582309316878524
0.7169249479288816
0.6183111372245332
0.46095560668163316
0.45412629074361927
0.5961385422474115
0.8890533054728373
0.9122749865545955
0.7722412965326397
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[5, 4, 6, 3, 2, 0, 7, 1]
tensor([5, 4, 6, 3, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 7, 2, 4, 0, 5, 3]
tensor([6, 1, 7, 2, 4, 0, 5, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 3, 1, 0, 7, 5]
tensor([2, 4, 6, 3, 1, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 0, 6, 4, 3, 1, 5, 7]
tensor([2, 0, 6, 4, 3, 1, 5, 7], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[3, 1, 0, 1, 2, 0, 2, 3]
tensor([3, 1, 0, 1, 2, 0, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 2, 1, 0, 2, 3, 3]
tensor([0, 1, 2, 1, 0, 2, 3, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[3, 1, 0, 1, 2, 0, 3, 2]
tensor([3, 1, 0, 1, 2, 0, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 2, 2, 3, 0, 3, 1]
tensor([0, 1, 2, 2, 3, 0, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[3, 0, 1, 2, 1, 2, 3, 0]
tensor([3, 0, 1, 2, 1, 2, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/21.pt
[179] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2645.57it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:37,  2.82s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:22,  1.45s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:06<02:54,  1.27s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:08<02:33,  1.14s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:20,  1.06s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:12,  1.01s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:05,  1.03it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<01:58,  1.07it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:17<01:57,  1.07it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:49,  1.12it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:44,  1.16it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:22<01:39,  1.20it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:34,  1.24it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:25<01:30,  1.27it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:38,  1.15it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:31,  1.21it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:27,  1.25it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:23,  1.29it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:20,  1.31it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:17,  1.34it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:14,  1.35it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:15,  1.31it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:11,  1.35it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:09,  1.38it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:06,  1.40it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:03,  1.43it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:44<01:01,  1.45it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<00:59,  1.46it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<01:03,  1.34it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<01:00,  1.38it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:56,  1.43it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:54,  1.44it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:52,  1.47it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:50,  1.50it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:47,  1.53it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:46,  1.53it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:46,  1.48it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:44,  1.51it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:42,  1.54it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:40,  1.56it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:38,  1.57it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:37,  1.58it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:35,  1.59it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:34,  1.60it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:34,  1.54it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:32,  1.56it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:31,  1.57it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:29,  1.59it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:27,  1.61it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:32,  1.34it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:28,  1.43it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:29,  1.30it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:26,  1.39it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:21<00:23,  1.46it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:23<00:28,  1.17it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:25<00:25,  1.22it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:26<00:21,  1.34it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:27<00:18,  1.43it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:28<00:17,  1.46it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:29<00:14,  1.54it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:31<00:13,  1.60it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:32<00:11,  1.65it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:33<00:10,  1.68it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:34<00:08,  1.71it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:35<00:07,  1.73it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:36<00:06,  1.75it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:37<00:05,  1.77it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:39<00:04,  1.64it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:40<00:02,  1.71it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:41<00:01,  1.74it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:42<00:00,  1.78it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:42<00:00,  1.39it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-8): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-30): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/179.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514987841760 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514987841760 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514987841760 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514987841760 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140516042667408 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140516042667408 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516042667408 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140516042667408 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[179] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
0.6796182015499347
0.7774502436639344
0.8164788896198145
0.9011095591650676
0.8564639597370403
0.9528866120532432
0.4784452972921274
0.48852492817861076
0.8397355586186119
0.7433051149723976
0.5457687574271932
0.891975958338049
0.3982001908036579
0.26652204443251004
0.14111026067874816
0.11763525057572936
0.8635004180455561
0.8741394655456793
0.5304527321557103
0.44617634270319845
0.7007455592903002
0.6799150362476898
0.3511365594568724
0.47145939463701925
0.4744673317050211
0.905435493752087
0.903612941640167
0.8893763849001028
0.6821933807976941
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 5, 4, 3, 2, 1, 7, 0]
tensor([6, 5, 4, 3, 2, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 2, 7, 4, 1, 0, 6, 3]
tensor([5, 2, 7, 4, 1, 0, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 6, 5, 2, 4, 0, 7, 1]
tensor([3, 6, 5, 2, 4, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 4, 6, 0, 2, 1, 7, 3]
tensor([5, 4, 6, 0, 2, 1, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 0, 7, 3, 5, 1, 6, 2]
tensor([4, 0, 7, 3, 5, 1, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 5, 1, 3, 2, 0, 1, 4]
tensor([0, 5, 1, 3, 2, 0, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 0, 0, 1, 1.0, 1.0, 1.0, 1.0]
tensor([1, 0, 0, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[1, 0, 1, 1.0, 1.0, 0, 1.0, 1.0]
tensor([1, 0, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1.0, 0, 1, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/179.pt
[220] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2507.33it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:53,  3.78s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:57,  1.71s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:00,  1.32s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:37,  1.16s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:27,  1.11s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:16,  1.04s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:07,  1.01it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:02,  1.04it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:55,  1.08it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:50,  1.11it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:43,  1.17it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:37,  1.22it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:32,  1.26it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:28,  1.29it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:25,  1.32it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:22,  1.35it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:31,  1.20it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:25,  1.25it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:21,  1.30it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:18,  1.32it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:15,  1.34it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:12,  1.36it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:09,  1.39it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:12,  1.31it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:07,  1.37it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:04,  1.42it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:01,  1.45it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<00:58,  1.49it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:59,  1.43it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:56,  1.48it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:53,  1.50it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:53,  1.47it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:50,  1.51it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:48,  1.54it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:46,  1.57it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:44,  1.58it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:43,  1.60it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:41,  1.61it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:40,  1.62it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:39,  1.58it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:38,  1.60it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:36,  1.62it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:34,  1.63it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:33,  1.65it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:33,  1.60it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:31,  1.62it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:30,  1.63it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:35,  1.33it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:31,  1.41it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:29,  1.47it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:26,  1.53it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:24,  1.59it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:22,  1.63it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:21,  1.66it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:19,  1.69it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:22<00:19,  1.58it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:23<00:17,  1.63it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:16,  1.66it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:14,  1.69it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:27<00:13,  1.71it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:28<00:12,  1.73it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:29<00:10,  1.75it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:30<00:09,  1.76it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:31<00:08,  1.77it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:32<00:07,  1.70it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:33<00:06,  1.73it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:35<00:05,  1.63it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:36<00:04,  1.69it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:37<00:02,  1.74it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:38<00:01,  1.78it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:39<00:00,  1.82it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:39<00:00,  1.43it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-25): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-25): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/220.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514428172384 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514428172384 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514428172384 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514428172384 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140516055699664 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140516055699664 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516055699664 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140516055699664 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[220] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.4211049002945932
0.6717558353282359
0.7474911564899941
0.6826176878179091
0.7797142635267413
0.8231499933202435
0.9112841579744179
0.9329117857417774
0.3884156651377716
0.7116719161574963
0.3163186199985964
0.7579641143181478
0.7487852880300513
0.644901206148365
0.6252899485594469
0.8300709483875389
0.9686405093585702
0.4827230969630938
0.4401250454367978
0.846541236433719
0.9874080634814415
0.6954384705363965
0.33457229698994645
0.3090774332022817
0.9161115178604439
0.6466213503285708
0.719815146262638
0.8765685188625579
0.30061586460742784
0.4211049002945932
0.6717558353282359
0.7474911564899941
0.6826176878179091
0.7797142635267413
0.8231499933202435
0.9112841579744179
0.9329117857417774
0.3884156651377716
0.7116719161574963
0.3163186199985964
0.7579641143181478
0.7487852880300513
0.644901206148365
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[2, 5, 6, 1, 4, 0, 7, 3]
tensor([2, 5, 6, 1, 4, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 1, 5, 0, 7, 3]
tensor([2, 4, 6, 1, 5, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 7, 0, 1, 3, 6, 5]
tensor([4, 2, 7, 0, 1, 3, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 3, 7, 1, 2, 0, 5, 4]
tensor([6, 3, 7, 1, 2, 0, 5, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 0, 0, 2, 3, 1, 1]
tensor([4, 5, 0, 0, 2, 3, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 4, 1, 2, 1, 0, 3, 5]
tensor([0, 4, 1, 2, 1, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 5, 0, 2, 1, 1, 4]
tensor([0, 3, 5, 0, 2, 1, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/220.pt
[243] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2493.27it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:24,  3.58s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:49,  1.65s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:03,  1.34s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:39,  1.18s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:25,  1.09s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:14,  1.03s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:07,  1.01it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:02,  1.03it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:56,  1.07it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:49,  1.13it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:43,  1.17it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:38,  1.21it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:33,  1.25it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:30,  1.27it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:31,  1.24it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:27,  1.26it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:34<02:20,  1.29s/it]Running loglikelihood requests:  25%|██▍       | 35/142 [00:35<02:00,  1.13s/it]Running loglikelihood requests:  26%|██▌       | 37/142 [00:37<01:50,  1.05s/it]Running loglikelihood requests:  27%|██▋       | 39/142 [00:39<01:37,  1.05it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:40<01:29,  1.13it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:42<01:22,  1.20it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:43<01:16,  1.26it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:44<01:12,  1.31it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:46<01:08,  1.36it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:47<01:05,  1.38it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:48<01:03,  1.39it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:50<01:00,  1.44it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:51<00:57,  1.47it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:52<00:55,  1.49it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:54<00:53,  1.51it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:55<00:51,  1.52it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:56<00:49,  1.54it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:58<00:51,  1.45it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:59<00:49,  1.49it/s]Running loglikelihood requests:  50%|█████     | 71/142 [01:01<00:53,  1.34it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:02<00:49,  1.40it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:03<00:46,  1.45it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:05<00:43,  1.49it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:06<00:41,  1.51it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:08<00:45,  1.33it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:09<00:42,  1.39it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:10<00:39,  1.45it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:12<00:36,  1.51it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:13<00:34,  1.55it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:14<00:32,  1.58it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:15<00:30,  1.60it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:16<00:29,  1.61it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:18<00:27,  1.63it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:19<00:27,  1.56it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:20<00:25,  1.58it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:21<00:24,  1.61it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:23<00:22,  1.62it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:24<00:21,  1.63it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:25<00:20,  1.64it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:26<00:18,  1.65it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:27<00:17,  1.66it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:29<00:16,  1.61it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:30<00:15,  1.63it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:31<00:13,  1.65it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:32<00:12,  1.66it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:33<00:11,  1.68it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:35<00:09,  1.70it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:36<00:08,  1.71it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:37<00:07,  1.72it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:38<00:06,  1.73it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:39<00:05,  1.68it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:40<00:04,  1.72it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:41<00:02,  1.75it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:43<00:01,  1.78it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:44<00:00,  1.81it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:44<00:00,  1.36it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-25): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-25): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/243.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140546769518320 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140546769518320 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140546769518320 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140546769518320 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514730717312 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514730717312 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514730717312 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514730717312 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[243] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.4211049002945932
0.6717558353282359
0.7474911564899941
0.6826176878179091
0.7797142635267413
0.8231499933202435
0.9112841579744179
0.9329117857417774
0.3884156651377716
0.7116719161574963
0.3163186199985964
0.7579641143181478
0.7487852880300513
0.644901206148365
0.6252899485594469
0.8300709483875389
0.9686405093585702
0.4827230969630938
0.4401250454367978
0.846541236433719
0.9874080634814415
0.6954384705363965
0.33457229698994645
0.3090774332022817
0.9161115178604439
0.6466213503285708
0.719815146262638
0.8765685188625579
0.30061586460742784
0.4211049002945932
0.6717558353282359
0.7474911564899941
0.6826176878179091
0.7797142635267413
0.8231499933202435
0.9112841579744179
0.9329117857417774
0.3884156651377716
0.7116719161574963
0.3163186199985964
0.7579641143181478
0.7487852880300513
0.644901206148365
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[2, 5, 6, 1, 4, 0, 7, 3]
tensor([2, 5, 6, 1, 4, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 1, 5, 0, 7, 3]
tensor([2, 4, 6, 1, 5, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 7, 0, 1, 3, 6, 5]
tensor([4, 2, 7, 0, 1, 3, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 3, 7, 1, 2, 0, 5, 4]
tensor([6, 3, 7, 1, 2, 0, 5, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 0, 0, 2, 3, 1, 1]
tensor([4, 5, 0, 0, 2, 3, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 4, 1, 2, 1, 0, 3, 5]
tensor([0, 4, 1, 2, 1, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 5, 0, 2, 1, 1, 4]
tensor([0, 3, 5, 0, 2, 1, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/243.pt
[172] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2617.36it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:17,  2.68s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:17,  1.42s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:06<02:41,  1.18s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:08<02:25,  1.08s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:21,  1.07s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:13,  1.02s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:05,  1.03it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:15<01:58,  1.07it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:17<01:52,  1.11it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:45,  1.16it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:20<01:44,  1.16it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:22<01:37,  1.21it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:23<01:33,  1.25it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:25<01:29,  1.29it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:26<01:25,  1.32it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:28<01:23,  1.34it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:29<01:20,  1.35it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:30,  1.18it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:25,  1.23it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:34<01:20,  1.28it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:35<01:16,  1.32it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:37<01:13,  1.35it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:38<01:10,  1.38it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:10,  1.34it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:41<01:11,  1.30it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:06,  1.36it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:44<01:03,  1.40it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:45<01:00,  1.44it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<00:57,  1.47it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<01:03,  1.30it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:59,  1.36it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:58,  1.34it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:55,  1.39it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:52,  1.43it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:49,  1.47it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:47,  1.51it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:44,  1.54it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:43,  1.55it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:43,  1.50it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:41,  1.53it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:39,  1.55it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:37,  1.57it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:35,  1.59it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:34,  1.60it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:32,  1.62it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:31,  1.62it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:30,  1.63it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:35,  1.33it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:31,  1.41it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:28,  1.49it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:26,  1.54it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:24,  1.59it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:22,  1.63it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:20<00:21,  1.66it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:22<00:24,  1.34it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:23<00:21,  1.44it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:24<00:19,  1.52it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:25<00:17,  1.58it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:26<00:15,  1.62it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:28<00:13,  1.66it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:29<00:12,  1.68it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:30<00:11,  1.71it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:31<00:09,  1.72it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:32<00:08,  1.68it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:34<00:08,  1.60it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:35<00:06,  1.65it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:36<00:05,  1.70it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:37<00:04,  1.74it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:38<00:02,  1.77it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:39<00:01,  1.80it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:40<00:00,  1.83it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:40<00:00,  1.41it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-13): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15-31): 17 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-13): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15-31): 17 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/172.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514733197264 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514733197264 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514733197264 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514733197264 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514959590128 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514959590128 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514959590128 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514959590128 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[172] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.36326485003451675
0.9393611658494853
0.5347697273308267
0.4619639883096971
0.26830156279453604
0.8380027464731019
0.8263233574501603
0.6320977880109986
0.7843146415103402
0.2072713135482749
0.3805984042723936
0.43289830555723763
0.19273781754260347
0.8181624071033468
0.5320194969946469
0.7203219641552613
0.7789354624632999
0.3567665334531696
0.2604208944916411
0.942273921845013
0.8798126471295904
0.8327076147848976
0.5471221204152548
0.8016285641373796
0.8787721123612405
0.7314094662215913
0.733830137407364
0.7399689895148621
0.8815844426907766
0.36326485003451675
0.9393611658494853
0.5347697273308267
0.4619639883096971
0.26830156279453604
0.8380027464731019
0.8263233574501603
0.6320977880109986
0.7843146415103402
0.2072713135482749
0.3805984042723936
0.43289830555723763
0.19273781754260347
0.8181624071033468
0.5320194969946469
0.7203219641552613
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[1, 2, 5, 4, 3, 0, 7, 6]
tensor([1, 2, 5, 4, 3, 0, 7, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 2, 6, 0, 1, 4, 7]
tensor([5, 3, 2, 6, 0, 1, 4, 7], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 2, 6, 7, 4, 0, 3, 5]
tensor([1, 2, 6, 7, 4, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 4, 5, 6, 2, 0, 7, 3]
tensor([1, 4, 5, 6, 2, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 1, 0, 0, 2, 1, 3]
tensor([4, 5, 1, 0, 0, 2, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[1, 5, 0, 3, 4, 0, 1, 2]
tensor([1, 5, 0, 3, 4, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 2, 0, 1, 3, 2]
tensor([0, 3, 1, 2, 0, 1, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1, 0, 1.0, 1.0]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
Model saved locally at saved_models/172.pt
[163] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2587.64it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:52,  2.93s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:30,  1.51s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:56,  1.29s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:34,  1.14s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:22,  1.07s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:12,  1.01s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:05,  1.03it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:02,  1.03it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:56,  1.07it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:49,  1.13it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:44,  1.16it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:22<01:38,  1.20it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:34,  1.24it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:34,  1.22it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:30,  1.25it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:26,  1.28it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:24,  1.29it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:24,  1.27it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:21,  1.28it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:18,  1.31it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:26,  1.17it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:21,  1.22it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:16,  1.27it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:12,  1.31it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:08,  1.35it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:05,  1.38it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:04,  1.39it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<01:10,  1.24it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:49<01:05,  1.30it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<01:01,  1.35it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:58,  1.38it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:53<00:55,  1.42it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:55<01:07,  1.14it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:57<01:04,  1.17it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:58<00:58,  1.26it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:59<00:53,  1.33it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:01<00:49,  1.39it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:02<00:46,  1.43it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:03<00:44,  1.45it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:05<00:42,  1.49it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:06<00:40,  1.52it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:07<00:39,  1.48it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:09<00:37,  1.50it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:10<00:35,  1.54it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:11<00:33,  1.56it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:12<00:32,  1.57it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:14<00:30,  1.59it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:15<00:29,  1.60it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:16<00:28,  1.61it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:17<00:27,  1.56it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:19<00:25,  1.58it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:20<00:24,  1.60it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:21<00:22,  1.61it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:22<00:21,  1.62it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:24<00:20,  1.62it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:25<00:18,  1.64it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:26<00:17,  1.64it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:27<00:17,  1.56it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:29<00:15,  1.59it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:30<00:14,  1.62it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:31<00:12,  1.63it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:32<00:11,  1.65it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:33<00:10,  1.66it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:34<00:08,  1.67it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:36<00:07,  1.69it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:37<00:06,  1.69it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:38<00:05,  1.62it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:39<00:04,  1.64it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:41<00:03,  1.64it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:42<00:01,  1.67it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:43<00:00,  1.72it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:43<00:00,  1.37it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-20): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (21): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-20): 11 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (21): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22-23): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/163.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140513028197264 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140513028197264 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140513028197264 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140513028197264 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140516042668368 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140516042668368 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140516042668368 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140516042668368 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[163] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8745617680355892
0.4870053329545456
0.7039635907698274
0.13683136283338845
0.45491781836309914
0.7394007339170455
0.7100810563328822
0.5089938007827006
0.6148932447522303
0.6097500975530942
0.2557945445996536
0.6585024343395449
0.3077968033302773
0.5696663704302732
0.4357302352587323
0.8508165638532892
0.2937402676601578
0.5546720889398816
0.456887590217546
0.8515148368106646
0.6351128178266827
0.587082322906938
0.4014945886309545
0.6816643170727646
0.7647343638504926
0.15578350583970074
0.7065213224778255
0.687952127174302
0.881700230921547
0.8745617680355892
0.4870053329545456
0.7039635907698274
0.13683136283338845
0.45491781836309914
0.7394007339170455
0.7100810563328822
0.5089938007827006
0.6148932447522303
0.6097500975530942
0.2557945445996536
0.6585024343395449
0.3077968033302773
0.5696663704302732
0.4357302352587323
0.8508165638532892
0.2937402676601578
0.5546720889398816
0.456887590217546
0.8515148368106646
0.6351128178266827
0.587082322906938
0.4014945886309545
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 6, 1, 0, 4, 2]
tensor([5, 3, 7, 6, 1, 0, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 7, 1, 5, 6, 0, 4, 2]
tensor([3, 7, 1, 5, 6, 0, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 3, 2, 4, 5, 1, 7, 0]
tensor([6, 3, 2, 4, 5, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 7, 5, 1, 3, 6, 0]
tensor([2, 4, 7, 5, 1, 3, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[0, 2, 4, 6, 5, 1, 7, 3]
tensor([0, 2, 4, 6, 5, 1, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 5, 3, 6, 0, 7, 1]
tensor([4, 2, 5, 3, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/163.pt
[211] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2622.22it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:35,  3.23s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:36,  1.56s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:55,  1.28s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:34,  1.14s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:32,  1.15s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:19,  1.06s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:09,  1.00s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:01,  1.05it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:54,  1.10it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:50,  1.12it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:44,  1.16it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:37,  1.21it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:32,  1.26it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:32,  1.24it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:28,  1.28it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:26,  1.29it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:27,  1.25it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:23,  1.29it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:19,  1.31it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:17,  1.33it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:14,  1.35it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:28,  1.12it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:41<01:28,  1.09it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:20,  1.18it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:13,  1.26it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:07,  1.35it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<01:07,  1.32it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<01:02,  1.40it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:49<00:58,  1.45it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<01:02,  1.32it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:52<00:57,  1.40it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:53<00:54,  1.45it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:54<00:54,  1.41it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:56<00:51,  1.47it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:57<00:48,  1.51it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:58<00:46,  1.54it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:59<00:43,  1.58it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:01<00:43,  1.54it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:02<00:41,  1.57it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:03<00:39,  1.60it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:37,  1.62it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:06<00:35,  1.64it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:07<00:34,  1.65it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:08<00:32,  1.67it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:31,  1.68it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:31,  1.62it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:29,  1.64it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:28,  1.67it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:26,  1.69it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:25,  1.70it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:23,  1.71it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:22,  1.72it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:21,  1.72it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:20<00:20,  1.74it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:19,  1.67it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:22<00:18,  1.69it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:23<00:16,  1.71it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:15,  1.72it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:26<00:14,  1.71it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:27<00:13,  1.72it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:28<00:12,  1.69it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:29<00:11,  1.72it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:30<00:09,  1.75it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:32<00:09,  1.51it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:33<00:08,  1.60it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:34<00:06,  1.66it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:35<00:05,  1.72it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:36<00:04,  1.65it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:38<00:02,  1.73it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:39<00:01,  1.79it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:40<00:00,  1.85it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:40<00:00,  1.42it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-6): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-15): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-21): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23-27): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29-30): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-6): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-15): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-21): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23-27): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29-30): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/211.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514967149344 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514967149344 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514967149344 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514967149344 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140541531830464 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140541531830464 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140541531830464 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140541531830464 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[211] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.7988766190696382
0.433785810695937
0.6525315214782026
0.871250377966373
0.7790022985921221
0.6581533546839238
0.6948782189696622
0.4773368074709654
0.7967725763717977
0.6899017375628973
0.8876656342513365
0.741048035098914
0.7711231987685904
0.658722026565161
0.8415498496492992
0.7230018765402745
0.5047290951856622
0.7541475774993723
0.5858483604351846
0.9316615143976832
0.7753381144708663
0.5505312367470351
0.97693166402346
0.9600861235134092
0.9818242386505116
0.9256028941559552
0.44553043946224297
0.3299453067275913
0.5544144775455869
0.7988766190696382
0.433785810695937
0.6525315214782026
0.871250377966373
0.7790022985921221
0.6581533546839238
0.6948782189696622
0.4773368074709654
0.7967725763717977
0.6899017375628973
0.8876656342513365
0.741048035098914
0.7711231987685904
0.658722026565161
0.8415498496492992
0.7230018765402745
0.5047290951856622
0.7541475774993723
Total groups 72 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 1, 2, 0, 6, 4]
tensor([5, 3, 7, 1, 2, 0, 6, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 4, 5, 2, 0, 7, 1]
tensor([6, 3, 4, 5, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 4, 7, 2, 5, 1, 6, 0]
tensor([3, 4, 7, 2, 5, 1, 6, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 4, 7, 1, 3, 0, 5, 2]
tensor([6, 4, 7, 1, 3, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 6, 2, 0, 1, 7, 4]
tensor([5, 3, 6, 2, 0, 1, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[3, 5, 0, 2, 4, 0, 1, 1]
tensor([3, 5, 0, 2, 4, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/211.pt
[146] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2621.58it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:45,  2.87s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:22,  1.45s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:06<02:47,  1.23s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:08<02:29,  1.11s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:17,  1.04s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:09,  1.01it/s]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:02,  1.05it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:15<01:56,  1.09it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:17<01:58,  1.06it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:49,  1.12it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:20<01:43,  1.17it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:22<01:43,  1.15it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:36,  1.21it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:25<01:31,  1.25it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:30,  1.24it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:28<01:26,  1.28it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:23,  1.30it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:22,  1.29it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:20,  1.30it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:34<01:19,  1.30it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:16,  1.32it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:19,  1.25it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:14,  1.31it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:10,  1.35it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:06,  1.39it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:03,  1.43it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:44<01:00,  1.46it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<00:58,  1.48it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<00:58,  1.45it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:48<00:56,  1.47it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:54,  1.49it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:52,  1.51it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:50,  1.52it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:48,  1.53it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:47,  1.55it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:45,  1.56it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:53,  1.29it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:48,  1.37it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:45,  1.43it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:42,  1.48it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:40,  1.52it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:38,  1.55it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:36,  1.56it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:34,  1.58it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:34,  1.55it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:32,  1.58it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:30,  1.60it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:28,  1.63it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:27,  1.64it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:14<00:26,  1.65it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:24,  1.66it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:23,  1.66it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:23,  1.56it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:22,  1.55it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:21,  1.57it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:22<00:19,  1.61it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:23<00:17,  1.65it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:16,  1.68it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:14,  1.69it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:13,  1.71it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:28<00:12,  1.73it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:29<00:11,  1.68it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:30<00:09,  1.71it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:31<00:08,  1.74it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:32<00:07,  1.76it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:33<00:06,  1.77it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:34<00:05,  1.79it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:35<00:03,  1.80it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:37<00:02,  1.80it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:38<00:01,  1.82it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:39<00:00,  1.77it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:39<00:00,  1.43it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/146.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514956725952 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514956725952 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514956725952 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514956725952 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514981366960 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514981366960 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514981366960 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514981366960 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[146] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8301327615319641
0.7817720914227475
0.8113678911982368
0.2257173763133808
0.2927105469616105
0.5177793359335131
0.8145737190612207
0.7363478990038121
0.20758834533702153
0.2533018140549718
0.30764875086796684
0.29044714199656413
0.9350374080783923
0.6277994360424477
0.603553220567354
0.78279661631625
0.6718149226457006
0.5353760149732133
0.6848376228333551
0.9446001512801305
0.7972906165117697
0.5043025319455414
0.5939887608746786
0.6455402209569181
0.2505215486241291
0.321550947803521
0.8361163063826382
0.7208169010376037
0.9633188772100769
0.8301327615319641
0.7817720914227475
0.8113678911982368
0.2257173763133808
0.2927105469616105
0.5177793359335131
0.8145737190612207
0.7363478990038121
0.20758834533702153
0.2533018140549718
0.30764875086796684
0.29044714199656413
0.9350374080783923
0.6277994360424477
0.603553220567354
0.78279661631625
0.6718149226457006
0.5353760149732133
0.6848376228333551
0.9446001512801305
0.7972906165117697
0.5043025319455414
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[6, 5, 4, 1, 3, 0, 7, 2]
tensor([6, 5, 4, 1, 3, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 5, 1, 0, 4, 2]
tensor([6, 3, 7, 5, 1, 0, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 4, 7, 0, 3, 1, 5, 2]
tensor([6, 4, 7, 0, 3, 1, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 5, 3, 4, 1, 0, 7, 6]
tensor([2, 5, 3, 4, 1, 0, 7, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 5, 0, 2, 4, 0, 1, 1]
tensor([3, 5, 0, 2, 4, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 0, 2, 1, 2, 3]
tensor([0, 3, 1, 0, 2, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 3, 1, 2, 2, 1, 3, 0]
tensor([0, 3, 1, 2, 2, 1, 3, 0], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 1, 2, 0, 3, 3, 2, 1]
tensor([0, 1, 2, 0, 3, 3, 2, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1, 0, 1.0, 1.0, 1.0, 1.0]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/146.pt
[5] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2607.90it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:02<06:54,  2.94s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:04<03:30,  1.52s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:06<02:46,  1.22s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:08<02:26,  1.09s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:14,  1.01s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:06,  1.04it/s]Running loglikelihood requests:   9%|▉         | 13/142 [00:13<01:59,  1.08it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:15<01:56,  1.09it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:17<01:50,  1.13it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:18<01:44,  1.18it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:20<01:39,  1.22it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:21<01:34,  1.26it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:23<01:31,  1.28it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:25<01:31,  1.26it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:26<01:28,  1.27it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:28<01:27,  1.27it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:29<01:26,  1.26it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:22,  1.30it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:32<01:18,  1.34it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:33<01:15,  1.37it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:35<01:16,  1.32it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:36<01:13,  1.36it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:38<01:09,  1.40it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:39<01:06,  1.43it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:40<01:03,  1.47it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:42<01:09,  1.30it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:44<01:04,  1.38it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:45<01:03,  1.38it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:46<00:58,  1.44it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:47<00:55,  1.49it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:49<00:52,  1.53it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:50<00:50,  1.57it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:51<00:48,  1.59it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:52<00:46,  1.61it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:54<00:44,  1.63it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:55<00:45,  1.57it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:56<00:42,  1.61it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:57<00:41,  1.63it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:58<00:39,  1.65it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:00<00:38,  1.65it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:01<00:36,  1.67it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:03<00:44,  1.33it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:04<00:40,  1.42it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:38,  1.43it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:35,  1.50it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:08<00:33,  1.54it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:09<00:31,  1.57it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:30,  1.55it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:28,  1.60it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:13<00:26,  1.64it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:14<00:24,  1.68it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:25,  1.51it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:23,  1.58it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:18<00:21,  1.63it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:19<00:19,  1.69it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:20<00:18,  1.65it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:21<00:17,  1.70it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:22<00:15,  1.74it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:24<00:14,  1.76it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:25<00:12,  1.79it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:26<00:12,  1.71it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:27<00:10,  1.75it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:28<00:09,  1.79it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:29<00:08,  1.81it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:30<00:07,  1.84it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:31<00:05,  1.85it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:32<00:04,  1.88it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:33<00:03,  1.90it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:34<00:02,  1.93it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:35<00:01,  1.95it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:36<00:00,  1.86it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:36<00:00,  1.46it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-7): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-13): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-23): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26-31): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-7): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-13): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-23): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26-31): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/5.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514965528000 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514965528000 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514965528000 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514965528000 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514718902384 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514718902384 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514718902384 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514718902384 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[5] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.4006974550645674
0.553274626438677
0.7114020264319938
0.6984504610379519
0.8708568984836823
0.6210210193880212
0.7638777300395475
0.4562187856309214
0.27778416422139596
0.7174549442708792
0.8874047516186899
0.7197371862802024
0.9973953807217149
0.8197334014617479
0.9421229754064668
0.9500023700992883
0.6312941692561447
0.47890039407555335
0.4338007663161437
0.9443928308635141
0.5752272779462101
0.5023096877772295
0.7917678755247893
0.42255885423676204
0.5830463630592772
0.884102890932443
0.7935069002678894
0.6274856483424485
0.841637545221562
0.4006974550645674
0.553274626438677
0.7114020264319938
0.6984504610379519
0.8708568984836823
0.6210210193880212
0.7638777300395475
0.4562187856309214
0.27778416422139596
0.7174549442708792
0.8874047516186899
0.7197371862802024
0.9973953807217149
0.8197334014617479
0.9421229754064668
0.9500023700992883
0.6312941692561447
0.47890039407555335
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[4, 2, 5, 1, 6, 3, 7, 0]
tensor([4, 2, 5, 1, 6, 3, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 4, 7, 5, 1, 0, 6, 2]
tensor([3, 4, 7, 5, 1, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 5, 6, 4, 3, 1, 7, 0]
tensor([2, 5, 6, 4, 3, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 1, 4, 3, 6, 0, 5, 2]
tensor([7, 1, 4, 3, 6, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[0, 5, 3, 1, 1, 0, 2, 4]
tensor([0, 5, 3, 1, 1, 0, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[2, 5, 1, 0, 1, 0, 4, 3]
tensor([2, 5, 1, 0, 1, 0, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[4, 0, 3, 0, 2, 1, 5, 1]
tensor([4, 0, 3, 0, 2, 1, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/5.pt
[175] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2588.20it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:29,  3.19s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:42,  1.60s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:53,  1.27s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:33,  1.14s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:21,  1.06s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:12,  1.01s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:08,  1.00it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<02:00,  1.05it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:56,  1.07it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:52,  1.09it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:46,  1.13it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:41,  1.17it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:38,  1.19it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:32,  1.24it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:27,  1.29it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:23,  1.33it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:20,  1.35it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:18,  1.37it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:15,  1.38it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:20,  1.28it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:16,  1.32it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:37<01:12,  1.36it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:08,  1.41it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:05,  1.44it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:41<01:03,  1.48it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:00,  1.50it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:44<00:58,  1.53it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:45<00:58,  1.49it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:46<00:55,  1.53it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:48<00:53,  1.55it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:49<00:51,  1.57it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:50<00:49,  1.58it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:51<00:48,  1.60it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:53<00:46,  1.61it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:54,  1.34it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:50,  1.41it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:57<00:46,  1.49it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:58<00:43,  1.55it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:59<00:40,  1.59it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:39,  1.61it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:02<00:37,  1.62it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:03<00:36,  1.61it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:46,  1.22it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:42,  1.31it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:37,  1.41it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:34,  1.49it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:31,  1.56it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:29,  1.61it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:30,  1.50it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:14<00:27,  1.57it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:27,  1.51it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:24,  1.58it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:22,  1.64it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:20,  1.68it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:19,  1.72it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:21<00:17,  1.74it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:22<00:16,  1.76it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:15,  1.79it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:13,  1.79it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:13,  1.71it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:27<00:12,  1.68it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:28<00:10,  1.74it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:29<00:09,  1.78it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:30<00:08,  1.81it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:31<00:07,  1.84it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:32<00:05,  1.85it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:33<00:04,  1.88it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:35<00:03,  1.90it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:36<00:03,  1.57it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:37<00:01,  1.67it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:38<00:00,  1.77it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:38<00:00,  1.44it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-25): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-25): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/175.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514972691888 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514972691888 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514972691888 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514972691888 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514728026160 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514728026160 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514728026160 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514728026160 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[175] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.4211049002945932
0.6717558353282359
0.7474911564899941
0.6826176878179091
0.7797142635267413
0.8231499933202435
0.9112841579744179
0.9329117857417774
0.3884156651377716
0.7116719161574963
0.3163186199985964
0.7579641143181478
0.7487852880300513
0.644901206148365
0.6252899485594469
0.8300709483875389
0.9686405093585702
0.4827230969630938
0.4401250454367978
0.846541236433719
0.9874080634814415
0.6954384705363965
0.33457229698994645
0.3090774332022817
0.9161115178604439
0.6466213503285708
0.719815146262638
0.8765685188625579
0.30061586460742784
0.4211049002945932
0.6717558353282359
0.7474911564899941
0.6826176878179091
0.7797142635267413
0.8231499933202435
0.9112841579744179
0.9329117857417774
0.3884156651377716
0.7116719161574963
0.3163186199985964
0.7579641143181478
0.7487852880300513
0.644901206148365
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[2, 5, 6, 1, 4, 0, 7, 3]
tensor([2, 5, 6, 1, 4, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 1, 5, 0, 7, 3]
tensor([2, 4, 6, 1, 5, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 7, 0, 1, 3, 6, 5]
tensor([4, 2, 7, 0, 1, 3, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 3, 7, 1, 2, 0, 5, 4]
tensor([6, 3, 7, 1, 2, 0, 5, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 0, 0, 2, 3, 1, 1]
tensor([4, 5, 0, 0, 2, 3, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 4, 1, 2, 1, 0, 3, 5]
tensor([0, 4, 1, 2, 1, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 5, 0, 2, 1, 1, 4]
tensor([0, 3, 5, 0, 2, 1, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/175.pt
[248] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2601.06it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:04<09:47,  4.17s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:13,  1.83s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:11,  1.39s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:44,  1.22s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:34,  1.16s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:31,  1.16s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:22,  1.11s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:09,  1.02s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<01:59,  1.04it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:22<02:19,  1.13s/it]Running loglikelihood requests:  15%|█▍        | 21/142 [00:24<02:03,  1.02s/it]Running loglikelihood requests:  16%|█▌        | 23/142 [00:25<01:50,  1.08it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:27<01:41,  1.15it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:34,  1.22it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:30<01:29,  1.27it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:31<01:28,  1.25it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:33<01:24,  1.29it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:34<01:20,  1.33it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:36<01:17,  1.36it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:37<01:14,  1.38it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:12,  1.40it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:40<01:09,  1.42it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:41<01:09,  1.39it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:43<01:06,  1.43it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:44<01:03,  1.47it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:00,  1.51it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<00:57,  1.54it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:48<00:55,  1.56it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:49<00:54,  1.57it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<00:53,  1.55it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:53<01:16,  1.05it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:55<01:06,  1.18it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:56<00:59,  1.30it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:57<00:53,  1.39it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:58<00:49,  1.47it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:59<00:46,  1.54it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:01<00:43,  1.59it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:02<00:43,  1.55it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:03<00:40,  1.60it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:04<00:38,  1.64it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:05<00:36,  1.67it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:06<00:34,  1.69it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:08<00:33,  1.71it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:09<00:31,  1.73it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:10<00:30,  1.74it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:11<00:29,  1.75it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:29,  1.67it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:27,  1.72it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:15<00:25,  1.74it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:16<00:24,  1.77it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:22,  1.79it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:21,  1.81it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:20,  1.82it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:20<00:19,  1.83it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:17,  1.84it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:23<00:19,  1.61it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:24<00:17,  1.67it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:25<00:15,  1.72it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:26<00:14,  1.76it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:27<00:12,  1.79it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:28<00:11,  1.82it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:29<00:10,  1.84it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:30<00:09,  1.86it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:32<00:09,  1.60it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:34<00:10,  1.22it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:35<00:08,  1.36it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:36<00:06,  1.49it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:37<00:04,  1.61it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:38<00:02,  1.71it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:39<00:01,  1.78it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:40<00:00,  1.85it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:40<00:00,  1.41it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-4): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-6): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-8): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-12): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-31): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-4): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-6): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7-8): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11-12): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-31): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/248.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514432514384 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514432514384 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514432514384 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514432514384 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514444018368 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514444018368 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514444018368 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514444018368 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[248] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9728816226148068
0.5587440604316873
0.8603958805228331
0.8559030347476725
0.9827298775692989
0.9752051704922662
0.9320451620794601
0.8534915859631292
0.8216376829910712
0.7152752058820862
0.865677149871932
0.6996438609344103
0.5450489764598446
0.9459412501293835
0.800938078166953
0.9388899316299849
0.6171018502143922
0.5700995146297211
0.9725502911927032
0.8593526631716398
0.8340908152739906
0.8867570952520308
0.8278223956249111
0.6726110030960961
0.822005113004547
0.952024055144357
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[2, 6, 4, 1, 3, 0, 7, 5]
tensor([2, 6, 4, 1, 3, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 6, 3, 1, 5, 0, 7, 2]
tensor([4, 6, 3, 1, 5, 0, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 1, 4, 2, 5, 0, 7, 3]
tensor([6, 1, 4, 2, 5, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 3, 7, 1, 6, 0, 5, 2]
tensor([4, 3, 7, 1, 6, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 6, 5, 1, 3, 0, 4, 2]
tensor([7, 6, 5, 1, 3, 0, 4, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 0, 5, 4, 3, 0, 1, 2]
tensor([1, 0, 5, 4, 3, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 3, 0, 2, 1, 3, 2]
tensor([0, 1, 3, 0, 2, 1, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/248.pt
[69] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2640.13it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<09:10,  3.91s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:23,  1.90s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:35,  1.57s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:57,  1.32s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:36,  1.17s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:21,  1.08s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:10,  1.01s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:12,  1.04s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:20<02:02,  1.02it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:52,  1.10it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:44,  1.16it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:38,  1.21it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:32,  1.26it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:31,  1.26it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:27,  1.29it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:24,  1.31it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:32<01:22,  1.33it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:19,  1.35it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:17,  1.36it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:14,  1.38it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:16,  1.32it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:13,  1.35it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:10,  1.38it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:08,  1.39it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:05,  1.42it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:02,  1.47it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<00:59,  1.50it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<00:57,  1.52it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:49<01:01,  1.38it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<00:57,  1.45it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:54,  1.49it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:51,  1.53it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:49,  1.56it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:55<00:47,  1.58it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:56<00:45,  1.60it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:46,  1.52it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:59<00:44,  1.56it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:42,  1.59it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:40,  1.62it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:38,  1.65it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:36,  1.67it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:35,  1.67it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:33,  1.68it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:32,  1.70it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:32,  1.64it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:30,  1.67it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:28,  1.69it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:27,  1.70it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:26,  1.72it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:14<00:24,  1.73it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:23,  1.74it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:22,  1.75it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:21,  1.75it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:20,  1.70it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:19,  1.72it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:21<00:17,  1.74it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:22<00:16,  1.77it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:23<00:15,  1.78it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:24<00:14,  1.78it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:25<00:12,  1.80it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:26<00:11,  1.80it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:27<00:10,  1.80it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:29<00:09,  1.71it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:30<00:08,  1.75it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:31<00:07,  1.76it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:32<00:06,  1.77it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:33<00:04,  1.80it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:34<00:03,  1.82it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:35<00:02,  1.83it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:36<00:01,  1.83it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:37<00:00,  1.81it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:37<00:00,  1.45it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-28): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-28): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/69.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514445554656 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514445554656 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514445554656 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514445554656 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514973617328 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514973617328 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514973617328 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514973617328 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[69] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6024646571659534
0.8979951240825792
0.626373129339755
0.44935206242731585
0.8784896158641048
0.6837759248511185
0.6015518094762149
0.3505110226931915
0.8777011536981446
0.9326494344559366
0.9677722596476288
0.8304234396005722
0.8190920250237561
0.9903242845857883
0.8267732717549456
0.828751210253346
0.9341138981557521
0.49221062986878583
0.3069647614672379
0.9540155558764357
0.17208911684447897
0.24981632210046514
0.9635863021858757
0.7455052133761656
0.8389345141460378
0.9946030373097186
0.6620732151580052
0.6342744878528105
0.8890512085844174
Total groups 67 exceeded the threshold, stopping comparison.
The group tensor is
[2, 4, 7, 1, 3, 0, 6, 5]
tensor([2, 4, 7, 1, 3, 0, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 7, 6, 5, 0, 1, 4, 3]
tensor([2, 7, 6, 5, 0, 1, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 6, 4, 3, 0, 1, 7, 2]
tensor([5, 6, 4, 3, 0, 1, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 4, 1, 0, 1, 2, 5, 3]
tensor([0, 4, 1, 0, 1, 2, 5, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 4, 0, 0, 5, 1, 1, 3]
tensor([2, 4, 0, 0, 5, 1, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1.0, 1.0, 1, 1.0, 1.0]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1.0, 0, 1, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 1, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1.0, 1.0, 1]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
Model saved locally at saved_models/69.pt
[57] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2621.65it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:11,  3.49s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:54,  2.12s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:26,  1.51s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:50,  1.26s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:30,  1.13s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:17,  1.05s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:12,  1.03s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:02,  1.04it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<01:54,  1.09it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:46,  1.15it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:40,  1.20it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:35,  1.25it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:33,  1.25it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:28,  1.30it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:25,  1.33it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:22,  1.34it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:20,  1.35it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:19,  1.34it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:18,  1.34it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:22,  1.25it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:17,  1.30it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:14,  1.33it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:10,  1.37it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:07,  1.41it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:04,  1.45it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:01,  1.48it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:00,  1.46it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<00:57,  1.50it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<00:55,  1.54it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:53,  1.56it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:51,  1.58it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:49,  1.59it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:47,  1.61it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:46,  1.62it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:44,  1.63it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:48,  1.45it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:45,  1.51it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:43,  1.55it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:40,  1.59it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:01<00:38,  1.62it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:37,  1.64it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:35,  1.66it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:34,  1.67it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:09<00:54,  1.02it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:10<00:45,  1.16it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:11<00:39,  1.28it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:35,  1.39it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:31,  1.48it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:29,  1.55it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:16<00:28,  1.53it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:25,  1.60it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:23,  1.64it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:22,  1.68it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:20<00:20,  1.70it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:19,  1.73it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:23<00:17,  1.74it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:24<00:16,  1.76it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:25<00:15,  1.77it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:26<00:15,  1.65it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:27<00:13,  1.69it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:28<00:12,  1.71it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:30<00:10,  1.75it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:31<00:09,  1.74it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:32<00:08,  1.74it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:33<00:07,  1.75it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:34<00:06,  1.75it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:35<00:05,  1.78it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:37<00:04,  1.55it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:38<00:03,  1.65it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:39<00:01,  1.73it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:40<00:00,  1.81it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:40<00:00,  1.41it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-29): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-29): 24 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/57.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514444200704 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514444200704 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514444200704 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514444200704 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514440212320 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514440212320 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514440212320 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514440212320 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[57] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4084507042253521, 'acc_stderr,none': 0.058751136942575236}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.9179357562486347
0.7304758827874278
0.9028361165350093
0.9177386064031547
0.736264115539795
0.7821589386855158
0.7475448337241333
0.6622221680289488
0.7583296634655016
0.33645990832698236
0.21982307453464583
0.17345324326533693
0.35977004925853934
0.8808900006571698
0.6260065234155616
0.8246960552033293
0.6144641592309815
0.6394144929277611
0.9223063206094516
0.7309208925817541
0.5091734316973643
0.7222829855220914
0.24969394334306674
0.508931879405537
0.8097279042671086
0.5258809052220982
0.4071243454156871
0.9510154626938295
0.3256867622893698
0.9179357562486347
0.7304758827874278
0.9028361165350093
0.9177386064031547
0.736264115539795
0.7821589386855158
0.7475448337241333
0.6622221680289488
0.7583296634655016
0.33645990832698236
0.21982307453464583
0.17345324326533693
0.35977004925853934
0.8808900006571698
0.6260065234155616
0.8246960552033293
0.6144641592309815
0.6394144929277611
0.9223063206094516
0.7309208925817541
0.5091734316973643
0.7222829855220914
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[1, 4, 3, 6, 2, 0, 7, 5]
tensor([1, 4, 3, 6, 2, 0, 7, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 1, 4, 7, 2, 0, 6, 5]
tensor([3, 1, 4, 7, 2, 0, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 6, 7, 0, 3, 2, 4, 1]
tensor([5, 6, 7, 0, 3, 2, 4, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 6, 3, 5, 0, 1, 7, 2]
tensor([4, 6, 3, 5, 0, 1, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 2, 5, 0, 4, 1, 1, 3]
tensor([0, 2, 5, 0, 4, 1, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 2, 3, 0, 2, 3, 1]
tensor([0, 1, 2, 3, 0, 2, 3, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[2, 1, 0, 1, 2, 0, 3, 3]
tensor([2, 1, 0, 1, 2, 0, 3, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[0, 2, 1, 3, 0, 1, 2, 3]
tensor([0, 2, 1, 3, 0, 1, 2, 3], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/57.pt
[136] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2642.42it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:28,  3.61s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:48,  2.08s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:23,  1.49s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:57,  1.31s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:34,  1.16s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:19,  1.06s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:09,  1.00s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:02,  1.04it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<01:55,  1.08it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:53,  1.08it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:48,  1.11it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:40,  1.18it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:34,  1.23it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:30,  1.27it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:26,  1.30it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:31<01:37,  1.13it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:32<01:30,  1.20it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:24,  1.27it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:35<01:20,  1.31it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:16,  1.34it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:13,  1.37it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:11,  1.39it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:41<01:12,  1.34it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:08,  1.38it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:06,  1.40it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:02,  1.45it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<00:59,  1.49it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<00:57,  1.52it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:55,  1.54it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<00:53,  1.56it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:53,  1.51it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:51,  1.55it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:54<00:48,  1.57it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:55<00:47,  1.59it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:56<00:45,  1.61it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:43,  1.62it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:42,  1.64it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:40,  1.65it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:40,  1.60it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:38,  1.62it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:37,  1.63it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:35,  1.64it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:34,  1.66it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:32,  1.68it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:31,  1.69it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:30,  1.69it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:28,  1.70it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:14<00:42,  1.10it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:15<00:36,  1.23it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:16<00:32,  1.34it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:28,  1.44it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:25,  1.51it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:20<00:24,  1.54it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:21<00:22,  1.54it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:22<00:21,  1.57it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:23<00:19,  1.59it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:24<00:17,  1.62it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:26<00:16,  1.62it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:27<00:15,  1.66it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:28<00:13,  1.72it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:29<00:11,  1.75it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:30<00:10,  1.79it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:31<00:09,  1.73it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:32<00:08,  1.78it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:33<00:07,  1.81it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:35<00:06,  1.78it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:36<00:04,  1.81it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:37<00:03,  1.84it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:38<00:02,  1.86it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:39<00:01,  1.90it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:40<00:00,  1.94it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:40<00:00,  1.42it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-7): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-13): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-23): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26-31): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-7): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-13): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-15): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-23): 7 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24-25): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26-31): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/136.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514444207520 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514444207520 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514444207520 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514444207520 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514981371760 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514981371760 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514981371760 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514981371760 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[136] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4225352112676056, 'acc_stderr,none': 0.059039842056825796}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.4006974550645674
0.553274626438677
0.7114020264319938
0.6984504610379519
0.8708568984836823
0.6210210193880212
0.7638777300395475
0.4562187856309214
0.27778416422139596
0.7174549442708792
0.8874047516186899
0.7197371862802024
0.9973953807217149
0.8197334014617479
0.9421229754064668
0.9500023700992883
0.6312941692561447
0.47890039407555335
0.4338007663161437
0.9443928308635141
0.5752272779462101
0.5023096877772295
0.7917678755247893
0.42255885423676204
0.5830463630592772
0.884102890932443
0.7935069002678894
0.6274856483424485
0.841637545221562
0.4006974550645674
0.553274626438677
0.7114020264319938
0.6984504610379519
0.8708568984836823
0.6210210193880212
0.7638777300395475
0.4562187856309214
0.27778416422139596
0.7174549442708792
0.8874047516186899
0.7197371862802024
0.9973953807217149
0.8197334014617479
0.9421229754064668
0.9500023700992883
0.6312941692561447
0.47890039407555335
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[4, 2, 5, 1, 6, 3, 7, 0]
tensor([4, 2, 5, 1, 6, 3, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 4, 7, 5, 1, 0, 6, 2]
tensor([3, 4, 7, 5, 1, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 5, 6, 4, 3, 1, 7, 0]
tensor([2, 5, 6, 4, 3, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[7, 1, 4, 3, 6, 0, 5, 2]
tensor([7, 1, 4, 3, 6, 0, 5, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[0, 5, 3, 1, 1, 0, 2, 4]
tensor([0, 5, 3, 1, 1, 0, 2, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[2, 5, 1, 0, 1, 0, 4, 3]
tensor([2, 5, 1, 0, 1, 0, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[4, 0, 3, 0, 2, 1, 5, 1]
tensor([4, 0, 3, 0, 2, 1, 5, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/136.pt
[160] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2570.55it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:54,  3.36s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:46,  1.63s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:22,  1.48s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:56,  1.31s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:36,  1.17s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:21,  1.08s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:10,  1.01s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:10,  1.02s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:20<02:12,  1.06s/it]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:59,  1.03it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:50,  1.10it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:42,  1.16it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:36,  1.21it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:28<01:44,  1.10it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:36,  1.18it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:31<01:29,  1.24it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:32<01:25,  1.28it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:34<01:21,  1.31it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:35<01:18,  1.33it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:37<01:18,  1.32it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:15,  1.35it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:12,  1.37it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:41<01:09,  1.40it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:06,  1.43it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:03,  1.46it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:00,  1.50it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<00:58,  1.52it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<00:59,  1.46it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:49<00:56,  1.49it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<00:54,  1.53it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:52,  1.54it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:50,  1.56it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:54<00:48,  1.58it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:55<00:47,  1.59it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:56<00:45,  1.61it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:58<00:46,  1.53it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:59<00:44,  1.56it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:42,  1.59it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:40,  1.60it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:38,  1.62it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:37,  1.64it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:05<00:35,  1.65it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:34,  1.66it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:34,  1.58it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:38,  1.39it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:11<00:36,  1.41it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:34,  1.43it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:31,  1.48it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:29,  1.51it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:16<00:27,  1.58it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:24,  1.65it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:23,  1.63it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:21,  1.68it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:20<00:20,  1.74it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:18,  1.78it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:22<00:17,  1.81it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:23<00:15,  1.84it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:14,  1.86it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:13,  1.87it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:12,  1.89it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:28<00:11,  1.81it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:29<00:10,  1.84it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:30<00:09,  1.87it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:31<00:07,  1.89it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:32<00:06,  1.91it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:33<00:05,  1.93it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:34<00:04,  1.95it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:35<00:03,  1.96it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:36<00:02,  1.98it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:37<00:01,  2.00it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:38<00:00,  1.89it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:38<00:00,  1.44it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-28): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-28): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:0)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/160.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514719286384 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514719286384 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514719286384 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514719286384 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514718899360 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514718899360 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514718899360 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514718899360 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[160] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6024646571659534
0.8979951240825792
0.626373129339755
0.44935206242731585
0.8784896158641048
0.6837759248511185
0.6015518094762149
0.3505110226931915
0.8777011536981446
0.9326494344559366
0.9677722596476288
0.8304234396005722
0.8190920250237561
0.9903242845857883
0.8267732717549456
0.828751210253346
0.9341138981557521
0.49221062986878583
0.3069647614672379
0.9540155558764357
0.17208911684447897
0.24981632210046514
0.9635863021858757
0.7455052133761656
0.8389345141460378
0.9946030373097186
0.6620732151580052
0.6342744878528105
0.8890512085844174
Total groups 67 exceeded the threshold, stopping comparison.
The group tensor is
[2, 4, 7, 1, 3, 0, 6, 5]
tensor([2, 4, 7, 1, 3, 0, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 7, 6, 5, 0, 1, 4, 3]
tensor([2, 7, 6, 5, 0, 1, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 6, 4, 3, 0, 1, 7, 2]
tensor([5, 6, 4, 3, 0, 1, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 4, 1, 0, 1, 2, 5, 3]
tensor([0, 4, 1, 0, 1, 2, 5, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 4, 0, 0, 5, 1, 1, 3]
tensor([2, 4, 0, 0, 5, 1, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1.0, 1.0, 1, 1.0, 1.0]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1.0, 0, 1, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 1, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1.0, 1.0, 1]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
Model saved locally at saved_models/160.pt
[247] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2616.58it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:04<10:32,  4.49s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:23,  1.89s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:12,  1.41s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:43,  1.21s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:48,  1.27s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:29,  1.14s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:15,  1.05s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:17,  1.08s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:20<02:04,  1.00it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:22<01:55,  1.06it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:47,  1.12it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:25<01:40,  1.18it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:34,  1.24it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:29,  1.28it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:26,  1.31it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:22,  1.34it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:32<01:23,  1.31it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:20,  1.34it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:35<01:17,  1.35it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:15,  1.37it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:13,  1.38it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:10,  1.40it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:07,  1.43it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:08,  1.39it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:06,  1.39it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:03,  1.44it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<01:00,  1.48it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<00:57,  1.51it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:55,  1.54it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<00:53,  1.55it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:51,  1.57it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:50,  1.58it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:54<00:58,  1.31it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:55<00:53,  1.39it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:57<00:50,  1.46it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:58<00:47,  1.51it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:59<00:44,  1.55it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:42,  1.57it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:02<00:41,  1.57it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:04<00:47,  1.33it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:05<00:46,  1.33it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:07<00:42,  1.38it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:08<00:41,  1.38it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:09<00:37,  1.46it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:10<00:34,  1.53it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:11<00:31,  1.60it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:13<00:29,  1.65it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:14<00:30,  1.57it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:15<00:27,  1.63it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:16<00:25,  1.68it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:17<00:23,  1.71it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:22,  1.75it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:20<00:20,  1.76it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:21<00:19,  1.79it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:22<00:18,  1.79it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:23<00:17,  1.81it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:24<00:16,  1.74it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:25<00:15,  1.77it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:26<00:14,  1.78it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:27<00:12,  1.80it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:28<00:11,  1.78it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:30<00:10,  1.73it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:31<00:10,  1.65it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:32<00:08,  1.70it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:34<00:08,  1.61it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:35<00:06,  1.70it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:36<00:05,  1.78it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:37<00:03,  1.85it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:37<00:02,  1.91it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:38<00:01,  1.96it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:39<00:00,  2.01it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:39<00:00,  1.42it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-28): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-28): 21 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30-31): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:1)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/247.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514715868224 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514715868224 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514715868224 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514715868224 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514715868944 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514715868944 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514715868944 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514715868944 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[247] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6024646571659534
0.8979951240825792
0.626373129339755
0.44935206242731585
0.8784896158641048
0.6837759248511185
0.6015518094762149
0.3505110226931915
0.8777011536981446
0.9326494344559366
0.9677722596476288
0.8304234396005722
0.8190920250237561
0.9903242845857883
0.8267732717549456
0.828751210253346
0.9341138981557521
0.49221062986878583
0.3069647614672379
0.9540155558764357
0.17208911684447897
0.24981632210046514
0.9635863021858757
0.7455052133761656
0.8389345141460378
0.9946030373097186
0.6620732151580052
0.6342744878528105
0.8890512085844174
Total groups 67 exceeded the threshold, stopping comparison.
The group tensor is
[2, 4, 7, 1, 3, 0, 6, 5]
tensor([2, 4, 7, 1, 3, 0, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 7, 6, 5, 0, 1, 4, 3]
tensor([2, 7, 6, 5, 0, 1, 4, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[5, 6, 4, 3, 0, 1, 7, 2]
tensor([5, 6, 4, 3, 0, 1, 7, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 4, 1, 0, 1, 2, 5, 3]
tensor([0, 4, 1, 0, 1, 2, 5, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[2, 4, 0, 0, 5, 1, 1, 3]
tensor([2, 4, 0, 0, 5, 1, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 0, 1, 1.0, 1.0, 1, 1.0, 1.0]
tensor([0, 0, 1, 1, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[0, 1, 1.0, 1.0, 0, 1, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1.0, 1, 1.0, 0]
tensor([0, 1, 1, 1, 1, 1, 1, 0], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 0, 1.0, 1.0, 1.0, 1]
tensor([0, 1, 1, 0, 1, 1, 1, 1], dtype=torch.int32)
[0, 1]
Model saved locally at saved_models/247.pt
[83] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2607.23it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:42,  3.70s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:54,  1.68s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:00,  1.32s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:45,  1.22s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:28,  1.12s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:17,  1.05s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:08,  1.00it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:00,  1.05it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:18<01:54,  1.09it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:49,  1.12it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:21<01:43,  1.17it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:37,  1.22it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:32,  1.26it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:26<01:28,  1.30it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:25,  1.33it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:29<01:22,  1.35it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:23,  1.30it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:32<01:20,  1.32it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:18,  1.34it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:16,  1.35it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:13,  1.37it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:11,  1.38it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:09,  1.40it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:09,  1.37it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:06,  1.41it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:43<01:02,  1.45it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:44<01:00,  1.48it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<00:58,  1.50it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:47<00:56,  1.51it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:48<00:54,  1.53it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:50<00:56,  1.43it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:51<00:53,  1.46it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:52<00:51,  1.51it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:54<00:49,  1.53it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:55<00:47,  1.53it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:56<00:46,  1.52it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:45,  1.51it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:45,  1.49it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:43,  1.48it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:40,  1.54it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:38,  1.58it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:36,  1.62it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:34,  1.66it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:32,  1.69it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:31,  1.71it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:08<00:29,  1.73it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:28,  1.74it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:30,  1.55it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:28,  1.61it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:13<00:25,  1.65it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:24,  1.68it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:18<00:36,  1.06it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:19<00:30,  1.20it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:21<00:28,  1.24it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:22<00:24,  1.34it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:23<00:21,  1.46it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:24<00:18,  1.56it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:25<00:16,  1.64it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:26<00:14,  1.69it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:27<00:13,  1.74it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:28<00:11,  1.79it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:29<00:10,  1.82it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:31<00:09,  1.71it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:32<00:08,  1.75it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:33<00:07,  1.79it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:34<00:06,  1.82it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:35<00:04,  1.84it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:36<00:03,  1.87it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:37<00:02,  1.91it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:38<00:01,  1.94it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:39<00:00,  1.98it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:39<00:00,  1.43it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-25): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-2): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3-4): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8-25): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27-31): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:2)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/83.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514440206320 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514440206320 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514440206320 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514440206320 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514967147376 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514967147376 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514967147376 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514967147376 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[83] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.4211049002945932
0.6717558353282359
0.7474911564899941
0.6826176878179091
0.7797142635267413
0.8231499933202435
0.9112841579744179
0.9329117857417774
0.3884156651377716
0.7116719161574963
0.3163186199985964
0.7579641143181478
0.7487852880300513
0.644901206148365
0.6252899485594469
0.8300709483875389
0.9686405093585702
0.4827230969630938
0.4401250454367978
0.846541236433719
0.9874080634814415
0.6954384705363965
0.33457229698994645
0.3090774332022817
0.9161115178604439
0.6466213503285708
0.719815146262638
0.8765685188625579
0.30061586460742784
0.4211049002945932
0.6717558353282359
0.7474911564899941
0.6826176878179091
0.7797142635267413
0.8231499933202435
0.9112841579744179
0.9329117857417774
0.3884156651377716
0.7116719161574963
0.3163186199985964
0.7579641143181478
0.7487852880300513
0.644901206148365
Total groups 75 exceeded the threshold, stopping comparison.
The group tensor is
[2, 5, 6, 1, 4, 0, 7, 3]
tensor([2, 5, 6, 1, 4, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 1, 5, 0, 7, 3]
tensor([2, 4, 6, 1, 5, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 2, 7, 0, 1, 3, 6, 5]
tensor([4, 2, 7, 0, 1, 3, 6, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 3, 7, 1, 2, 0, 5, 4]
tensor([6, 3, 7, 1, 2, 0, 5, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 0, 0, 2, 3, 1, 1]
tensor([4, 5, 0, 0, 2, 3, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 4, 1, 2, 1, 0, 3, 5]
tensor([0, 4, 1, 2, 1, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 5, 0, 2, 1, 1, 4]
tensor([0, 3, 5, 0, 2, 1, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/83.pt
[144] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2629.45it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:04<10:51,  4.62s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:07<04:54,  2.12s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:08<03:26,  1.51s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:10<02:50,  1.26s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:12<02:29,  1.12s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:14<02:16,  1.04s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:16<02:06,  1.02it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:18<02:05,  1.01it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<01:56,  1.08it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:21<01:47,  1.14it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:41,  1.19it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:35,  1.24it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:30,  1.29it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:32,  1.24it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:27,  1.29it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:23,  1.32it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:20,  1.35it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:19,  1.35it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:16,  1.36it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:35<01:14,  1.38it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:37<01:14,  1.36it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:38<01:11,  1.38it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:08,  1.42it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:41<01:05,  1.45it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:42<01:02,  1.48it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:44<01:00,  1.51it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<00:58,  1.53it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:46<00:57,  1.51it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<01:06,  1.28it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<01:05,  1.26it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<01:03,  1.27it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:53<00:58,  1.36it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:54<00:53,  1.45it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:55<00:49,  1.51it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:56<00:46,  1.57it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:58<00:46,  1.54it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:59<00:43,  1.59it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:00<00:40,  1.64it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:38,  1.68it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:36,  1.72it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:35,  1.74it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:33,  1.75it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:32,  1.73it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:32,  1.71it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:32,  1.62it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:10<00:33,  1.51it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:11<00:31,  1.58it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:12<00:28,  1.65it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:26,  1.71it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:14<00:24,  1.76it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:22,  1.79it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:21,  1.82it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:20,  1.83it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:20,  1.68it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:19,  1.73it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:21<00:20,  1.51it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:22<00:17,  1.61it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:16,  1.69it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:14,  1.74it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:12,  1.80it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:27<00:11,  1.80it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:28<00:10,  1.85it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:29<00:09,  1.78it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:30<00:08,  1.84it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:31<00:06,  1.89it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:32<00:05,  1.93it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:33<00:04,  1.96it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:34<00:03,  1.99it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:35<00:02,  2.01it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:36<00:01,  2.04it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:37<00:00,  2.08it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:37<00:00,  1.46it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-5): 6 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6-31): 26 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:3)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/144.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514440204640 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514440204640 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514440204640 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514440204640 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514433519152 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514433519152 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514433519152 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514433519152 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[144] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4084507042253521, 'acc_stderr,none': 0.058751136942575236}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.8470147925890528
0.7476864669376683
0.7057458194715966
0.9259961826035564
0.9063486644110201
0.24963708122926456
0.19827660218091203
0.8342224461031358
0.674768686362321
0.8317740713616216
0.8052770539851904
0.87411142785436
0.9423251627051763
0.6325449231299249
0.5709019604280002
0.492272078021527
0.5470094105753499
0.8106788106419917
0.8007286389438932
0.8514363351777328
0.5678533410855942
0.6964663000476322
0.43791892189616227
0.24333097709743945
0.6773756911035087
0.7468917488225975
0.9536421875896308
0.6970547096788922
0.8446279043078407
0.8470147925890528
0.7476864669376683
0.7057458194715966
0.9259961826035564
0.9063486644110201
0.24963708122926456
0.19827660218091203
0.8342224461031358
0.674768686362321
0.8317740713616216
0.8052770539851904
0.87411142785436
0.9423251627051763
0.6325449231299249
0.5709019604280002
0.492272078021527
Total groups 76 exceeded the threshold, stopping comparison.
The group tensor is
[3, 4, 6, 5, 2, 0, 7, 1]
tensor([3, 4, 6, 5, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 4, 6, 5, 3, 0, 7, 1]
tensor([2, 4, 6, 5, 3, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[6, 3, 4, 5, 2, 0, 7, 1]
tensor([6, 3, 4, 5, 2, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 4, 7, 2, 5, 0, 6, 3]
tensor([1, 4, 7, 2, 5, 0, 6, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[0, 3, 5, 2, 0, 1, 1, 4]
tensor([0, 3, 5, 2, 0, 1, 1, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[4, 3, 0, 5, 2, 0, 1, 1]
tensor([4, 3, 0, 5, 2, 0, 1, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[0, 3, 5, 1, 4, 0, 1, 2]
tensor([0, 3, 5, 1, 4, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1, 0, 1.0, 1.0, 1.0]
tensor([0, 1, 1, 1, 0, 1, 1, 1], dtype=torch.int32)
[0, 1]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/144.pt
[12] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2610.09it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:21,  3.56s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:51,  1.67s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:08,  1.37s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:42,  1.21s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:28,  1.12s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:18,  1.06s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:10,  1.01s/it]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:07,  1.01s/it]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:00,  1.04it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:52,  1.09it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:22<01:46,  1.13it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:23<01:40,  1.18it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:25<01:35,  1.22it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:35,  1.21it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:28<01:30,  1.24it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:27,  1.27it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:31<01:24,  1.29it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:21,  1.31it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:34<01:19,  1.31it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:17,  1.33it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:23,  1.21it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:18,  1.26it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:14,  1.31it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:12,  1.31it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:44<01:13,  1.27it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:11,  1.27it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:47<01:13,  1.21it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:48<01:10,  1.24it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:50<01:05,  1.30it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:51<00:59,  1.39it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:52<00:55,  1.46it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:53<00:52,  1.52it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:55<00:49,  1.57it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:56<00:46,  1.60it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:57<00:46,  1.57it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:58<00:44,  1.59it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [01:00<00:42,  1.63it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [01:01<00:43,  1.55it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:02<00:40,  1.59it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:03<00:38,  1.63it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:04<00:36,  1.67it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:06<00:34,  1.70it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:07<00:37,  1.51it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:08<00:34,  1.59it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:09<00:32,  1.64it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:11<00:30,  1.68it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:12<00:28,  1.72it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:13<00:26,  1.74it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:14<00:25,  1.76it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:15<00:24,  1.78it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:16<00:22,  1.79it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:17<00:22,  1.73it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:18<00:21,  1.76it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:20<00:19,  1.78it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:21<00:18,  1.79it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:22<00:17,  1.82it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:23<00:15,  1.83it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:24<00:14,  1.85it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:13,  1.86it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:12,  1.87it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:27<00:11,  1.79it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:28<00:10,  1.82it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:29<00:09,  1.84it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:30<00:08,  1.86it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:31<00:06,  1.87it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:32<00:05,  1.89it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:33<00:04,  1.90it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:35<00:03,  1.91it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:35<00:02,  1.94it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:36<00:01,  1.96it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:38<00:00,  1.60it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:38<00:00,  1.44it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-10): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12-13): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-31): 15 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-1): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2-3): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-7): 3 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9-10): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12-13): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-31): 15 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:4)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/12.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514433317824 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514433317824 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514433317824 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514433317824 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514443867968 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514443867968 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514443867968 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514443867968 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[12] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.43661971830985913, 'acc_stderr,none': 0.05927935558412972}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6384788962207051
0.4946572039289874
0.7062468225311351
0.5924118081316296
0.8055799477986278
0.4290834385090486
0.5207152243980063
0.40307410421369466
0.575849756312845
0.37833122092903004
0.559402975459772
0.9202435099718248
0.7328649512638977
0.4656521136449837
0.6226167455222542
0.6500475926241498
0.14933051029764177
0.852848973497393
0.7020336348742873
0.8036038458930006
0.25840240277743587
0.45348406473870356
0.7291236246725161
0.5999541971262276
0.7071125031923391
0.6523270915574834
0.5982105985982268
0.6520903105513685
0.39933491227192136
0.6384788962207051
0.4946572039289874
0.7062468225311351
0.5924118081316296
0.8055799477986278
0.4290834385090486
0.5207152243980063
0.40307410421369466
0.575849756312845
0.37833122092903004
0.559402975459772
0.9202435099718248
0.7328649512638977
0.4656521136449837
0.6226167455222542
0.6500475926241498
0.14933051029764177
0.852848973497393
0.7020336348742873
0.8036038458930006
0.25840240277743587
0.45348406473870356
0.7291236246725161
0.5999541971262276
0.7071125031923391
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[6, 3, 2, 1, 5, 0, 7, 4]
tensor([6, 3, 2, 1, 5, 0, 7, 4], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 5, 4, 2, 6, 0, 7, 1]
tensor([3, 5, 4, 2, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[2, 7, 6, 3, 4, 1, 5, 0]
tensor([2, 7, 6, 3, 4, 1, 5, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[3, 2, 5, 4, 6, 0, 7, 1]
tensor([3, 2, 5, 4, 6, 0, 7, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[4, 5, 7, 0, 3, 2, 6, 1]
tensor([4, 5, 7, 0, 3, 2, 6, 1], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 6, 3, 4, 2, 1, 7, 0]
tensor([5, 6, 3, 4, 2, 1, 7, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/12.pt
[156] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2646.48it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<07:59,  3.40s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:38,  1.57s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:50,  1.24s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:08<02:28,  1.10s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:16,  1.03s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:07,  1.03it/s]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:03,  1.05it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:15<01:55,  1.10it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:17<01:49,  1.14it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:43,  1.19it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:20<01:38,  1.23it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:21<01:32,  1.28it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:23<01:32,  1.26it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:25<01:27,  1.31it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:26<01:24,  1.35it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:27<01:21,  1.37it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:29<01:19,  1.37it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:30<01:18,  1.37it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:29,  1.17it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:34<01:30,  1.14it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:24,  1.20it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:37<01:18,  1.26it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:39<01:13,  1.32it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:08,  1.39it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:41<01:04,  1.45it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:42<01:00,  1.50it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:44<01:00,  1.46it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:45<00:58,  1.48it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:46<00:56,  1.51it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:48<00:54,  1.53it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:49<00:52,  1.55it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:50<00:49,  1.59it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:51<00:47,  1.62it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:52<00:45,  1.64it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:54<00:45,  1.61it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:55<00:44,  1.60it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:56<00:42,  1.63it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:57<00:40,  1.65it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [00:59<00:38,  1.67it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:00<00:37,  1.69it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:01<00:35,  1.70it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:02<00:34,  1.71it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:03<00:33,  1.72it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:44,  1.23it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:07<00:39,  1.36it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:08<00:34,  1.46it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:09<00:31,  1.55it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:10<00:29,  1.61it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:11<00:27,  1.66it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:13<00:25,  1.70it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:31,  1.31it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:27,  1.43it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:24,  1.53it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:18<00:21,  1.61it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:19<00:19,  1.67it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:20<00:18,  1.72it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:21<00:16,  1.76it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:22<00:15,  1.79it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:24<00:13,  1.80it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:25<00:13,  1.75it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:26<00:11,  1.79it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:27<00:10,  1.83it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:28<00:09,  1.85it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:29<00:08,  1.87it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:30<00:06,  1.89it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:31<00:05,  1.90it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:33<00:05,  1.70it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:34<00:03,  1.77it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:35<00:02,  1.76it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:36<00:01,  1.83it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:37<00:00,  1.90it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:37<00:00,  1.46it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-11): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-31): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-11): 8 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14-31): 18 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:5)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/156.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514722039712 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514722039712 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514722039712 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514722039712 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514722053104 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514722053104 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514722053104 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514722053104 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[156] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4788732394366197, 'acc_stderr,none': 0.05970805879899505}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.6890224269714
0.6519987454814186
0.44009979043140846
0.5066155659876163
0.7217003976797293
0.12899593918836094
0.4620116191334168
0.3468065225687064
0.10107939599729882
0.5600103662507451
0.577132167051305
0.6748045019607131
0.4004557650420357
0.700349463047757
0.9095160148001629
0.6131975246160634
0.5678386109581075
0.8437866782628755
0.3536670789186643
0.44960924435079147
0.5862896519346712
0.8246697057441258
0.5484674172302575
0.3288911762125467
0.6478473442183775
0.571596663207821
0.7179565348178579
0.38406312361402845
0.42349247998810474
0.6890224269714
0.6519987454814186
0.44009979043140846
0.5066155659876163
0.7217003976797293
0.12899593918836094
0.4620116191334168
0.3468065225687064
0.10107939599729882
0.5600103662507451
0.577132167051305
0.6748045019607131
0.4004557650420357
0.700349463047757
0.9095160148001629
0.6131975246160634
0.5678386109581075
0.8437866782628755
0.3536670789186643
0.44960924435079147
0.5862896519346712
0.8246697057441258
0.5484674172302575
0.3288911762125467
0.6478473442183775
0.571596663207821
Total groups 70 exceeded the threshold, stopping comparison.
The group tensor is
[5, 3, 7, 6, 2, 1, 4, 0]
tensor([5, 3, 7, 6, 2, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 5, 1, 3, 4, 0, 6, 2]
tensor([7, 5, 1, 3, 4, 0, 6, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 2, 1, 0, 4, 5]
tensor([6, 3, 7, 2, 1, 0, 4, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[6, 3, 7, 5, 2, 1, 4, 0]
tensor([6, 3, 7, 5, 2, 1, 4, 0], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[7, 3, 2, 5, 1, 0, 4, 6]
tensor([7, 3, 2, 5, 1, 0, 4, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 2, 3, 0, 3, 2, 1]
tensor([0, 1, 2, 3, 0, 3, 2, 1], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
Model saved locally at saved_models/156.pt
[84] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2588.67it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:03<08:08,  3.47s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:05<03:42,  1.60s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<02:52,  1.26s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:29,  1.11s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:10<02:17,  1.03s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:12<02:08,  1.02it/s]Running loglikelihood requests:   9%|▉         | 13/142 [00:14<02:05,  1.02it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:16<01:57,  1.08it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:17<01:50,  1.13it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:19<01:43,  1.18it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:20<01:38,  1.22it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:22<01:33,  1.27it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:24<01:38,  1.19it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:25<01:32,  1.24it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:27<01:28,  1.27it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:28<01:24,  1.31it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:30<01:22,  1.32it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:31<01:21,  1.30it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:33<01:19,  1.32it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:34<01:18,  1.31it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:36<01:15,  1.35it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:37<01:12,  1.37it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:38<01:09,  1.39it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:40<01:09,  1.37it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:41<01:06,  1.41it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:42<01:02,  1.46it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:45<01:16,  1.17it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<01:22,  1.05it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<01:12,  1.18it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:50<01:04,  1.29it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:58,  1.38it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:54,  1.45it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:50,  1.51it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:55<00:49,  1.51it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:56<00:46,  1.56it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:44,  1.61it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:42,  1.64it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:40,  1.66it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:00<00:38,  1.68it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:37,  1.70it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:35,  1.72it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:35,  1.64it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:05<00:34,  1.67it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:06<00:32,  1.70it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:30,  1.72it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:29,  1.73it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:28,  1.74it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:26,  1.75it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:12<00:25,  1.76it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:13<00:24,  1.77it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:27,  1.48it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:24,  1.56it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:22,  1.62it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:18<00:20,  1.67it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:19<00:19,  1.71it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:21<00:17,  1.74it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:22<00:16,  1.76it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:23<00:16,  1.61it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:24<00:15,  1.59it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:13,  1.65it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:27<00:12,  1.69it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:28<00:10,  1.73it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:29<00:09,  1.76it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:30<00:08,  1.78it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:31<00:07,  1.81it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:32<00:06,  1.83it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:33<00:04,  1.85it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:34<00:03,  1.87it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:36<00:03,  1.60it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:37<00:01,  1.70it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:38<00:00,  1.78it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:38<00:00,  1.44it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-13): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15-31): 17 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-3): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4-8): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10-13): 4 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15-31): 17 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:6)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/84.pt
INFO:save_model:Node info successfully sent
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Using pre-initialized model
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but aggregation is not. using default aggregation=mean
WARNING:lm_eval.api.task:[Task: wnli] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/glue/glue.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/glue HTTP/1.1" 307 61
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "GET /api/datasets/nyu-mll/glue HTTP/1.1" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 233
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): hf-mirror.com:443
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 307 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "HEAD /datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_infos.json HTTP/1.1" 404 0
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 307 113
DEBUG:urllib3.connectionpool:https://hf-mirror.com:443 "POST /api/datasets/nyu-mll/glue/paths-info/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c HTTP/1.1" 200 236
DEBUG:filelock:Attempting to acquire lock 140514433307264 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514433307264 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514433307264 on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Lock 140514433307264 released on /public/home/zouyifei001/.cache/huggingface/datasets/_public_home_zouyifei001_.cache_huggingface_datasets_glue_wnli_0.0.0_bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c.lock
DEBUG:filelock:Attempting to acquire lock 140514728028416 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514728028416 acquired on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:fsspec.local:open file: /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/dataset_info.json
DEBUG:filelock:Attempting to release lock 140514728028416 on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:filelock:Lock 140514728028416 released on /public/home/zouyifei001/.cache/huggingface/datasets/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c_builder.lock
DEBUG:lm_eval.api.task:No custom filters defined. Using default 'take_first' filter for handling repeats.
WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wnli from None to 0
INFO:lm_eval.api.task:Building contexts for wnli on rank 0...
[84] Inference Results (Before Update): {'wnli': {'alias': 'wnli', 'acc,none': 0.4507042253521127, 'acc_stderr,none': 0.05947027187738001}}
length of layer 0 is :8
length of layer 1 is :8
length of layer 2 is :8
length of layer 3 is :8
length of layer 4 is :8
length of layer 5 is :8
length of layer 6 is :8
length of layer 7 is :8
length of layer 8 is :8
length of layer 9 is :8
length of layer 10 is :8
length of layer 11 is :8
length of layer 12 is :8
length of layer 13 is :8
length of layer 14 is :8
length of layer 15 is :8
length of layer 16 is :8
length of layer 17 is :8
length of layer 18 is :8
length of layer 19 is :8
length of layer 20 is :8
length of layer 21 is :8
length of layer 22 is :8
length of layer 23 is :8
length of layer 24 is :8
length of layer 25 is :8
length of layer 26 is :8
length of layer 27 is :8
length of layer 28 is :8
length of layer 29 is :8
length of layer 30 is :8
length of layer 31 is :8
0.36326485003451675
0.9393611658494853
0.5347697273308267
0.4619639883096971
0.26830156279453604
0.8380027464731019
0.8263233574501603
0.6320977880109986
0.7843146415103402
0.2072713135482749
0.3805984042723936
0.43289830555723763
0.19273781754260347
0.8181624071033468
0.5320194969946469
0.7203219641552613
0.7789354624632999
0.3567665334531696
0.2604208944916411
0.942273921845013
0.8798126471295904
0.8327076147848976
0.5471221204152548
0.8016285641373796
0.8787721123612405
0.7314094662215913
0.733830137407364
0.7399689895148621
0.8815844426907766
0.36326485003451675
0.9393611658494853
0.5347697273308267
0.4619639883096971
0.26830156279453604
0.8380027464731019
0.8263233574501603
0.6320977880109986
0.7843146415103402
0.2072713135482749
0.3805984042723936
0.43289830555723763
0.19273781754260347
0.8181624071033468
0.5320194969946469
0.7203219641552613
Total groups 74 exceeded the threshold, stopping comparison.
The group tensor is
[1, 2, 5, 4, 3, 0, 7, 6]
tensor([1, 2, 5, 4, 3, 0, 7, 6], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[5, 3, 2, 6, 0, 1, 4, 7]
tensor([5, 3, 2, 6, 0, 1, 4, 7], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[1, 2, 6, 7, 4, 0, 3, 5]
tensor([1, 2, 6, 7, 4, 0, 3, 5], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[1, 4, 5, 6, 2, 0, 7, 3]
tensor([1, 4, 5, 6, 2, 0, 7, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5, 6, 7]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[4, 5, 1, 0, 0, 2, 1, 3]
tensor([4, 5, 1, 0, 0, 2, 1, 3], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[1, 5, 0, 3, 4, 0, 1, 2]
tensor([1, 5, 0, 3, 4, 0, 1, 2], dtype=torch.int32)
[0, 1, 2, 3, 4, 5]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 3, 1, 2, 0, 1, 3, 2]
tensor([0, 3, 1, 2, 0, 1, 3, 2], dtype=torch.int32)
[0, 1, 2, 3]
The group tensor is
[-2, -2, -2, -2, -2, -2, -2, -2]
tensor([-2, -2, -2, -2, -2, -2, -2, -2], dtype=torch.int32)
[-2]
The group tensor is
[0, 1, 1.0, 1.0, 1, 0, 1.0, 1.0]
tensor([0, 1, 1, 1, 1, 0, 1, 1], dtype=torch.int32)
[0, 1]
Model saved locally at saved_models/84.pt
[6] Inference Step Starting
  0%|          | 0/71 [00:00<?, ?it/s]100%|██████████| 71/71 [00:00<00:00, 2618.95it/s]
DEBUG:lm_eval.evaluator:Task: wnli; number of requests on this rank: 142
INFO:lm_eval.evaluator:Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/142 [00:00<?, ?it/s]Running loglikelihood requests:   1%|          | 1/142 [00:04<09:39,  4.11s/it]Running loglikelihood requests:   2%|▏         | 3/142 [00:06<04:07,  1.78s/it]Running loglikelihood requests:   4%|▎         | 5/142 [00:07<03:06,  1.36s/it]Running loglikelihood requests:   5%|▍         | 7/142 [00:09<02:40,  1.19s/it]Running loglikelihood requests:   6%|▋         | 9/142 [00:11<02:28,  1.12s/it]Running loglikelihood requests:   8%|▊         | 11/142 [00:13<02:17,  1.05s/it]Running loglikelihood requests:   9%|▉         | 13/142 [00:15<02:08,  1.00it/s]Running loglikelihood requests:  11%|█         | 15/142 [00:17<02:01,  1.05it/s]Running loglikelihood requests:  12%|█▏        | 17/142 [00:19<02:02,  1.02it/s]Running loglikelihood requests:  13%|█▎        | 19/142 [00:20<01:55,  1.07it/s]Running loglikelihood requests:  15%|█▍        | 21/142 [00:23<01:55,  1.04it/s]Running loglikelihood requests:  16%|█▌        | 23/142 [00:24<01:48,  1.10it/s]Running loglikelihood requests:  18%|█▊        | 25/142 [00:26<01:41,  1.16it/s]Running loglikelihood requests:  19%|█▉        | 27/142 [00:27<01:36,  1.19it/s]Running loglikelihood requests:  20%|██        | 29/142 [00:29<01:35,  1.19it/s]Running loglikelihood requests:  22%|██▏       | 31/142 [00:30<01:30,  1.23it/s]Running loglikelihood requests:  23%|██▎       | 33/142 [00:32<01:29,  1.21it/s]Running loglikelihood requests:  25%|██▍       | 35/142 [00:33<01:24,  1.27it/s]Running loglikelihood requests:  26%|██▌       | 37/142 [00:35<01:20,  1.31it/s]Running loglikelihood requests:  27%|██▋       | 39/142 [00:36<01:16,  1.35it/s]Running loglikelihood requests:  29%|██▉       | 41/142 [00:38<01:13,  1.37it/s]Running loglikelihood requests:  30%|███       | 43/142 [00:39<01:11,  1.39it/s]Running loglikelihood requests:  32%|███▏      | 45/142 [00:40<01:08,  1.41it/s]Running loglikelihood requests:  33%|███▎      | 47/142 [00:42<01:08,  1.39it/s]Running loglikelihood requests:  35%|███▍      | 49/142 [00:43<01:04,  1.44it/s]Running loglikelihood requests:  36%|███▌      | 51/142 [00:45<01:01,  1.47it/s]Running loglikelihood requests:  37%|███▋      | 53/142 [00:46<00:59,  1.50it/s]Running loglikelihood requests:  39%|███▊      | 55/142 [00:47<00:56,  1.53it/s]Running loglikelihood requests:  40%|████      | 57/142 [00:48<00:54,  1.56it/s]Running loglikelihood requests:  42%|████▏     | 59/142 [00:49<00:52,  1.58it/s]Running loglikelihood requests:  43%|████▎     | 61/142 [00:51<00:51,  1.59it/s]Running loglikelihood requests:  44%|████▍     | 63/142 [00:52<00:51,  1.54it/s]Running loglikelihood requests:  46%|████▌     | 65/142 [00:53<00:49,  1.56it/s]Running loglikelihood requests:  47%|████▋     | 67/142 [00:55<00:47,  1.58it/s]Running loglikelihood requests:  49%|████▊     | 69/142 [00:56<00:45,  1.60it/s]Running loglikelihood requests:  50%|█████     | 71/142 [00:57<00:44,  1.61it/s]Running loglikelihood requests:  51%|█████▏    | 73/142 [00:58<00:42,  1.63it/s]Running loglikelihood requests:  53%|█████▎    | 75/142 [00:59<00:40,  1.64it/s]Running loglikelihood requests:  54%|█████▍    | 77/142 [01:01<00:39,  1.65it/s]Running loglikelihood requests:  56%|█████▌    | 79/142 [01:02<00:37,  1.67it/s]Running loglikelihood requests:  57%|█████▋    | 81/142 [01:03<00:39,  1.56it/s]Running loglikelihood requests:  58%|█████▊    | 83/142 [01:04<00:37,  1.59it/s]Running loglikelihood requests:  60%|█████▉    | 85/142 [01:06<00:35,  1.62it/s]Running loglikelihood requests:  61%|██████▏   | 87/142 [01:07<00:33,  1.65it/s]Running loglikelihood requests:  63%|██████▎   | 89/142 [01:08<00:31,  1.66it/s]Running loglikelihood requests:  64%|██████▍   | 91/142 [01:09<00:30,  1.68it/s]Running loglikelihood requests:  65%|██████▌   | 93/142 [01:10<00:28,  1.69it/s]Running loglikelihood requests:  67%|██████▋   | 95/142 [01:11<00:27,  1.71it/s]Running loglikelihood requests:  68%|██████▊   | 97/142 [01:13<00:27,  1.65it/s]Running loglikelihood requests:  70%|██████▉   | 99/142 [01:14<00:25,  1.68it/s]Running loglikelihood requests:  71%|███████   | 101/142 [01:15<00:24,  1.69it/s]Running loglikelihood requests:  73%|███████▎  | 103/142 [01:16<00:22,  1.70it/s]Running loglikelihood requests:  74%|███████▍  | 105/142 [01:17<00:21,  1.72it/s]Running loglikelihood requests:  75%|███████▌  | 107/142 [01:19<00:20,  1.73it/s]Running loglikelihood requests:  77%|███████▋  | 109/142 [01:20<00:19,  1.71it/s]Running loglikelihood requests:  78%|███████▊  | 111/142 [01:21<00:18,  1.72it/s]Running loglikelihood requests:  80%|███████▉  | 113/142 [01:22<00:16,  1.73it/s]Running loglikelihood requests:  81%|████████  | 115/142 [01:23<00:17,  1.59it/s]Running loglikelihood requests:  82%|████████▏ | 117/142 [01:25<00:15,  1.65it/s]Running loglikelihood requests:  84%|████████▍ | 119/142 [01:26<00:13,  1.70it/s]Running loglikelihood requests:  85%|████████▌ | 121/142 [01:27<00:12,  1.73it/s]Running loglikelihood requests:  87%|████████▋ | 123/142 [01:28<00:10,  1.76it/s]Running loglikelihood requests:  88%|████████▊ | 125/142 [01:29<00:09,  1.78it/s]Running loglikelihood requests:  89%|████████▉ | 127/142 [01:30<00:08,  1.80it/s]Running loglikelihood requests:  91%|█████████ | 129/142 [01:31<00:07,  1.81it/s]Running loglikelihood requests:  92%|█████████▏| 131/142 [01:32<00:06,  1.83it/s]Running loglikelihood requests:  94%|█████████▎| 133/142 [01:34<00:05,  1.69it/s]Running loglikelihood requests:  95%|█████████▌| 135/142 [01:35<00:03,  1.75it/s]Running loglikelihood requests:  96%|█████████▋| 137/142 [01:36<00:02,  1.80it/s]Running loglikelihood requests:  98%|█████████▊| 139/142 [01:37<00:01,  1.83it/s]Running loglikelihood requests:  99%|█████████▉| 141/142 [01:38<00:00,  1.86it/s]Running loglikelihood requests: 100%|██████████| 142/142 [01:38<00:00,  1.44it/s]
DEBUG:lm_eval.models.huggingface:Failed to get model SHA for LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-16): 12 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-18): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (19): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20-31): 12 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
) at revision main. Error: Repo id must be a string, not <class 'transformers_modules.LLaMA-MoE-v1-3_5B-2_8.modeling_llama_moe_hf.LlamaMoEForCausalLM'>: 'LlamaMoEForCausalLM(
  (model): LlamaMoEModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-4): 5 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1376x4096]
                  (6): Parameter containing: [torch.float32 of size 1376x4096]
                  (7): Parameter containing: [torch.float32 of size 1376x4096]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 4096x1376]
                  (6): Parameter containing: [torch.float32 of size 4096x1376]
                  (7): Parameter containing: [torch.float32 of size 4096x1376]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5-16): 12 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17-18): 2 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1376x4096]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 4096x1376]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (19): LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1376x4096]
                  (3): Parameter containing: [torch.float32 of size 1376x4096]
                  (4): Parameter containing: [torch.float32 of size 1376x4096]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 4096x1376]
                  (3): Parameter containing: [torch.float32 of size 4096x1376]
                  (4): Parameter containing: [torch.float32 of size 4096x1376]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20-31): 12 x LlamaMoEDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LinearGLUMoELayer(
          (gate): TopKBalancedNoisyGate(
            (gate_network): Sequential(
              (0): Linear(in_features=4096, out_features=8, bias=False)
              (1): Tanh()
              (2): Linear(in_features=8, out_features=8, bias=False)
            )
            (softmax): Softmax(dim=1)
            (weight_noise): Linear(in_features=4096, out_features=8, bias=False)
            (softplus): Softplus(beta=1.0, threshold=20.0)
          )
          (calculator): UniversalCalculator(
            (experts): LinearGLUExperts(
              in_features=4096, hidden_features=11008, out_features=4096, hidden_act=silu, num_experts=8, size_experts=[1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376], bias=False
              (act_fn): SiLU()
              (weight_gate): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_up): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 1376x4096]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
              (weight_down): ParameterList(
                  (0): Parameter containing: [torch.float32 of size 4096x1376]
                  (1): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (2): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (3): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (4): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (5): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (6): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
                  (7): Parameter containing: [torch.float32 of size 1x1 (cuda:7)]
              )
            )
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)'.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
WARNING:save_model:Expert index -2 is negative, resetting to 0.
INFO:save_model:Model saved successfully at saved_models/6.pt
INFO:save_model:Node info successfully sent
